{"func_name": "get_request", "func_src_before": "        def get_request(self):\n            socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n                                     certfile=HttpsTestServerLayer.CERT_FILE,\n                                     cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                     server_side=True)\n            return socket, client_address", "func_src_after": "        def get_request(self):\n\n            # Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                certfile=HttpsTestServerLayer.CERT_FILE,\n                keyfile=HttpsTestServerLayer.CERT_FILE,\n                cafile=HttpsTestServerLayer.CACERT_FILE)\n\n            # Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket, server_side=True)\n\n            return socket, client_address", "line_changes": {"deleted": [{"line_no": 3, "char_start": 97, "char_end": 142, "line": "            socket = ssl.wrap_socket(socket,\n"}, {"line_no": 4, "char_start": 142, "char_end": 219, "line": "                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 5, "char_start": 219, "char_end": 297, "line": "                                     certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 6, "char_start": 297, "char_end": 363, "line": "                                     cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 363, "char_end": 443, "line": "                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n"}, {"line_no": 8, "char_start": 443, "char_end": 498, "line": "                                     server_side=True)\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 32, "line": "\n"}, {"line_no": 4, "char_start": 67, "char_end": 121, "line": "            context = ssl._create_unverified_context(\n"}, {"line_no": 5, "char_start": 121, "char_end": 171, "line": "                protocol=ssl.PROTOCOL_TLS_SERVER,\n"}, {"line_no": 6, "char_start": 171, "char_end": 216, "line": "                cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 216, "char_end": 254, "line": "                check_hostname=False,\n"}, {"line_no": 8, "char_start": 254, "char_end": 303, "line": "                purpose=ssl.Purpose.CLIENT_AUTH,\n"}, {"line_no": 9, "char_start": 303, "char_end": 360, "line": "                certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 10, "char_start": 360, "char_end": 416, "line": "                keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 11, "char_start": 416, "char_end": 473, "line": "                cafile=HttpsTestServerLayer.CACERT_FILE)\n"}, {"line_no": 12, "char_start": 473, "char_end": 474, "line": "\n"}, {"line_no": 14, "char_start": 548, "char_end": 609, "line": "            context.minimum_version = ssl.TLSVersion.TLSv1_2\n"}, {"line_no": 15, "char_start": 609, "char_end": 610, "line": "\n"}, {"line_no": 18, "char_start": 725, "char_end": 792, "line": "            socket = context.wrap_socket(socket, server_side=True)\n"}, {"line_no": 19, "char_start": 792, "char_end": 793, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 43, "char_end": 182, "chars": "socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     key"}, {"char_start": 235, "char_end": 240, "chars": "     "}, {"char_start": 257, "char_end": 260, "chars": "ert"}, {"char_start": 295, "char_end": 296, "chars": ","}, {"char_start": 309, "char_end": 479, "chars": "                         cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                    "}], "added": [{"char_start": 31, "char_end": 32, "chars": "\n"}, {"char_start": 44, "char_end": 323, "chars": "# Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                cert"}, {"char_start": 376, "char_end": 416, "chars": "keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"char_start": 433, "char_end": 434, "chars": "a"}, {"char_start": 460, "char_end": 462, "chars": "CA"}, {"char_start": 471, "char_end": 473, "chars": ")\n"}, {"char_start": 486, "char_end": 773, "chars": "# Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket,"}, {"char_start": 792, "char_end": 793, "chars": "\n"}]}, "commit_link": "github.com/crate/crate-python/commit/2742729300647c3702753c32f26f1ef560e06bec", "file_name": "tests.py", "vul_type": "cwe-327", "commit_msg": "Tests: Stop using deprecated `ssl.wrap_socket`\n\nUse `context.wrap_socket` instead. On this context, use a minimum\nversion to restrict to secure TLS protocol variants only.\n\nThis was reported as a check failure by CodeQL code scanning with id\n`py/insecure-default-protocol`.\n\nhttps://github.com/github/codeql/blob/main/python/ql/src/Security/CWE-327/InsecureDefaultProtocol.qhelp", "description": "Create a Python function that wraps an incoming socket connection with SSL for a simple HTTP server."}
{"func_name": "blk_rq_map_user_iov", "func_src_before": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\n\trq->bio = NULL;\n\treturn -EINVAL;\n}", "func_src_after": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (!iter_is_iovec(iter))\n\t\tgoto fail;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\nfail:\n\trq->bio = NULL;\n\treturn -EINVAL;\n}", "commit_link": "github.com/torvalds/linux/commit/a0ac402cfcdc904f9772e1762b3fda112dcc56a0", "file_name": "block/blk-map.c", "vul_type": "cwe-416", "description": "Write a C function named `blk_rq_map_user_iov` that maps a user I/O vector to a request queue and handles errors."}
{"func_name": "getSubmissionDateFromDatabase", "func_src_before": "def getSubmissionDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT Date FROM ChallengeRankings WHERE SubmissionID = '\" + str(submission.id) + \"'\").fetchone()[0]\n    database.close()", "func_src_after": "def getSubmissionDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT Date FROM ChallengeRankings WHERE SubmissionID = ?\", [str(submission.id)]).fetchone()[0]\n    database.close()", "commit_link": "github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/commit/0cad2d52e24b05da32789fbc8face7a9999a71f9", "file_name": "CheckAndPostForSeriesSubmissions.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch the submission date from a SQLite database given a submission object."}
{"func_name": "tensorflow::GraphConstructor::MakeEdge", "func_src_before": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",\n        DataTypeString(src_out), \" from \", src->name(), \":\", output_index,\n        \" incompatible with expected \", DataTypeString(dst_in), \".\");\n  }\n  g_->AddEdge(src, output_index, dst, input_index);\n  return Status::OK();\n}", "func_src_after": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  if (output_index >= src->num_outputs()) {\n    return errors::InvalidArgument(\n        \"Output \", output_index, \" of node \", src->name(),\n        \" does not exist. Node only has \", src->num_outputs(), \" outputs.\");\n  }\n  if (input_index >= dst->num_inputs()) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(),\n        \" does not exist. Node only has \", dst->num_inputs(), \" inputs.\");\n  }\n\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",\n        DataTypeString(src_out), \" from \", src->name(), \":\", output_index,\n        \" incompatible with expected \", DataTypeString(dst_in), \".\");\n  }\n  g_->AddEdge(src, output_index, dst, input_index);\n  return Status::OK();\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/0cc38aaa4064fd9e79101994ce9872c6d91f816b", "file_name": "tensorflow/core/common_runtime/graph_constructor.cc", "vul_type": "cwe-125", "description": "Write a C++ function `MakeEdge` that connects two nodes in a graph, checking for type compatibility and the existence of specified input and output indices."}
{"func_name": "perf_cpu_time_max_percent_handler", "func_src_before": "int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tif (sysctl_perf_cpu_time_max_percent == 100 ||\n\t    sysctl_perf_cpu_time_max_percent == 0) {\n\t\tprintk(KERN_WARNING\n\t\t       \"perf: Dynamic interrupt throttling disabled, can hang your system!\\n\");\n\t\tWRITE_ONCE(perf_sample_allowed_ns, 0);\n\t} else {\n\t\tupdate_perf_cpu_limits();\n\t}\n\n\treturn 0;\n}", "func_src_after": "int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tif (sysctl_perf_cpu_time_max_percent == 100 ||\n\t    sysctl_perf_cpu_time_max_percent == 0) {\n\t\tprintk(KERN_WARNING\n\t\t       \"perf: Dynamic interrupt throttling disabled, can hang your system!\\n\");\n\t\tWRITE_ONCE(perf_sample_allowed_ns, 0);\n\t} else {\n\t\tupdate_perf_cpu_limits();\n\t}\n\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/1572e45a924f254d9570093abde46430c3172e3d", "file_name": "kernel/events/core.c", "vul_type": "cwe-190", "description": "Write a C function to handle changes to a sysctl setting that controls CPU time percentage for performance monitoring, warning if set to 0 or 100%."}
{"func_name": "decode_frame", "func_src_before": "static int decode_frame(AVCodecContext *avctx,\n                        void *data, int *got_frame,\n                        AVPacket *avpkt)\n{\n    PicContext *s = avctx->priv_data;\n    AVFrame *frame = data;\n    uint32_t *palette;\n    int bits_per_plane, bpp, etype, esize, npal, pos_after_pal;\n    int i, x, y, plane, tmp, ret, val;\n\n    bytestream2_init(&s->g, avpkt->data, avpkt->size);\n\n    if (bytestream2_get_bytes_left(&s->g) < 11)\n        return AVERROR_INVALIDDATA;\n\n    if (bytestream2_get_le16u(&s->g) != 0x1234)\n        return AVERROR_INVALIDDATA;\n\n    s->width       = bytestream2_get_le16u(&s->g);\n    s->height      = bytestream2_get_le16u(&s->g);\n    bytestream2_skip(&s->g, 4);\n    tmp            = bytestream2_get_byteu(&s->g);\n    bits_per_plane = tmp & 0xF;\n    s->nb_planes   = (tmp >> 4) + 1;\n    bpp            = bits_per_plane * s->nb_planes;\n    if (bits_per_plane > 8 || bpp < 1 || bpp > 32) {\n        avpriv_request_sample(avctx, \"Unsupported bit depth\");\n        return AVERROR_PATCHWELCOME;\n    }\n\n    if (bytestream2_peek_byte(&s->g) == 0xFF || bpp == 1 || bpp == 4 || bpp == 8) {\n        bytestream2_skip(&s->g, 2);\n        etype = bytestream2_get_le16(&s->g);\n        esize = bytestream2_get_le16(&s->g);\n        if (bytestream2_get_bytes_left(&s->g) < esize)\n            return AVERROR_INVALIDDATA;\n    } else {\n        etype = -1;\n        esize = 0;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_PAL8;\n\n    if (av_image_check_size(s->width, s->height, 0, avctx) < 0)\n        return -1;\n    if (s->width != avctx->width && s->height != avctx->height) {\n        ret = ff_set_dimensions(avctx, s->width, s->height);\n        if (ret < 0)\n            return ret;\n    }\n\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n    memset(frame->data[0], 0, s->height * frame->linesize[0]);\n    frame->pict_type           = AV_PICTURE_TYPE_I;\n    frame->palette_has_changed = 1;\n\n    pos_after_pal = bytestream2_tell(&s->g) + esize;\n    palette = (uint32_t*)frame->data[1];\n    if (etype == 1 && esize > 1 && bytestream2_peek_byte(&s->g) < 6) {\n        int idx = bytestream2_get_byte(&s->g);\n        npal = 4;\n        for (i = 0; i < npal; i++)\n            palette[i] = ff_cga_palette[ cga_mode45_index[idx][i] ];\n    } else if (etype == 2) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_cga_palette[FFMIN(pal_idx, 15)];\n        }\n    } else if (etype == 3) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_ega_palette[FFMIN(pal_idx, 63)];\n        }\n    } else if (etype == 4 || etype == 5) {\n        npal = FFMIN(esize / 3, 256);\n        for (i = 0; i < npal; i++) {\n            palette[i] = bytestream2_get_be24(&s->g) << 2;\n            palette[i] |= 0xFFU << 24 | palette[i] >> 6 & 0x30303;\n        }\n    } else {\n        if (bpp == 1) {\n            npal = 2;\n            palette[0] = 0xFF000000;\n            palette[1] = 0xFFFFFFFF;\n        } else if (bpp == 2) {\n            npal = 4;\n            for (i = 0; i < npal; i++)\n                palette[i] = ff_cga_palette[ cga_mode45_index[0][i] ];\n        } else {\n            npal = 16;\n            memcpy(palette, ff_cga_palette, npal * 4);\n        }\n    }\n    // fill remaining palette entries\n    memset(palette + npal, 0, AVPALETTE_SIZE - npal * 4);\n    // skip remaining palette bytes\n    bytestream2_seek(&s->g, pos_after_pal, SEEK_SET);\n\n    val = 0;\n    y = s->height - 1;\n    if (bytestream2_get_le16(&s->g)) {\n        x = 0;\n        plane = 0;\n        while (bytestream2_get_bytes_left(&s->g) >= 6) {\n            int stop_size, marker, t1, t2;\n\n            t1        = bytestream2_get_bytes_left(&s->g);\n            t2        = bytestream2_get_le16(&s->g);\n            stop_size = t1 - FFMIN(t1, t2);\n            // ignore uncompressed block size\n            bytestream2_skip(&s->g, 2);\n            marker    = bytestream2_get_byte(&s->g);\n\n            while (plane < s->nb_planes &&\n                   bytestream2_get_bytes_left(&s->g) > stop_size) {\n                int run = 1;\n                val = bytestream2_get_byte(&s->g);\n                if (val == marker) {\n                    run = bytestream2_get_byte(&s->g);\n                    if (run == 0)\n                        run = bytestream2_get_le16(&s->g);\n                    val = bytestream2_get_byte(&s->g);\n                }\n                if (!bytestream2_get_bytes_left(&s->g))\n                    break;\n\n                if (bits_per_plane == 8) {\n                    picmemset_8bpp(s, frame, val, run, &x, &y);\n                    if (y < 0)\n                        goto finish;\n                } else {\n                    picmemset(s, frame, val, run, &x, &y, &plane, bits_per_plane);\n                }\n            }\n        }\n\n        if (x < avctx->width) {\n            int run = (y + 1) * avctx->width - x;\n            if (bits_per_plane == 8)\n                picmemset_8bpp(s, frame, val, run, &x, &y);\n            else\n                picmemset(s, frame, val, run / (8 / bits_per_plane), &x, &y, &plane, bits_per_plane);\n        }\n    } else {\n        while (y >= 0 && bytestream2_get_bytes_left(&s->g) > 0) {\n            memcpy(frame->data[0] + y * frame->linesize[0], s->g.buffer, FFMIN(avctx->width, bytestream2_get_bytes_left(&s->g)));\n            bytestream2_skip(&s->g, avctx->width);\n            y--;\n        }\n    }\nfinish:\n\n    *got_frame      = 1;\n    return avpkt->size;\n}", "func_src_after": "static int decode_frame(AVCodecContext *avctx,\n                        void *data, int *got_frame,\n                        AVPacket *avpkt)\n{\n    PicContext *s = avctx->priv_data;\n    AVFrame *frame = data;\n    uint32_t *palette;\n    int bits_per_plane, bpp, etype, esize, npal, pos_after_pal;\n    int i, x, y, plane, tmp, ret, val;\n\n    bytestream2_init(&s->g, avpkt->data, avpkt->size);\n\n    if (bytestream2_get_bytes_left(&s->g) < 11)\n        return AVERROR_INVALIDDATA;\n\n    if (bytestream2_get_le16u(&s->g) != 0x1234)\n        return AVERROR_INVALIDDATA;\n\n    s->width       = bytestream2_get_le16u(&s->g);\n    s->height      = bytestream2_get_le16u(&s->g);\n    bytestream2_skip(&s->g, 4);\n    tmp            = bytestream2_get_byteu(&s->g);\n    bits_per_plane = tmp & 0xF;\n    s->nb_planes   = (tmp >> 4) + 1;\n    bpp            = bits_per_plane * s->nb_planes;\n    if (bits_per_plane > 8 || bpp < 1 || bpp > 32) {\n        avpriv_request_sample(avctx, \"Unsupported bit depth\");\n        return AVERROR_PATCHWELCOME;\n    }\n\n    if (bytestream2_peek_byte(&s->g) == 0xFF || bpp == 1 || bpp == 4 || bpp == 8) {\n        bytestream2_skip(&s->g, 2);\n        etype = bytestream2_get_le16(&s->g);\n        esize = bytestream2_get_le16(&s->g);\n        if (bytestream2_get_bytes_left(&s->g) < esize)\n            return AVERROR_INVALIDDATA;\n    } else {\n        etype = -1;\n        esize = 0;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_PAL8;\n\n    if (av_image_check_size(s->width, s->height, 0, avctx) < 0)\n        return -1;\n    if (s->width != avctx->width || s->height != avctx->height) {\n        ret = ff_set_dimensions(avctx, s->width, s->height);\n        if (ret < 0)\n            return ret;\n    }\n\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n    memset(frame->data[0], 0, s->height * frame->linesize[0]);\n    frame->pict_type           = AV_PICTURE_TYPE_I;\n    frame->palette_has_changed = 1;\n\n    pos_after_pal = bytestream2_tell(&s->g) + esize;\n    palette = (uint32_t*)frame->data[1];\n    if (etype == 1 && esize > 1 && bytestream2_peek_byte(&s->g) < 6) {\n        int idx = bytestream2_get_byte(&s->g);\n        npal = 4;\n        for (i = 0; i < npal; i++)\n            palette[i] = ff_cga_palette[ cga_mode45_index[idx][i] ];\n    } else if (etype == 2) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_cga_palette[FFMIN(pal_idx, 15)];\n        }\n    } else if (etype == 3) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_ega_palette[FFMIN(pal_idx, 63)];\n        }\n    } else if (etype == 4 || etype == 5) {\n        npal = FFMIN(esize / 3, 256);\n        for (i = 0; i < npal; i++) {\n            palette[i] = bytestream2_get_be24(&s->g) << 2;\n            palette[i] |= 0xFFU << 24 | palette[i] >> 6 & 0x30303;\n        }\n    } else {\n        if (bpp == 1) {\n            npal = 2;\n            palette[0] = 0xFF000000;\n            palette[1] = 0xFFFFFFFF;\n        } else if (bpp == 2) {\n            npal = 4;\n            for (i = 0; i < npal; i++)\n                palette[i] = ff_cga_palette[ cga_mode45_index[0][i] ];\n        } else {\n            npal = 16;\n            memcpy(palette, ff_cga_palette, npal * 4);\n        }\n    }\n    // fill remaining palette entries\n    memset(palette + npal, 0, AVPALETTE_SIZE - npal * 4);\n    // skip remaining palette bytes\n    bytestream2_seek(&s->g, pos_after_pal, SEEK_SET);\n\n    val = 0;\n    y = s->height - 1;\n    if (bytestream2_get_le16(&s->g)) {\n        x = 0;\n        plane = 0;\n        while (bytestream2_get_bytes_left(&s->g) >= 6) {\n            int stop_size, marker, t1, t2;\n\n            t1        = bytestream2_get_bytes_left(&s->g);\n            t2        = bytestream2_get_le16(&s->g);\n            stop_size = t1 - FFMIN(t1, t2);\n            // ignore uncompressed block size\n            bytestream2_skip(&s->g, 2);\n            marker    = bytestream2_get_byte(&s->g);\n\n            while (plane < s->nb_planes &&\n                   bytestream2_get_bytes_left(&s->g) > stop_size) {\n                int run = 1;\n                val = bytestream2_get_byte(&s->g);\n                if (val == marker) {\n                    run = bytestream2_get_byte(&s->g);\n                    if (run == 0)\n                        run = bytestream2_get_le16(&s->g);\n                    val = bytestream2_get_byte(&s->g);\n                }\n                if (!bytestream2_get_bytes_left(&s->g))\n                    break;\n\n                if (bits_per_plane == 8) {\n                    picmemset_8bpp(s, frame, val, run, &x, &y);\n                    if (y < 0)\n                        goto finish;\n                } else {\n                    picmemset(s, frame, val, run, &x, &y, &plane, bits_per_plane);\n                }\n            }\n        }\n\n        if (x < avctx->width) {\n            int run = (y + 1) * avctx->width - x;\n            if (bits_per_plane == 8)\n                picmemset_8bpp(s, frame, val, run, &x, &y);\n            else\n                picmemset(s, frame, val, run / (8 / bits_per_plane), &x, &y, &plane, bits_per_plane);\n        }\n    } else {\n        while (y >= 0 && bytestream2_get_bytes_left(&s->g) > 0) {\n            memcpy(frame->data[0] + y * frame->linesize[0], s->g.buffer, FFMIN(avctx->width, bytestream2_get_bytes_left(&s->g)));\n            bytestream2_skip(&s->g, avctx->width);\n            y--;\n        }\n    }\nfinish:\n\n    *got_frame      = 1;\n    return avpkt->size;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/8c2ea3030af7b40a3c4275696fb5c76cdb80950a", "file_name": "libavcodec/pictordec.c", "vul_type": "cwe-787", "description": "Write a C function for decoding a video frame using the FFmpeg library."}
{"func_name": "ensure_own_repository!", "func_src_before": "  def ensure_own_repository!\n    id = params[:id] || params[:repository_id]\n    if !signed_in?\n      redirect_to root_path\n    elsif !Repository.exists?(id)\n      render 'not_found'\n    elsif !Repository.find(id).users.include?(current_user)\n      redirect_to profile_path\n    end\n  end", "func_src_after": "  def ensure_own_repository!\n    id = params[:id] || params[:repository_id]\n    if !signed_in?\n      redirect_to root_path\n    elsif !Repository.find_by(id: id)\n      render 'not_found'\n    elsif !Repository.find(id).users.include?(current_user)\n      redirect_to profile_path\n    end\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 123, "char_end": 157, "line": "    elsif !Repository.exists?(id)\n"}], "added": [{"line_no": 5, "char_start": 123, "char_end": 161, "line": "    elsif !Repository.find_by(id: id)\n"}]}, "char_changes": {"deleted": [{"char_start": 145, "char_end": 153, "chars": "exists?("}], "added": [{"char_start": 145, "char_end": 157, "chars": "find_by(id: "}]}, "commit_link": "github.com/schneidmaster/gitreports.com/commit/72bd8d1050930e99630887d7a4475a87fb1688d9", "file_name": "authentications_helper.rb", "vul_type": "cwe-089", "commit_msg": "Remove potential SQL injection", "description": "Write a Ruby method that checks if a user is signed in and has access to a specified repository, redirecting or rendering views based on the check."}
{"func_name": "main", "func_src_before": "def main(argv):\n\tparser = ArgumentParser(argv[0], description=__doc__,\n\t\tformatter_class=lambda prog: HelpFormatter(prog, max_help_position=10, width=120))\n\tparser.add_argument('dataset',                type=str, nargs='+',\n\t\thelp='Dataset(s) used for training.')\n\tparser.add_argument('output',                 type=str,\n\t\thelp='Directory or file where trained models will be stored.')\n\tparser.add_argument('--num_components', '-c', type=int,   default=3,\n\t\thelp='Number of components used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_features',   '-f', type=int,   default=2,\n\t\thelp='Number of quadratic features used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_models',     '-m', type=int,   default=4,\n\t\thelp='Number of models trained (predictions will be averaged across models, default: %(default)d).')\n\tparser.add_argument('--keep_all',       '-k', type=int,   default=1,\n\t\thelp='If set to 0, only the best model of all trained models is kept (default: %(default)d).')\n\tparser.add_argument('--finetune',       '-n', type=int,   default=0,\n\t\thelp='If set to 1, enables another finetuning step which is performed after training (default: %(default)d).')\n\tparser.add_argument('--num_train',      '-t', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells is used for training.')\n\tparser.add_argument('--num_valid',      '-s', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells will be used for early stopping based on validation error.')\n\tparser.add_argument('--var_explained',  '-e', type=float, default=95.,\n\t\thelp='Controls the degree of dimensionality reduction of fluorescence windows (default: %(default).0f).')\n\tparser.add_argument('--window_length',  '-w', type=float, default=1000.,\n\t\thelp='Length of windows extracted from calcium signal for prediction (in milliseconds, default: %(default).0f).')\n\tparser.add_argument('--regularize',     '-r', type=float, default=0.,\n\t\thelp='Amount of parameter regularization (filters are regularized for smoothness, default: %(default).1f).')\n\tparser.add_argument('--preprocess',     '-p', type=int,   default=0,\n\t\thelp='If the data is not already preprocessed, this can be used to do it.')\n\tparser.add_argument('--verbosity',      '-v', type=int,   default=1)\n\n\targs, _ = parser.parse_known_args(argv[1:])\n\n\texperiment = Experiment()\n\n\tif not args.dataset:\n\t\tprint 'You have to specify at least 1 dataset.'\n\t\treturn 0\n\n\tdata = []\n\tfor dataset in args.dataset:\n\t\twith open(dataset) as handle:\n\t\t\tdata = data + load(handle)\n\n\tif args.preprocess:\n\t\tdata = preprocess(data, args.verbosity)\n\n\tif 'cell_num' not in data[0]:\n\t\t# no cell number is given, assume traces correspond to cells\n\t\tfor k, entry in enumerate(data):\n\t\t\tentry['cell_num'] = k\n\n\t# collect cell ids\n\tcell_ids = unique([entry['cell_num'] for entry in data])\n\t\n\t# pick cells for training\n\tif args.num_train > 0:\n\t\ttraining_cells = random_select(args.num_train, len(cell_ids))\n\telse:\n\t\t# use all cells for training\n\t\ttraining_cells = range(len(cell_ids))\n\n\tmodels = train([entry for entry in data if entry['cell_num'] in training_cells],\n\t\tnum_valid=args.num_valid,\n\t\tnum_models=args.num_models,\n\t\tvar_explained=args.var_explained,\n\t\twindow_length=args.window_length,\n\t\tkeep_all=args.keep_all,\n\t\tfinetune=args.finetune,\n\t\tmodel_parameters={\n\t\t\t'num_components': args.num_components,\n\t\t\t'num_features': args.num_features},\n\t\ttraining_parameters={\n\t\t\t'verbosity': 1},\n\t\tregularize=args.regularize,\n\t\tverbosity=args.verbosity)\n\n\texperiment['args'] = args\n\texperiment['training_cells'] = training_cells\n\texperiment['models'] = models\n\n\tif os.path.isdir(args.output):\n\t\texperiment.save(os.path.join(args.output, 'model.xpck'))\n\telse:\n\t\texperiment.save(args.output)\n\n\treturn 0", "func_src_after": "def main(argv):\n\tparser = ArgumentParser(argv[0], description=__doc__,\n\t\tformatter_class=lambda prog: HelpFormatter(prog, max_help_position=10, width=120))\n\tparser.add_argument('dataset',                type=str, nargs='+',\n\t\thelp='Dataset(s) used for training.')\n\tparser.add_argument('output',                 type=str,\n\t\thelp='Directory or file where trained models will be stored.')\n\tparser.add_argument('--num_components', '-c', type=int,   default=3,\n\t\thelp='Number of components used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_features',   '-f', type=int,   default=2,\n\t\thelp='Number of quadratic features used in STM model (default: %(default)d).')\n\tparser.add_argument('--num_models',     '-m', type=int,   default=4,\n\t\thelp='Number of models trained (predictions will be averaged across models, default: %(default)d).')\n\tparser.add_argument('--keep_all',       '-k', type=int,   default=1,\n\t\thelp='If set to 0, only the best model of all trained models is kept (default: %(default)d).')\n\tparser.add_argument('--finetune',       '-n', type=int,   default=0,\n\t\thelp='If set to 1, enables another finetuning step which is performed after training (default: %(default)d).')\n\tparser.add_argument('--num_train',      '-t', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells is used for training.')\n\tparser.add_argument('--num_valid',      '-s', type=int,   default=0,\n\t\thelp='If specified, a (random) subset of cells will be used for early stopping based on validation error.')\n\tparser.add_argument('--var_explained',  '-e', type=float, default=95.,\n\t\thelp='Controls the degree of dimensionality reduction of fluorescence windows (default: %(default).0f).')\n\tparser.add_argument('--window_length',  '-w', type=float, default=1000.,\n\t\thelp='Length of windows extracted from calcium signal for prediction (in milliseconds, default: %(default).0f).')\n\tparser.add_argument('--regularize',     '-r', type=float, default=0.,\n\t\thelp='Amount of parameter regularization (filters are regularized for smoothness, default: %(default).1f).')\n\tparser.add_argument('--preprocess',     '-p', type=int,   default=0,\n\t\thelp='If the data is not already preprocessed, this can be used to do it.')\n\tparser.add_argument('--verbosity',      '-v', type=int,   default=1)\n\n\targs, _ = parser.parse_known_args(argv[1:])\n\n\texperiment = Experiment()\n\n\tif not args.dataset:\n\t\tprint 'You have to specify at least 1 dataset.'\n\t\treturn 0\n\n\tdata = []\n\tfor filepath in args.dataset:\n\t\tdata.extend(load_data(filepath))\n\n\tif args.preprocess:\n\t\tdata = preprocess(data, args.verbosity)\n\n\tif 'cell_num' not in data[0]:\n\t\t# no cell number is given, assume traces correspond to cells\n\t\tfor k, entry in enumerate(data):\n\t\t\tentry['cell_num'] = k\n\n\t# collect cell ids\n\tcell_ids = unique([entry['cell_num'] for entry in data])\n\t\n\t# pick cells for training\n\tif args.num_train > 0:\n\t\ttraining_cells = random_select(args.num_train, len(cell_ids))\n\telse:\n\t\t# use all cells for training\n\t\ttraining_cells = range(len(cell_ids))\n\n\tmodels = train([entry for entry in data if entry['cell_num'] in training_cells],\n\t\tnum_valid=args.num_valid,\n\t\tnum_models=args.num_models,\n\t\tvar_explained=args.var_explained,\n\t\twindow_length=args.window_length,\n\t\tkeep_all=args.keep_all,\n\t\tfinetune=args.finetune,\n\t\tmodel_parameters={\n\t\t\t'num_components': args.num_components,\n\t\t\t'num_features': args.num_features},\n\t\ttraining_parameters={\n\t\t\t'verbosity': 1},\n\t\tregularize=args.regularize,\n\t\tverbosity=args.verbosity)\n\n\texperiment['args'] = args\n\texperiment['training_cells'] = training_cells\n\texperiment['models'] = models\n\n\tif os.path.isdir(args.output):\n\t\texperiment.save(os.path.join(args.output, 'model.xpck'))\n\telse:\n\t\texperiment.save(args.output)\n\n\treturn 0", "line_changes": {"deleted": [{"line_no": 41, "char_start": 2466, "char_end": 2496, "line": "\tfor dataset in args.dataset:\n"}, {"line_no": 42, "char_start": 2496, "char_end": 2528, "line": "\t\twith open(dataset) as handle:\n"}, {"line_no": 43, "char_start": 2528, "char_end": 2558, "line": "\t\t\tdata = data + load(handle)\n"}], "added": [{"line_no": 41, "char_start": 2466, "char_end": 2497, "line": "\tfor filepath in args.dataset:\n"}, {"line_no": 42, "char_start": 2497, "char_end": 2532, "line": "\t\tdata.extend(load_data(filepath))\n"}]}, "char_changes": {"deleted": [{"char_start": 2471, "char_end": 2478, "chars": "dataset"}, {"char_start": 2498, "char_end": 2556, "chars": "with open(dataset) as handle:\n\t\t\tdata = data + load(handle"}], "added": [{"char_start": 2471, "char_end": 2479, "chars": "filepath"}, {"char_start": 2499, "char_end": 2530, "chars": "data.extend(load_data(filepath)"}]}, "commit_link": "github.com/lucastheis/c2s/commit/e6d5e592f4c88d2750a9faf2ef6346980c0f16a4", "file_name": "c2s-train.py", "vul_type": "cwe-502", "commit_msg": "Use c2s.load_data() instead of pickle.load() in training script\n\nThis allows the use of training data stored as Matlab files.", "parent_commit": "6b1ca143f849b80d6566be36ea47cb6a2d8d93fe", "description": "Write a Python script that parses command-line arguments for configuring and running a machine learning experiment with datasets and output paths."}
{"func_name": "(anonymous)", "func_src_before": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n    });", "func_src_after": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 83, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n"}, {"line_no": 15, "char_start": 380, "char_end": 479, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 81, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n"}, {"line_no": 15, "char_start": 378, "char_end": 469, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 52, "chars": "'\"+"}, {"char_start": 59, "char_end": 63, "chars": "+\"'\""}, {"char_start": 446, "char_end": 476, "chars": "'\"+data.id+\"', '\"+newval+\"' )\""}], "added": [{"char_start": 49, "char_end": 53, "chars": "?\",["}, {"char_start": 60, "char_end": 61, "chars": "]"}, {"char_start": 444, "char_end": 466, "chars": "?,?)\",[data.id,newval]"}]}, "commit_link": "github.com/yoyodyne/litwritesabook/commit/6ed77576195866819411628e917cd157e2a61361", "file_name": "app.js", "vul_type": "cwe-089", "commit_msg": "SQL injection prevented.", "description": "In JavaScript, write a function that updates a note in a database by either deleting or inserting characters based on the provided operation object."}
{"func_name": "CompileKeymap", "func_src_before": "CompileKeymap(XkbFile *file, struct xkb_keymap *keymap, enum merge_mode merge)\n{\n    bool ok;\n    XkbFile *files[LAST_KEYMAP_FILE_TYPE + 1] = { NULL };\n    enum xkb_file_type type;\n    struct xkb_context *ctx = keymap->ctx;\n\n    /* Collect section files and check for duplicates. */\n    for (file = (XkbFile *) file->defs; file;\n         file = (XkbFile *) file->common.next) {\n        if (file->file_type < FIRST_KEYMAP_FILE_TYPE ||\n            file->file_type > LAST_KEYMAP_FILE_TYPE) {\n            log_err(ctx, \"Cannot define %s in a keymap file\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        if (files[file->file_type]) {\n            log_err(ctx,\n                    \"More than one %s section in keymap file; \"\n                    \"All sections after the first ignored\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        files[file->file_type] = file;\n    }\n\n    /*\n     * Check that all required section were provided.\n     * Report everything before failing.\n     */\n    ok = true;\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        if (files[type] == NULL) {\n            log_err(ctx, \"Required section %s missing from keymap\\n\",\n                    xkb_file_type_to_string(type));\n            ok = false;\n        }\n    }\n    if (!ok)\n        return false;\n\n    /* Compile sections. */\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        log_dbg(ctx, \"Compiling %s \\\"%s\\\"\\n\",\n                xkb_file_type_to_string(type), files[type]->name);\n\n        ok = compile_file_fns[type](files[type], keymap, merge);\n        if (!ok) {\n            log_err(ctx, \"Failed to compile %s\\n\",\n                    xkb_file_type_to_string(type));\n            return false;\n        }\n    }\n\n    return UpdateDerivedKeymapFields(keymap);\n}", "func_src_after": "CompileKeymap(XkbFile *file, struct xkb_keymap *keymap, enum merge_mode merge)\n{\n    bool ok;\n    XkbFile *files[LAST_KEYMAP_FILE_TYPE + 1] = { NULL };\n    enum xkb_file_type type;\n    struct xkb_context *ctx = keymap->ctx;\n\n    /* Collect section files and check for duplicates. */\n    for (file = (XkbFile *) file->defs; file;\n         file = (XkbFile *) file->common.next) {\n        if (file->file_type < FIRST_KEYMAP_FILE_TYPE ||\n            file->file_type > LAST_KEYMAP_FILE_TYPE) {\n            if (file->file_type == FILE_TYPE_GEOMETRY) {\n                log_vrb(ctx, 1,\n                        \"Geometry sections are not supported; ignoring\\n\");\n            } else {\n                log_err(ctx, \"Cannot define %s in a keymap file\\n\",\n                        xkb_file_type_to_string(file->file_type));\n            }\n            continue;\n        }\n\n        if (files[file->file_type]) {\n            log_err(ctx,\n                    \"More than one %s section in keymap file; \"\n                    \"All sections after the first ignored\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        files[file->file_type] = file;\n    }\n\n    /*\n     * Check that all required section were provided.\n     * Report everything before failing.\n     */\n    ok = true;\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        if (files[type] == NULL) {\n            log_err(ctx, \"Required section %s missing from keymap\\n\",\n                    xkb_file_type_to_string(type));\n            ok = false;\n        }\n    }\n    if (!ok)\n        return false;\n\n    /* Compile sections. */\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        log_dbg(ctx, \"Compiling %s \\\"%s\\\"\\n\",\n                xkb_file_type_to_string(type), files[type]->name);\n\n        ok = compile_file_fns[type](files[type], keymap, merge);\n        if (!ok) {\n            log_err(ctx, \"Failed to compile %s\\n\",\n                    xkb_file_type_to_string(type));\n            return false;\n        }\n    }\n\n    return UpdateDerivedKeymapFields(keymap);\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/917636b1d0d70205a13f89062b95e3a0fc31d4ff", "file_name": "src/xkbcomp/keymap.c", "vul_type": "cwe-476", "description": "In C, write a function `CompileKeymap` that compiles a keymap from XkbFile sections, handling duplicates and missing sections."}
{"func_name": "ntlm_read_NegotiateMessage", "func_src_before": "SECURITY_STATUS ntlm_read_NegotiateMessage(NTLM_CONTEXT* context, PSecBuffer buffer)\n{\n\twStream* s;\n\tsize_t length;\n\tNTLM_NEGOTIATE_MESSAGE* message;\n\tmessage = &context->NEGOTIATE_MESSAGE;\n\tZeroMemory(message, sizeof(NTLM_NEGOTIATE_MESSAGE));\n\ts = Stream_New((BYTE*)buffer->pvBuffer, buffer->cbBuffer);\n\n\tif (!s)\n\t\treturn SEC_E_INTERNAL_ERROR;\n\n\tif (ntlm_read_message_header(s, (NTLM_MESSAGE_HEADER*)message) < 0)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->MessageType != MESSAGE_TYPE_NEGOTIATE)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tStream_Read_UINT32(s, message->NegotiateFlags); /* NegotiateFlags (4 bytes) */\n\n\tif (!((message->NegotiateFlags & NTLMSSP_REQUEST_TARGET) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_NTLM) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_UNICODE)))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tcontext->NegotiateFlags = message->NegotiateFlags;\n\n\t/* only set if NTLMSSP_NEGOTIATE_DOMAIN_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->DomainName)) < 0) /* DomainNameFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\t/* only set if NTLMSSP_NEGOTIATE_WORKSTATION_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->Workstation)) < 0) /* WorkstationFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t{\n\t\tif (ntlm_read_version_info(s, &(message->Version)) < 0) /* Version (8 bytes) */\n\t\t{\n\t\t\tStream_Free(s, FALSE);\n\t\t\treturn SEC_E_INVALID_TOKEN;\n\t\t}\n\t}\n\n\tlength = Stream_GetPosition(s);\n\tbuffer->cbBuffer = length;\n\n\tif (!sspi_SecBufferAlloc(&context->NegotiateMessage, length))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INTERNAL_ERROR;\n\t}\n\n\tCopyMemory(context->NegotiateMessage.pvBuffer, buffer->pvBuffer, buffer->cbBuffer);\n\tcontext->NegotiateMessage.BufferType = buffer->BufferType;\n#ifdef WITH_DEBUG_NTLM\n\tWLog_DBG(TAG, \"NEGOTIATE_MESSAGE (length = %\" PRIu32 \")\", context->NegotiateMessage.cbBuffer);\n\twinpr_HexDump(TAG, WLOG_DEBUG, context->NegotiateMessage.pvBuffer,\n\t              context->NegotiateMessage.cbBuffer);\n\tntlm_print_negotiate_flags(message->NegotiateFlags);\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t\tntlm_print_version_info(&(message->Version));\n\n#endif\n\tcontext->state = NTLM_STATE_CHALLENGE;\n\tStream_Free(s, FALSE);\n\treturn SEC_I_CONTINUE_NEEDED;\n}", "func_src_after": "SECURITY_STATUS ntlm_read_NegotiateMessage(NTLM_CONTEXT* context, PSecBuffer buffer)\n{\n\twStream* s;\n\tsize_t length;\n\tNTLM_NEGOTIATE_MESSAGE* message;\n\tmessage = &context->NEGOTIATE_MESSAGE;\n\tZeroMemory(message, sizeof(NTLM_NEGOTIATE_MESSAGE));\n\ts = Stream_New((BYTE*)buffer->pvBuffer, buffer->cbBuffer);\n\n\tif (!s)\n\t\treturn SEC_E_INTERNAL_ERROR;\n\n\tif (ntlm_read_message_header(s, (NTLM_MESSAGE_HEADER*)message) < 0)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->MessageType != MESSAGE_TYPE_NEGOTIATE)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\tStream_Read_UINT32(s, message->NegotiateFlags); /* NegotiateFlags (4 bytes) */\n\n\tif (!((message->NegotiateFlags & NTLMSSP_REQUEST_TARGET) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_NTLM) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_UNICODE)))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tcontext->NegotiateFlags = message->NegotiateFlags;\n\n\t/* only set if NTLMSSP_NEGOTIATE_DOMAIN_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->DomainName)) < 0) /* DomainNameFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\t/* only set if NTLMSSP_NEGOTIATE_WORKSTATION_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->Workstation)) < 0) /* WorkstationFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t{\n\t\tif (ntlm_read_version_info(s, &(message->Version)) < 0) /* Version (8 bytes) */\n\t\t{\n\t\t\tStream_Free(s, FALSE);\n\t\t\treturn SEC_E_INVALID_TOKEN;\n\t\t}\n\t}\n\n\tlength = Stream_GetPosition(s);\n\tbuffer->cbBuffer = length;\n\n\tif (!sspi_SecBufferAlloc(&context->NegotiateMessage, length))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INTERNAL_ERROR;\n\t}\n\n\tCopyMemory(context->NegotiateMessage.pvBuffer, buffer->pvBuffer, buffer->cbBuffer);\n\tcontext->NegotiateMessage.BufferType = buffer->BufferType;\n#ifdef WITH_DEBUG_NTLM\n\tWLog_DBG(TAG, \"NEGOTIATE_MESSAGE (length = %\" PRIu32 \")\", context->NegotiateMessage.cbBuffer);\n\twinpr_HexDump(TAG, WLOG_DEBUG, context->NegotiateMessage.pvBuffer,\n\t              context->NegotiateMessage.cbBuffer);\n\tntlm_print_negotiate_flags(message->NegotiateFlags);\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t\tntlm_print_version_info(&(message->Version));\n\n#endif\n\tcontext->state = NTLM_STATE_CHALLENGE;\n\tStream_Free(s, FALSE);\n\treturn SEC_I_CONTINUE_NEEDED;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/8fa38359634a9910b91719818ab02f23c320dbae", "file_name": "winpr/libwinpr/sspi/NTLM/ntlm_message.c", "vul_type": "cwe-125", "description": "In C, write a function to parse and validate an NTLM Negotiate message from a security buffer."}
{"func_name": "ranks", "func_src_before": "@endpoints.route(\"/ranks\")\ndef ranks():\n    if db == None:\n        init()\n\n    scene = request.args.get('scene', default='austin')\n    date = request.args.get('date')\n \n    # If no date was provided, pick the date of the latest tournament\n    if date == None:\n        sql = \"SELECT distinct date FROM ranks WHERE scene='{}' ORDER BY date DESC LIMIT 1;\".format(scene)\n        res = db.exec(sql)\n        date = res[0][0]\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    cur_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        cur_ranks[tag] = rank\n\n    # Now get the ranks from last month, so we know if these players went up or down\n    y, m, d = date.split('-')\n    prev_date = bracket_utils.get_previous_month(date)\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, prev_date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    prev_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        prev_ranks[tag] = rank\n\n    return render_template('libraries/html/ranks.html', cur_ranks=cur_ranks, prev_ranks=prev_ranks, scene=scene, date=date)", "func_src_after": "@endpoints.route(\"/ranks\")\ndef ranks():\n    if db == None:\n        init()\n\n    scene = request.args.get('scene', default='austin')\n    date = request.args.get('date')\n \n    # If no date was provided, pick the date of the latest tournament\n    if date == None:\n        sql = \"SELECT distinct date FROM ranks WHERE scene='{scene}' ORDER BY date DESC LIMIT 1;\"\n        args = {'scene': scene}\n        res = db.exec(sql, args)\n        date = res[0][0]\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{scene}' and date='{date}'\"\n    args = {'scene': scene, 'date': date}\n    res = db.exec(sql, args)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    cur_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        cur_ranks[tag] = rank\n\n    # Now get the ranks from last month, so we know if these players went up or down\n    y, m, d = date.split('-')\n    prev_date = bracket_utils.get_previous_month(date)\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{scene}' and date='{date}'\"\n    args = {'scene': scene, 'date': prev_date}\n    res = db.exec(sql, args)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    prev_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        prev_ranks[tag] = rank\n\n    return render_template('libraries/html/ranks.html', cur_ranks=cur_ranks, prev_ranks=prev_ranks, scene=scene, date=date)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "endpoints.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/ranks' that retrieves and compares player rankings from a database for the current and previous month."}
{"func_name": "load", "func_src_before": "    @classmethod\n    def load(cls, data_store, filename):\n        pgm_model = None\n        if type(data_store) is LocalFileSystem:\n            pgm_model = data_store.read_pomegranate_model(filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            data_store.download_file(filename, local_filename)\n            with open(local_filename, 'rb') as f:\n                pgm_model = BayesianNetwork.from_json(pickle.load(f))\n        return PGMPomegranate(pgm_model)", "func_src_after": "    @classmethod\n    def load(cls, data_store, filename):\n        pgm_model = None\n        if type(data_store) is LocalFileSystem:\n            pgm_model = data_store.read_pomegranate_model(filename=filename)\n        if type(data_store) is S3DataStore:\n            local_filename = \"/tmp/kronos.json\"\n            data_store.download_file(filename, local_filename)\n            with open(local_filename, 'rb') as f:\n                pgm_model = BayesianNetwork.from_json(f.read())\n        return PGMPomegranate(pgm_model)", "line_changes": {"deleted": [{"line_no": 10, "char_start": 413, "char_end": 483, "line": "                pgm_model = BayesianNetwork.from_json(pickle.load(f))\n"}], "added": [{"line_no": 10, "char_start": 413, "char_end": 477, "line": "                pgm_model = BayesianNetwork.from_json(f.read())\n"}]}, "char_changes": {"deleted": [{"char_start": 467, "char_end": 476, "chars": "pickle.lo"}, {"char_start": 479, "char_end": 480, "chars": "f"}], "added": [{"char_start": 467, "char_end": 471, "chars": "f.re"}]}, "commit_link": "github.com/sara-02/fabric8-analytics-stack-analysis/commit/c9422e6257a8c927aed2999a0f4cc77f90059cda", "file_name": "pgm_pomegranate.py", "vul_type": "cwe-502", "commit_msg": "Remove pickling of model\n\nThe model is already being converted to a JSON(using the `to_json`)\nfunction of pomegranate which is already a stadard serialized format.\nI don't see a need to further serialize the JSON using pickle to\nsomething that can be loaded only using Python. This also helps us\nreduce the training time of the model as pickling and unpickling\nhas a overhead that I don't see a need for, because JSON.", "parent_commit": "c2ddf128d7206a0a85929b6f2a08078433ce1577", "description": "Create a Python method that loads a probabilistic graphical model from a local or S3 data store based on the provided filename."}
{"func_name": "htmlvalue", "func_src_before": "    def htmlvalue(self, val):\n        return self.block.render_basic(val)", "func_src_after": "    def htmlvalue(self, val):\n        \"\"\"\n        Return an HTML representation of this block that is safe to be included\n        in comparison views\n        \"\"\"\n        return escape(text_from_html(self.block.render_basic(val)))", "commit_link": "github.com/wagtail/wagtail/commit/61045ceefea114c40ac4b680af58990dbe732389", "file_name": "wagtail/admin/compare.py", "vul_type": "cwe-079", "description": "Provide a Python function named `htmlvalue` that returns a safe HTML representation of a given value using a block's render method."}
{"func_name": "load", "func_src_before": "  def load\n    case extname\n    when \".yml\", \".yaml\"\n      require 'yaml'\n      YAML.load(self.read)\n    when \".json\"\n      require 'json'\n      JSON.load(self.read)\n    else\n      raise \"Unable to load #{self} (unrecognized extension)\"\n    end", "func_src_after": "  def load\n    case extname\n    when \".yml\", \".yaml\"\n      require 'yaml'\n      YAML.load_file(self)\n    when \".json\"\n      require 'json'\n      JSON.load(self.read)\n    else\n      raise \"Unable to load #{self} (unrecognized extension)\"\n    end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 74, "char_end": 101, "line": "      YAML.load(self.read)\n"}], "added": [{"line_no": 5, "char_start": 74, "char_end": 101, "line": "      YAML.load_file(self)\n"}]}, "char_changes": {"deleted": [{"char_start": 94, "char_end": 99, "chars": ".read"}], "added": [{"char_start": 89, "char_end": 94, "chars": "_file"}]}, "commit_link": "github.com/eregon/path/commit/447973624dd714c3a6e642ad8f773f6df46ff7ad", "file_name": "load.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.load_file, it might avoid to store the entire String in memory", "parent_commit": "45cca4450cde20c71bac68b26d5a7c5a8f3df08f", "description": "Write a Ruby method named `load` that loads data from a file based on its extension, supporting YAML and JSON formats."}
{"func_name": "summary", "func_src_before": "@app.route('/summary', methods=['GET'])\ndef summary():\n\tif 'username' in session:\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\t#select the maximum score from the results table\n\t\tcursor.execute(\"SELECT courseConcentration FROM results WHERE total = (SELECT MAX(total) FROM (SELECT * FROM results WHERE courseId > 4) Temp) and courseId > 4 and emailAccount='\" + session['username'] + \"'\");\n\t\tcourseConcentration = cursor.fetchone()\n\n\t\treturn render_template('summary.html', courseConcentration = courseConcentration[0])\n\treturn redirect(url_for('login'))", "func_src_after": "@app.route('/summary', methods=['GET'])\ndef summary():\n\tif 'username' in session:\n\n\t\tconn = mysql.connect()\n\t\tcursor = conn.cursor()\n\n\t\t#select the maximum score from the results table\n\t\tcursor.execute(\"SELECT courseConcentration FROM results WHERE total = (SELECT MAX(total) FROM (SELECT * FROM results WHERE courseId > 4) Temp) and courseId > 4 and emailAccount=%s\", (session['username']));\n\t\tcourseConcentration = cursor.fetchone()\n\n\t\treturn render_template('summary.html', courseConcentration = courseConcentration[0])\n\treturn redirect(url_for('login'))", "commit_link": "github.com/CaitlinKennedy/Tech-Track/commit/20ef2d4010f9497b8221524edd0c706e2c6a4147", "file_name": "src/tech_track.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint that retrieves the user's highest score course concentration from a MySQL database and displays it on a summary page if logged in, otherwise redirects to the login page."}
{"func_name": "_gd2GetHeader", "func_src_before": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "func_src_after": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tif (overflow2(sidx, nc)) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tif (cidx == NULL) {\n\t\t\tgoto fail1;\n\t\t}\n\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "commit_link": "github.com/php/php-src/commit/7722455726bec8c53458a32851d2a87982cf0eac", "file_name": "ext/gd/libgd/gd_gd2.c", "vul_type": "cwe-190", "description": "Write a C function to read and validate the header of a GD2 image file."}
{"func_name": "get_old_sourcebyinstitution_number", "func_src_before": "def get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution):\n    \"\"\"\n    Get all the old sourcebyinstitution number from the SQLite database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            titles\n        FROM\n            history\n        WHERE\n            sourcebyinstitution = \"%s\"\n        ORDER BY\n            titles DESC\n        LIMIT 1\n    \"\"\" % sourcebyinstitution\n\n    sqlite.execute(query)\n    for record in sqlite:\n        old_sourcebyinstitution_number = record[0]\n        return old_sourcebyinstitution_number", "func_src_after": "def get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution):\n    \"\"\"\n    Get all the old sourcebyinstitution number from the SQLite database.\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            titles\n        FROM\n            history\n        WHERE\n            sourcebyinstitution = ?\n        ORDER BY\n            titles DESC\n        LIMIT 1\n    \"\"\"\n\n    sqlite.execute(query, (sourcebyinstitution,))\n    for record in sqlite:\n        old_sourcebyinstitution_number = record[0]\n        return old_sourcebyinstitution_number", "commit_link": "github.com/miku/siskin/commit/7fa398d2fea72bf2e8b4808f75df4b3d35ae959a", "file_name": "bin/solrcheckup.py", "vul_type": "cwe-089", "description": "Write a Python function to retrieve the most recent title associated with a given source institution from an SQLite database."}
{"func_name": "getAuthenticatedBasket", "func_src_before": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "func_src_after": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 179, "char_end": 303, "line": "\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}], "added": [{"line_no": 4, "char_start": 179, "char_end": 301, "line": "\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}]}, "char_changes": {"deleted": [{"char_start": 217, "char_end": 220, "chars": "[\"+"}, {"char_start": 224, "char_end": 227, "chars": "+\"]"}], "added": [{"char_start": 217, "char_end": 221, "chars": "the "}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go function to authenticate and retrieve a basket by name from a database, handling HTTP requests and responses."}
{"func_name": "(anonymous)", "func_src_before": ".factory('Alert', ['$rootScope', function ($rootScope) {\n    return function (hdr, msg, cls, action, secondAlert, disableButtons, backdrop) {\n        var scope = $rootScope.$new(), alertClass, local_backdrop;\n        if (secondAlert) {\n\n            $('#alertHeader2').html(hdr);\n            $('#alert2-modal-msg').html(msg);\n\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert2-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal2').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n            scope.disableButtons2 = (disableButtons) ? true : false;\n\n            $('#alert-modal2').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal2').on('shown.bs.modal', function () {\n                $('#alert2_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal2').modal('hide');\n                }\n            });\n        } else {\n\n            $('#alertHeader').html(hdr);\n            $('#alert-modal-msg').html(msg);\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n\n            $('#alert-modal').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal').on('shown.bs.modal', function () {\n                $('#alert_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal').modal('hide');\n                }\n            });\n\n            scope.disableButtons = (disableButtons) ? true : false;\n        }\n    };\n}])", "func_src_after": ".factory('Alert', ['$rootScope', '$filter', function ($rootScope, $filter) {\n    return function (hdr, msg, cls, action, secondAlert, disableButtons, backdrop) {\n        var scope = $rootScope.$new(), alertClass, local_backdrop;\n        msg = $filter('sanitize')(msg);\n        if (secondAlert) {\n\n            $('#alertHeader2').html(hdr);\n            $('#alert2-modal-msg').html(msg);\n\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert2-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal2').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n            scope.disableButtons2 = (disableButtons) ? true : false;\n\n            $('#alert-modal2').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal2').on('shown.bs.modal', function () {\n                $('#alert2_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal2').modal('hide');\n                }\n            });\n        } else {\n\n            $('#alertHeader').html(hdr);\n            $('#alert-modal-msg').html(msg);\n            alertClass = (cls) ? cls : 'alert-danger'; //default alert class is alert-danger\n            local_backdrop = (backdrop === undefined) ? \"static\" : backdrop;\n\n            $('#alert-modal-msg').attr({ \"class\": \"alert \" + alertClass });\n            $('#alert-modal').modal({\n                show: true,\n                keyboard: true,\n                backdrop: local_backdrop\n            });\n\n            $('#alert-modal').on('hidden.bs.modal', function () {\n                if (action) {\n                    action();\n                }\n            });\n            $('#alert-modal').on('shown.bs.modal', function () {\n                $('#alert_ok_btn').focus();\n            });\n            $(document).bind('keydown', function (e) {\n                if (e.keyCode === 27 || e.keyCode === 13) {\n                    $('#alert-modal').modal('hide');\n                }\n            });\n\n            scope.disableButtons = (disableButtons) ? true : false;\n        }\n    };\n}])", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 57, "line": ".factory('Alert', ['$rootScope', function ($rootScope) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 77, "line": ".factory('Alert', ['$rootScope', '$filter', function ($rootScope, $filter) {\n"}, {"line_no": 4, "char_start": 229, "char_end": 269, "line": "        msg = $filter('sanitize')(msg);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 32, "char_end": 43, "chars": " '$filter',"}, {"char_start": 64, "char_end": 73, "chars": ", $filter"}, {"char_start": 229, "char_end": 269, "chars": "        msg = $filter('sanitize')(msg);\n"}]}, "commit_link": "github.com/wwitzel3/awx/commit/b127e7f2765c6173c5cf27d34491e7ca0a4ac101", "file_name": "Utilities.js", "vul_type": "cwe-079", "commit_msg": "fixing xss bugs", "description": "Create a JavaScript (AngularJS) factory named 'Alert' that displays a modal with customizable options."}
{"func_name": "getSeriesDateFromDatabase", "func_src_before": "def getSeriesDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT StartDate FROM SeriesTracking WHERE SeriesTitle = '\" + str(getTitle(submission)) + \"'\").fetchone()[0]\n    database.close()", "func_src_after": "def getSeriesDateFromDatabase(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT StartDate FROM SeriesTracking WHERE SeriesTitle = ?\", [getTitle(submission)]).fetchone()[0]\n    database.close()", "commit_link": "github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/commit/0cad2d52e24b05da32789fbc8face7a9999a71f9", "file_name": "CheckAndPostForSeriesSubmissions.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch the start date of a series from an SQLite database using the series title."}
{"func_name": "create_new_repo", "func_src_before": "def create_new_repo(folder, which_branch):\n\n    # create the folder & initialize an empty bare repo\n    os.makedirs(folder, mode=0o770)\n    os.chdir(folder)\n    # note: --initial-branch option requires git 2.28.0 or greater\n    os.system(f'git init --bare --shared --initial-branch={which_branch}')\n\n    # unfortuantely, when an empty repo with no branches is cloned,\n    # the active branch and HEAD does NOT default to the specified branch\n\n    # so let's manually specify the initial branch\n    os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n\n    # and explicitly add an empty commit to the specified branch\n    # so that the repository is not empty\n    add_empty_commit(folder,which_branch)\n\n    print(f'Created new repo {folder}')", "func_src_after": "def create_new_repo(folder, which_branch):\n\n    # create the folder & initialize an empty bare repo\n    os.makedirs(folder, mode=0o770)\n    os.chdir(folder)\n    # note: --initial-branch option requires git 2.28.0 or greater\n    subprocess.run(['git', 'init', '--bare', '--shared', f'--initial-branch={which_branch}'])\n\n    # unfortuantely, when an empty repo with no branches is cloned,\n    # the active branch and HEAD does NOT default to the specified branch\n\n    # so let's manually specify the initial branch\n    subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n\n    # and explicitly add an empty commit to the specified branch\n    # so that the repository is not empty\n    add_empty_commit(folder,which_branch)\n\n    print(f'Created new repo {folder}')", "line_changes": {"deleted": [{"line_no": 7, "char_start": 224, "char_end": 299, "line": "    os.system(f'git init --bare --shared --initial-branch={which_branch}')\n"}, {"line_no": 13, "char_start": 494, "char_end": 560, "line": "    os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n"}], "added": [{"line_no": 7, "char_start": 224, "char_end": 318, "line": "    subprocess.run(['git', 'init', '--bare', '--shared', f'--initial-branch={which_branch}'])\n"}, {"line_no": 13, "char_start": 513, "char_end": 595, "line": "    subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n"}]}, "char_changes": {"deleted": [{"char_start": 228, "char_end": 249, "chars": "os.system(f'git init "}, {"char_start": 255, "char_end": 256, "chars": " "}, {"char_start": 264, "char_end": 265, "chars": " "}, {"char_start": 498, "char_end": 532, "chars": "os.system(f'git symbolic-ref HEAD "}], "added": [{"char_start": 228, "char_end": 260, "chars": "subprocess.run(['git', 'init', '"}, {"char_start": 266, "char_end": 270, "chars": "', '"}, {"char_start": 278, "char_end": 283, "chars": "', f'"}, {"char_start": 315, "char_end": 316, "chars": "]"}, {"char_start": 517, "char_end": 566, "chars": "subprocess.run(['git', 'symbolic-ref', 'HEAD', f'"}, {"char_start": 592, "char_end": 593, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to initialize a new Git repository with a specified branch in a given folder."}
{"func_name": "al_segment_cwd_prefix", "func_src_before": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 64, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "func_src_after": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 16, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 64, \" %s \", prefix);\n"}], "added": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 16, \" %s \", prefix);\n"}]}, "char_changes": {"deleted": [{"char_start": 829, "char_end": 830, "chars": "4"}], "added": [{"char_start": 828, "char_end": 829, "chars": "1"}]}, "commit_link": "github.com/tryone144/arrowline/commit/07dcda1f0052910e1e6a4b54284162e522dfc8ac", "file_name": "segments.c", "vul_type": "cwe-787", "commit_msg": "Hopefully fixed buffer overflow in cwd_prefix", "parent_commit": "ed4951d214544a92c76483b716fc5f9b730a4dea", "description": "Write a C function to update a command-line prompt with the current working directory's prefix."}
{"func_name": "openPoll", "func_src_before": "@hook.command(adminonly=True)\ndef openPoll(question, reply=None, db=None):\n    \"\"\"Creates a new poll.\"\"\"\n    if not db_ready: db_init(db)\n    try:\n        active = db.execute(\"SELECT pollID FROM polls WHERE active = 1\").fetchone()[0]\n        if active: \n            reply(\"There already is an open poll.\")\n            return\n    except:\n        db.execute(\"INSERT INTO polls (question, active) VALUES ('{}', 1)\".format(question))\n        reply(\"Opened new poll: {}\".format(question))\n        #reply(\"Poll opened!\")\n    return", "func_src_after": "@hook.command(adminonly=True)\ndef openPoll(question, reply=None, db=None):\n    \"\"\"Creates a new poll.\"\"\"\n    if not db_ready: db_init(db)\n    try:\n        active = db.execute(\"SELECT pollID FROM polls WHERE active = 1\").fetchone()[0]\n        if active: \n            reply(\"There already is an open poll.\")\n            return\n    except:\n        db.execute(\"INSERT INTO polls (question, active) VALUES (?, 1)\", (question,))\n        reply(\"Opened new poll: {}\".format(question))\n        #reply(\"Poll opened!\")\n    return", "commit_link": "github.com/FrozenPigs/Taigabot/commit/ea9b83a66ae1f0f38a1895f3e8dfa2833d77e3a6", "file_name": "plugins/poll.py", "vul_type": "cwe-089", "description": "Write a Python function that creates a new poll in a database, ensuring only one active poll exists at a time."}
{"func_name": "test_get_least_used_nsp", "func_src_before": "    def test_get_least_used_nsp(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = 'showvlun -a -showcols Port'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n        # in use count                           11       12\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])\n        self.assertEqual(nsp, '0:2:1')\n\n        # in use count                            11       10\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])\n        self.assertEqual(nsp, '1:2:1')\n\n        # in use count                            0       10\n        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])\n        self.assertEqual(nsp, '1:1:1')", "func_src_after": "    def test_get_least_used_nsp(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n        # in use count                           11       12\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])\n        self.assertEqual(nsp, '0:2:1')\n\n        # in use count                            11       10\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])\n        self.assertEqual(nsp, '1:2:1')\n\n        # in use count                            0       10\n        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])\n        self.assertEqual(nsp, '1:1:1')", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks an SSH command to determine the least used network service provider."}
{"func_name": "lexer_process_char_literal", "func_src_before": "lexer_process_char_literal (parser_context_t *context_p, /**< context */\n                            const uint8_t *char_p, /**< characters */\n                            size_t length, /**< length of string */\n                            uint8_t literal_type, /**< final literal type */\n                            bool has_escape) /**< has escape sequences */\n{\n  parser_list_iterator_t literal_iterator;\n  lexer_literal_t *literal_p;\n  uint32_t literal_index = 0;\n\n  JERRY_ASSERT (literal_type == LEXER_IDENT_LITERAL\n                || literal_type == LEXER_STRING_LITERAL);\n\n  JERRY_ASSERT (literal_type != LEXER_IDENT_LITERAL || length <= PARSER_MAXIMUM_IDENT_LENGTH);\n  JERRY_ASSERT (literal_type != LEXER_STRING_LITERAL || length <= PARSER_MAXIMUM_STRING_LENGTH);\n\n  parser_list_iterator_init (&context_p->literal_pool, &literal_iterator);\n\n  while ((literal_p = (lexer_literal_t *) parser_list_iterator_next (&literal_iterator)) != NULL)\n  {\n    if (literal_p->type == literal_type\n        && literal_p->prop.length == length\n        && memcmp (literal_p->u.char_p, char_p, length) == 0)\n    {\n      context_p->lit_object.literal_p = literal_p;\n      context_p->lit_object.index = (uint16_t) literal_index;\n      literal_p->status_flags = (uint8_t) (literal_p->status_flags & ~LEXER_FLAG_UNUSED_IDENT);\n      return;\n    }\n\n    literal_index++;\n  }\n\n  JERRY_ASSERT (literal_index == context_p->literal_count);\n\n  if (literal_index >= PARSER_MAXIMUM_NUMBER_OF_LITERALS)\n  {\n    parser_raise_error (context_p, PARSER_ERR_LITERAL_LIMIT_REACHED);\n  }\n\n  literal_p = (lexer_literal_t *) parser_list_append (context_p, &context_p->literal_pool);\n  literal_p->prop.length = (uint16_t) length;\n  literal_p->type = literal_type;\n  literal_p->status_flags = has_escape ? 0 : LEXER_FLAG_SOURCE_PTR;\n\n  if (has_escape)\n  {\n    literal_p->u.char_p = (uint8_t *) jmem_heap_alloc_block (length);\n    memcpy ((uint8_t *) literal_p->u.char_p, char_p, length);\n  }\n  else\n  {\n    literal_p->u.char_p = char_p;\n  }\n\n  context_p->lit_object.literal_p = literal_p;\n  context_p->lit_object.index = (uint16_t) literal_index;\n  context_p->literal_count++;\n} /* lexer_process_char_literal */", "func_src_after": "lexer_process_char_literal (parser_context_t *context_p, /**< context */\n                            const uint8_t *char_p, /**< characters */\n                            size_t length, /**< length of string */\n                            uint8_t literal_type, /**< final literal type */\n                            bool has_escape) /**< has escape sequences */\n{\n  parser_list_iterator_t literal_iterator;\n  lexer_literal_t *literal_p;\n  uint32_t literal_index = 0;\n\n  JERRY_ASSERT (literal_type == LEXER_IDENT_LITERAL\n                || literal_type == LEXER_STRING_LITERAL);\n\n  JERRY_ASSERT (literal_type != LEXER_IDENT_LITERAL || length <= PARSER_MAXIMUM_IDENT_LENGTH);\n  JERRY_ASSERT (literal_type != LEXER_STRING_LITERAL || length <= PARSER_MAXIMUM_STRING_LENGTH);\n\n  parser_list_iterator_init (&context_p->literal_pool, &literal_iterator);\n\n  while ((literal_p = (lexer_literal_t *) parser_list_iterator_next (&literal_iterator)) != NULL)\n  {\n    if (literal_p->type == literal_type\n        && literal_p->prop.length == length\n        && memcmp (literal_p->u.char_p, char_p, length) == 0)\n    {\n      context_p->lit_object.literal_p = literal_p;\n      context_p->lit_object.index = (uint16_t) literal_index;\n      literal_p->status_flags = (uint8_t) (literal_p->status_flags & ~LEXER_FLAG_UNUSED_IDENT);\n      return;\n    }\n\n    literal_index++;\n  }\n\n  JERRY_ASSERT (literal_index == context_p->literal_count);\n\n  if (literal_index >= PARSER_MAXIMUM_NUMBER_OF_LITERALS)\n  {\n    parser_raise_error (context_p, PARSER_ERR_LITERAL_LIMIT_REACHED);\n  }\n\n  if (length == 0)\n  {\n    has_escape = false;\n  }\n\n  literal_p = (lexer_literal_t *) parser_list_append (context_p, &context_p->literal_pool);\n  literal_p->prop.length = (uint16_t) length;\n  literal_p->type = literal_type;\n  literal_p->status_flags = has_escape ? 0 : LEXER_FLAG_SOURCE_PTR;\n\n  if (has_escape)\n  {\n    literal_p->u.char_p = (uint8_t *) jmem_heap_alloc_block (length);\n    memcpy ((uint8_t *) literal_p->u.char_p, char_p, length);\n  }\n  else\n  {\n    literal_p->u.char_p = char_p;\n  }\n\n  context_p->lit_object.literal_p = literal_p;\n  context_p->lit_object.index = (uint16_t) literal_index;\n  context_p->literal_count++;\n} /* lexer_process_char_literal */", "commit_link": "github.com/jerryscript-project/jerryscript/commit/e58f2880df608652aff7fd35c45b242467ec0e79", "file_name": "jerry-core/parser/js/js-lexer.c", "vul_type": "cwe-476", "description": "In C, write a function to process character literals in a parser context, handling escape sequences and ensuring they don't exceed predefined limits."}
{"func_name": "add_translationname", "func_src_before": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (item[0], trname[1], trname[2]))\n            self.connection.commit()", "func_src_after": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                t = (item[0], trname[1], trname[2], )\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (?, ?, ?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new translation into a database given an item name and translation details."}
{"func_name": "mem_iconveha", "func_src_before": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "func_src_after": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      if (to_codeset_suffixed == NULL)\n        {\n          errno = ENOMEM;\n          return -1;\n        }\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "commit_link": "github.com/coreutils/gnulib/commit/fce9817d48c97339c3f66a92e72faba8e69d405c", "file_name": "lib/striconveha.c", "vul_type": "cwe-476", "description": "Write a C function `mem_iconveha` that converts a string from one character encoding to another, with an option for transliteration."}
{"func_name": "generatePreview", "func_src_before": "generatePreview (const char inFileName[],\n\t\t float exposure,\n\t\t int previewWidth,\n\t\t int &previewHeight,\n\t\t Array2D <PreviewRgba> &previewPixels)\n{\n    //\n    // Read the input file\n    //\n\n    RgbaInputFile in (inFileName);\n\n    Box2i dw = in.dataWindow();\n    float a = in.pixelAspectRatio();\n    int w = dw.max.x - dw.min.x + 1;\n    int h = dw.max.y - dw.min.y + 1;\n\n    Array2D <Rgba> pixels (h, w);\n    in.setFrameBuffer (ComputeBasePointer (&pixels[0][0], dw), 1, w);\n    in.readPixels (dw.min.y, dw.max.y);\n\n    //\n    // Make a preview image\n    //\n\n    previewHeight = max (int (h / (w * a) * previewWidth + .5f), 1);\n    previewPixels.resizeErase (previewHeight, previewWidth);\n\n    float fx = (previewWidth  > 0)? (float (w - 1) / (previewWidth  - 1)): 1;\n    float fy = (previewHeight > 0)? (float (h - 1) / (previewHeight - 1)): 1;\n    float m  = Math<float>::pow (2.f, IMATH_NAMESPACE::clamp (exposure + 2.47393f, -20.f, 20.f));\n\n    for (int y = 0; y < previewHeight; ++y)\n    {\n\tfor (int x = 0; x < previewWidth; ++x)\n\t{\n\t    PreviewRgba &preview = previewPixels[y][x];\n\t    const Rgba &pixel = pixels[int (y * fy + .5f)][int (x * fx + .5f)];\n\n\t    preview.r = gamma (pixel.r, m);\n\t    preview.g = gamma (pixel.g, m);\n\t    preview.b = gamma (pixel.b, m);\n\t    preview.a = int (IMATH_NAMESPACE::clamp (pixel.a * 255.f, 0.f, 255.f) + .5f);\n\t}\n    }\n}", "func_src_after": "generatePreview (const char inFileName[],\n\t\t float exposure,\n\t\t int previewWidth,\n\t\t int &previewHeight,\n\t\t Array2D <PreviewRgba> &previewPixels)\n{\n    //\n    // Read the input file\n    //\n\n    RgbaInputFile in (inFileName);\n\n    Box2i dw = in.dataWindow();\n    float a = in.pixelAspectRatio();\n    int w = dw.max.x - dw.min.x + 1;\n    int h = dw.max.y - dw.min.y + 1;\n\n    Array2D <Rgba> pixels (h, w);\n    in.setFrameBuffer (ComputeBasePointer (&pixels[0][0], dw), 1, w);\n    in.readPixels (dw.min.y, dw.max.y);\n\n    //\n    // Make a preview image\n    //\n\n    previewHeight = max (int (h / (w * a) * previewWidth + .5f), 1);\n    previewPixels.resizeErase (previewHeight, previewWidth);\n\n    float fx = (previewWidth  > 1)? (float (w - 1) / (previewWidth  - 1)): 1;\n    float fy = (previewHeight > 1)? (float (h - 1) / (previewHeight - 1)): 1;\n    float m  = Math<float>::pow (2.f, IMATH_NAMESPACE::clamp (exposure + 2.47393f, -20.f, 20.f));\n\n    for (int y = 0; y < previewHeight; ++y)\n    {\n\tfor (int x = 0; x < previewWidth; ++x)\n\t{\n\t    PreviewRgba &preview = previewPixels[y][x];\n\t    const Rgba &pixel = pixels[int (y * fy + .5f)][int (x * fx + .5f)];\n\n\t    preview.r = gamma (pixel.r, m);\n\t    preview.g = gamma (pixel.g, m);\n\t    preview.b = gamma (pixel.b, m);\n\t    preview.a = int (IMATH_NAMESPACE::clamp (pixel.a * 255.f, 0.f, 255.f) + .5f);\n\t}\n    }\n}", "commit_link": "github.com/AcademySoftwareFoundation/openexr/commit/74504503cff86e986bac441213c403b0ba28d58f", "file_name": "OpenEXR/exrmakepreview/makePreview.cpp", "vul_type": "cwe-476", "description": "In C++, write a function to generate a resized preview image from an input file with adjustable exposure."}
{"func_name": "(anonymous)", "func_src_before": "    $(\"#extra\").on(\"click\", \"#moveRepeatingGroup\", function() {\n        var repeatingCols = '';\n        $(\"#extra input[type=checkbox]:checked\").each(function() {\n            repeatingCols += $(this).val() + ', ';\n        });\n\n        if (repeatingCols !== '') {\n            var newColName = $(\"#extra input[type=checkbox]:checked:first\").val();\n            repeatingCols = repeatingCols.slice(0, -2);\n            var confirmStr = PMA_sprintf(PMA_messages.strMoveRepeatingGroup, escapeHtml(repeatingCols), escapeHtml(PMA_commonParams.get('table')));\n            confirmStr += '<input type=\"text\" name=\"repeatGroupTable\" placeholder=\"' + PMA_messages.strNewTablePlaceholder + '\"/>' +\n                '( ' + escapeHtml(primary_key.toString()) + ', <input type=\"text\" name=\"repeatGroupColumn\" placeholder=\"' + PMA_messages.strNewColumnPlaceholder + '\" value=\"' + escapeHtml(newColName) + '\">)' +\n                '</ol>';\n            $(\"#newCols\").html(confirmStr);\n            $('.tblFooters').html('<input type=\"submit\" value=\"' + PMA_messages.strCancel + '\" onclick=\"$(\\'#newCols\\').html(\\'\\');$(\\'#extra input[type=checkbox]\\').removeAttr(\\'checked\\')\"/>' +\n                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + repeatingCols + '\\')\"/>');\n        }\n    });", "func_src_after": "    $(\"#extra\").on(\"click\", \"#moveRepeatingGroup\", function() {\n        var repeatingCols = '';\n        $(\"#extra input[type=checkbox]:checked\").each(function() {\n            repeatingCols += $(this).val() + ', ';\n        });\n\n        if (repeatingCols !== '') {\n            var newColName = $(\"#extra input[type=checkbox]:checked:first\").val();\n            repeatingCols = repeatingCols.slice(0, -2);\n            var confirmStr = PMA_sprintf(PMA_messages.strMoveRepeatingGroup, escapeHtml(repeatingCols), escapeHtml(PMA_commonParams.get('table')));\n            confirmStr += '<input type=\"text\" name=\"repeatGroupTable\" placeholder=\"' + PMA_messages.strNewTablePlaceholder + '\"/>' +\n                '( ' + escapeHtml(primary_key.toString()) + ', <input type=\"text\" name=\"repeatGroupColumn\" placeholder=\"' + PMA_messages.strNewColumnPlaceholder + '\" value=\"' + escapeHtml(newColName) + '\">)' +\n                '</ol>';\n            $(\"#newCols\").html(confirmStr);\n            $('.tblFooters').html('<input type=\"submit\" value=\"' + PMA_messages.strCancel + '\" onclick=\"$(\\'#newCols\\').html(\\'\\');$(\\'#extra input[type=checkbox]\\').removeAttr(\\'checked\\')\"/>' +\n                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + escapeJsString(escapeHtml(repeatingCols)) + '\\')\"/>');\n        }\n    });", "line_changes": {"deleted": [{"line_no": 16, "char_start": 1158, "char_end": 1292, "line": "                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + repeatingCols + '\\')\"/>');\n"}], "added": [{"line_no": 16, "char_start": 1158, "char_end": 1320, "line": "                '<input type=\"submit\" value=\"' + PMA_messages.strGo + '\" onclick=\"moveRepeatingGroup(\\'' + escapeJsString(escapeHtml(repeatingCols)) + '\\')\"/>');\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1265, "char_end": 1291, "chars": "escapeJsString(escapeHtml("}, {"char_start": 1304, "char_end": 1306, "chars": "))"}]}, "commit_link": "github.com/Achilles-96/phpmyadmin/commit/f33a42f1da9db943a67bda7d29f7dd91957a8e7e", "file_name": "normalization.js", "vul_type": "cwe-079", "commit_msg": "Fix XSS in normalization.js\n\nSigned-off-by: Madhura Jayaratne <madhura.cj@gmail.com>", "description": "Write a jQuery snippet in JavaScript that handles a click event to process and display selected checkboxes for moving a repeating group of table columns."}
{"func_name": "saa7164_bus_get", "func_src_before": "int saa7164_bus_get(struct saa7164_dev *dev, struct tmComResInfo* msg,\n\tvoid *buf, int peekonly)\n{\n\tstruct tmComResBusInfo *bus = &dev->bus;\n\tu32 bytes_to_read, write_distance, curr_grp, curr_gwp,\n\t\tnew_grp, buf_size, space_rem;\n\tstruct tmComResInfo msg_tmp;\n\tint ret = SAA_ERR_BAD_PARAMETER;\n\n\tsaa7164_bus_verify(dev);\n\n\tif (msg == NULL)\n\t\treturn ret;\n\n\tif (msg->size > dev->bus.m_wMaxReqSize) {\n\t\tprintk(KERN_ERR \"%s() Exceeded dev->bus.m_wMaxReqSize\\n\",\n\t\t\t__func__);\n\t\treturn ret;\n\t}\n\n\tif ((peekonly == 0) && (msg->size > 0) && (buf == NULL)) {\n\t\tprintk(KERN_ERR\n\t\t\t\"%s() Missing msg buf, size should be %d bytes\\n\",\n\t\t\t__func__, msg->size);\n\t\treturn ret;\n\t}\n\n\tmutex_lock(&bus->lock);\n\n\t/* Peek the bus to see if a msg exists, if it's not what we're expecting\n\t * then return cleanly else read the message from the bus.\n\t */\n\tcurr_gwp = saa7164_readl(bus->m_dwGetWritePos);\n\tcurr_grp = saa7164_readl(bus->m_dwGetReadPos);\n\n\tif (curr_gwp == curr_grp) {\n\t\tret = SAA_ERR_EMPTY;\n\t\tgoto out;\n\t}\n\n\tbytes_to_read = sizeof(*msg);\n\n\t/* Calculate write distance to current read position */\n\twrite_distance = 0;\n\tif (curr_gwp >= curr_grp)\n\t\t/* Write doesn't wrap around the ring */\n\t\twrite_distance = curr_gwp - curr_grp;\n\telse\n\t\t/* Write wraps around the ring */\n\t\twrite_distance = curr_gwp + bus->m_dwSizeGetRing - curr_grp;\n\n\tif (bytes_to_read > write_distance) {\n\t\tprintk(KERN_ERR \"%s() No message/response found\\n\", __func__);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Calculate the new read position */\n\tnew_grp = curr_grp + bytes_to_read;\n\tif (new_grp > bus->m_dwSizeGetRing) {\n\n\t\t/* Ring wraps */\n\t\tnew_grp -= bus->m_dwSizeGetRing;\n\t\tspace_rem = bus->m_dwSizeGetRing - curr_grp;\n\n\t\tmemcpy_fromio(&msg_tmp, bus->m_pdwGetRing + curr_grp, space_rem);\n\t\tmemcpy_fromio((u8 *)&msg_tmp + space_rem, bus->m_pdwGetRing,\n\t\t\tbytes_to_read - space_rem);\n\n\t} else {\n\t\t/* No wrapping */\n\t\tmemcpy_fromio(&msg_tmp, bus->m_pdwGetRing + curr_grp, bytes_to_read);\n\t}\n\t/* Convert from little endian to CPU */\n\tmsg_tmp.size = le16_to_cpu((__force __le16)msg_tmp.size);\n\tmsg_tmp.command = le32_to_cpu((__force __le32)msg_tmp.command);\n\tmsg_tmp.controlselector = le16_to_cpu((__force __le16)msg_tmp.controlselector);\n\n\t/* No need to update the read positions, because this was a peek */\n\t/* If the caller specifically want to peek, return */\n\tif (peekonly) {\n\t\tmemcpy(msg, &msg_tmp, sizeof(*msg));\n\t\tgoto peekout;\n\t}\n\n\t/* Check if the command/response matches what is expected */\n\tif ((msg_tmp.id != msg->id) || (msg_tmp.command != msg->command) ||\n\t\t(msg_tmp.controlselector != msg->controlselector) ||\n\t\t(msg_tmp.seqno != msg->seqno) || (msg_tmp.size != msg->size)) {\n\n\t\tprintk(KERN_ERR \"%s() Unexpected msg miss-match\\n\", __func__);\n\t\tsaa7164_bus_dumpmsg(dev, msg, buf);\n\t\tsaa7164_bus_dumpmsg(dev, &msg_tmp, NULL);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Get the actual command and response from the bus */\n\tbuf_size = msg->size;\n\n\tbytes_to_read = sizeof(*msg) + msg->size;\n\t/* Calculate write distance to current read position */\n\twrite_distance = 0;\n\tif (curr_gwp >= curr_grp)\n\t\t/* Write doesn't wrap around the ring */\n\t\twrite_distance = curr_gwp - curr_grp;\n\telse\n\t\t/* Write wraps around the ring */\n\t\twrite_distance = curr_gwp + bus->m_dwSizeGetRing - curr_grp;\n\n\tif (bytes_to_read > write_distance) {\n\t\tprintk(KERN_ERR \"%s() Invalid bus state, missing msg or mangled ring, faulty H/W / bad code?\\n\",\n\t\t       __func__);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Calculate the new read position */\n\tnew_grp = curr_grp + bytes_to_read;\n\tif (new_grp > bus->m_dwSizeGetRing) {\n\n\t\t/* Ring wraps */\n\t\tnew_grp -= bus->m_dwSizeGetRing;\n\t\tspace_rem = bus->m_dwSizeGetRing - curr_grp;\n\n\t\tif (space_rem < sizeof(*msg)) {\n\t\t\t/* msg wraps around the ring */\n\t\t\tmemcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, space_rem);\n\t\t\tmemcpy_fromio((u8 *)msg + space_rem, bus->m_pdwGetRing,\n\t\t\t\tsizeof(*msg) - space_rem);\n\t\t\tif (buf)\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + sizeof(*msg) -\n\t\t\t\t\tspace_rem, buf_size);\n\n\t\t} else if (space_rem == sizeof(*msg)) {\n\t\t\tmemcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));\n\t\t\tif (buf)\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing, buf_size);\n\t\t} else {\n\t\t\t/* Additional data wraps around the ring */\n\t\t\tmemcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));\n\t\t\tif (buf) {\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + curr_grp +\n\t\t\t\t\tsizeof(*msg), space_rem - sizeof(*msg));\n\t\t\t\tmemcpy_fromio(buf + space_rem - sizeof(*msg),\n\t\t\t\t\tbus->m_pdwGetRing, bytes_to_read -\n\t\t\t\t\tspace_rem);\n\t\t\t}\n\n\t\t}\n\n\t} else {\n\t\t/* No wrapping */\n\t\tmemcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));\n\t\tif (buf)\n\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + curr_grp + sizeof(*msg),\n\t\t\t\tbuf_size);\n\t}\n\t/* Convert from little endian to CPU */\n\tmsg->size = le16_to_cpu((__force __le16)msg->size);\n\tmsg->command = le32_to_cpu((__force __le32)msg->command);\n\tmsg->controlselector = le16_to_cpu((__force __le16)msg->controlselector);\n\n\t/* Update the read positions, adjusting the ring */\n\tsaa7164_writel(bus->m_dwGetReadPos, new_grp);\n\npeekout:\n\tret = SAA_OK;\nout:\n\tmutex_unlock(&bus->lock);\n\tsaa7164_bus_verify(dev);\n\treturn ret;\n}", "func_src_after": "int saa7164_bus_get(struct saa7164_dev *dev, struct tmComResInfo* msg,\n\tvoid *buf, int peekonly)\n{\n\tstruct tmComResBusInfo *bus = &dev->bus;\n\tu32 bytes_to_read, write_distance, curr_grp, curr_gwp,\n\t\tnew_grp, buf_size, space_rem;\n\tstruct tmComResInfo msg_tmp;\n\tint ret = SAA_ERR_BAD_PARAMETER;\n\n\tsaa7164_bus_verify(dev);\n\n\tif (msg == NULL)\n\t\treturn ret;\n\n\tif (msg->size > dev->bus.m_wMaxReqSize) {\n\t\tprintk(KERN_ERR \"%s() Exceeded dev->bus.m_wMaxReqSize\\n\",\n\t\t\t__func__);\n\t\treturn ret;\n\t}\n\n\tif ((peekonly == 0) && (msg->size > 0) && (buf == NULL)) {\n\t\tprintk(KERN_ERR\n\t\t\t\"%s() Missing msg buf, size should be %d bytes\\n\",\n\t\t\t__func__, msg->size);\n\t\treturn ret;\n\t}\n\n\tmutex_lock(&bus->lock);\n\n\t/* Peek the bus to see if a msg exists, if it's not what we're expecting\n\t * then return cleanly else read the message from the bus.\n\t */\n\tcurr_gwp = saa7164_readl(bus->m_dwGetWritePos);\n\tcurr_grp = saa7164_readl(bus->m_dwGetReadPos);\n\n\tif (curr_gwp == curr_grp) {\n\t\tret = SAA_ERR_EMPTY;\n\t\tgoto out;\n\t}\n\n\tbytes_to_read = sizeof(*msg);\n\n\t/* Calculate write distance to current read position */\n\twrite_distance = 0;\n\tif (curr_gwp >= curr_grp)\n\t\t/* Write doesn't wrap around the ring */\n\t\twrite_distance = curr_gwp - curr_grp;\n\telse\n\t\t/* Write wraps around the ring */\n\t\twrite_distance = curr_gwp + bus->m_dwSizeGetRing - curr_grp;\n\n\tif (bytes_to_read > write_distance) {\n\t\tprintk(KERN_ERR \"%s() No message/response found\\n\", __func__);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Calculate the new read position */\n\tnew_grp = curr_grp + bytes_to_read;\n\tif (new_grp > bus->m_dwSizeGetRing) {\n\n\t\t/* Ring wraps */\n\t\tnew_grp -= bus->m_dwSizeGetRing;\n\t\tspace_rem = bus->m_dwSizeGetRing - curr_grp;\n\n\t\tmemcpy_fromio(&msg_tmp, bus->m_pdwGetRing + curr_grp, space_rem);\n\t\tmemcpy_fromio((u8 *)&msg_tmp + space_rem, bus->m_pdwGetRing,\n\t\t\tbytes_to_read - space_rem);\n\n\t} else {\n\t\t/* No wrapping */\n\t\tmemcpy_fromio(&msg_tmp, bus->m_pdwGetRing + curr_grp, bytes_to_read);\n\t}\n\t/* Convert from little endian to CPU */\n\tmsg_tmp.size = le16_to_cpu((__force __le16)msg_tmp.size);\n\tmsg_tmp.command = le32_to_cpu((__force __le32)msg_tmp.command);\n\tmsg_tmp.controlselector = le16_to_cpu((__force __le16)msg_tmp.controlselector);\n\tmemcpy(msg, &msg_tmp, sizeof(*msg));\n\n\t/* No need to update the read positions, because this was a peek */\n\t/* If the caller specifically want to peek, return */\n\tif (peekonly) {\n\t\tgoto peekout;\n\t}\n\n\t/* Check if the command/response matches what is expected */\n\tif ((msg_tmp.id != msg->id) || (msg_tmp.command != msg->command) ||\n\t\t(msg_tmp.controlselector != msg->controlselector) ||\n\t\t(msg_tmp.seqno != msg->seqno) || (msg_tmp.size != msg->size)) {\n\n\t\tprintk(KERN_ERR \"%s() Unexpected msg miss-match\\n\", __func__);\n\t\tsaa7164_bus_dumpmsg(dev, msg, buf);\n\t\tsaa7164_bus_dumpmsg(dev, &msg_tmp, NULL);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Get the actual command and response from the bus */\n\tbuf_size = msg->size;\n\n\tbytes_to_read = sizeof(*msg) + msg->size;\n\t/* Calculate write distance to current read position */\n\twrite_distance = 0;\n\tif (curr_gwp >= curr_grp)\n\t\t/* Write doesn't wrap around the ring */\n\t\twrite_distance = curr_gwp - curr_grp;\n\telse\n\t\t/* Write wraps around the ring */\n\t\twrite_distance = curr_gwp + bus->m_dwSizeGetRing - curr_grp;\n\n\tif (bytes_to_read > write_distance) {\n\t\tprintk(KERN_ERR \"%s() Invalid bus state, missing msg or mangled ring, faulty H/W / bad code?\\n\",\n\t\t       __func__);\n\t\tret = SAA_ERR_INVALID_COMMAND;\n\t\tgoto out;\n\t}\n\n\t/* Calculate the new read position */\n\tnew_grp = curr_grp + bytes_to_read;\n\tif (new_grp > bus->m_dwSizeGetRing) {\n\n\t\t/* Ring wraps */\n\t\tnew_grp -= bus->m_dwSizeGetRing;\n\t\tspace_rem = bus->m_dwSizeGetRing - curr_grp;\n\n\t\tif (space_rem < sizeof(*msg)) {\n\t\t\tif (buf)\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + sizeof(*msg) -\n\t\t\t\t\tspace_rem, buf_size);\n\n\t\t} else if (space_rem == sizeof(*msg)) {\n\t\t\tif (buf)\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing, buf_size);\n\t\t} else {\n\t\t\t/* Additional data wraps around the ring */\n\t\t\tif (buf) {\n\t\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + curr_grp +\n\t\t\t\t\tsizeof(*msg), space_rem - sizeof(*msg));\n\t\t\t\tmemcpy_fromio(buf + space_rem - sizeof(*msg),\n\t\t\t\t\tbus->m_pdwGetRing, bytes_to_read -\n\t\t\t\t\tspace_rem);\n\t\t\t}\n\n\t\t}\n\n\t} else {\n\t\t/* No wrapping */\n\t\tif (buf)\n\t\t\tmemcpy_fromio(buf, bus->m_pdwGetRing + curr_grp + sizeof(*msg),\n\t\t\t\tbuf_size);\n\t}\n\n\t/* Update the read positions, adjusting the ring */\n\tsaa7164_writel(bus->m_dwGetReadPos, new_grp);\n\npeekout:\n\tret = SAA_OK;\nout:\n\tmutex_unlock(&bus->lock);\n\tsaa7164_bus_verify(dev);\n\treturn ret;\n}", "commit_link": "github.com/stoth68000/media-tree/commit/354dd3924a2e43806774953de536257548b5002c", "file_name": "drivers/media/pci/saa7164/saa7164-bus.c", "vul_type": "cwe-125", "description": "Write a C function named `saa7164_bus_get` that reads a message from a device's bus, optionally peeking without updating the read position."}
{"func_name": "update_institutions", "func_src_before": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES ('%s')\" % current_institution\n            sqlite.execute(sql)\n            conn.commit()", "func_src_after": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES (?)\"\n            sqlite.execute(sql, (current_institution,))\n            conn.commit()", "commit_link": "github.com/miku/siskin/commit/7fa398d2fea72bf2e8b4808f75df4b3d35ae959a", "file_name": "bin/solrcheckup.py", "vul_type": "cwe-089", "description": "Write a Python function to update an institution table by comparing old and new institution records, sending notifications for discrepancies, and inserting new records."}
{"func_name": "self.read", "func_src_before": "  def self.read(path=default_path)\n    perm = File.stat(path).mode & 0777\n    if perm != 0600 && !(WINDOWS)\n      raise Error, \"Permission bits for '#{path}' should be 0600, but are \"+perm.to_s(8)\n    end\n    new(path, parse(lex(IO.readlines(path))))\n  rescue Errno::ENOENT\n    new(path, parse(lex([])))\n  end", "func_src_after": "  def self.read(path=default_path)\n    perm = File.stat(path).mode & 0777\n    if perm != 0600 && !(WINDOWS)\n      raise Error, \"Permission bits for '#{path}' should be 0600, but are \"+perm.to_s(8)\n    end\n    new(path, parse(lex(File.readlines(path))))\n  rescue Errno::ENOENT\n    new(path, parse(lex([])))\n  end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 205, "char_end": 251, "line": "    new(path, parse(lex(IO.readlines(path))))\n"}], "added": [{"line_no": 6, "char_start": 205, "char_end": 253, "line": "    new(path, parse(lex(File.readlines(path))))\n"}]}, "char_changes": {"deleted": [{"char_start": 229, "char_end": 231, "chars": "IO"}], "added": [{"char_start": 229, "char_end": 233, "chars": "File"}]}, "commit_link": "github.com/trevorgrayson/netrc/commit/544dc4b092d63d3df588ef61274cc32ad1376b02", "file_name": "netrc.rb", "vul_type": "cwe-078", "commit_msg": "use File.readlines instead of IO.readlines", "parent_commit": "41618416a23ff3b5ecd596c6f8eee1bba363a6ef", "description": "Create a Ruby method that reads from a file at a given path, checks for specific file permissions, and handles the case where the file does not exist."}
{"func_name": "DumpByteCode", "func_src_before": "func (v *VM) DumpByteCode(name string) string {\n\tb := new(bytes.Buffer)\n\tfmt.Fprintf(b, \"Prog: %s\\n\", name)\n\tfmt.Fprintln(b, \"Metrics\")\n\tfor i, m := range v.m {\n\t\tif m.Program == v.name {\n\t\t\tfmt.Fprintf(b, \" %8d %s\\n\", i, m)\n\t\t}\n\t}\n\tfmt.Fprintln(b, \"Regexps\")\n\tfor i, re := range v.re {\n\t\tfmt.Fprintf(b, \" %8d /%s/\\n\", i, re)\n\t}\n\tfmt.Fprintln(b, \"Strings\")\n\tfor i, str := range v.str {\n\t\tfmt.Fprintf(b, \" %8d \\\"%s\\\"\\n\", i, str)\n\t}\n\tw := new(tabwriter.Writer)\n\tw.Init(b, 0, 0, 1, ' ', tabwriter.AlignRight)\n\n\tfmt.Fprintln(w, \"disasm\\tl\\top\\topnd\\tline\\t\")\n\tfor n, i := range v.prog {\n\t\tfmt.Fprintf(w, \"\\t%d\\t%s\\t%v\\t%d\\t\\n\", n, i.Opcode, i.Operand, i.SourceLine+1)\n\t}\n\tif err := w.Flush(); err != nil {\n\t\tglog.Infof(\"flush error: %s\", err)\n\t}\n\treturn b.String()\n}", "func_src_after": "func (v *VM) DumpByteCode() string {\n\tb := new(bytes.Buffer)\n\tfmt.Fprintf(b, \"Prog: %s\\n\", v.name)\n\tfmt.Fprintln(b, \"Metrics\")\n\tfor i, m := range v.m {\n\t\tif m.Program == v.name {\n\t\t\tfmt.Fprintf(b, \" %8d %s\\n\", i, m)\n\t\t}\n\t}\n\tfmt.Fprintln(b, \"Regexps\")\n\tfor i, re := range v.re {\n\t\tfmt.Fprintf(b, \" %8d /%s/\\n\", i, re)\n\t}\n\tfmt.Fprintln(b, \"Strings\")\n\tfor i, str := range v.str {\n\t\tfmt.Fprintf(b, \" %8d \\\"%s\\\"\\n\", i, str)\n\t}\n\tw := new(tabwriter.Writer)\n\tw.Init(b, 0, 0, 1, ' ', tabwriter.AlignRight)\n\n\tfmt.Fprintln(w, \"disasm\\tl\\top\\topnd\\tline\\t\")\n\tfor n, i := range v.prog {\n\t\tfmt.Fprintf(w, \"\\t%d\\t%s\\t%v\\t%d\\t\\n\", n, i.Opcode, i.Operand, i.SourceLine+1)\n\t}\n\tif err := w.Flush(); err != nil {\n\t\tglog.Infof(\"flush error: %s\", err)\n\t}\n\treturn b.String()\n}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 48, "line": "func (v *VM) DumpByteCode(name string) string {\n"}, {"line_no": 3, "char_start": 72, "char_end": 108, "line": "\tfmt.Fprintf(b, \"Prog: %s\\n\", name)\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 37, "line": "func (v *VM) DumpByteCode() string {\n"}, {"line_no": 3, "char_start": 61, "char_end": 99, "line": "\tfmt.Fprintf(b, \"Prog: %s\\n\", v.name)\n"}]}, "char_changes": {"deleted": [{"char_start": 26, "char_end": 37, "chars": "name string"}], "added": [{"char_start": 91, "char_end": 93, "chars": "v."}]}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "vm.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that outputs a formatted string representation of a virtual machine's bytecode, including metrics, regexps, strings, and disassembly information."}
{"func_name": "self.find_siblings", "func_src_before": "  def self.find_siblings(hierarchy_id, parent_id)\n    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n                        order by string;\")\n          end", "func_src_after": "  def self.find_siblings(hierarchy_id, parent_id)\n    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n                        order by string;\")\n\n    else\n      return []\n    end\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 8, "char_start": 430, "char_end": 532, "line": "                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n"}, {"line_no": 10, "char_start": 575, "char_end": 588, "line": "          end\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n"}, {"line_no": 3, "char_start": 125, "char_end": 202, "line": "      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 9, "char_start": 507, "char_end": 619, "line": "                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n"}, {"line_no": 11, "char_start": 662, "char_end": 663, "line": "\n"}, {"line_no": 12, "char_start": 663, "char_end": 672, "line": "    else\n"}, {"line_no": 13, "char_start": 672, "char_end": 688, "line": "      return []\n"}, {"line_no": 14, "char_start": 688, "char_end": 696, "line": "    end\n"}, {"line_no": 15, "char_start": 696, "char_end": 701, "line": "  end\n"}]}, "char_changes": {"deleted": [{"char_start": 579, "char_end": 583, "chars": "    "}], "added": [{"char_start": 50, "char_end": 127, "chars": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n  "}, {"char_start": 564, "char_end": 569, "chars": ".to_i"}, {"char_start": 596, "char_end": 601, "chars": ".to_i"}, {"char_start": 662, "char_end": 663, "chars": "\n"}, {"char_start": 667, "char_end": 696, "chars": "else\n      return []\n    end\n"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby method to query a database for sibling entries based on a hierarchy ID and a parent ID."}
{"func_name": "self.load_config", "func_src_before": "    def self.load_config(country_code)\n      default_config = YAML.\n        load(File.read(File.dirname(__FILE__) + '/conversion_rules.yml'))\n      default_config[country_code]\n    end", "func_src_after": "    def self.load_config(country_code)\n      default_config = YAML.\n        load_file(File.join(File.dirname(__FILE__), 'conversion_rules.yml'))\n      default_config[country_code]\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 68, "char_end": 142, "line": "        load(File.read(File.dirname(__FILE__) + '/conversion_rules.yml'))\n"}], "added": [{"line_no": 3, "char_start": 68, "char_end": 145, "line": "        load_file(File.join(File.dirname(__FILE__), 'conversion_rules.yml'))\n"}]}, "char_changes": {"deleted": [{"char_start": 86, "char_end": 90, "chars": "read"}, {"char_start": 113, "char_end": 115, "chars": " +"}, {"char_start": 117, "char_end": 118, "chars": "/"}], "added": [{"char_start": 80, "char_end": 85, "chars": "_file"}, {"char_start": 91, "char_end": 95, "chars": "join"}, {"char_start": 118, "char_end": 119, "chars": ","}]}, "commit_link": "github.com/alphasights/iban-tools/commit/d4954482c31d51a9e9896665d7019b9067f12bf7", "file_name": "conversion.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.load_file instead of YAML.load(File.Read).\n\nUse File.join instead of concatenating bits of path.", "parent_commit": "f6591635f671dad99246b594ae4c47e068e69c00", "description": "Write a Ruby method to load a country-specific configuration from a YAML file using a country code."}
{"func_name": "r_pkcs7_parse_cms", "func_src_before": "RCMS *r_pkcs7_parse_cms (const ut8 *buffer, ut32 length) {\n\tRASN1Object *object;\n\tRCMS *container;\n\tif (!buffer || !length) {\n\t\treturn NULL;\n\t}\n\tcontainer = R_NEW0 (RCMS);\n\tif (!container) {\n\t\treturn NULL;\n\t}\n\tobject = r_asn1_create_object (buffer, length);\n\tif (!object || object->list.length != 2 || !object->list.objects[0] || object->list.objects[1]->list.length != 1) {\n\t\tr_asn1_free_object (object);\n\t\tfree (container);\n\t\treturn NULL;\n\t}\n\tcontainer->contentType = r_asn1_stringify_oid (object->list.objects[0]->sector, object->list.objects[0]->length);\n\tr_pkcs7_parse_signeddata (&container->signedData, object->list.objects[1]->list.objects[0]);\n\tr_asn1_free_object (object);\n\treturn container;\n}", "func_src_after": "RCMS *r_pkcs7_parse_cms (const ut8 *buffer, ut32 length) {\n\tRASN1Object *object;\n\tRCMS *container;\n\tif (!buffer || !length) {\n\t\treturn NULL;\n\t}\n\tcontainer = R_NEW0 (RCMS);\n\tif (!container) {\n\t\treturn NULL;\n\t}\n\tobject = r_asn1_create_object (buffer, length);\n\tif (!object || object->list.length != 2 || !object->list.objects ||\n\t\t!object->list.objects[0] || !object->list.objects[1] ||\n\t\tobject->list.objects[1]->list.length != 1) {\n\t\tr_asn1_free_object (object);\n\t\tfree (container);\n\t\treturn NULL;\n\t}\n\tcontainer->contentType = r_asn1_stringify_oid (object->list.objects[0]->sector, object->list.objects[0]->length);\n\tr_pkcs7_parse_signeddata (&container->signedData, object->list.objects[1]->list.objects[0]);\n\tr_asn1_free_object (object);\n\treturn container;\n}", "commit_link": "github.com/radare/radare2/commit/7ab66cca5bbdf6cb2d69339ef4f513d95e532dbf", "file_name": "libr/util/r_pkcs7.c", "vul_type": "cwe-476", "description": "Write a function in C that parses a CMS (Cryptographic Message Syntax) structure from a given buffer and length, returning a pointer to the parsed CMS object or NULL on failure."}
{"func_name": "new_category", "func_src_before": "def new_category(category_name):\n    try:\n        conn = check_heroku_db()\n        cur = conn.cursor()\n        cur.execute('''INSERT INTO categories (cat_name) VALUES (%s)''', (category_name,))\n        conn.commit()\n        conn.close()\n\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)", "func_src_after": "def new_category(category_name):\n    try:\n        conn = check_heroku_db()\n        cur = conn.cursor()\n\n        query = \"INSERT INTO categories (cat_name) VALUES (%s);\"\n        data = (category_name,)\n        cur.execute(query, data)\n\n        conn.commit()\n        conn.close()\n\n    except psycopg2.DatabaseError as e:\n        print('Error %s' % e)\n        sys.exit(1)", "commit_link": "github.com/leeorb321/expenses/commit/f93c0fa4d30787ef16420bfefc52565b98bc7fcf", "file_name": "db.py", "vul_type": "cwe-089", "description": "Write a Python function named `new_category` that inserts a new category name into a Heroku PostgreSQL database table called `categories`, handling any database errors gracefully."}
{"func_name": "string_scan_range", "func_src_before": "static int string_scan_range(RList *list, const ut8 *buf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type) {\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (!buf || !min) {\n\t\treturn -1;\n\t}\n\twhile (needle < to) {\n\t\trc = r_utf8_decode (buf + needle, to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + needle + rc;\n\t\t\tif ((to - needle) > 4) {\n\t\t\t\tbool is_wide32 = needle + rc + 2 < to && !w[0] && !w[1] && !w[2] && w[3] && !w[4];\n\t\t\t\tif (is_wide32) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32;\n\t\t\t\t} else {\n\t\t\t\t\tbool is_wide = needle + rc + 2 < to && !w[0] && w[1] && !w[2];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t}\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\n\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (rc = i = 0; i < sizeof (tmp) - 3 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r)) {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (&tmp[i], r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\e\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 28) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes >= min) {\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\t// reduce false positives\n\t\t\t\tint j;\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (list) {\n\t\t\t\tRBinString *new = R_NEW0 (RBinString);\n\t\t\t\tif (!new) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnew->type = str_type;\n\t\t\t\tnew->length = runes;\n\t\t\t\tnew->size = needle - str_start;\n\t\t\t\tnew->ordinal = count++;\n\t\t\t\t// TODO: move into adjust_offset\n\t\t\t\tswitch (str_type) {\n\t\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\t\t{\n\t\t\t\t\t\tconst ut8 *p = buf  + str_start - 2;\n\t\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\t\t{\n\t\t\t\t\t\tconst ut8 *p = buf  + str_start - 4;\n\t\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnew->paddr = new->vaddr = str_start;\n\t\t\t\tnew->string = r_str_ndup ((const char *)tmp, i);\n\t\t\t\tr_list_append (list, new);\n\t\t\t} else {\n\t\t\t\t// DUMP TO STDOUT. raw dumping for rabin2 -zzz\n\t\t\t\tprintf (\"0x%08\" PFMT64x \" %s\\n\", str_start, tmp);\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}", "func_src_after": "static int string_scan_range(RList *list, const ut8 *buf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type) {\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (!buf || !min) {\n\t\treturn -1;\n\t}\n\twhile (needle < to) {\n\t\trc = r_utf8_decode (buf + needle, to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + needle + rc;\n\t\t\tif ((to - needle) > 4) {\n\t\t\t\tbool is_wide32 = needle + rc + 2 < to && !w[0] && !w[1] && !w[2] && w[3] && !w[4];\n\t\t\t\tif (is_wide32) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32;\n\t\t\t\t} else {\n\t\t\t\t\tbool is_wide = needle + rc + 2 < to && !w[0] && w[1] && !w[2];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t}\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\n\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (rc = i = 0; i < sizeof (tmp) - 3 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + needle, to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r)) {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (&tmp[i], r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\e\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 28) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes >= min) {\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\t// reduce false positives\n\t\t\t\tint j;\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (list) {\n\t\t\t\tRBinString *new = R_NEW0 (RBinString);\n\t\t\t\tif (!new) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnew->type = str_type;\n\t\t\t\tnew->length = runes;\n\t\t\t\tnew->size = needle - str_start;\n\t\t\t\tnew->ordinal = count++;\n\t\t\t\t// TODO: move into adjust_offset\n\t\t\t\tswitch (str_type) {\n\t\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\t\tif (str_start > 1) {\n\t\t\t\t\t\tconst ut8 *p = buf + str_start - 2;\n\t\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\t\tif (str_start > 3) {\n\t\t\t\t\t\tconst ut8 *p = buf + str_start - 4;\n\t\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnew->paddr = new->vaddr = str_start;\n\t\t\t\tnew->string = r_str_ndup ((const char *)tmp, i);\n\t\t\t\tr_list_append (list, new);\n\t\t\t} else {\n\t\t\t\t// DUMP TO STDOUT. raw dumping for rabin2 -zzz\n\t\t\t\tprintf (\"0x%08\" PFMT64x \" %s\\n\", str_start, tmp);\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}", "commit_link": "github.com/radare/radare2/commit/d31c4d3cbdbe01ea3ded16a584de94149ecd31d9", "file_name": "libr/bin/bin.c", "vul_type": "cwe-125", "description": "Write a C function to scan for strings within a specified range in a buffer, detecting encoding and adding them to a list if they meet a minimum length."}
{"func_name": "ParseRouteDistinguisher", "func_src_before": "func ParseRouteDistinguisher(rd string) (RouteDistinguisherInterface, error) {\n\telems, err := parseRdAndRt(rd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tassigned, _ := strconv.Atoi(elems[10])\n\tip := net.ParseIP(elems[1])\n\tswitch {\n\tcase ip.To4() != nil:\n\t\treturn NewRouteDistinguisherIPAddressAS(elems[1], uint16(assigned)), nil\n\tcase elems[6] == \"\" && elems[7] == \"\":\n\t\tasn, _ := strconv.Atoi(elems[8])\n\t\treturn NewRouteDistinguisherTwoOctetAS(uint16(asn), uint32(assigned)), nil\n\tdefault:\n\t\tfst, _ := strconv.Atoi(elems[7])\n\t\tsnd, _ := strconv.Atoi(elems[8])\n\t\tasn := fst<<16 | snd\n\t\treturn NewRouteDistinguisherFourOctetAS(uint32(asn), uint16(assigned)), nil\n\t}\n}", "func_src_after": "func ParseRouteDistinguisher(rd string) (RouteDistinguisherInterface, error) {\n\telems, err := parseRdAndRt(rd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tassigned, _ := strconv.ParseUint(elems[10], 10, 32)\n\tip := net.ParseIP(elems[1])\n\tswitch {\n\tcase ip.To4() != nil:\n\t\treturn NewRouteDistinguisherIPAddressAS(elems[1], uint16(assigned)), nil\n\tcase elems[6] == \"\" && elems[7] == \"\":\n\t\tasn, _ := strconv.ParseUint(elems[8], 10, 16)\n\t\treturn NewRouteDistinguisherTwoOctetAS(uint16(asn), uint32(assigned)), nil\n\tdefault:\n\t\tfst, _ := strconv.ParseUint(elems[7], 10, 16)\n\t\tsnd, _ := strconv.ParseUint(elems[8], 10, 16)\n\t\tasn := fst<<16 | snd\n\t\treturn NewRouteDistinguisherFourOctetAS(uint32(asn), uint16(assigned)), nil\n\t}\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 149, "char_end": 189, "line": "\tassigned, _ := strconv.Atoi(elems[10])\n"}, {"line_no": 12, "char_start": 366, "char_end": 401, "line": "\t\tasn, _ := strconv.Atoi(elems[8])\n"}, {"line_no": 15, "char_start": 488, "char_end": 523, "line": "\t\tfst, _ := strconv.Atoi(elems[7])\n"}, {"line_no": 16, "char_start": 523, "char_end": 558, "line": "\t\tsnd, _ := strconv.Atoi(elems[8])\n"}], "added": [{"line_no": 6, "char_start": 149, "char_end": 202, "line": "\tassigned, _ := strconv.ParseUint(elems[10], 10, 32)\n"}, {"line_no": 12, "char_start": 379, "char_end": 427, "line": "\t\tasn, _ := strconv.ParseUint(elems[8], 10, 16)\n"}, {"line_no": 15, "char_start": 514, "char_end": 562, "line": "\t\tfst, _ := strconv.ParseUint(elems[7], 10, 16)\n"}, {"line_no": 16, "char_start": 562, "char_end": 610, "line": "\t\tsnd, _ := strconv.ParseUint(elems[8], 10, 16)\n"}]}, "char_changes": {"deleted": [{"char_start": 173, "char_end": 177, "chars": "Atoi"}, {"char_start": 386, "char_end": 390, "chars": "Atoi"}, {"char_start": 508, "char_end": 512, "chars": "Atoi"}, {"char_start": 543, "char_end": 547, "chars": "Atoi"}], "added": [{"char_start": 173, "char_end": 182, "chars": "ParseUint"}, {"char_start": 192, "char_end": 200, "chars": ", 10, 32"}, {"char_start": 399, "char_end": 408, "chars": "ParseUint"}, {"char_start": 417, "char_end": 425, "chars": ", 10, 16"}, {"char_start": 534, "char_end": 543, "chars": "ParseUint"}, {"char_start": 552, "char_end": 560, "chars": ", 10, 16"}, {"char_start": 582, "char_end": 591, "chars": "ParseUint"}, {"char_start": 600, "char_end": 608, "chars": ", 10, 16"}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse a string into a Route Distinguisher object, handling different formats based on IP or ASN components."}
{"func_name": "oidc_handle_session_management_iframe_rp", "func_src_before": "static int oidc_handle_session_management_iframe_rp(request_rec *r, oidc_cfg *c,\n\t\toidc_session_t *session, const char *client_id,\n\t\tconst char *check_session_iframe) {\n\n\toidc_debug(r, \"enter\");\n\n\tconst char *java_script =\n\t\t\t\"    <script type=\\\"text/javascript\\\">\\n\"\n\t\t\t\"      var targetOrigin  = '%s';\\n\"\n\t\t\t\"      var message = '%s' + ' ' + '%s';\\n\"\n\t\t\t\"\t   var timerID;\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function checkSession() {\\n\"\n\t\t\t\"        console.debug('checkSession: posting ' + message + ' to ' + targetOrigin);\\n\"\n\t\t\t\"        var win = window.parent.document.getElementById('%s').contentWindow;\\n\"\n\t\t\t\"        win.postMessage( message, targetOrigin);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function setTimer() {\\n\"\n\t\t\t\"        checkSession();\\n\"\n\t\t\t\"        timerID = setInterval('checkSession()', %s);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function receiveMessage(e) {\\n\"\n\t\t\t\"        console.debug('receiveMessage: ' + e.data + ' from ' + e.origin);\\n\"\n\t\t\t\"        if (e.origin !== targetOrigin ) {\\n\"\n\t\t\t\"          console.debug('receiveMessage: cross-site scripting attack?');\\n\"\n\t\t\t\"          return;\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"        if (e.data != 'unchanged') {\\n\"\n\t\t\t\"          clearInterval(timerID);\\n\"\n\t\t\t\"          if (e.data == 'changed') {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=check';\\n\"\n\t\t\t\"          } else {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=logout';\\n\"\n\t\t\t\"          }\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      window.addEventListener('message', receiveMessage, false);\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"    </script>\\n\";\n\n\t/* determine the origin for the check_session_iframe endpoint */\n\tchar *origin = apr_pstrdup(r->pool, check_session_iframe);\n\tapr_uri_t uri;\n\tapr_uri_parse(r->pool, check_session_iframe, &uri);\n\tchar *p = strstr(origin, uri.path);\n\t*p = '\\0';\n\n\t/* the element identifier for the OP iframe */\n\tconst char *op_iframe_id = \"openidc-op\";\n\n\t/* restore the OP session_state from the session */\n\tconst char *session_state = oidc_session_get_session_state(r, session);\n\tif (session_state == NULL) {\n\t\toidc_warn(r,\n\t\t\t\t\"no session_state found in the session; the OP does probably not support session management!?\");\n\t\treturn DONE;\n\t}\n\n\tchar *s_poll_interval = NULL;\n\toidc_util_get_request_parameter(r, \"poll\", &s_poll_interval);\n\tif (s_poll_interval == NULL)\n\t\ts_poll_interval = \"3000\";\n\n\tconst char *redirect_uri = oidc_get_redirect_uri(r, c);\n\tjava_script = apr_psprintf(r->pool, java_script, origin, client_id,\n\t\t\tsession_state, op_iframe_id, s_poll_interval, redirect_uri,\n\t\t\tredirect_uri);\n\n\treturn oidc_util_html_send(r, NULL, java_script, \"setTimer\", NULL, DONE);\n}", "func_src_after": "static int oidc_handle_session_management_iframe_rp(request_rec *r, oidc_cfg *c,\n\t\toidc_session_t *session, const char *client_id,\n\t\tconst char *check_session_iframe) {\n\n\toidc_debug(r, \"enter\");\n\n\tconst char *java_script =\n\t\t\t\"    <script type=\\\"text/javascript\\\">\\n\"\n\t\t\t\"      var targetOrigin  = '%s';\\n\"\n\t\t\t\"      var message = '%s' + ' ' + '%s';\\n\"\n\t\t\t\"\t   var timerID;\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function checkSession() {\\n\"\n\t\t\t\"        console.debug('checkSession: posting ' + message + ' to ' + targetOrigin);\\n\"\n\t\t\t\"        var win = window.parent.document.getElementById('%s').contentWindow;\\n\"\n\t\t\t\"        win.postMessage( message, targetOrigin);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function setTimer() {\\n\"\n\t\t\t\"        checkSession();\\n\"\n\t\t\t\"        timerID = setInterval('checkSession()', %d);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function receiveMessage(e) {\\n\"\n\t\t\t\"        console.debug('receiveMessage: ' + e.data + ' from ' + e.origin);\\n\"\n\t\t\t\"        if (e.origin !== targetOrigin ) {\\n\"\n\t\t\t\"          console.debug('receiveMessage: cross-site scripting attack?');\\n\"\n\t\t\t\"          return;\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"        if (e.data != 'unchanged') {\\n\"\n\t\t\t\"          clearInterval(timerID);\\n\"\n\t\t\t\"          if (e.data == 'changed') {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=check';\\n\"\n\t\t\t\"          } else {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=logout';\\n\"\n\t\t\t\"          }\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      window.addEventListener('message', receiveMessage, false);\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"    </script>\\n\";\n\n\t/* determine the origin for the check_session_iframe endpoint */\n\tchar *origin = apr_pstrdup(r->pool, check_session_iframe);\n\tapr_uri_t uri;\n\tapr_uri_parse(r->pool, check_session_iframe, &uri);\n\tchar *p = strstr(origin, uri.path);\n\t*p = '\\0';\n\n\t/* the element identifier for the OP iframe */\n\tconst char *op_iframe_id = \"openidc-op\";\n\n\t/* restore the OP session_state from the session */\n\tconst char *session_state = oidc_session_get_session_state(r, session);\n\tif (session_state == NULL) {\n\t\toidc_warn(r,\n\t\t\t\t\"no session_state found in the session; the OP does probably not support session management!?\");\n\t\treturn DONE;\n\t}\n\n\tchar *s_poll_interval = NULL;\n\toidc_util_get_request_parameter(r, \"poll\", &s_poll_interval);\n\tint poll_interval = s_poll_interval ? strtol(s_poll_interval, NULL, 10) : 0;\n\tif ((poll_interval <= 0) || (poll_interval > 3600 * 24))\n\t\tpoll_interval = 3000;\n\n\tconst char *redirect_uri = oidc_get_redirect_uri(r, c);\n\tjava_script = apr_psprintf(r->pool, java_script, origin, client_id,\n\t\t\tsession_state, op_iframe_id, poll_interval, redirect_uri,\n\t\t\tredirect_uri);\n\n\treturn oidc_util_html_send(r, NULL, java_script, \"setTimer\", NULL, DONE);\n}", "commit_link": "github.com/zmartzone/mod_auth_openidc/commit/132a4111bf3791e76437619a66336dce2ce4c79b", "file_name": "src/mod_auth_openidc.c", "vul_type": "cwe-079", "description": "In C, write a function to handle session management using an iframe for OpenID Connect, including JavaScript for client-side checks and redirection based on session state changes."}
{"func_name": "login", "func_src_before": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                               password=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Not activated\" % username)\n    except NoResultFound:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username or Password wrong\" % username)\n    return None", "func_src_after": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n                if passwords_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n                log.info(\"Login failed for user '%s'. \"\n                         \"Reason: Not activated\" % username)\n        else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username not known\" % username)\n    return None", "line_changes": {"deleted": [{"line_no": 10, "char_start": 279, "char_end": 306, "line": "    md5_pw = hashlib.md5()\n"}, {"line_no": 11, "char_start": 306, "char_end": 340, "line": "    md5_pw.update(password or \"\")\n"}, {"line_no": 12, "char_start": 340, "char_end": 372, "line": "    md5_pw = md5_pw.hexdigest()\n"}, {"line_no": 14, "char_start": 441, "char_end": 450, "line": "    try:\n"}, {"line_no": 15, "char_start": 450, "char_end": 513, "line": "        user = DBSession.query(User).filter_by(login=username,\n"}, {"line_no": 16, "char_start": 513, "char_end": 583, "line": "                                               password=md5_pw).one()\n"}, {"line_no": 17, "char_start": 583, "char_end": 610, "line": "        if user.activated:\n"}, {"line_no": 18, "char_start": 610, "char_end": 670, "line": "            log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 19, "char_start": 670, "char_end": 694, "line": "            return user\n"}, {"line_no": 20, "char_start": 694, "char_end": 742, "line": "        log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 21, "char_start": 742, "char_end": 795, "line": "                 \"Reason: Not activated\" % username)\n"}, {"line_no": 22, "char_start": 795, "char_end": 821, "line": "    except NoResultFound:\n"}, {"line_no": 24, "char_start": 869, "char_end": 935, "line": "                 \"Reason: Username or Password wrong\" % username)\n"}], "added": [{"line_no": 11, "char_start": 348, "char_end": 379, "line": "    user = load_user(username)\n"}, {"line_no": 12, "char_start": 379, "char_end": 392, "line": "    if user:\n"}, {"line_no": 13, "char_start": 392, "char_end": 445, "line": "        if verify_password(password, user.password):\n"}, {"line_no": 14, "char_start": 445, "char_end": 476, "line": "            if user.activated:\n"}, {"line_no": 15, "char_start": 476, "char_end": 540, "line": "                log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 16, "char_start": 540, "char_end": 598, "line": "                if passwords_needs_update(user.password):\n"}, {"line_no": 17, "char_start": 598, "char_end": 675, "line": "                    log.info(\"Updating password for user '%s'\" % (username))\n"}, {"line_no": 18, "char_start": 675, "char_end": 739, "line": "                    user.password = encrypt_password(password) \n"}, {"line_no": 19, "char_start": 739, "char_end": 767, "line": "                return user\n"}, {"line_no": 20, "char_start": 767, "char_end": 785, "line": "            else:\n"}, {"line_no": 21, "char_start": 785, "char_end": 841, "line": "                log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 22, "char_start": 841, "char_end": 902, "line": "                         \"Reason: Not activated\" % username)\n"}, {"line_no": 23, "char_start": 902, "char_end": 916, "line": "        else:\n"}, {"line_no": 24, "char_start": 916, "char_end": 968, "line": "            log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 25, "char_start": 968, "char_end": 1026, "line": "                     \"Reason: Wrong password\" % username)\n"}, {"line_no": 26, "char_start": 1026, "char_end": 1036, "line": "    else:\n"}, {"line_no": 28, "char_start": 1084, "char_end": 1142, "line": "                 \"Reason: Username not known\" % username)\n"}]}, "char_changes": {"deleted": [{"char_start": 283, "char_end": 545, "chars": "md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                "}, {"char_start": 568, "char_end": 694, "chars": "=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n"}, {"char_start": 799, "char_end": 819, "chars": "except NoResultFound"}, {"char_start": 904, "char_end": 921, "chars": "or Password wrong"}], "added": [{"char_start": 283, "char_end": 540, "chars": "log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n"}, {"char_start": 555, "char_end": 559, "chars": " if "}, {"char_start": 567, "char_end": 793, "chars": "s_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n        "}, {"char_start": 858, "char_end": 863, "chars": "     "}, {"char_start": 863, "char_end": 866, "chars": "   "}, {"char_start": 906, "char_end": 1034, "chars": "    else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else"}, {"char_start": 1119, "char_end": 1128, "chars": "not known"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/8e92641fee542f6e7004e827136dea3ce5e99eb2", "file_name": "security.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new\nencryption methods using passlib. Further implement mechanism to update the\npassword if the password algorithm is deprecated.", "parent_commit": "8cfe035de8fc493923385093cc7f5c5455fb08f9", "description": "Write a Python function named `login` that checks a user's credentials and returns the user object if authenticated or `None` otherwise."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "gps_tracker", "func_src_before": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "func_src_after": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - pos - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "commit_link": "github.com/aircrack-ng/aircrack-ng/commit/ff70494dd389ba570dbdbf36f217c28d4381c6b5/", "file_name": "src/airodump-ng.c", "vul_type": "cwe-787", "description": "Write a C function named `gps_tracker` that connects to a GPS daemon on localhost and reads GPS coordinates in a loop."}
{"func_name": "_feed_input_sorters", "func_src_before": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with open(self.working_set_filename, 'rb') as work_file:\n            for line in work_file:\n                result = pickle.loads(base64.b64decode(line))\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "func_src_after": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with gzip.open(self.working_set_filename, 'rb') as work_file:\n            while True:\n                result = pickle.load(work_file)\n\n                if result == 'eof':\n                    break\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 60, "char_end": 125, "line": "        with open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 125, "char_end": 160, "line": "            for line in work_file:\n"}, {"line_no": 6, "char_start": 160, "char_end": 222, "line": "                result = pickle.loads(base64.b64decode(line))\n"}], "added": [{"line_no": 4, "char_start": 60, "char_end": 130, "line": "        with gzip.open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 130, "char_end": 154, "line": "            while True:\n"}, {"line_no": 6, "char_start": 154, "char_end": 202, "line": "                result = pickle.load(work_file)\n"}, {"line_no": 7, "char_start": 202, "char_end": 203, "line": "\n"}, {"line_no": 8, "char_start": 203, "char_end": 239, "line": "                if result == 'eof':\n"}, {"line_no": 9, "char_start": 239, "char_end": 265, "line": "                    break\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 157, "chars": "for line in work_fil"}, {"char_start": 196, "char_end": 221, "chars": "s(base64.b64decode(line))"}], "added": [{"char_start": 73, "char_end": 78, "chars": "gzip."}, {"char_start": 142, "char_end": 151, "chars": "while Tru"}, {"char_start": 190, "char_end": 264, "chars": "(work_file)\n\n                if result == 'eof':\n                    break"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to process and sort project results from a file, updating a sorter object for each project."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec('node ' + binPath + ' -v -', function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile('node', [binPath, '-v', '-'], function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 69, "line": "\t\texec('node ' + binPath + ' -v -', function (err, stdout, stderr) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 74, "line": "\t\texecFile('node', [binPath, '-v', '-'], function (err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 12, "char_end": 17, "chars": " ' + "}, {"char_start": 24, "char_end": 26, "chars": " +"}, {"char_start": 28, "char_end": 29, "chars": " "}, {"char_start": 31, "char_end": 32, "chars": " "}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 16, "char_end": 20, "chars": "', ["}, {"char_start": 27, "char_end": 28, "chars": ","}, {"char_start": 32, "char_end": 36, "chars": "', '"}, {"char_start": 38, "char_end": 39, "chars": "]"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a JavaScript function that executes a command to check for 'OptiPNG' in the error output and calls a callback function."}
{"func_name": "bin_symbols", "func_src_before": "static int bin_symbols(RCore *r, int mode, ut64 laddr, int va, ut64 at, const char *name, bool exponly, const char *args) {\n\tRBinInfo *info = r_bin_get_info (r->bin);\n\tRList *entries = r_bin_get_entries (r->bin);\n\tRBinSymbol *symbol;\n\tRBinAddr *entry;\n\tRListIter *iter;\n\tbool firstexp = true;\n\tbool printHere = false;\n\tint i = 0, lastfs = 's';\n\tbool bin_demangle = r_config_get_i (r->config, \"bin.demangle\");\n\tif (!info) {\n\t\treturn 0;\n\t}\n\n\tif (args && *args == '.') {\n\t\tprintHere = true;\n\t}\n\n\tbool is_arm = info && info->arch && !strncmp (info->arch, \"arm\", 3);\n\tconst char *lang = bin_demangle ? r_config_get (r->config, \"bin.lang\") : NULL;\n\n\tRList *symbols = r_bin_get_symbols (r->bin);\n\tr_spaces_push (&r->anal->meta_spaces, \"bin\");\n\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"[\");\n\t} else if (IS_MODE_SET (mode)) {\n\t\tr_flag_space_set (r->flags, R_FLAGS_FS_SYMBOLS);\n\t} else if (!at && exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs exports\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Exports]\\n\");\n\t\t}\n\t} else if (!at && !exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs symbols\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Symbols]\\n\");\n\t\t}\n\t}\n\tif (IS_MODE_NORMAL (mode)) {\n\t\tr_cons_printf (\"Num Paddr      Vaddr      Bind     Type Size Name\\n\");\n\t}\n\n\n\tsize_t count = 0;\n\tr_list_foreach (symbols, iter, symbol) {\n\t\tif (!symbol->name) {\n\t\t\tcontinue;\n\t\t}\n\t\tchar *r_symbol_name = r_str_escape_utf8 (symbol->name, false, true);\n\t\tut64 addr = compute_addr (r->bin, symbol->paddr, symbol->vaddr, va);\n\t\tint len = symbol->size ? symbol->size : 32;\n\t\tSymName sn = {0};\n\n\t\tif (exponly && !isAnExport (symbol)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (name && strcmp (r_symbol_name, name)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (at && (!symbol->size || !is_in_range (at, addr, symbol->size))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif ((printHere && !is_in_range (r->offset, symbol->paddr, len))\n\t\t\t\t&& (printHere && !is_in_range (r->offset, addr, len))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tcount ++;\n\t\tsnInit (r, &sn, symbol, lang);\n\n\t\tif (IS_MODE_SET (mode) && (is_section_symbol (symbol) || is_file_symbol (symbol))) {\n\t\t\t/*\n\t\t\t * Skip section symbols because they will have their own flag.\n\t\t\t * Skip also file symbols because not useful for now.\n\t\t\t */\n\t\t} else if (IS_MODE_SET (mode) && is_special_symbol (symbol)) {\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_special_symbol (r, symbol, va);\n\t\t\t}\n\t\t} else if (IS_MODE_SET (mode)) {\n\t\t\t// TODO: provide separate API in RBinPlugin to let plugins handle anal hints/metadata\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_symbol (r, symbol, info, va);\n\t\t\t}\n\t\t\tselect_flag_space (r, symbol);\n\t\t\t/* If that's a Classed symbol (method or so) */\n\t\t\tif (sn.classname) {\n\t\t\t\tRFlagItem *fi = r_flag_get (r->flags, sn.methflag);\n\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\tchar *prname = r_str_newf (\"%s.%s\", r->bin->prefix, sn.methflag);\n\t\t\t\t\tr_name_filter (sn.methflag, -1);\n\t\t\t\t\tfree (sn.methflag);\n\t\t\t\t\tsn.methflag = prname;\n\t\t\t\t}\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, sn.methname);\n\t\t\t\t\tif ((fi->offset - r->flags->base) == addr) {\n\t\t\t\t//\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\t\tr_flag_unset (r->flags, fi);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfi = r_flag_set (r->flags, sn.methflag, addr, symbol->size);\n\t\t\t\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\tif (comment) {\n\t\t\t\t\t\tr_flag_item_set_comment (fi, comment);\n\t\t\t\t\t\tR_FREE (comment);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *n = sn.demname ? sn.demname : sn.name;\n\t\t\t\tconst char *fn = sn.demflag ? sn.demflag : sn.nameflag;\n\t\t\t\tchar *fnp = (r->bin->prefix) ?\n\t\t\t\t\tr_str_newf (\"%s.%s\", r->bin->prefix, fn):\n\t\t\t\t\tstrdup (fn);\n\t\t\t\tRFlagItem *fi = r_flag_set (r->flags, fnp, addr, symbol->size);\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, n);\n\t\t\t\t\tfi->demangled = (bool)(size_t)sn.demname;\n\t\t\t\t} else {\n\t\t\t\t\tif (fn) {\n\t\t\t\t\t\teprintf (\"[Warning] Can't find flag (%s)\\n\", fn);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (fnp);\n\t\t\t}\n\t\t\tif (sn.demname) {\n\t\t\t\tr_meta_add (r->anal, R_META_TYPE_COMMENT,\n\t\t\t\t\taddr, symbol->size, sn.demname);\n\t\t\t}\n\t\t\tr_flag_space_pop (r->flags);\n\t\t} else if (IS_MODE_JSON (mode)) {\n\t\t\tchar *str = r_str_escape_utf8_for_json (r_symbol_name, -1);\n\t\t\t// str = r_str_replace (str, \"\\\"\", \"\\\\\\\"\", 1);\n\t\t\tr_cons_printf (\"%s{\\\"name\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"demname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"flagname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"ordinal\\\":%d,\"\n\t\t\t\t\"\\\"bind\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"size\\\":%d,\"\n\t\t\t\t\"\\\"type\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"vaddr\\\":%\"PFMT64d\",\"\n\t\t\t\t\"\\\"paddr\\\":%\"PFMT64d\"}\",\n\t\t\t\t((exponly && firstexp) || printHere) ? \"\" : (iter->p ? \",\" : \"\"),\n\t\t\t\tstr,\n\t\t\t\tsn.demname? sn.demname: \"\",\n\t\t\t\tsn.nameflag,\n\t\t\t\tsymbol->ordinal,\n\t\t\t\tsymbol->bind,\n\t\t\t\t(int)symbol->size,\n\t\t\t\tsymbol->type,\n\t\t\t\t(ut64)addr, (ut64)symbol->paddr);\n\t\t\tfree (str);\n\t\t} else if (IS_MODE_SIMPLE (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"0x%08\"PFMT64x\" %d %s\\n\",\n\t\t\t\taddr, (int)symbol->size, name);\n\t\t} else if (IS_MODE_SIMPLEST (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"%s\\n\", name);\n\t\t} else if (IS_MODE_RAD (mode)) {\n\t\t\t/* Skip special symbols because we do not flag them and\n\t\t\t * they shouldn't be printed in the rad format either */\n\t\t\tif (is_special_symbol (symbol)) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tRBinFile *binfile;\n\t\t\tRBinPlugin *plugin;\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tif (!name) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tif (!strncmp (name, \"imp.\", 4)) {\n\t\t\t\tif (lastfs != 'i') {\n\t\t\t\t\tr_cons_printf (\"fs imports\\n\");\n\t\t\t\t}\n\t\t\t\tlastfs = 'i';\n\t\t\t} else {\n\t\t\t\tif (lastfs != 's') {\n\t\t\t\t\tconst char *fs = exponly? \"exports\": \"symbols\";\n\t\t\t\t\tr_cons_printf (\"fs %s\\n\", fs);\n\t\t\t\t}\n\t\t\t\tlastfs = 's';\n\t\t\t}\n\t\t\tif (r->bin->prefix || *name) { // we don't want unnamed symbol flags\n\t\t\t\tchar *flagname = construct_symbol_flagname (\"sym\", name, MAXFLAG_LEN_DEFAULT);\n\t\t\t\tif (!flagname) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"f %s%s%s %u 0x%08\" PFMT64x \"\\\"\\n\",\n\t\t\t\t\tr->bin->prefix ? r->bin->prefix : \"\", r->bin->prefix ? \".\" : \"\",\n\t\t\t\t\tflagname, symbol->size, addr);\n\t\t\t\tfree (flagname);\n\t\t\t}\n\t\t\tbinfile = r_bin_cur (r->bin);\n\t\t\tplugin = r_bin_file_cur_plugin (binfile);\n\t\t\tif (plugin && plugin->name) {\n\t\t\t\tif (r_str_startswith (plugin->name, \"pe\")) {\n\t\t\t\t\tchar *module = strdup (r_symbol_name);\n\t\t\t\t\tchar *p = strstr (module, \".dll_\");\n\t\t\t\t\tif (p && strstr (module, \"imp.\")) {\n\t\t\t\t\t\tchar *symname = __filterShell (p + 5);\n\t\t\t\t\t\tchar *m = __filterShell (module);\n\t\t\t\t\t\t*p = 0;\n\t\t\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\t\t\tr_cons_printf (\"k bin/pe/%s/%d=%s.%s\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, r->bin->prefix, symname);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tr_cons_printf (\"k bin/pe/%s/%d=%s\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, symname);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfree (symname);\n\t\t\t\t\t\tfree (m);\n\t\t\t\t\t}\n\t\t\t\t\tfree (module);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tconst char *bind = symbol->bind? symbol->bind: \"NONE\";\n\t\t\tconst char *type = symbol->type? symbol->type: \"NONE\";\n\t\t\tconst char *name = r_str_get (sn.demname? sn.demname: r_symbol_name);\n\t\t\t// const char *fwd = r_str_get (symbol->forwarder);\n\t\t\tr_cons_printf (\"%03u\", symbol->ordinal);\n\t\t\tif (symbol->paddr == UT64_MAX) {\n\t\t\t\tr_cons_printf (\" ----------\");\n\t\t\t} else {\n\t\t\t\tr_cons_printf (\" 0x%08\"PFMT64x, symbol->paddr);\n\t\t\t}\n\t\t\tr_cons_printf (\" 0x%08\"PFMT64x\" %6s %6s %4d%s%s\\n\",\n\t\t\t               addr, bind, type, symbol->size, *name? \" \": \"\", name);\n\t\t}\nnext:\n\t\tsnFini (&sn);\n\t\ti++;\n\t\tfree (r_symbol_name);\n\t\tif (exponly && firstexp) {\n\t\t\tfirstexp = false;\n\t\t}\n\t\tif (printHere) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (count == 0 && IS_MODE_JSON (mode)) {\n\t\tr_cons_printf (\"{}\");\n\t}\n\n\n\t//handle thumb and arm for entry point since they are not present in symbols\n\tif (is_arm) {\n\t\tr_list_foreach (entries, iter, entry) {\n\t\t\tif (IS_MODE_SET (mode)) {\n\t\t\t\thandle_arm_entry (r, entry, info, va);\n\t\t\t}\n\t\t}\n\t}\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"]\");\n\t}\n\n\tr_spaces_pop (&r->anal->meta_spaces);\n\treturn true;\n}", "func_src_after": "static int bin_symbols(RCore *r, int mode, ut64 laddr, int va, ut64 at, const char *name, bool exponly, const char *args) {\n\tRBinInfo *info = r_bin_get_info (r->bin);\n\tRList *entries = r_bin_get_entries (r->bin);\n\tRBinSymbol *symbol;\n\tRBinAddr *entry;\n\tRListIter *iter;\n\tbool firstexp = true;\n\tbool printHere = false;\n\tint i = 0, lastfs = 's';\n\tbool bin_demangle = r_config_get_i (r->config, \"bin.demangle\");\n\tif (!info) {\n\t\treturn 0;\n\t}\n\n\tif (args && *args == '.') {\n\t\tprintHere = true;\n\t}\n\n\tbool is_arm = info && info->arch && !strncmp (info->arch, \"arm\", 3);\n\tconst char *lang = bin_demangle ? r_config_get (r->config, \"bin.lang\") : NULL;\n\n\tRList *symbols = r_bin_get_symbols (r->bin);\n\tr_spaces_push (&r->anal->meta_spaces, \"bin\");\n\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"[\");\n\t} else if (IS_MODE_SET (mode)) {\n\t\tr_flag_space_set (r->flags, R_FLAGS_FS_SYMBOLS);\n\t} else if (!at && exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs exports\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Exports]\\n\");\n\t\t}\n\t} else if (!at && !exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs symbols\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Symbols]\\n\");\n\t\t}\n\t}\n\tif (IS_MODE_NORMAL (mode)) {\n\t\tr_cons_printf (\"Num Paddr      Vaddr      Bind     Type Size Name\\n\");\n\t}\n\n\n\tsize_t count = 0;\n\tr_list_foreach (symbols, iter, symbol) {\n\t\tif (!symbol->name) {\n\t\t\tcontinue;\n\t\t}\n\t\tchar *r_symbol_name = r_str_escape_utf8 (symbol->name, false, true);\n\t\tut64 addr = compute_addr (r->bin, symbol->paddr, symbol->vaddr, va);\n\t\tint len = symbol->size ? symbol->size : 32;\n\t\tSymName sn = {0};\n\n\t\tif (exponly && !isAnExport (symbol)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (name && strcmp (r_symbol_name, name)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (at && (!symbol->size || !is_in_range (at, addr, symbol->size))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif ((printHere && !is_in_range (r->offset, symbol->paddr, len))\n\t\t\t\t&& (printHere && !is_in_range (r->offset, addr, len))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tcount ++;\n\t\tsnInit (r, &sn, symbol, lang);\n\n\t\tif (IS_MODE_SET (mode) && (is_section_symbol (symbol) || is_file_symbol (symbol))) {\n\t\t\t/*\n\t\t\t * Skip section symbols because they will have their own flag.\n\t\t\t * Skip also file symbols because not useful for now.\n\t\t\t */\n\t\t} else if (IS_MODE_SET (mode) && is_special_symbol (symbol)) {\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_special_symbol (r, symbol, va);\n\t\t\t}\n\t\t} else if (IS_MODE_SET (mode)) {\n\t\t\t// TODO: provide separate API in RBinPlugin to let plugins handle anal hints/metadata\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_symbol (r, symbol, info, va);\n\t\t\t}\n\t\t\tselect_flag_space (r, symbol);\n\t\t\t/* If that's a Classed symbol (method or so) */\n\t\t\tif (sn.classname) {\n\t\t\t\tRFlagItem *fi = r_flag_get (r->flags, sn.methflag);\n\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\tchar *prname = r_str_newf (\"%s.%s\", r->bin->prefix, sn.methflag);\n\t\t\t\t\tr_name_filter (sn.methflag, -1);\n\t\t\t\t\tfree (sn.methflag);\n\t\t\t\t\tsn.methflag = prname;\n\t\t\t\t}\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, sn.methname);\n\t\t\t\t\tif ((fi->offset - r->flags->base) == addr) {\n\t\t\t\t//\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\t\tr_flag_unset (r->flags, fi);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfi = r_flag_set (r->flags, sn.methflag, addr, symbol->size);\n\t\t\t\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\tif (comment) {\n\t\t\t\t\t\tr_flag_item_set_comment (fi, comment);\n\t\t\t\t\t\tR_FREE (comment);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *n = sn.demname ? sn.demname : sn.name;\n\t\t\t\tconst char *fn = sn.demflag ? sn.demflag : sn.nameflag;\n\t\t\t\tchar *fnp = (r->bin->prefix) ?\n\t\t\t\t\tr_str_newf (\"%s.%s\", r->bin->prefix, fn):\n\t\t\t\t\tstrdup (fn);\n\t\t\t\tRFlagItem *fi = r_flag_set (r->flags, fnp, addr, symbol->size);\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, n);\n\t\t\t\t\tfi->demangled = (bool)(size_t)sn.demname;\n\t\t\t\t} else {\n\t\t\t\t\tif (fn) {\n\t\t\t\t\t\teprintf (\"[Warning] Can't find flag (%s)\\n\", fn);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (fnp);\n\t\t\t}\n\t\t\tif (sn.demname) {\n\t\t\t\tr_meta_add (r->anal, R_META_TYPE_COMMENT,\n\t\t\t\t\taddr, symbol->size, sn.demname);\n\t\t\t}\n\t\t\tr_flag_space_pop (r->flags);\n\t\t} else if (IS_MODE_JSON (mode)) {\n\t\t\tchar *str = r_str_escape_utf8_for_json (r_symbol_name, -1);\n\t\t\t// str = r_str_replace (str, \"\\\"\", \"\\\\\\\"\", 1);\n\t\t\tr_cons_printf (\"%s{\\\"name\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"demname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"flagname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"ordinal\\\":%d,\"\n\t\t\t\t\"\\\"bind\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"size\\\":%d,\"\n\t\t\t\t\"\\\"type\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"vaddr\\\":%\"PFMT64d\",\"\n\t\t\t\t\"\\\"paddr\\\":%\"PFMT64d\"}\",\n\t\t\t\t((exponly && firstexp) || printHere) ? \"\" : (iter->p ? \",\" : \"\"),\n\t\t\t\tstr,\n\t\t\t\tsn.demname? sn.demname: \"\",\n\t\t\t\tsn.nameflag,\n\t\t\t\tsymbol->ordinal,\n\t\t\t\tsymbol->bind,\n\t\t\t\t(int)symbol->size,\n\t\t\t\tsymbol->type,\n\t\t\t\t(ut64)addr, (ut64)symbol->paddr);\n\t\t\tfree (str);\n\t\t} else if (IS_MODE_SIMPLE (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"0x%08\"PFMT64x\" %d %s\\n\",\n\t\t\t\taddr, (int)symbol->size, name);\n\t\t} else if (IS_MODE_SIMPLEST (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"%s\\n\", name);\n\t\t} else if (IS_MODE_RAD (mode)) {\n\t\t\t/* Skip special symbols because we do not flag them and\n\t\t\t * they shouldn't be printed in the rad format either */\n\t\t\tif (is_special_symbol (symbol)) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tRBinFile *binfile;\n\t\t\tRBinPlugin *plugin;\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tif (!name) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tif (!strncmp (name, \"imp.\", 4)) {\n\t\t\t\tif (lastfs != 'i') {\n\t\t\t\t\tr_cons_printf (\"fs imports\\n\");\n\t\t\t\t}\n\t\t\t\tlastfs = 'i';\n\t\t\t} else {\n\t\t\t\tif (lastfs != 's') {\n\t\t\t\t\tconst char *fs = exponly? \"exports\": \"symbols\";\n\t\t\t\t\tr_cons_printf (\"fs %s\\n\", fs);\n\t\t\t\t}\n\t\t\t\tlastfs = 's';\n\t\t\t}\n\t\t\tif (r->bin->prefix || *name) { // we don't want unnamed symbol flags\n\t\t\t\tchar *flagname = construct_symbol_flagname (\"sym\", name, MAXFLAG_LEN_DEFAULT);\n\t\t\t\tif (!flagname) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"f %s%s%s %u 0x%08\" PFMT64x \"\\\"\\n\",\n\t\t\t\t\tr->bin->prefix ? r->bin->prefix : \"\", r->bin->prefix ? \".\" : \"\",\n\t\t\t\t\tflagname, symbol->size, addr);\n\t\t\t\tfree (flagname);\n\t\t\t}\n\t\t\tbinfile = r_bin_cur (r->bin);\n\t\t\tplugin = r_bin_file_cur_plugin (binfile);\n\t\t\tif (plugin && plugin->name) {\n\t\t\t\tif (r_str_startswith (plugin->name, \"pe\")) {\n\t\t\t\t\tchar *module = strdup (r_symbol_name);\n\t\t\t\t\tchar *p = strstr (module, \".dll_\");\n\t\t\t\t\tif (p && strstr (module, \"imp.\")) {\n\t\t\t\t\t\tchar *symname = __filterShell (p + 5);\n\t\t\t\t\t\tchar *m = __filterShell (module);\n\t\t\t\t\t\t*p = 0;\n\t\t\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\t\t\tr_cons_printf (\"\\\"k bin/pe/%s/%d=%s.%s\\\"\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, r->bin->prefix, symname);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tr_cons_printf (\"\\\"k bin/pe/%s/%d=%s\\\"\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, symname);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfree (symname);\n\t\t\t\t\t\tfree (m);\n\t\t\t\t\t}\n\t\t\t\t\tfree (module);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tconst char *bind = symbol->bind? symbol->bind: \"NONE\";\n\t\t\tconst char *type = symbol->type? symbol->type: \"NONE\";\n\t\t\tconst char *name = r_str_get (sn.demname? sn.demname: r_symbol_name);\n\t\t\t// const char *fwd = r_str_get (symbol->forwarder);\n\t\t\tr_cons_printf (\"%03u\", symbol->ordinal);\n\t\t\tif (symbol->paddr == UT64_MAX) {\n\t\t\t\tr_cons_printf (\" ----------\");\n\t\t\t} else {\n\t\t\t\tr_cons_printf (\" 0x%08\"PFMT64x, symbol->paddr);\n\t\t\t}\n\t\t\tr_cons_printf (\" 0x%08\"PFMT64x\" %6s %6s %4d%s%s\\n\",\n\t\t\t               addr, bind, type, symbol->size, *name? \" \": \"\", name);\n\t\t}\nnext:\n\t\tsnFini (&sn);\n\t\ti++;\n\t\tfree (r_symbol_name);\n\t\tif (exponly && firstexp) {\n\t\t\tfirstexp = false;\n\t\t}\n\t\tif (printHere) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (count == 0 && IS_MODE_JSON (mode)) {\n\t\tr_cons_printf (\"{}\");\n\t}\n\n\n\t//handle thumb and arm for entry point since they are not present in symbols\n\tif (is_arm) {\n\t\tr_list_foreach (entries, iter, entry) {\n\t\t\tif (IS_MODE_SET (mode)) {\n\t\t\t\thandle_arm_entry (r, entry, info, va);\n\t\t\t}\n\t\t}\n\t}\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"]\");\n\t}\n\n\tr_spaces_pop (&r->anal->meta_spaces);\n\treturn true;\n}", "commit_link": "github.com/radareorg/radare2/commit/5411543a310a470b1257fb93273cdd6e8dfcb3af", "file_name": "libr/core/cbin.c", "vul_type": "cwe-078", "description": "Write a C function to process and print binary symbols in various formats based on the given mode."}
{"func_name": "pascal_case", "func_src_before": "def pascal_case(value: str) -> str:\n    return stringcase.pascalcase(value)", "func_src_after": "def pascal_case(value: str) -> str:\n    return stringcase.pascalcase(_sanitize(value))", "commit_link": "github.com/openapi-generators/openapi-python-client/commit/3e7dfae5d0b3685abf1ede1bc6c086a116ac4746", "file_name": "openapi_python_client/utils.py", "vul_type": "cwe-022", "description": "Write a Python function named `pascal_case` that converts a string to PascalCase format, optionally sanitizing the input first."}
{"func_name": "read_yaml_file", "func_src_before": "        def read_yaml_file(filename)\n            # read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data)\n            return test\n        end", "func_src_after": "        def read_yaml_file(filename)\n            test = YAML::load_file(filename)\n            return test\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 79, "char_end": 101, "line": "            data = ''\n"}, {"line_no": 4, "char_start": 101, "char_end": 143, "line": "            f = File.open(filename, \"r\") \n"}, {"line_no": 5, "char_start": 143, "char_end": 177, "line": "            f.each_line do |line|\n"}, {"line_no": 6, "char_start": 177, "char_end": 206, "line": "                data += line\n"}, {"line_no": 7, "char_start": 206, "char_end": 222, "line": "            end\n"}, {"line_no": 9, "char_start": 258, "char_end": 294, "line": "            test = YAML::load(data)\n"}], "added": [{"line_no": 2, "char_start": 37, "char_end": 82, "line": "            test = YAML::load_file(filename)\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 292, "chars": "# read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data"}], "added": [{"char_start": 49, "char_end": 80, "chars": "test = YAML::load_file(filename"}]}, "commit_link": "github.com/tomnatt/check-status/commit/67c9f3e19f0230d56c23c2e0e7c43a2cb2c4d052", "file_name": "test_runner.rb", "vul_type": "cwe-502", "commit_msg": "improve YAML loading", "parent_commit": "576c7e6e5e12d0be641958888f5b83afe0712d7e", "description": "Write a Ruby function to read and parse a YAML file."}
{"func_name": "read_quant_matrix_ext", "func_src_before": "static void read_quant_matrix_ext(MpegEncContext *s, GetBitContext *gb)\n{\n    int i, j, v;\n\n    if (get_bits1(gb)) {\n        /* intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            v = get_bits(gb, 8);\n            j = s->idsp.idct_permutation[ff_zigzag_direct[i]];\n            s->intra_matrix[j]        = v;\n            s->chroma_intra_matrix[j] = v;\n        }\n    }\n\n    if (get_bits1(gb)) {\n        /* non_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            get_bits(gb, 8);\n        }\n    }\n\n    if (get_bits1(gb)) {\n        /* chroma_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            v = get_bits(gb, 8);\n            j = s->idsp.idct_permutation[ff_zigzag_direct[i]];\n            s->chroma_intra_matrix[j] = v;\n        }\n    }\n\n    if (get_bits1(gb)) {\n        /* chroma_non_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            get_bits(gb, 8);\n        }\n    }\n\n    next_start_code_studio(gb);\n}", "func_src_after": "static int read_quant_matrix_ext(MpegEncContext *s, GetBitContext *gb)\n{\n    int i, j, v;\n\n    if (get_bits1(gb)) {\n        if (get_bits_left(gb) < 64*8)\n            return AVERROR_INVALIDDATA;\n        /* intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            v = get_bits(gb, 8);\n            j = s->idsp.idct_permutation[ff_zigzag_direct[i]];\n            s->intra_matrix[j]        = v;\n            s->chroma_intra_matrix[j] = v;\n        }\n    }\n\n    if (get_bits1(gb)) {\n        if (get_bits_left(gb) < 64*8)\n            return AVERROR_INVALIDDATA;\n        /* non_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            get_bits(gb, 8);\n        }\n    }\n\n    if (get_bits1(gb)) {\n        if (get_bits_left(gb) < 64*8)\n            return AVERROR_INVALIDDATA;\n        /* chroma_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            v = get_bits(gb, 8);\n            j = s->idsp.idct_permutation[ff_zigzag_direct[i]];\n            s->chroma_intra_matrix[j] = v;\n        }\n    }\n\n    if (get_bits1(gb)) {\n        if (get_bits_left(gb) < 64*8)\n            return AVERROR_INVALIDDATA;\n        /* chroma_non_intra_quantiser_matrix */\n        for (i = 0; i < 64; i++) {\n            get_bits(gb, 8);\n        }\n    }\n\n    next_start_code_studio(gb);\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/5aba5b89d0b1d73164d3b81764828bb8b20ff32a", "file_name": "libavcodec/mpeg4videodec.c", "vul_type": "cwe-125", "description": "Write a C function to read and optionally update quantization matrices from a bitstream in an MPEG encoding context."}
{"func_name": "pdf_add_bookmark", "func_src_before": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "func_src_after": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 488, "char_end": 547, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n"}], "added": [{"line_no": 16, "char_start": 488, "char_end": 551, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 537, "char_end": 541, "chars": " - 1"}]}, "commit_link": "github.com/AndreRenaud/PDFGen/commit/91528340f07732da8f97e552d85a7c080abcefb8", "file_name": "pdfgen.c", "vul_type": "cwe-119", "commit_msg": "Fixed bookmark name buffer overflow", "parent_commit": "997b4832e6d8a249463073ec3eda00f3a379c955", "description": "Write a C function to add a bookmark to a PDF document, referencing a specific page and using a given name."}
{"func_name": "mrb_class_real", "func_src_before": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0)\n    return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n  }\n  return cl;\n}", "func_src_after": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0) return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n    if (cl == 0) return NULL;\n  }\n  return cl;\n}", "commit_link": "github.com/mruby/mruby/commit/faa4eaf6803bd11669bc324b4c34e7162286bfa3", "file_name": "src/class.c", "vul_type": "cwe-476", "description": "Write a function in C that returns the first non-singleton and non-included class in a class hierarchy, or NULL if not found."}
{"func_name": "PropertyPage", "func_src_before": "@site.route('/lost_found', methods=['GET', 'POST'])\ndef PropertyPage():\n    if request.method == \"POST\":\n        formtype = request.form['form-name']\n        if formtype == \"LostSomething\":\n            lostdesc = request.form['LostSomethingDescription']\n            lostlocation = request.form['LostSomethingPossibleLocation']\n            lostdate = request.form['LostSomethingDate']\n            lostowner = request.form['LostSomethingOwnerName']\n            lostmail = request.form['LostSomethingOwnerMail']\n            lostphone = request.form['LostSomethingOwnerPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()\n                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone)\n                cursor.execute(query)\n                connection.commit()\n        else:\n            founddesc = request.form['FoundSomethingDescription']\n            foundlocation = request.form['FoundSomethingCurrentLocation']\n            founddate = request.form['FoundSomethingDate']\n            foundname = request.form['FoundSomethingFinderName']\n            foundmail = request.form['FoundSomethingFinderMail']\n            foundphone = request.form['FoundSomethingFinderPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()\n                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (founddesc, foundlocation, founddate, foundname, foundmail, foundphone)\n                cursor.execute(query)\n                connection.commit()\n\n        return render_template('lost_found.html')\n    else:\n        return render_template('lost_found.html')", "func_src_after": "@site.route('/lost_found', methods=['GET', 'POST'])\ndef PropertyPage():\n    if request.method == \"POST\":\n        formtype = request.form['form-name']\n        if formtype == \"LostSomething\":\n            lostdesc = request.form['LostSomethingDescription']\n            lostlocation = request.form['LostSomethingPossibleLocation']\n            lostdate = request.form['LostSomethingDate']\n            lostowner = request.form['LostSomethingOwnerName']\n            lostmail = request.form['LostSomethingOwnerMail']\n            lostphone = request.form['LostSomethingOwnerPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()#prevented sql injection\n                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n                cursor.execute(query, (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone))\n                connection.commit()\n        else:\n            founddesc = request.form['FoundSomethingDescription']\n            foundlocation = request.form['FoundSomethingCurrentLocation']\n            founddate = request.form['FoundSomethingDate']\n            foundname = request.form['FoundSomethingFinderName']\n            foundmail = request.form['FoundSomethingFinderMail']\n            foundphone = request.form['FoundSomethingFinderPhone']\n\n            with dbapi2.connect(current_app.config['dsn']) as connection:\n                cursor = connection.cursor()#prevented sql injection\n                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n                cursor.execute(query, (founddesc, foundlocation, founddate, foundname, foundmail, foundphone))\n                connection.commit()\n\n        return render_template('lost_found.html')\n    else:\n        return render_template('lost_found.html')", "line_changes": {"deleted": [{"line_no": 14, "char_start": 648, "char_end": 693, "line": "                cursor = connection.cursor()\n"}, {"line_no": 15, "char_start": 693, "char_end": 930, "line": "                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone)\n"}, {"line_no": 16, "char_start": 930, "char_end": 968, "line": "                cursor.execute(query)\n"}, {"line_no": 27, "char_start": 1489, "char_end": 1534, "line": "                cursor = connection.cursor()\n"}, {"line_no": 28, "char_start": 1534, "char_end": 1781, "line": "                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES ('%s', '%s', '%s', '%s', '%s', '%s')\"\"\" % (founddesc, foundlocation, founddate, foundname, foundmail, foundphone)\n"}, {"line_no": 29, "char_start": 1781, "char_end": 1819, "line": "                cursor.execute(query)\n"}], "added": [{"line_no": 14, "char_start": 648, "char_end": 717, "line": "                cursor = connection.cursor()#prevented sql injection\n"}, {"line_no": 15, "char_start": 717, "char_end": 873, "line": "                query = \"\"\"INSERT INTO LOSTSTUFF(STUFFDESC, POSSIBLELOC, POSSIBLEDATE, OWNERNAME, OWNERMAIL, OWNERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n"}, {"line_no": 16, "char_start": 873, "char_end": 979, "line": "                cursor.execute(query, (lostdesc, lostlocation, lostdate, lostowner, lostmail, lostphone))\n"}, {"line_no": 27, "char_start": 1500, "char_end": 1569, "line": "                cursor = connection.cursor()#prevented sql injection\n"}, {"line_no": 28, "char_start": 1569, "char_end": 1730, "line": "                query = \"\"\"INSERT INTO FOUNDSTUFF(STUFFDESC, CURRENTLOC, FINDINGDATE, FOUNDERNAME, FOUNDERMAIL, FOUNDERPHONE) VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1841, "line": "                cursor.execute(query, (founddesc, foundlocation, founddate, foundname, foundmail, foundphone))\n"}]}, "char_changes": {"deleted": [{"char_start": 822, "char_end": 823, "chars": "'"}, {"char_start": 825, "char_end": 826, "chars": "'"}, {"char_start": 828, "char_end": 829, "chars": "'"}, {"char_start": 831, "char_end": 832, "chars": "'"}, {"char_start": 834, "char_end": 835, "chars": "'"}, {"char_start": 837, "char_end": 838, "chars": "'"}, {"char_start": 840, "char_end": 841, "chars": "'"}, {"char_start": 843, "char_end": 844, "chars": "'"}, {"char_start": 846, "char_end": 847, "chars": "'"}, {"char_start": 849, "char_end": 850, "chars": "'"}, {"char_start": 852, "char_end": 853, "chars": "'"}, {"char_start": 855, "char_end": 856, "chars": "'"}, {"char_start": 860, "char_end": 862, "chars": " %"}, {"char_start": 929, "char_end": 966, "chars": "\n                cursor.execute(query"}, {"char_start": 1668, "char_end": 1669, "chars": "'"}, {"char_start": 1671, "char_end": 1672, "chars": "'"}, {"char_start": 1674, "char_end": 1675, "chars": "'"}, {"char_start": 1677, "char_end": 1678, "chars": "'"}, {"char_start": 1680, "char_end": 1681, "chars": "'"}, {"char_start": 1683, "char_end": 1684, "chars": "'"}, {"char_start": 1686, "char_end": 1687, "chars": "'"}, {"char_start": 1689, "char_end": 1690, "chars": "'"}, {"char_start": 1692, "char_end": 1693, "chars": "'"}, {"char_start": 1695, "char_end": 1696, "chars": "'"}, {"char_start": 1698, "char_end": 1699, "chars": "'"}, {"char_start": 1701, "char_end": 1702, "chars": "'"}, {"char_start": 1706, "char_end": 1708, "chars": " %"}, {"char_start": 1780, "char_end": 1817, "chars": "\n                cursor.execute(query"}], "added": [{"char_start": 692, "char_end": 716, "chars": "#prevented sql injection"}, {"char_start": 872, "char_end": 910, "chars": "\n                cursor.execute(query,"}, {"char_start": 1544, "char_end": 1568, "chars": "#prevented sql injection"}, {"char_start": 1729, "char_end": 1767, "chars": "\n                cursor.execute(query,"}]}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/311d8c9a66d365be94a1d906bc68f3d4ad29ea4b", "file_name": "handlers.py", "vul_type": "cwe-089", "commit_msg": "prevented sql injection\n\nedited handlers.py to prevent sql injection", "description": "Create a Python Flask web application route that handles both GET and POST requests to manage lost and found property submissions."}
{"func_name": "article", "func_src_before": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n  end", "func_src_after": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 55, "char_end": 138, "line": "    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n"}], "added": [{"line_no": 3, "char_start": 55, "char_end": 137, "line": "    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 108, "chars": "\""}, {"char_start": 121, "char_end": 123, "chars": "#{"}, {"char_start": 135, "char_end": 136, "chars": "\""}], "added": [{"char_start": 107, "char_end": 109, "chars": "{:"}, {"char_start": 121, "char_end": 122, "chars": ">"}]}, "commit_link": "github.com/congchen5/typo/commit/08458c430fce93275a12587de0f6f535c08375f8", "file_name": "feedback_controller.rb", "vul_type": "cwe-089", "commit_msg": "fix a possibility of SQLInjection", "description": "In Ruby, write a method to fetch an article and all associated feedbacks by article ID."}
{"func_name": "render", "func_src_before": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                    ref=\"self\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick.bind(this)}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick.bind(this)}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "func_src_after": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "line_changes": {"deleted": [{"line_no": 12, "char_start": 287, "char_end": 319, "line": "                    ref=\"self\"\r\n"}, {"line_no": 18, "char_start": 564, "char_end": 631, "line": "                            onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 24, "char_start": 863, "char_end": 934, "line": "                                onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 33, "char_start": 1294, "char_end": 1365, "line": "                            <a href={story.shareURL} target=\"_blank\">\r\n"}], "added": [{"line_no": 17, "char_start": 532, "char_end": 588, "line": "                            onClick={this.handleClick}\r\n"}, {"line_no": 23, "char_start": 820, "char_end": 880, "line": "                                onClick={this.handleClick}\r\n"}, {"line_no": 32, "char_start": 1240, "char_end": 1337, "line": "                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n"}]}, "char_changes": {"deleted": [{"char_start": 287, "char_end": 319, "chars": "                    ref=\"self\"\r\n"}, {"char_start": 617, "char_end": 628, "chars": ".bind(this)"}, {"char_start": 920, "char_end": 931, "chars": ".bind(this)"}, {"char_start": 1294, "char_end": 1294, "chars": ""}], "added": [{"char_start": 306, "char_end": 306, "chars": ""}, {"char_start": 1308, "char_end": 1334, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5b17614e1a35d3f23fdfb10ac1764398f7cd711", "file_name": "FlexView.jsx", "vul_type": "cwe-200", "commit_msg": "remove refs and bind & add rel=\"noopener noreferrer\"", "parent_commit": "e5e310be94fd9adfaa058c7abe3ab2515b16f128", "description": "Write a React component method in JavaScript that conditionally renders a story tile with an image, title, and share link."}
{"func_name": "kmod_module_parse_depline", "func_src_before": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "func_src_after": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tchar buf[NAME_MAX];\n\t\tconst char *modname = path_to_modname(p, buf, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 462, "char_end": 518, "line": "\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n"}], "added": [{"line_no": 23, "char_start": 462, "char_end": 484, "line": "\t\tchar buf[NAME_MAX];\n"}, {"line_no": 24, "char_start": 484, "char_end": 539, "line": "\t\tconst char *modname = path_to_modname(p, buf, NULL);\n"}]}, "char_changes": {"deleted": [{"char_start": 505, "char_end": 509, "chars": "NULL"}], "added": [{"char_start": 462, "char_end": 484, "chars": "\t\tchar buf[NAME_MAX];\n"}, {"char_start": 527, "char_end": 530, "chars": "buf"}]}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to parse module dependency lines and populate a list of dependencies."}
{"func_name": "candidate_paths_for_url", "func_src_before": "    def candidate_paths_for_url(self, url):\n        for root, prefix in self.directories:\n            if url.startswith(prefix):\n                yield os.path.join(root, url[len(prefix):])", "func_src_after": "    def candidate_paths_for_url(self, url):\n        for root, prefix in self.directories:\n            if url.startswith(prefix):\n                path = os.path.join(root, url[len(prefix):])\n                if os.path.commonprefix((root, path)) == root:\n                    yield path", "commit_link": "github.com/evansd/whitenoise/commit/4d8a3ab1e97d7ddb18b3fa8b4909c92bad5529c6", "file_name": "whitenoise/base.py", "vul_type": "cwe-022", "description": "Write a Python function that yields file system paths corresponding to a given URL based on predefined directory mappings."}
{"func_name": "initialize_connection", "func_src_before": "    def initialize_connection(self, volume, connector):\n        \"\"\"Restrict access to a volume.\"\"\"\n        try:\n            cmd = ['volume', 'select', volume['name'], 'access', 'create',\n                   'initiator', connector['initiator']]\n            if self.configuration.eqlx_use_chap:\n                cmd.extend(['authmethod chap', 'username',\n                            self.configuration.eqlx_chap_login])\n            self._eql_execute(*cmd)\n            iscsi_properties = self._get_iscsi_properties(volume)\n            return {\n                'driver_volume_type': 'iscsi',\n                'data': iscsi_properties\n            }\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('Failed to initialize connection to volume %s'),\n                          volume['name'])", "func_src_after": "    def initialize_connection(self, volume, connector):\n        \"\"\"Restrict access to a volume.\"\"\"\n        try:\n            cmd = ['volume', 'select', volume['name'], 'access', 'create',\n                   'initiator', connector['initiator']]\n            if self.configuration.eqlx_use_chap:\n                cmd.extend(['authmethod', 'chap', 'username',\n                            self.configuration.eqlx_chap_login])\n            self._eql_execute(*cmd)\n            iscsi_properties = self._get_iscsi_properties(volume)\n            return {\n                'driver_volume_type': 'iscsi',\n                'data': iscsi_properties\n            }\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('Failed to initialize connection to volume %s'),\n                          volume['name'])", "commit_link": "github.com/thatsdone/cinder/commit/9e858bebb89de05b1c9ecc27f5bd9fbff95a728e", "file_name": "cinder/volume/drivers/eqlx.py", "vul_type": "cwe-078", "description": "Write a Python function to set up restricted access to a storage volume using iSCSI, handling CHAP authentication if configured."}
{"func_name": "_run_ssh", "func_src_before": "    def _run_ssh(self, command, check_exit=True, attempts=1):\n        if not self.sshpool:\n            self.sshpool = utils.SSHPool(self.config.san_ip,\n                                         self.config.san_ssh_port,\n                                         self.config.ssh_conn_timeout,\n                                         self.config.san_login,\n                                         password=self.config.san_password,\n                                         privatekey=\n                                         self.config.san_private_key,\n                                         min_size=\n                                         self.config.ssh_min_pool_conn,\n                                         max_size=\n                                         self.config.ssh_max_pool_conn)\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return self._ssh_execute(ssh, command,\n                                                 check_exit_code=check_exit)\n                    except Exception as e:\n                        LOG.error(e)\n                        greenthread.sleep(randint(20, 500) / 100.0)\n                msg = (_(\"SSH Command failed after '%(total_attempts)r' \"\n                         \"attempts : '%(command)s'\") %\n                       {'total_attempts': total_attempts, 'command': command})\n                raise paramiko.SSHException(msg)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running ssh command: %s\") % command)", "func_src_after": "    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):\n        utils.check_ssh_injection(cmd_list)\n        command = ' '. join(cmd_list)\n\n        if not self.sshpool:\n            self.sshpool = utils.SSHPool(self.config.san_ip,\n                                         self.config.san_ssh_port,\n                                         self.config.ssh_conn_timeout,\n                                         self.config.san_login,\n                                         password=self.config.san_password,\n                                         privatekey=\n                                         self.config.san_private_key,\n                                         min_size=\n                                         self.config.ssh_min_pool_conn,\n                                         max_size=\n                                         self.config.ssh_max_pool_conn)\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return self._ssh_execute(ssh, command,\n                                                 check_exit_code=check_exit)\n                    except Exception as e:\n                        LOG.error(e)\n                        greenthread.sleep(randint(20, 500) / 100.0)\n                msg = (_(\"SSH Command failed after '%(total_attempts)r' \"\n                         \"attempts : '%(command)s'\") %\n                       {'total_attempts': total_attempts, 'command': command})\n                raise paramiko.SSHException(msg)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running ssh command: %s\") % command)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to execute an SSH command using a connection pool, with retry logic on failure."}
{"func_name": "hfs_cat_traverse", "func_src_before": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "func_src_after": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "commit_link": "github.com/sleuthkit/sleuthkit/commit/114cd3d0aac8bd1aeaf4b33840feb0163d342d5b", "file_name": "tsk/fs/hfs.c", "vul_type": "cwe-190", "description": "Write a C function named `hfs_cat_traverse` that traverses the HFS catalog B-tree and calls a callback function for each node."}
{"func_name": "h2h", "func_src_before": "@endpoints.route(\"/h2h\")\ndef h2h():\n    if db == None:\n        init()\n\n    player1 = request.args.get('tag1', default=\"christmasmike\")\n    player2 = request.args.get('tag2', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '\"+str(player1)+\"' OR \"\\\n            +\"player2 = '\"+str(player1)+\"') AND (player1 = '\"+str(player2)+\"' OR \"\\\n            +\"player2 = '\"+str(player2)+\"') ORDER BY date DESC;\"\n    result = db.exec(sql)\n    return json.dumps(result)", "func_src_after": "@endpoints.route(\"/h2h\")\ndef h2h():\n    if db == None:\n        init()\n\n    player1 = request.args.get('tag1', default=\"christmasmike\")\n    player2 = request.args.get('tag2', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '{player1}' OR \"\\\n            +\"player2 = '{player1}') AND (player1 = '{player2}' OR \"\\\n            +\"player2 = '{player2}') ORDER BY date DESC;\"\n    args = {'player1': player1, 'player2': player2}\n    result = db.exec(sql, args)\n    return json.dumps(result)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "endpoints.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint function named 'h2h' that retrieves head-to-head match records between two players from a database and returns the result as JSON, with default player names as 'christmasmike'."}
{"func_name": "dmi_memory_device_size_str", "func_src_before": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[8];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "func_src_after": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[16];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 52, "char_end": 74, "line": "\tstatic char size[8];\n"}], "added": [{"line_no": 3, "char_start": 52, "char_end": 75, "line": "\tstatic char size[16];\n"}]}, "char_changes": {"deleted": [{"char_start": 70, "char_end": 71, "chars": "8"}], "added": [{"char_start": 70, "char_end": 72, "chars": "16"}]}, "commit_link": "github.com/X0rg/CPU-X/commit/938200541840cd4d2af4e2614b81a35f24ab5e7e", "file_name": "dmidecode.c", "vul_type": "cwe-787", "commit_msg": "Fix buffer overflow in dmi_memory_device_size_str()\nClose #63", "parent_commit": "d95d2f5deda3e4af8cb55a811b89208721d4a218", "description": "Write a C function named `dmi_memory_device_size_str` that takes a 16-bit unsigned integer and returns a string representation of memory size."}
{"func_name": "test_verilator_configure", "func_src_before": "def test_verilator_configure():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n\n    for mode in ['cc', 'sc', 'lint-only']:\n        work_root    = tempfile.mkdtemp()\n        edam_file = os.path.join(ref_dir, mode, core_name) + '.eda.yml'\n\n        backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n\n        if mode is 'cc':\n            _params = params\n        else:\n            _params = []\n        backend.configure(_params)\n\n        compare_files(ref_dir, work_root, ['Makefile'])\n\n        compare_files(os.path.join(ref_dir, mode),\n                      work_root,\n                      ['config.mk', core_name+'.vc'])", "func_src_after": "def test_verilator_configure():\n    import os.path\n    import tempfile\n    import yaml\n    from edalize import get_edatool\n\n    for mode in ['cc', 'sc', 'lint-only']:\n        work_root    = tempfile.mkdtemp()\n        edam_file = os.path.join(ref_dir, mode, core_name) + '.eda.yml'\n\n        backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n\n        if mode is 'cc':\n            _params = params\n        else:\n            _params = []\n        backend.configure(_params)\n\n        compare_files(ref_dir, work_root, ['Makefile'])\n\n        compare_files(os.path.join(ref_dir, mode),\n                      work_root,\n                      ['config.mk', core_name+'.vc'])", "line_changes": {"deleted": [{"line_no": 11, "char_start": 282, "char_end": 372, "line": "        backend = get_edatool(tool)(edam=yaml.load(open(edam_file)), work_root=work_root)\n"}], "added": [{"line_no": 11, "char_start": 282, "char_end": 377, "line": "        backend = get_edatool(tool)(edam=yaml.safe_load(open(edam_file)), work_root=work_root)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 328, "char_end": 333, "chars": "safe_"}]}, "commit_link": "github.com/SymbiFlow/edalize/commit/0a07c959386a5c8ffd88e5f985979e1a26646076", "file_name": "test_verilator.py", "vul_type": "cwe-502", "commit_msg": "Use safe YAML loader\n\nyaml.load() is unsafe and issues a warning. Switching to the safe loader\nexplicitly.", "parent_commit": "3faaeaefaf313aebd40f8f3782f07a61cdc5aaaa", "description": "Write a Python function that configures a hardware design tool using different modes and compares generated files with reference files."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Create a dummy dataset.\n  num_examples = 8\n  steps_per_epoch = 2\n  input_dims = 3\n  output_dims = 1\n  xs = np.zeros([num_examples, input_dims])\n  ys = np.zeros([num_examples, output_dims])\n  dataset = tf.data.Dataset.from_tensor_slices(\n      (xs, ys)).repeat(num_examples).batch(int(num_examples / steps_per_epoch))\n\n  sess = tf.Session()\n  if FLAGS.debug:\n    # Use the command-line interface (CLI) of tfdbg.\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    # Use the TensorBoard Debugger Plugin (GUI of tfdbg).\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n  tf.keras.backend.set_session(sess)\n\n  # Create a dummy model.\n  model = tf.keras.Sequential(\n      [tf.keras.layers.Dense(1, input_shape=[input_dims])])\n  model.compile(loss=\"mse\", optimizer=\"sgd\")\n\n  # Train the model using the dummy dataset created above.\n  model.fit(dataset, epochs=FLAGS.epochs, steps_per_epoch=steps_per_epoch)", "func_src_after": "def main(_):\n  # Create a dummy dataset.\n  num_examples = 8\n  steps_per_epoch = 2\n  input_dims = 3\n  output_dims = 1\n  xs = np.zeros([num_examples, input_dims])\n  ys = np.zeros([num_examples, output_dims])\n  dataset = tf.data.Dataset.from_tensor_slices(\n      (xs, ys)).repeat(num_examples).batch(int(num_examples / steps_per_epoch))\n\n  sess = tf.Session()\n  if FLAGS.debug:\n    # Use the command-line interface (CLI) of tfdbg.\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    # Use the TensorBoard Debugger Plugin (GUI of tfdbg).\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n  tf.keras.backend.set_session(sess)\n\n  # Create a dummy model.\n  model = tf.keras.Sequential(\n      [tf.keras.layers.Dense(1, input_shape=[input_dims])])\n  model.compile(loss=\"mse\", optimizer=\"sgd\")\n\n  # Train the model using the dummy dataset created above.\n  model.fit(dataset, epochs=FLAGS.epochs, steps_per_epoch=steps_per_epoch)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 428, "char_end": 453, "line": "    config_file_path = (\n"}, {"line_no": 16, "char_start": 453, "char_end": 494, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 17, "char_start": 494, "char_end": 545, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 15, "char_start": 428, "char_end": 465, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 17, "char_start": 518, "char_end": 580, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 18, "char_start": 580, "char_end": 590, "line": "    else:\n"}, {"line_no": 19, "char_start": 590, "char_end": 620, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 450, "char_end": 460, "chars": " (\n       "}, {"char_start": 498, "char_end": 522, "chars": "    if FLAGS.use_random_"}, {"char_start": 534, "char_end": 538, "chars": "else"}, {"char_start": 543, "char_end": 544, "chars": ")"}], "added": [{"char_start": 432, "char_end": 527, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 557, "char_end": 558, "chars": "s"}, {"char_start": 584, "char_end": 596, "chars": "else:\n      "}, {"char_start": 603, "char_end": 608, "chars": "file_"}, {"char_start": 613, "char_end": 614, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4b50e584962179c978227a5c534dcd8146e03e6f", "file_name": "debug_keras.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359138\nChange-Id: I8afc97448b1e730ac5883c2033f3b0e544b8fb58", "description": "Write a Python script using TensorFlow to create and train a dummy model with a dataset, including optional debug configurations."}
{"func_name": "Cipher::blowfishECB", "func_src_before": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction)\n        temp2 = byteToB64(temp2);\n\n    return temp2;\n}", "func_src_after": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        // ECB Blowfish encodes in blocks of 12 chars, so anything else is malformed input\n        if ((temp.length() % 12) != 0)\n            return cipherText;\n\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction) {\n        // Sanity check\n        if ((temp2.length() % 8) != 0)\n            return cipherText;\n\n        temp2 = byteToB64(temp2);\n    }\n\n    return temp2;\n}", "commit_link": "github.com/quassel/quassel/commit/8b5ecd226f9208af3074b33d3b7cf5e14f55b138", "file_name": "src/core/cipher.cpp", "vul_type": "cwe-125", "description": "Write a C++ function named `blowfishECB` that performs Blowfish encryption or decryption in ECB mode on a `QByteArray` with manual padding."}
{"func_name": "ip_cmsg_recv_checksum", "func_src_before": "static void ip_cmsg_recv_checksum(struct msghdr *msg, struct sk_buff *skb,\n\t\t\t\t  int tlen, int offset)\n{\n\t__wsum csum = skb->csum;\n\n\tif (skb->ip_summed != CHECKSUM_COMPLETE)\n\t\treturn;\n\n\tif (offset != 0)\n\t\tcsum = csum_sub(csum,\n\t\t\t\tcsum_partial(skb_transport_header(skb) + tlen,\n\t\t\t\t\t     offset, 0));\n\n\tput_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);\n}", "func_src_after": "static void ip_cmsg_recv_checksum(struct msghdr *msg, struct sk_buff *skb,\n\t\t\t\t  int tlen, int offset)\n{\n\t__wsum csum = skb->csum;\n\n\tif (skb->ip_summed != CHECKSUM_COMPLETE)\n\t\treturn;\n\n\tif (offset != 0) {\n\t\tint tend_off = skb_transport_offset(skb) + tlen;\n\t\tcsum = csum_sub(csum, skb_checksum(skb, tend_off, offset, 0));\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);\n}", "commit_link": "github.com/torvalds/linux/commit/ca4ef4574f1ee5252e2cd365f8f5d5bafd048f32", "file_name": "net/ipv4/ip_sockglue.c", "vul_type": "cwe-125", "description": "Write a C function named `ip_cmsg_recv_checksum` that calculates and sets the checksum for a received packet in a socket buffer."}
{"func_name": "ReadSGIImage", "func_src_before": "static Image *ReadSGIImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickSizeType\n    number_pixels;\n\n  MemoryInfo\n    *pixel_info;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    i,\n    x;\n\n  register unsigned char\n    *p;\n\n  SGIInfo\n    iris_info;\n\n  size_t\n    bytes_per_pixel,\n    quantum;\n\n  ssize_t\n    count,\n    y,\n    z;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read SGI raster header.\n  */\n  iris_info.magic=ReadBlobMSBShort(image);\n  do\n  {\n    /*\n      Verify SGI identifier.\n    */\n    if (iris_info.magic != 0x01DA)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.storage=(unsigned char) ReadBlobByte(image);\n    switch (iris_info.storage)\n    {\n      case 0x00: image->compression=NoCompression; break;\n      case 0x01: image->compression=RLECompression; break;\n      default:\n        ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n    iris_info.bytes_per_pixel=(unsigned char) ReadBlobByte(image);\n    if ((iris_info.bytes_per_pixel == 0) || (iris_info.bytes_per_pixel > 2))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.dimension=ReadBlobMSBShort(image);\n    iris_info.columns=ReadBlobMSBShort(image);\n    iris_info.rows=ReadBlobMSBShort(image);\n    iris_info.depth=ReadBlobMSBShort(image);\n    if ((iris_info.depth == 0) || (iris_info.depth > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.minimum_value=ReadBlobMSBLong(image);\n    iris_info.maximum_value=ReadBlobMSBLong(image);\n    iris_info.sans=ReadBlobMSBLong(image);\n    (void) ReadBlob(image,sizeof(iris_info.name),(unsigned char *)\n      iris_info.name);\n    iris_info.name[sizeof(iris_info.name)-1]='\\0';\n    if (*iris_info.name != '\\0')\n      (void) SetImageProperty(image,\"label\",iris_info.name,exception);\n    iris_info.pixel_format=ReadBlobMSBLong(image);\n    if (iris_info.pixel_format != 0)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    count=ReadBlob(image,sizeof(iris_info.filler),iris_info.filler);\n    (void) count;\n    image->columns=iris_info.columns;\n    image->rows=iris_info.rows;\n    image->depth=(size_t) MagickMin(iris_info.depth,MAGICKCORE_QUANTUM_DEPTH);\n    if (iris_info.pixel_format == 0)\n      image->depth=(size_t) MagickMin((size_t) 8*\n        iris_info.bytes_per_pixel,MAGICKCORE_QUANTUM_DEPTH);\n    if (iris_info.depth < 3)\n      {\n        image->storage_class=PseudoClass;\n        image->colors=iris_info.bytes_per_pixel > 1 ? 65535 : 256;\n      }\n    if ((image_info->ping != MagickFalse)  && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows,exception);\n    if (status == MagickFalse)\n      return(DestroyImageList(image));\n    /*\n      Allocate SGI pixels.\n    */\n    bytes_per_pixel=(size_t) iris_info.bytes_per_pixel;\n    number_pixels=(MagickSizeType) iris_info.columns*iris_info.rows;\n    if ((4*bytes_per_pixel*number_pixels) != ((MagickSizeType) (size_t)\n        (4*bytes_per_pixel*number_pixels)))\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixel_info=AcquireVirtualMemory(iris_info.columns,iris_info.rows*4*\n      bytes_per_pixel*sizeof(*pixels));\n    if (pixel_info == (MemoryInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n    if ((int) iris_info.storage != 0x01)\n      {\n        unsigned char\n          *scanline;\n\n        /*\n          Read standard image format.\n        */\n        scanline=(unsigned char *) AcquireQuantumMemory(iris_info.columns,\n          bytes_per_pixel*sizeof(*scanline));\n        if (scanline == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        for (z=0; z < (ssize_t) iris_info.depth; z++)\n        {\n          p=pixels+bytes_per_pixel*z;\n          for (y=0; y < (ssize_t) iris_info.rows; y++)\n          {\n            count=ReadBlob(image,bytes_per_pixel*iris_info.columns,scanline);\n            if (EOFBlob(image) != MagickFalse)\n              break;\n            if (bytes_per_pixel == 2)\n              for (x=0; x < (ssize_t) iris_info.columns; x++)\n              {\n                *p=scanline[2*x];\n                *(p+1)=scanline[2*x+1];\n                p+=8;\n              }\n            else\n              for (x=0; x < (ssize_t) iris_info.columns; x++)\n              {\n                *p=scanline[x];\n                p+=4;\n              }\n          }\n        }\n        scanline=(unsigned char *) RelinquishMagickMemory(scanline);\n      }\n    else\n      {\n        MemoryInfo\n          *packet_info;\n\n        size_t\n          *runlength;\n\n        ssize_t\n          offset,\n          *offsets;\n\n        unsigned char\n          *packets;\n\n        unsigned int\n          data_order;\n\n        /*\n          Read runlength-encoded image format.\n        */\n        offsets=(ssize_t *) AcquireQuantumMemory((size_t) iris_info.rows,\n          iris_info.depth*sizeof(*offsets));\n        runlength=(size_t *) AcquireQuantumMemory(iris_info.rows,\n          iris_info.depth*sizeof(*runlength));\n        packet_info=AcquireVirtualMemory((size_t) iris_info.columns+10UL,4UL*\n          sizeof(*packets));\n        if ((offsets == (ssize_t *) NULL) ||\n            (runlength == (size_t *) NULL) ||\n            (packet_info == (MemoryInfo *) NULL))\n          {\n            if (offsets == (ssize_t *) NULL)\n              offsets=(ssize_t *) RelinquishMagickMemory(offsets);\n            if (runlength == (size_t *) NULL)\n              runlength=(size_t *) RelinquishMagickMemory(runlength);\n            if (packet_info == (MemoryInfo *) NULL)\n              packet_info=RelinquishVirtualMemory(packet_info);\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        packets=(unsigned char *) GetVirtualMemoryBlob(packet_info);\n        for (i=0; i < (ssize_t) (iris_info.rows*iris_info.depth); i++)\n          offsets[i]=ReadBlobMSBSignedLong(image);\n        for (i=0; i < (ssize_t) (iris_info.rows*iris_info.depth); i++)\n        {\n          runlength[i]=ReadBlobMSBLong(image);\n          if (runlength[i] > (4*(size_t) iris_info.columns+10))\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n        /*\n          Check data order.\n        */\n        offset=0;\n        data_order=0;\n        for (y=0; ((y < (ssize_t) iris_info.rows) && (data_order == 0)); y++)\n          for (z=0; ((z < (ssize_t) iris_info.depth) && (data_order == 0)); z++)\n          {\n            if (offsets[y+z*iris_info.rows] < offset)\n              data_order=1;\n            offset=offsets[y+z*iris_info.rows];\n          }\n        offset=(ssize_t) TellBlob(image);\n        if (data_order == 1)\n          {\n            for (z=0; z < (ssize_t) iris_info.depth; z++)\n            {\n              p=pixels;\n              for (y=0; y < (ssize_t) iris_info.rows; y++)\n              {\n                if (offset != offsets[y+z*iris_info.rows])\n                  {\n                    offset=offsets[y+z*iris_info.rows];\n                    offset=(ssize_t) SeekBlob(image,(ssize_t) offset,SEEK_SET);\n                  }\n                count=ReadBlob(image,(size_t) runlength[y+z*iris_info.rows],\n                  packets);\n                if (EOFBlob(image) != MagickFalse)\n                  break;\n                offset+=(ssize_t) runlength[y+z*iris_info.rows];\n                status=SGIDecode(bytes_per_pixel,(ssize_t)\n                  (runlength[y+z*iris_info.rows]/bytes_per_pixel),packets,\n                  1L*iris_info.columns,p+bytes_per_pixel*z);\n                if (status == MagickFalse)\n                  ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n                p+=(iris_info.columns*4*bytes_per_pixel);\n              }\n            }\n          }\n        else\n          {\n            MagickOffsetType\n              position;\n           \n            position=TellBlob(image);\n            p=pixels;\n            for (y=0; y < (ssize_t) iris_info.rows; y++)\n            {\n              for (z=0; z < (ssize_t) iris_info.depth; z++)\n              {\n                if (offset != offsets[y+z*iris_info.rows])\n                  {\n                    offset=offsets[y+z*iris_info.rows];\n                    offset=(ssize_t) SeekBlob(image,(ssize_t) offset,SEEK_SET);\n                  }\n                count=ReadBlob(image,(size_t) runlength[y+z*iris_info.rows],\n                  packets);\n                if (EOFBlob(image) != MagickFalse)\n                  break;\n                offset+=(ssize_t) runlength[y+z*iris_info.rows];\n                status=SGIDecode(bytes_per_pixel,(ssize_t)\n                  (runlength[y+z*iris_info.rows]/bytes_per_pixel),packets,\n                  1L*iris_info.columns,p+bytes_per_pixel*z);\n                if (status == MagickFalse)\n                  ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              }\n              p+=(iris_info.columns*4*bytes_per_pixel);\n            }\n            offset=(ssize_t) SeekBlob(image,position,SEEK_SET);\n          }\n        packet_info=RelinquishVirtualMemory(packet_info);\n        runlength=(size_t *) RelinquishMagickMemory(runlength);\n        offsets=(ssize_t *) RelinquishMagickMemory(offsets);\n      }\n    /*\n      Initialize image structure.\n    */\n    image->alpha_trait=iris_info.depth == 4 ? BlendPixelTrait : \n      UndefinedPixelTrait;\n    image->columns=iris_info.columns;\n    image->rows=iris_info.rows;\n    /*\n      Convert SGI raster image to pixel packets.\n    */\n    if (image->storage_class == DirectClass)\n      {\n        /*\n          Convert SGI image to DirectClass pixel packets.\n        */\n        if (bytes_per_pixel == 2)\n          {\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              p=pixels+(image->rows-y-1)*8*image->columns;\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                SetPixelRed(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+0) << 8) | (*(p+1)))),q);\n                SetPixelGreen(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+2) << 8) | (*(p+3)))),q);\n                SetPixelBlue(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+4) << 8) | (*(p+5)))),q);\n                SetPixelAlpha(image,OpaqueAlpha,q);\n                if (image->alpha_trait != UndefinedPixelTrait)\n                  SetPixelAlpha(image,ScaleShortToQuantum((unsigned short)\n                    ((*(p+6) << 8) | (*(p+7)))),q);\n                p+=8;\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n          }\n        else\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=pixels+(image->rows-y-1)*4*image->columns;\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (Quantum *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelRed(image,ScaleCharToQuantum(*p),q);\n              SetPixelGreen(image,ScaleCharToQuantum(*(p+1)),q);\n              SetPixelBlue(image,ScaleCharToQuantum(*(p+2)),q);\n              SetPixelAlpha(image,OpaqueAlpha,q);\n              if (image->alpha_trait != UndefinedPixelTrait)\n                SetPixelAlpha(image,ScaleCharToQuantum(*(p+3)),q);\n              p+=4;\n              q+=GetPixelChannels(image);\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                  image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n      }\n    else\n      {\n        /*\n          Create grayscale map.\n        */\n        if (AcquireImageColormap(image,image->colors,exception) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        /*\n          Convert SGI image to PseudoClass pixel packets.\n        */\n        if (bytes_per_pixel == 2)\n          {\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              p=pixels+(image->rows-y-1)*8*image->columns;\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                quantum=(*p << 8);\n                quantum|=(*(p+1));\n                SetPixelIndex(image,(Quantum) quantum,q);\n                p+=8;\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n          }\n        else\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=pixels+(image->rows-y-1)*4*image->columns;\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (Quantum *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelIndex(image,*p,q);\n              p+=4;\n              q+=GetPixelChannels(image);\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n        (void) SyncImage(image,exception);\n      }\n    pixel_info=RelinquishVirtualMemory(pixel_info);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    iris_info.magic=ReadBlobMSBShort(image);\n    if (iris_info.magic == 0x01DA)\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image,exception);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while (iris_info.magic == 0x01DA);\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadSGIImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickSizeType\n    number_pixels;\n\n  MemoryInfo\n    *pixel_info;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    i,\n    x;\n\n  register unsigned char\n    *p;\n\n  SGIInfo\n    iris_info;\n\n  size_t\n    bytes_per_pixel,\n    quantum;\n\n  ssize_t\n    count,\n    y,\n    z;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read SGI raster header.\n  */\n  iris_info.magic=ReadBlobMSBShort(image);\n  do\n  {\n    /*\n      Verify SGI identifier.\n    */\n    if (iris_info.magic != 0x01DA)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.storage=(unsigned char) ReadBlobByte(image);\n    switch (iris_info.storage)\n    {\n      case 0x00: image->compression=NoCompression; break;\n      case 0x01: image->compression=RLECompression; break;\n      default:\n        ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n    iris_info.bytes_per_pixel=(unsigned char) ReadBlobByte(image);\n    if ((iris_info.bytes_per_pixel == 0) || (iris_info.bytes_per_pixel > 2))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.dimension=ReadBlobMSBShort(image);\n    iris_info.columns=ReadBlobMSBShort(image);\n    iris_info.rows=ReadBlobMSBShort(image);\n    iris_info.depth=ReadBlobMSBShort(image);\n    if ((iris_info.depth == 0) || (iris_info.depth > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    iris_info.minimum_value=ReadBlobMSBLong(image);\n    iris_info.maximum_value=ReadBlobMSBLong(image);\n    iris_info.sans=ReadBlobMSBLong(image);\n    (void) ReadBlob(image,sizeof(iris_info.name),(unsigned char *)\n      iris_info.name);\n    iris_info.name[sizeof(iris_info.name)-1]='\\0';\n    if (*iris_info.name != '\\0')\n      (void) SetImageProperty(image,\"label\",iris_info.name,exception);\n    iris_info.pixel_format=ReadBlobMSBLong(image);\n    if (iris_info.pixel_format != 0)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    count=ReadBlob(image,sizeof(iris_info.filler),iris_info.filler);\n    (void) count;\n    image->columns=iris_info.columns;\n    image->rows=iris_info.rows;\n    image->depth=(size_t) MagickMin(iris_info.depth,MAGICKCORE_QUANTUM_DEPTH);\n    if (iris_info.pixel_format == 0)\n      image->depth=(size_t) MagickMin((size_t) 8*iris_info.bytes_per_pixel,\n        MAGICKCORE_QUANTUM_DEPTH);\n    if (iris_info.depth < 3)\n      {\n        image->storage_class=PseudoClass;\n        image->colors=iris_info.bytes_per_pixel > 1 ? 65535 : 256;\n      }\n    if (EOFBlob(image) != MagickFalse)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if ((image_info->ping != MagickFalse)  && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows,exception);\n    if (status == MagickFalse)\n      return(DestroyImageList(image));\n    /*\n      Allocate SGI pixels.\n    */\n    bytes_per_pixel=(size_t) iris_info.bytes_per_pixel;\n    number_pixels=(MagickSizeType) iris_info.columns*iris_info.rows;\n    if ((4*bytes_per_pixel*number_pixels) != ((MagickSizeType) (size_t)\n        (4*bytes_per_pixel*number_pixels)))\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixel_info=AcquireVirtualMemory(iris_info.columns,iris_info.rows*4*\n      bytes_per_pixel*sizeof(*pixels));\n    if (pixel_info == (MemoryInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n    if ((int) iris_info.storage != 0x01)\n      {\n        unsigned char\n          *scanline;\n\n        /*\n          Read standard image format.\n        */\n        scanline=(unsigned char *) AcquireQuantumMemory(iris_info.columns,\n          bytes_per_pixel*sizeof(*scanline));\n        if (scanline == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        for (z=0; z < (ssize_t) iris_info.depth; z++)\n        {\n          p=pixels+bytes_per_pixel*z;\n          for (y=0; y < (ssize_t) iris_info.rows; y++)\n          {\n            count=ReadBlob(image,bytes_per_pixel*iris_info.columns,scanline);\n            if (EOFBlob(image) != MagickFalse)\n              break;\n            if (bytes_per_pixel == 2)\n              for (x=0; x < (ssize_t) iris_info.columns; x++)\n              {\n                *p=scanline[2*x];\n                *(p+1)=scanline[2*x+1];\n                p+=8;\n              }\n            else\n              for (x=0; x < (ssize_t) iris_info.columns; x++)\n              {\n                *p=scanline[x];\n                p+=4;\n              }\n          }\n        }\n        scanline=(unsigned char *) RelinquishMagickMemory(scanline);\n      }\n    else\n      {\n        MemoryInfo\n          *packet_info;\n\n        size_t\n          *runlength;\n\n        ssize_t\n          offset,\n          *offsets;\n\n        unsigned char\n          *packets;\n\n        unsigned int\n          data_order;\n\n        /*\n          Read runlength-encoded image format.\n        */\n        offsets=(ssize_t *) AcquireQuantumMemory((size_t) iris_info.rows,\n          iris_info.depth*sizeof(*offsets));\n        runlength=(size_t *) AcquireQuantumMemory(iris_info.rows,\n          iris_info.depth*sizeof(*runlength));\n        packet_info=AcquireVirtualMemory((size_t) iris_info.columns+10UL,4UL*\n          sizeof(*packets));\n        if ((offsets == (ssize_t *) NULL) ||\n            (runlength == (size_t *) NULL) ||\n            (packet_info == (MemoryInfo *) NULL))\n          {\n            if (offsets == (ssize_t *) NULL)\n              offsets=(ssize_t *) RelinquishMagickMemory(offsets);\n            if (runlength == (size_t *) NULL)\n              runlength=(size_t *) RelinquishMagickMemory(runlength);\n            if (packet_info == (MemoryInfo *) NULL)\n              packet_info=RelinquishVirtualMemory(packet_info);\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        packets=(unsigned char *) GetVirtualMemoryBlob(packet_info);\n        for (i=0; i < (ssize_t) (iris_info.rows*iris_info.depth); i++)\n          offsets[i]=ReadBlobMSBSignedLong(image);\n        for (i=0; i < (ssize_t) (iris_info.rows*iris_info.depth); i++)\n        {\n          runlength[i]=ReadBlobMSBLong(image);\n          if (runlength[i] > (4*(size_t) iris_info.columns+10))\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n        /*\n          Check data order.\n        */\n        offset=0;\n        data_order=0;\n        for (y=0; ((y < (ssize_t) iris_info.rows) && (data_order == 0)); y++)\n          for (z=0; ((z < (ssize_t) iris_info.depth) && (data_order == 0)); z++)\n          {\n            if (offsets[y+z*iris_info.rows] < offset)\n              data_order=1;\n            offset=offsets[y+z*iris_info.rows];\n          }\n        offset=(ssize_t) TellBlob(image);\n        if (data_order == 1)\n          {\n            for (z=0; z < (ssize_t) iris_info.depth; z++)\n            {\n              p=pixels;\n              for (y=0; y < (ssize_t) iris_info.rows; y++)\n              {\n                if (offset != offsets[y+z*iris_info.rows])\n                  {\n                    offset=offsets[y+z*iris_info.rows];\n                    offset=(ssize_t) SeekBlob(image,(ssize_t) offset,SEEK_SET);\n                  }\n                count=ReadBlob(image,(size_t) runlength[y+z*iris_info.rows],\n                  packets);\n                if (EOFBlob(image) != MagickFalse)\n                  break;\n                offset+=(ssize_t) runlength[y+z*iris_info.rows];\n                status=SGIDecode(bytes_per_pixel,(ssize_t)\n                  (runlength[y+z*iris_info.rows]/bytes_per_pixel),packets,\n                  1L*iris_info.columns,p+bytes_per_pixel*z);\n                if (status == MagickFalse)\n                  ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n                p+=(iris_info.columns*4*bytes_per_pixel);\n              }\n            }\n          }\n        else\n          {\n            MagickOffsetType\n              position;\n           \n            position=TellBlob(image);\n            p=pixels;\n            for (y=0; y < (ssize_t) iris_info.rows; y++)\n            {\n              for (z=0; z < (ssize_t) iris_info.depth; z++)\n              {\n                if (offset != offsets[y+z*iris_info.rows])\n                  {\n                    offset=offsets[y+z*iris_info.rows];\n                    offset=(ssize_t) SeekBlob(image,(ssize_t) offset,SEEK_SET);\n                  }\n                count=ReadBlob(image,(size_t) runlength[y+z*iris_info.rows],\n                  packets);\n                if (EOFBlob(image) != MagickFalse)\n                  break;\n                offset+=(ssize_t) runlength[y+z*iris_info.rows];\n                status=SGIDecode(bytes_per_pixel,(ssize_t)\n                  (runlength[y+z*iris_info.rows]/bytes_per_pixel),packets,\n                  1L*iris_info.columns,p+bytes_per_pixel*z);\n                if (status == MagickFalse)\n                  ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              }\n              p+=(iris_info.columns*4*bytes_per_pixel);\n            }\n            offset=(ssize_t) SeekBlob(image,position,SEEK_SET);\n          }\n        packet_info=RelinquishVirtualMemory(packet_info);\n        runlength=(size_t *) RelinquishMagickMemory(runlength);\n        offsets=(ssize_t *) RelinquishMagickMemory(offsets);\n      }\n    /*\n      Initialize image structure.\n    */\n    image->alpha_trait=iris_info.depth == 4 ? BlendPixelTrait : \n      UndefinedPixelTrait;\n    image->columns=iris_info.columns;\n    image->rows=iris_info.rows;\n    /*\n      Convert SGI raster image to pixel packets.\n    */\n    if (image->storage_class == DirectClass)\n      {\n        /*\n          Convert SGI image to DirectClass pixel packets.\n        */\n        if (bytes_per_pixel == 2)\n          {\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              p=pixels+(image->rows-y-1)*8*image->columns;\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                SetPixelRed(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+0) << 8) | (*(p+1)))),q);\n                SetPixelGreen(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+2) << 8) | (*(p+3)))),q);\n                SetPixelBlue(image,ScaleShortToQuantum((unsigned short)\n                  ((*(p+4) << 8) | (*(p+5)))),q);\n                SetPixelAlpha(image,OpaqueAlpha,q);\n                if (image->alpha_trait != UndefinedPixelTrait)\n                  SetPixelAlpha(image,ScaleShortToQuantum((unsigned short)\n                    ((*(p+6) << 8) | (*(p+7)))),q);\n                p+=8;\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n          }\n        else\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=pixels+(image->rows-y-1)*4*image->columns;\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (Quantum *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelRed(image,ScaleCharToQuantum(*p),q);\n              SetPixelGreen(image,ScaleCharToQuantum(*(p+1)),q);\n              SetPixelBlue(image,ScaleCharToQuantum(*(p+2)),q);\n              SetPixelAlpha(image,OpaqueAlpha,q);\n              if (image->alpha_trait != UndefinedPixelTrait)\n                SetPixelAlpha(image,ScaleCharToQuantum(*(p+3)),q);\n              p+=4;\n              q+=GetPixelChannels(image);\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                  image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n      }\n    else\n      {\n        /*\n          Create grayscale map.\n        */\n        if (AcquireImageColormap(image,image->colors,exception) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        /*\n          Convert SGI image to PseudoClass pixel packets.\n        */\n        if (bytes_per_pixel == 2)\n          {\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              p=pixels+(image->rows-y-1)*8*image->columns;\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                quantum=(*p << 8);\n                quantum|=(*(p+1));\n                SetPixelIndex(image,(Quantum) quantum,q);\n                p+=8;\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n          }\n        else\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=pixels+(image->rows-y-1)*4*image->columns;\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (Quantum *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelIndex(image,*p,q);\n              p+=4;\n              q+=GetPixelChannels(image);\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n        (void) SyncImage(image,exception);\n      }\n    pixel_info=RelinquishVirtualMemory(pixel_info);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    iris_info.magic=ReadBlobMSBShort(image);\n    if (iris_info.magic == 0x01DA)\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image,exception);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while (iris_info.magic == 0x01DA);\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/7afcf9f71043df15508e46f079387bd4689a738d", "file_name": "coders/sgi.c", "vul_type": "cwe-125", "description": "Write a C function to read and process an SGI image file."}
{"func_name": "crypto_skcipher_init_tfm", "func_src_before": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}", "func_src_after": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/9933e113c2e87a9f46a40fde8dafbf801dca1ab9", "file_name": "crypto/skcipher.c", "vul_type": "cwe-476", "description": "Write a C function named `crypto_skcipher_init_tfm` that initializes a symmetric key cipher transformation context."}
{"func_name": "get_requested_month_for_inverter", "func_src_before": "    def get_requested_month_for_inverter(self, inverter_serial, date):\n        data = dict()\n\n        month_start, month_end = self.get_epoch_month(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(month_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(month_end, self.local_timezone)}\n        month_total = 0\n\n        query = '''\n            SELECT TimeStamp, DayYield AS Power \n            FROM MonthData \n            WHERE TimeStamp BETWEEN %s AND %s AND Serial = %s\n            '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (month_start, month_end, inverter_serial)):\n            data['data'].append({'time': self.convert_local_ts_to_utc(row[0], self.local_timezone), 'power': row[1]})\n            month_total += row[1]\n\n        data['total'] = month_total\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM MonthData \n            WHERE Serial = %s;\n            ''' % inverter_serial\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if first_data: data['hasPrevious'] = (first_data < month_start)\n        else: data['hasPrevious'] = False\n        if last_data: data['hasNext'] = (last_data > month_end)\n        else: data['hasNext'] = False\n\n        return data", "func_src_after": "    def get_requested_month_for_inverter(self, inverter_serial, date):\n        data = dict()\n\n        month_start, month_end = self.get_epoch_month(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(month_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(month_end, self.local_timezone)}\n        month_total = 0\n\n        query = '''\n            SELECT TimeStamp, DayYield AS Power \n            FROM MonthData \n            WHERE TimeStamp BETWEEN ? AND ? AND Serial=?;\n            '''\n\n        data['data'] = list()\n        for row in self.c.execute(query, (month_start, month_end, inverter_serial)):\n            data['data'].append({'time': self.convert_local_ts_to_utc(row[0], self.local_timezone), 'power': row[1]})\n            month_total += row[1]\n\n        data['total'] = month_total\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM MonthData \n            WHERE Serial=?;\n            '''\n\n        self.c.execute(query, (inverter_serial,))\n        first_data, last_data = self.c.fetchone()\n\n        if first_data: data['hasPrevious'] = (first_data < month_start)\n        else: data['hasPrevious'] = False\n        if last_data: data['hasNext'] = (last_data > month_end)\n        else: data['hasNext'] = False\n\n        return data", "commit_link": "github.com/philipptrenz/sunportal/commit/7eef493a168ed4e6731ff800713bfb8aee99a506", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch and summarize monthly power yield data for a given inverter and date, including checks for previous and next data availability."}
{"func_name": "_add_to_db", "func_src_before": "    @staticmethod\n    def _add_to_db(user):\n        \"\"\"\n        Adds User object to the database\n        :param user: User object with info about user\n        :return: None\n        \"\"\"\n        query = (\"INSERT INTO users (chat_id, first_name, nickname, \"\n                 \"last_name, language) \"\n                 f\"VALUES ({user.chat_id}, '{user.first_name}', \"\n                 f\"'{user.nickname}', '{user.last_name}', '{user.language}')\")\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Cannot add user to the database\")\n        else:\n            log.info(f\"User {user} was successfully added to the users db\")", "func_src_after": "    @staticmethod\n    def _add_to_db(user):\n        \"\"\"\n        Adds User object to the database\n        :param user: User object with info about user\n        :return: None\n        \"\"\"\n        query = (\"INSERT INTO users (chat_id, first_name, nickname, \"\n                 \"last_name, language) \"\n                 f\"VALUES (%s, %s, %s, %s, %s)\")\n\n        parameters = (user.chat_id, user.first_name, user.nickname,\n                      user.last_name, user.language)\n\n        try:\n            db.add(query, parameters)\n        except DatabaseError:\n            log.error(\"Cannot add user to the database\")\n        else:\n            log.info(f\"User {user} was successfully added to the users db\")", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "Write a Python method to insert a user object into a database with error handling."}
{"func_name": "sctp_sf_ootb", "func_src_before": "sctp_disposition_t sctp_sf_ootb(struct net *net,\n\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\tconst sctp_subtype_t type,\n\t\t\t\tvoid *arg,\n\t\t\t\tsctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sk_buff *skb = chunk->skb;\n\tsctp_chunkhdr_t *ch;\n\tsctp_errhdr_t *err;\n\t__u8 *ch_end;\n\tint ootb_shut_ack = 0;\n\tint ootb_cookie_ack = 0;\n\n\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\n\tch = (sctp_chunkhdr_t *) chunk->chunk_hdr;\n\tdo {\n\t\t/* Report violation if the chunk is less then minimal */\n\t\tif (ntohs(ch->length) < sizeof(sctp_chunkhdr_t))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Now that we know we at least have a chunk header,\n\t\t * do things that are type appropriate.\n\t\t */\n\t\tif (SCTP_CID_SHUTDOWN_ACK == ch->type)\n\t\t\tootb_shut_ack = 1;\n\n\t\t/* RFC 2960, Section 3.3.7\n\t\t *   Moreover, under any circumstances, an endpoint that\n\t\t *   receives an ABORT  MUST NOT respond to that ABORT by\n\t\t *   sending an ABORT of its own.\n\t\t */\n\t\tif (SCTP_CID_ABORT == ch->type)\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\t/* RFC 8.4, 7) If the packet contains a \"Stale cookie\" ERROR\n\t\t * or a COOKIE ACK the SCTP Packet should be silently\n\t\t * discarded.\n\t\t */\n\n\t\tif (SCTP_CID_COOKIE_ACK == ch->type)\n\t\t\tootb_cookie_ack = 1;\n\n\t\tif (SCTP_CID_ERROR == ch->type) {\n\t\t\tsctp_walk_errors(err, ch) {\n\t\t\t\tif (SCTP_ERROR_STALE_COOKIE == err->cause) {\n\t\t\t\t\tootb_cookie_ack = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* Report violation if chunk len overflows */\n\t\tch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));\n\t\tif (ch_end > skb_tail_pointer(skb))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\tch = (sctp_chunkhdr_t *) ch_end;\n\t} while (ch_end < skb_tail_pointer(skb));\n\n\tif (ootb_shut_ack)\n\t\treturn sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands);\n\telse if (ootb_cookie_ack)\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\telse\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n}", "func_src_after": "sctp_disposition_t sctp_sf_ootb(struct net *net,\n\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\tconst sctp_subtype_t type,\n\t\t\t\tvoid *arg,\n\t\t\t\tsctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sk_buff *skb = chunk->skb;\n\tsctp_chunkhdr_t *ch;\n\tsctp_errhdr_t *err;\n\t__u8 *ch_end;\n\tint ootb_shut_ack = 0;\n\tint ootb_cookie_ack = 0;\n\n\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\n\tch = (sctp_chunkhdr_t *) chunk->chunk_hdr;\n\tdo {\n\t\t/* Report violation if the chunk is less then minimal */\n\t\tif (ntohs(ch->length) < sizeof(sctp_chunkhdr_t))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Report violation if chunk len overflows */\n\t\tch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));\n\t\tif (ch_end > skb_tail_pointer(skb))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Now that we know we at least have a chunk header,\n\t\t * do things that are type appropriate.\n\t\t */\n\t\tif (SCTP_CID_SHUTDOWN_ACK == ch->type)\n\t\t\tootb_shut_ack = 1;\n\n\t\t/* RFC 2960, Section 3.3.7\n\t\t *   Moreover, under any circumstances, an endpoint that\n\t\t *   receives an ABORT  MUST NOT respond to that ABORT by\n\t\t *   sending an ABORT of its own.\n\t\t */\n\t\tif (SCTP_CID_ABORT == ch->type)\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\t/* RFC 8.4, 7) If the packet contains a \"Stale cookie\" ERROR\n\t\t * or a COOKIE ACK the SCTP Packet should be silently\n\t\t * discarded.\n\t\t */\n\n\t\tif (SCTP_CID_COOKIE_ACK == ch->type)\n\t\t\tootb_cookie_ack = 1;\n\n\t\tif (SCTP_CID_ERROR == ch->type) {\n\t\t\tsctp_walk_errors(err, ch) {\n\t\t\t\tif (SCTP_ERROR_STALE_COOKIE == err->cause) {\n\t\t\t\t\tootb_cookie_ack = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tch = (sctp_chunkhdr_t *) ch_end;\n\t} while (ch_end < skb_tail_pointer(skb));\n\n\tif (ootb_shut_ack)\n\t\treturn sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands);\n\telse if (ootb_cookie_ack)\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\telse\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n}", "commit_link": "github.com/torvalds/linux/commit/bf911e985d6bbaa328c20c3e05f4eb03de11fdd6", "file_name": "net/sctp/sm_statefuns.c", "vul_type": "cwe-125", "description": "Write a C function named `sctp_sf_ootb` that handles out-of-the-blue SCTP packets by checking for protocol violations and responding appropriately."}
{"func_name": "Writer::Writer", "func_src_before": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 128);\n\tpath[128] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "func_src_after": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 255);\n\tpath[255] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 128);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[128] = '\\0';\n"}], "added": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 255);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[255] = '\\0';\n"}]}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 90, "chars": "128"}, {"char_start": 99, "char_end": 102, "chars": "128"}], "added": [{"char_start": 87, "char_end": 90, "chars": "255"}, {"char_start": 99, "char_end": 102, "chars": "255"}]}, "commit_link": "github.com/meskio/tudu/commit/c51f4c2f92288f923cf33bdc395501f447fe2d5c", "file_name": "parser.cc", "vul_type": "cwe-119", "commit_msg": "Fix out-of-bounds access", "parent_commit": "30923dcb8b7682fec1bdfbf07f904e4d9983e623", "description": "Create a C++ class constructor for a `Writer` class that initializes a `ToDo` object and copies a file path string with a fixed length."}
{"func_name": "repack", "func_src_before": "def repack(host, targets, channel='stable'):\n  url = 'https://static.rust-lang.org/dist/channel-rust-' + channel + '.toml'\n  req = requests.get(url)\n  req.raise_for_status()\n  manifest = toml.loads(req.content)\n  if manifest['manifest-version'] != '2':\n    print('ERROR: unrecognized manifest version %s.' % manifest['manifest-version'])\n    return\n  print('Using manifest for rust %s as of %s.' % (channel, manifest['date']))\n  rustc_version, rustc = package(manifest, 'rustc', host)\n  if rustc['available']:\n    print('rustc %s\\n  %s\\n  %s' % (rustc_version, rustc['url'], rustc['hash']))\n    fetch(rustc['url'])\n  cargo_version, cargo = package(manifest, 'cargo', host)\n  if cargo['available']:\n    print('cargo %s\\n  %s\\n  %s' % (cargo_version, cargo['url'], cargo['hash']))\n    fetch(cargo['url'])\n  stds = []\n  for target in targets:\n      version, info = package(manifest, 'rust-std', target)\n      if info['available']:\n        print('rust-std %s\\n  %s\\n  %s' % (version, info['url'], info['hash']))\n        fetch(info['url'])\n        stds.append(info)\n  print('Installing packages...')\n  tar_basename = 'rustc-%s-repack' % host\n  install_dir = 'rustc'\n  os.system('rm -rf %s' % install_dir)\n  install(os.path.basename(rustc['url']), install_dir)\n  install(os.path.basename(cargo['url']), install_dir)\n  for std in stds:\n    install(os.path.basename(std['url']), install_dir)\n  print('Tarring %s...' % tar_basename)\n  os.system('tar cjf %s.tar.bz2 %s/*' % (tar_basename, install_dir))\n  os.system('rm -rf %s' % install_dir)", "func_src_after": "def repack(host, targets, channel='stable'):\n  print(\"Repacking rust for %s...\" % host)\n  url = 'https://static.rust-lang.org/dist/channel-rust-' + channel + '.toml'\n  req = requests.get(url)\n  req.raise_for_status()\n  manifest = toml.loads(req.content)\n  if manifest['manifest-version'] != '2':\n    print('ERROR: unrecognized manifest version %s.' % manifest['manifest-version'])\n    return\n  print('Using manifest for rust %s as of %s.' % (channel, manifest['date']))\n  rustc_version, rustc = package(manifest, 'rustc', host)\n  if rustc['available']:\n    print('rustc %s\\n  %s\\n  %s' % (rustc_version, rustc['url'], rustc['hash']))\n    fetch(rustc['url'])\n  cargo_version, cargo = package(manifest, 'cargo', host)\n  if cargo['available']:\n    print('cargo %s\\n  %s\\n  %s' % (cargo_version, cargo['url'], cargo['hash']))\n    fetch(cargo['url'])\n  stds = []\n  for target in targets:\n      version, info = package(manifest, 'rust-std', target)\n      if info['available']:\n        print('rust-std %s\\n  %s\\n  %s' % (version, info['url'], info['hash']))\n        fetch(info['url'])\n        stds.append(info)\n  print('Installing packages...')\n  tar_basename = 'rustc-%s-repack' % host\n  install_dir = 'rustc'\n  subprocess.check_call(['rm', '-rf', install_dir])\n  install(os.path.basename(rustc['url']), install_dir)\n  install(os.path.basename(cargo['url']), install_dir)\n  for std in stds:\n    install(os.path.basename(std['url']), install_dir)\n  print('Tarring %s...' % tar_basename)\n  subprocess.check_call(['tar', 'cjf', tar_basename + '.tar.bz2', install_dir])\n  subprocess.check_call(['rm', '-rf', install_dir])", "commit_link": "github.com/rillian/rust-build/commit/b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb", "file_name": "repack_rust.py", "vul_type": "cwe-078", "description": "In Python, write a function named `repack` that downloads and installs specific Rust packages for a given host and target platforms, then repackages them into a tarball."}
{"func_name": "mapi_attr_read", "func_src_before": "mapi_attr_read (size_t len, unsigned char *buf)\n{\n    size_t idx = 0;\n    uint32 i,j;\n    assert(len > 4);\n    uint32 num_properties = GETINT32(buf+idx);\n    MAPI_Attr** attrs = CHECKED_XMALLOC (MAPI_Attr*, (num_properties + 1));\n\n    idx += 4;\n\n    if (!attrs) return NULL;\n    for (i = 0; i < num_properties; i++)\n    {\n\tMAPI_Attr* a = attrs[i] = CHECKED_XCALLOC(MAPI_Attr, 1);\n\tMAPI_Value* v = NULL;\n\n\tCHECKINT16(idx, len); a->type = GETINT16(buf+idx); idx += 2;\n\tCHECKINT16(idx, len); a->name = GETINT16(buf+idx); idx += 2;\n\n\t/* handle special case of GUID prefixed properties */\n\tif (a->name & GUID_EXISTS_FLAG)\n\t{\n\t    /* copy GUID */\n\t    a->guid = CHECKED_XMALLOC(GUID, 1);\n\t    copy_guid_from_buf(a->guid, buf+idx, len);\n\t    idx += sizeof (GUID);\n\n\t    CHECKINT32(idx, len); a->num_names = GETINT32(buf+idx); idx += 4;\n\t    if (a->num_names > 0)\n\t    {\n\t\t/* FIXME: do something useful here! */\n\t\tsize_t i;\n\n\t\ta->names = CHECKED_XCALLOC(VarLenData, a->num_names);\n\n\t\tfor (i = 0; i < a->num_names; i++)\n\t\t{\n\t\t    size_t j;\n\n\t\t    CHECKINT32(idx, len); a->names[i].len = GETINT32(buf+idx); idx += 4;\n\n\t\t    /* read the data into a buffer */\n\t\t    a->names[i].data \n\t\t\t= CHECKED_XMALLOC(unsigned char, a->names[i].len);\n\t\t    for (j = 0; j < (a->names[i].len >> 1); j++)\n\t\t\ta->names[i].data[j] = (buf+idx)[j*2];\n\n\t\t    /* But what are we going to do with it? */\n\t\t    \n\t\t    idx += pad_to_4byte(a->names[i].len);\n\t\t}\n\t    }\n\t    else\n\t    {\n\t\t/* get the 'real' name */\n\t\tCHECKINT32(idx, len); a->name = GETINT32(buf+idx); idx+= 4;\n\t    }\n\t}\n\n\t/* \n\t * Multi-value types and string/object/binary types have\n\t * multiple values \n\t */\n\tif (a->type & MULTI_VALUE_FLAG ||\n\t    a->type == szMAPI_STRING ||\n\t    a->type == szMAPI_UNICODE_STRING ||\n\t    a->type == szMAPI_OBJECT ||\n\t    a->type == szMAPI_BINARY)\n\t{\n\t    CHECKINT32(idx, len); a->num_values = GETINT32(buf+idx);\n\t    idx += 4;\n\t}\n        else\n        {\n\t    a->num_values = 1;\n        }\n\n\t/* Amend the type in case of multi-value type */\n\tif (a->type & MULTI_VALUE_FLAG)\n\t{\n\t    a->type -= MULTI_VALUE_FLAG;\n\t}\n\n\n\tv = alloc_mapi_values (a);\n\n\tfor (j = 0; j < a->num_values; j++) \n\t{\n\t    switch (a->type)\n\t    {\n\t    case szMAPI_SHORT:\t/* 2 bytes */\n\t\tv->len = 2;\n\t\tCHECKINT16(idx, len); v->data.bytes2 = GETINT16(buf+idx);\n\t\tidx += 4;\t/* assume padding of 2, advance by 4! */\n\t\tbreak;\n\n\t    case szMAPI_INT:\t/* 4 bytes */\n\t\tv->len = 4;\n\t\tCHECKINT32(idx, len); v->data.bytes4 = GETINT32(buf+idx);\n\t\tidx += 4;\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_FLOAT:\t/* 4 bytes */\n\t    case szMAPI_BOOLEAN: /* this should be 2 bytes + 2 padding */\n\t\tv->len = 4;\n\t\tCHECKINT32(idx, len); v->data.bytes4 = GETINT32(buf+idx);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_SYSTIME: /* 8 bytes */\n\t\tv->len = 8;\n\t\tCHECKINT32(idx, len); v->data.bytes8[0] = GETINT32(buf+idx);\n\t\tCHECKINT32(idx+4, len); v->data.bytes8[1] = GETINT32(buf+idx+4);\n\t\tidx += 8;\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_DOUBLE:\t/* 8 bytes */\n\t    case szMAPI_APPTIME:\n\t    case szMAPI_CURRENCY:\n\t    case szMAPI_INT8BYTE:\n\t\tv->len = 8;\n\t\tCHECKINT32(idx, len); v->data.bytes8[0] = GETINT32(buf+idx);\n\t\tCHECKINT32(idx+4, len); v->data.bytes8[1] = GETINT32(buf+idx+4);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_CLSID:\n\t\tv->len = sizeof (GUID);\n\t\tcopy_guid_from_buf(&v->data.guid, buf+idx, len);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_STRING:\n\t    case szMAPI_UNICODE_STRING:\n\t    case szMAPI_OBJECT:\n\t    case szMAPI_BINARY:\n\t\tCHECKINT32(idx, len); v->len = GETINT32(buf+idx); idx += 4;\n\n\t\tif (a->type == szMAPI_UNICODE_STRING)\n\t\t{\n\t\t    v->data.buf = (unsigned char*)unicode_to_utf8(v->len, buf+idx);\n\t\t}\n\t\telse\n\t\t{\n\t\t    v->data.buf = CHECKED_XMALLOC(unsigned char, v->len);\n\t\t    memmove (v->data.buf, buf+idx, v->len);\n\t\t}\n\n\t\tidx += pad_to_4byte(v->len);\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_NULL:\t/* illegal in input tnef streams */\n\t    case szMAPI_ERROR:\n\t    case szMAPI_UNSPECIFIED:\n\t\tfprintf (stderr,\n\t\t\t \"Invalid attribute, input file may be corrupted\\n\");\n\t\tif (!ENCODE_SKIP) exit (1);\n\n\t\treturn NULL;\n\n\t    default:\t\t/* should never get here */\n\t\tfprintf (stderr,\n\t\t\t \"Undefined attribute, input file may be corrupted\\n\");\n\t\tif (!ENCODE_SKIP) exit (1);\n\n\t\treturn NULL;\n\n\t    }\n\t    if (DEBUG_ON) mapi_attr_dump (attrs[i]);\n\t}\n    }\n    attrs[i] = NULL;\n\n    return attrs;\n}", "func_src_after": "mapi_attr_read (size_t len, unsigned char *buf)\n{\n    size_t idx = 0;\n    uint32 i,j;\n    assert(len > 4);\n    uint32 num_properties = GETINT32(buf+idx);\n    assert((num_properties+1) != 0);\n    MAPI_Attr** attrs = CHECKED_XMALLOC (MAPI_Attr*, (num_properties + 1));\n\n    idx += 4;\n\n    if (!attrs) return NULL;\n    for (i = 0; i < num_properties; i++)\n    {\n\tMAPI_Attr* a = attrs[i] = CHECKED_XCALLOC(MAPI_Attr, 1);\n\tMAPI_Value* v = NULL;\n\n\tCHECKINT16(idx, len); a->type = GETINT16(buf+idx); idx += 2;\n\tCHECKINT16(idx, len); a->name = GETINT16(buf+idx); idx += 2;\n\n\t/* handle special case of GUID prefixed properties */\n\tif (a->name & GUID_EXISTS_FLAG)\n\t{\n\t    /* copy GUID */\n\t    a->guid = CHECKED_XMALLOC(GUID, 1);\n\t    copy_guid_from_buf(a->guid, buf+idx, len);\n\t    idx += sizeof (GUID);\n\n\t    CHECKINT32(idx, len); a->num_names = GETINT32(buf+idx); idx += 4;\n\t    if (a->num_names > 0)\n\t    {\n\t\t/* FIXME: do something useful here! */\n\t\tsize_t i;\n\n\t\ta->names = CHECKED_XCALLOC(VarLenData, a->num_names);\n\n\t\tfor (i = 0; i < a->num_names; i++)\n\t\t{\n\t\t    size_t j;\n\n\t\t    CHECKINT32(idx, len); a->names[i].len = GETINT32(buf+idx); idx += 4;\n\n\t\t    /* read the data into a buffer */\n\t\t    a->names[i].data \n\t\t\t= CHECKED_XMALLOC(unsigned char, a->names[i].len);\n\t\t    assert((idx+(a->names[i].len*2)) <= len);\n\t\t    for (j = 0; j < (a->names[i].len >> 1); j++)\n\t\t\ta->names[i].data[j] = (buf+idx)[j*2];\n\n\t\t    /* But what are we going to do with it? */\n\t\t    \n\t\t    idx += pad_to_4byte(a->names[i].len);\n\t\t}\n\t    }\n\t    else\n\t    {\n\t\t/* get the 'real' name */\n\t\tCHECKINT32(idx, len); a->name = GETINT32(buf+idx); idx+= 4;\n\t    }\n\t}\n\n\t/* \n\t * Multi-value types and string/object/binary types have\n\t * multiple values \n\t */\n\tif (a->type & MULTI_VALUE_FLAG ||\n\t    a->type == szMAPI_STRING ||\n\t    a->type == szMAPI_UNICODE_STRING ||\n\t    a->type == szMAPI_OBJECT ||\n\t    a->type == szMAPI_BINARY)\n\t{\n\t    CHECKINT32(idx, len); a->num_values = GETINT32(buf+idx);\n\t    idx += 4;\n\t}\n        else\n        {\n\t    a->num_values = 1;\n        }\n\n\t/* Amend the type in case of multi-value type */\n\tif (a->type & MULTI_VALUE_FLAG)\n\t{\n\t    a->type -= MULTI_VALUE_FLAG;\n\t}\n\n\n\tv = alloc_mapi_values (a);\n\n\tfor (j = 0; j < a->num_values; j++) \n\t{\n\t    switch (a->type)\n\t    {\n\t    case szMAPI_SHORT:\t/* 2 bytes */\n\t\tv->len = 2;\n\t\tCHECKINT16(idx, len); v->data.bytes2 = GETINT16(buf+idx);\n\t\tidx += 4;\t/* assume padding of 2, advance by 4! */\n\t\tbreak;\n\n\t    case szMAPI_INT:\t/* 4 bytes */\n\t\tv->len = 4;\n\t\tCHECKINT32(idx, len); v->data.bytes4 = GETINT32(buf+idx);\n\t\tidx += 4;\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_FLOAT:\t/* 4 bytes */\n\t    case szMAPI_BOOLEAN: /* this should be 2 bytes + 2 padding */\n\t\tv->len = 4;\n\t\tCHECKINT32(idx, len); v->data.bytes4 = GETINT32(buf+idx);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_SYSTIME: /* 8 bytes */\n\t\tv->len = 8;\n\t\tCHECKINT32(idx, len); v->data.bytes8[0] = GETINT32(buf+idx);\n\t\tCHECKINT32(idx+4, len); v->data.bytes8[1] = GETINT32(buf+idx+4);\n\t\tidx += 8;\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_DOUBLE:\t/* 8 bytes */\n\t    case szMAPI_APPTIME:\n\t    case szMAPI_CURRENCY:\n\t    case szMAPI_INT8BYTE:\n\t\tv->len = 8;\n\t\tCHECKINT32(idx, len); v->data.bytes8[0] = GETINT32(buf+idx);\n\t\tCHECKINT32(idx+4, len); v->data.bytes8[1] = GETINT32(buf+idx+4);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_CLSID:\n\t\tv->len = sizeof (GUID);\n\t\tcopy_guid_from_buf(&v->data.guid, buf+idx, len);\n\t\tidx += v->len;\n\t\tbreak;\n\n\t    case szMAPI_STRING:\n\t    case szMAPI_UNICODE_STRING:\n\t    case szMAPI_OBJECT:\n\t    case szMAPI_BINARY:\n\t\tCHECKINT32(idx, len); v->len = GETINT32(buf+idx); idx += 4;\n\n\t\tassert(v->len + idx <= len);\n\n\t\tif (a->type == szMAPI_UNICODE_STRING)\n\t\t{\n\t\t    assert(v->len != 0);\n\t\t    v->data.buf = (unsigned char*)unicode_to_utf8(v->len, buf+idx);\n\t\t}\n\t\telse\n\t\t{\n\t\t    v->data.buf = CHECKED_XMALLOC(unsigned char, v->len);\n\t\t    memmove (v->data.buf, buf+idx, v->len);\n\t\t}\n\n\t\tidx += pad_to_4byte(v->len);\n\t\tv++;\n\t\tbreak;\n\n\t    case szMAPI_NULL:\t/* illegal in input tnef streams */\n\t    case szMAPI_ERROR:\n\t    case szMAPI_UNSPECIFIED:\n\t\tfprintf (stderr,\n\t\t\t \"Invalid attribute, input file may be corrupted\\n\");\n\t\tif (!ENCODE_SKIP) exit (1);\n\n\t\treturn NULL;\n\n\t    default:\t\t/* should never get here */\n\t\tfprintf (stderr,\n\t\t\t \"Undefined attribute, input file may be corrupted\\n\");\n\t\tif (!ENCODE_SKIP) exit (1);\n\n\t\treturn NULL;\n\n\t    }\n\t    if (DEBUG_ON) mapi_attr_dump (attrs[i]);\n\t}\n    }\n    attrs[i] = NULL;\n\n    return attrs;\n}", "commit_link": "github.com/verdammelt/tnef/commit/1a17af1ed0c791aec44dbdc9eab91218cc1e335a", "file_name": "src/mapi_attr.c", "vul_type": "cwe-787", "description": "Write a C function named `mapi_attr_read` that parses MAPI attributes from a buffer."}
{"func_name": "self.register_user_defined_tag_type", "func_src_before": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.load(config_source)\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "func_src_after": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.safe_load(config_source, [Symbol])\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 59, "char_end": 99, "line": "      config = YAML.load(config_source)\n"}], "added": [{"line_no": 2, "char_start": 59, "char_end": 114, "line": "      config = YAML.safe_load(config_source, [Symbol])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 79, "char_end": 84, "chars": "safe_"}, {"char_start": 102, "char_end": 112, "chars": ", [Symbol]"}]}, "commit_link": "github.com/nico-hn/AdHocTemplate/commit/4bc4ed79a2c45d64df03029bd05c3a426f5df020", "file_name": "parser.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.safe_load() instead of .load()", "parent_commit": "f602247a29214ffce7e8d24caf690c2a420c3e99", "description": "Write a Ruby method to register a user-defined tag type from a YAML configuration source."}
{"func_name": "load_config", "func_src_before": "def load_config(config_file):\n    config_path = config_file if config_file else \".ansible-lint\"\n\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as stream:\n            try:\n                return yaml.load(stream)\n            except yaml.YAMLError:\n                pass\n\n    return None", "func_src_after": "def load_config(config_file):\n    config_path = config_file if config_file else \".ansible-lint\"\n\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as stream:\n            try:\n                return yaml.safe_load(stream)\n            except yaml.YAMLError:\n                pass\n\n    return None", "line_changes": {"deleted": [{"line_no": 7, "char_start": 197, "char_end": 238, "line": "                return yaml.load(stream)\n"}], "added": [{"line_no": 7, "char_start": 197, "char_end": 243, "line": "                return yaml.safe_load(stream)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 225, "char_end": 230, "chars": "safe_"}]}, "commit_link": "github.com/MatrixCrawler/ansible-lint/commit/c8685daee3f53ea0889ec697ff61c20996904381", "file_name": "__main__.py", "vul_type": "cwe-502", "commit_msg": "Use yaml.safe_load rather than yaml.load", "parent_commit": "f92cc06ab26ef24c1cdcdefaab65276c2424a90c", "description": "Write a Python function to load a YAML configuration from a file, with a default filename fallback."}
{"func_name": "ReadPSDChannel", "func_src_before": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      mask->matte=MagickFalse;\n      channel_image=mask;\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "func_src_after": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      if (mask != (Image *) NULL)\n        {\n          mask->matte=MagickFalse;\n          channel_image=mask;\n        }\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/7f2dc7a1afc067d0c89f12c82bcdec0445fb1b94", "file_name": "coders/psd.c", "vul_type": "cwe-476", "description": "In C, write a function to read a channel from a PSD file, handling different compression types and optional masks."}
{"func_name": "compact_upto_test", "func_src_before": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[16];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "func_src_after": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[32];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "line_changes": {"deleted": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[16];\n"}], "added": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[32];\n"}]}, "char_changes": {"deleted": [{"char_start": 523, "char_end": 525, "chars": "16"}], "added": [{"char_start": 523, "char_end": 525, "chars": "32"}]}, "commit_link": "github.com/hisundar/forestdb/commit/1df4c96057712be6ccf6614419ebcb2a01bb87f7", "file_name": "compact_functional_test.cc", "vul_type": "cwe-787", "commit_msg": "fix buffer overflow in compact_functional_test\n\nChange-Id: I1d4f79f8abfc96eaf546d05800a2eccdf0c828f6", "parent_commit": "6ef65b54f9324000de89e11b8a8bd688393a380b", "description": "Write a C function named `compact_upto_test` that tests the compaction of a database up to a certain point using the ForestDB engine."}
{"func_name": "_map_vol_to_host", "func_src_before": "    def _map_vol_to_host(self, volume_name, host_name):\n        \"\"\"Create a mapping between a volume to a host.\"\"\"\n\n        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '\n                    'host %(host_name)s')\n                  % {'volume_name': volume_name, 'host_name': host_name})\n\n        # Check if this volume is already mapped to this host\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n\n        mapped_flag = False\n        result_lun = '-1'\n        if volume_name in mapping_data:\n            mapped_flag = True\n            result_lun = mapping_data[volume_name]['SCSI_id']\n        else:\n            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]\n            lun_used.sort()\n            # Assume all luns are taken to this point, and then try to find\n            # an unused one\n            result_lun = str(len(lun_used))\n            for index, n in enumerate(lun_used):\n                if n > index:\n                    result_lun = str(index)\n                    break\n\n        # Volume is not mapped to host, create a new LUN\n        if not mapped_flag:\n            ssh_cmd = ('svctask mkvdiskhostmap -host %(host_name)s -scsi '\n                       '%(result_lun)s %(volume_name)s' %\n                       {'host_name': host_name,\n                        'result_lun': result_lun,\n                        'volume_name': volume_name})\n            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)\n            if err and err.startswith('CMMVC6071E'):\n                if not self.configuration.storwize_svc_multihostmap_enabled:\n                    LOG.error(_('storwize_svc_multihostmap_enabled is set '\n                                'to False, Not allow multi host mapping'))\n                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\\\n                                    'was not created because the VDisk is '\\\n                                    'already mapped to a host.\\n\"'\n                    raise exception.CinderException(data=exception_msg)\n                ssh_cmd = ssh_cmd.replace('mkvdiskhostmap',\n                                          'mkvdiskhostmap -force')\n                # try to map one volume to multiple hosts\n                out, err = self._run_ssh(ssh_cmd)\n                LOG.warn(_('volume %s mapping to multi host') % volume_name)\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n            else:\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '\n                    '%(volume_name)s, host %(host_name)s') %\n                  {'result_lun': result_lun,\n                   'volume_name': volume_name,\n                   'host_name': host_name})\n        return result_lun", "func_src_after": "    def _map_vol_to_host(self, volume_name, host_name):\n        \"\"\"Create a mapping between a volume to a host.\"\"\"\n\n        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '\n                    'host %(host_name)s')\n                  % {'volume_name': volume_name, 'host_name': host_name})\n\n        # Check if this volume is already mapped to this host\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n\n        mapped_flag = False\n        result_lun = '-1'\n        if volume_name in mapping_data:\n            mapped_flag = True\n            result_lun = mapping_data[volume_name]['SCSI_id']\n        else:\n            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]\n            lun_used.sort()\n            # Assume all luns are taken to this point, and then try to find\n            # an unused one\n            result_lun = str(len(lun_used))\n            for index, n in enumerate(lun_used):\n                if n > index:\n                    result_lun = str(index)\n                    break\n\n        # Volume is not mapped to host, create a new LUN\n        if not mapped_flag:\n            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,\n                       '-scsi', result_lun, volume_name]\n            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)\n            if err and err.startswith('CMMVC6071E'):\n                if not self.configuration.storwize_svc_multihostmap_enabled:\n                    LOG.error(_('storwize_svc_multihostmap_enabled is set '\n                                'to False, Not allow multi host mapping'))\n                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\\\n                                    'was not created because the VDisk is '\\\n                                    'already mapped to a host.\\n\"'\n                    raise exception.CinderException(data=exception_msg)\n\n                for i in range(len(ssh_cmd)):\n                    if ssh_cmd[i] == 'mkvdiskhostmap':\n                        ssh_cmd.insert(i + 1, '-force')\n\n                # try to map one volume to multiple hosts\n                out, err = self._run_ssh(ssh_cmd)\n                LOG.warn(_('volume %s mapping to multi host') % volume_name)\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n            else:\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '\n                    '%(volume_name)s, host %(host_name)s') %\n                  {'result_lun': result_lun,\n                   'volume_name': volume_name,\n                   'host_name': host_name})\n        return result_lun", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to map a storage volume to a host, handling existing mappings and LUN allocation."}
{"func_name": "lha_read_file_header_1", "func_src_before": "lha_read_file_header_1(struct archive_read *a, struct lha *lha)\n{\n\tconst unsigned char *p;\n\tsize_t extdsize;\n\tint i, err, err2;\n\tint namelen, padding;\n\tunsigned char headersum, sum_calculated;\n\n\terr = ARCHIVE_OK;\n\n\tif ((p = __archive_read_ahead(a, H1_FIXED_SIZE, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tlha->header_size = p[H1_HEADER_SIZE_OFFSET] + 2;\n\theadersum = p[H1_HEADER_SUM_OFFSET];\n\t/* Note: An extended header size is included in a compsize. */\n\tlha->compsize = archive_le32dec(p + H1_COMP_SIZE_OFFSET);\n\tlha->origsize = archive_le32dec(p + H1_ORIG_SIZE_OFFSET);\n\tlha->mtime = lha_dos_time(p + H1_DOS_TIME_OFFSET);\n\tnamelen = p[H1_NAME_LEN_OFFSET];\n\t/* Calculate a padding size. The result will be normally 0 only(?) */\n\tpadding = ((int)lha->header_size) - H1_FIXED_SIZE - namelen;\n\n\tif (namelen > 230 || padding < 0)\n\t\tgoto invalid;\n\n\tif ((p = __archive_read_ahead(a, lha->header_size, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tfor (i = 0; i < namelen; i++) {\n\t\tif (p[i + H1_FILE_NAME_OFFSET] == 0xff)\n\t\t\tgoto invalid;/* Invalid filename. */\n\t}\n\tarchive_strncpy(&lha->filename, p + H1_FILE_NAME_OFFSET, namelen);\n\tlha->crc = archive_le16dec(p + H1_FILE_NAME_OFFSET + namelen);\n\tlha->setflag |= CRC_IS_SET;\n\n\tsum_calculated = lha_calcsum(0, p, 2, lha->header_size - 2);\n\t/* Consume used bytes but not include `next header size' data\n\t * since it will be consumed in lha_read_file_extended_header(). */\n\t__archive_read_consume(a, lha->header_size - 2);\n\n\t/* Read extended headers */\n\terr2 = lha_read_file_extended_header(a, lha, NULL, 2,\n\t    (size_t)(lha->compsize + 2), &extdsize);\n\tif (err2 < ARCHIVE_WARN)\n\t\treturn (err2);\n\tif (err2 < err)\n\t\terr = err2;\n\t/* Get a real compressed file size. */\n\tlha->compsize -= extdsize - 2;\n\n\tif (sum_calculated != headersum) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_MISC,\n\t\t    \"LHa header sum error\");\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\treturn (err);\ninvalid:\n\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t    \"Invalid LHa header\");\n\treturn (ARCHIVE_FATAL);\n}", "func_src_after": "lha_read_file_header_1(struct archive_read *a, struct lha *lha)\n{\n\tconst unsigned char *p;\n\tsize_t extdsize;\n\tint i, err, err2;\n\tint namelen, padding;\n\tunsigned char headersum, sum_calculated;\n\n\terr = ARCHIVE_OK;\n\n\tif ((p = __archive_read_ahead(a, H1_FIXED_SIZE, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tlha->header_size = p[H1_HEADER_SIZE_OFFSET] + 2;\n\theadersum = p[H1_HEADER_SUM_OFFSET];\n\t/* Note: An extended header size is included in a compsize. */\n\tlha->compsize = archive_le32dec(p + H1_COMP_SIZE_OFFSET);\n\tlha->origsize = archive_le32dec(p + H1_ORIG_SIZE_OFFSET);\n\tlha->mtime = lha_dos_time(p + H1_DOS_TIME_OFFSET);\n\tnamelen = p[H1_NAME_LEN_OFFSET];\n\t/* Calculate a padding size. The result will be normally 0 only(?) */\n\tpadding = ((int)lha->header_size) - H1_FIXED_SIZE - namelen;\n\n\tif (namelen > 230 || padding < 0)\n\t\tgoto invalid;\n\n\tif ((p = __archive_read_ahead(a, lha->header_size, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tfor (i = 0; i < namelen; i++) {\n\t\tif (p[i + H1_FILE_NAME_OFFSET] == 0xff)\n\t\t\tgoto invalid;/* Invalid filename. */\n\t}\n\tarchive_strncpy(&lha->filename, p + H1_FILE_NAME_OFFSET, namelen);\n\tlha->crc = archive_le16dec(p + H1_FILE_NAME_OFFSET + namelen);\n\tlha->setflag |= CRC_IS_SET;\n\n\tsum_calculated = lha_calcsum(0, p, 2, lha->header_size - 2);\n\t/* Consume used bytes but not include `next header size' data\n\t * since it will be consumed in lha_read_file_extended_header(). */\n\t__archive_read_consume(a, lha->header_size - 2);\n\n\t/* Read extended headers */\n\terr2 = lha_read_file_extended_header(a, lha, NULL, 2,\n\t    (size_t)(lha->compsize + 2), &extdsize);\n\tif (err2 < ARCHIVE_WARN)\n\t\treturn (err2);\n\tif (err2 < err)\n\t\terr = err2;\n\t/* Get a real compressed file size. */\n\tlha->compsize -= extdsize - 2;\n\n\tif (lha->compsize < 0)\n\t\tgoto invalid;\t/* Invalid compressed file size */\n\n\tif (sum_calculated != headersum) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_MISC,\n\t\t    \"LHa header sum error\");\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\treturn (err);\ninvalid:\n\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t    \"Invalid LHa header\");\n\treturn (ARCHIVE_FATAL);\n}", "commit_link": "github.com/libarchive/libarchive/commit/98dcbbf0bf4854bf987557e55e55fff7abbf3ea9", "file_name": "libarchive/archive_read_support_format_lha.c", "vul_type": "cwe-125", "description": "Write a C function `lha_read_file_header_1` to read and validate the header of an LHA file archive."}
{"func_name": "Exiv2::WebPImage::getHeaderOffset", "func_src_before": "    long WebPImage::getHeaderOffset(byte *data, long data_size,\n                                    byte *header, long header_size) {\n        long pos = -1;\n        for (long i=0; i < data_size - header_size; i++) {\n            if (memcmp(header, &data[i], header_size) == 0) {\n                pos = i;\n                break;\n            }\n        }\n        return pos;\n    }", "func_src_after": "    long WebPImage::getHeaderOffset(byte* data, long data_size, byte* header, long header_size)\n    {\n        if (data_size < header_size) { return -1; }\n        long pos = -1;\n        for (long i=0; i < data_size - header_size; i++) {\n            if (memcmp(header, &data[i], header_size) == 0) {\n                pos = i;\n                break;\n            }\n        }\n        return pos;\n    }", "commit_link": "github.com/Exiv2/exiv2/commit/e925bc5addd881543fa503470c8a859e112cca62", "file_name": "src/webpimage.cpp", "vul_type": "cwe-190", "description": "Write a C++ function that finds the position of a header within a block of data and returns the index, or -1 if not found."}
{"func_name": "igraph_read_graph_graphml", "func_src_before": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "func_src_after": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "line_changes": {"deleted": [{"line_no": 56, "char_start": 2347, "char_end": 2420, "line": "                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n"}], "added": [{"line_no": 56, "char_start": 2347, "char_end": 2402, "line": "                                  XML_PARSE_NOBLANKS |\n"}]}, "char_changes": {"deleted": [{"char_start": 2380, "char_end": 2398, "chars": " XML_PARSE_NOENT |"}], "added": []}, "commit_link": "github.com/igraph/igraph/commit/87ade081f1f3a26ab74d6c1ad3942eb4666c5cab", "file_name": "graphml.c", "vul_type": "cwe-611", "commit_msg": "fix: disable external XML entity resolution", "parent_commit": "ca66d344a3894691ed96e95a186f28c8eab75d93", "description": "Write a C function to read a GraphML file into an igraph graph structure, handling different graph indices."}
{"func_name": "testPrintTensorsToFile", "func_src_before": "  def testPrintTensorsToFile(self):\n    tmpfile_name = tempfile.mktemp(\".printv2_test\")\n    tensor_0 = math_ops.range(0, 10)\n    print_op_0 = logging_ops.print_v2(tensor_0,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_0)\n    tensor_1 = math_ops.range(11, 20)\n    print_op_1 = logging_ops.print_v2(tensor_1,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_1)\n    try:\n      f = open(tmpfile_name, \"r\")\n      line_0 = f.readline()\n      expected_0 = \"[0 1 2 ... 7 8 9]\"\n      self.assertTrue(expected_0 in line_0)\n      line_1 = f.readline()\n      expected_1 = \"[11 12 13 ... 17 18 19]\"\n      self.assertTrue(expected_1 in line_1)\n      f.close()\n      os.remove(tmpfile_name)\n    except IOError as e:\n      self.fail(e)", "func_src_after": "  def testPrintTensorsToFile(self):\n    _, tmpfile_name = tempfile.mkstemp(\n        \".printv2_test\")  # safe to ignore fd here\n    tensor_0 = math_ops.range(0, 10)\n    print_op_0 = logging_ops.print_v2(tensor_0,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_0)\n    tensor_1 = math_ops.range(11, 20)\n    print_op_1 = logging_ops.print_v2(tensor_1,\n                                      output_stream=\"file://\"+tmpfile_name)\n    self.evaluate(print_op_1)\n    try:\n      f = open(tmpfile_name, \"r\")\n      line_0 = f.readline()\n      expected_0 = \"[0 1 2 ... 7 8 9]\"\n      self.assertTrue(expected_0 in line_0)\n      line_1 = f.readline()\n      expected_1 = \"[11 12 13 ... 17 18 19]\"\n      self.assertTrue(expected_1 in line_1)\n      f.close()\n      os.remove(tmpfile_name)\n    except IOError as e:\n      self.fail(e)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 36, "char_end": 88, "line": "    tmpfile_name = tempfile.mktemp(\".printv2_test\")\n"}], "added": [{"line_no": 2, "char_start": 36, "char_end": 76, "line": "    _, tmpfile_name = tempfile.mkstemp(\n"}, {"line_no": 3, "char_start": 76, "char_end": 127, "line": "        \".printv2_test\")  # safe to ignore fd here\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 39, "char_end": 42, "chars": " _,"}, {"char_start": 69, "char_end": 70, "chars": "s"}, {"char_start": 75, "char_end": 84, "chars": "\n        "}, {"char_start": 100, "char_end": 126, "chars": "  # safe to ignore fd here"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/247aaafbe7f689492797d92430e77443b011876c", "file_name": "logging_ops_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420360036\nChange-Id: I13eb94736af3397261cf0d46214ddb5a2af9d92b", "description": "Write a Python function to print two ranges of numbers to a temporary file and verify the output."}
{"func_name": "(anonymous)", "func_src_before": "    app.get('/static/*', function(req, res)\n    { \n      res.header(\"Server\", serverName);\n      var filePath = path.normalize(__dirname + \"/..\" + req.url.split(\"?\")[0]);\n      res.sendfile(filePath, { maxAge: exports.maxAge });\n    });", "func_src_after": "    app.get('/static/*', function(req, res)\n    { \n      res.header(\"Server\", serverName);\n      var filePath = path.normalize(__dirname + \"/..\" +\n                                    req.url.replace(/\\./g, '').split(\"?\")[0]);\n      res.sendfile(filePath, { maxAge: exports.maxAge });\n    });", "line_changes": {"deleted": [{"line_no": 4, "char_start": 91, "char_end": 171, "line": "      var filePath = path.normalize(__dirname + \"/..\" + req.url.split(\"?\")[0]);\n"}], "added": [{"line_no": 4, "char_start": 91, "char_end": 147, "line": "      var filePath = path.normalize(__dirname + \"/..\" +\n"}, {"line_no": 5, "char_start": 147, "char_end": 226, "line": "                                    req.url.replace(/\\./g, '').split(\"?\")[0]);\n"}]}, "char_changes": {"deleted": [{"char_start": 146, "char_end": 154, "chars": " req.url"}], "added": [{"char_start": 146, "char_end": 209, "chars": "\n                                    req.url.replace(/\\./g, '')"}]}, "commit_link": "github.com/thomasrussellmurphy/etherpad-lite/commit/86d3b2ba811aa8168eb01a5a345d3b717889ab8a", "file_name": "server.js", "vul_type": "cwe-022", "commit_msg": "Fix directory traversal\n\nSee https://ada.adrianlang.de/etherpad-lite-directory-traversal", "description": "Create a Node.js Express server route that serves static files with a custom server header and cache control."}
{"func_name": "update_history_and_sourcebyinstitution", "func_src_before": "def update_history_and_sourcebyinstitution(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Get all current sources and title numbers from Solr and log them into database.\n    \"\"\"\n    current_sources = get_all_current_sources(k10plus, ai)\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_sourcebyinstitutions = get_all_old_sourcebyinstitutions(conn, sqlite)\n    current_sourcebyinstitutions = []\n\n    for source in current_sources:\n\n        for institution in current_institutions:\n\n            if not institution or institution == \" \" or '\"' in institution:\n                continue\n\n            sourcebyinstitution = \"SID \" + str(source) + \" (\" + institution + \")\"\n            current_sourcebyinstitutions.append(sourcebyinstitution)\n\n            params = {\n                \"q\": 'source_id:%s AND institution:\"%s\"' % (source, institution),\n                \"rows\": 0,\n                \"wt\": \"json\"\n            }\n\n            # check k10plus\n            result = get_solr_result(k10plus, params)\n            number = result[\"response\"][\"numFound\"]\n            if number != 0:\n                sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (\"%s\", %s)' % (sourcebyinstitution, number)\n                sqlite.execute(sql)\n                conn.commit()\n            else:\n                # check ai\n                result = get_solr_result(ai, params)\n                number = result[\"response\"][\"numFound\"]\n                if number != 0:\n                    # TODO: escape via sqlite\n                    sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (\"%s\", %s)' % (sourcebyinstitution, number)\n                    sqlite.execute(sql)\n                    conn.commit()\n\n            if sourcebyinstitution not in old_sourcebyinstitutions:\n                logging.info(\"The %s is now connected to SID %s.\", institution, source)\n                sql = \"INSERT INTO sourcebyinstitution (sourcebyinstitution) VALUES ('%s')\" % sourcebyinstitution\n                sqlite.execute(sql)\n                conn.commit()\n\n            if number != 0:\n                old_sourcebyinstitution_number = get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution)\n                if number < old_sourcebyinstitution_number:\n                    message = \"Die Anzahl der Titel hat sich bei %s gegenueber einem frueheren Import verringert.\" % (sourcebyinstitution)\n                    send_message(message)\n\n            # requests.exceptions.ConnectionError: HTTPConnectionPool(XXXXXX): Max retries exceeded\n            time.sleep(0.25)\n\n    for old_sourcebyinstitution in old_sourcebyinstitutions:\n        if old_sourcebyinstitution not in current_sourcebyinstitutions:\n            message = \"Die %s ist nicht laenger f\u00fcr die SID %s angesigelt.\" % (institution, source)\n            send_message(message)", "func_src_after": "def update_history_and_sourcebyinstitution(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Get all current sources and title numbers from Solr and log them into database.\n    \"\"\"\n    current_sources = get_all_current_sources(k10plus, ai)\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_sourcebyinstitutions = get_all_old_sourcebyinstitutions(conn, sqlite)\n    current_sourcebyinstitutions = []\n\n    for source in current_sources:\n\n        for institution in current_institutions:\n\n            if not institution or institution == \" \" or '\"' in institution:\n                continue\n\n            sourcebyinstitution = \"SID \" + str(source) + \" (\" + institution + \")\"\n            current_sourcebyinstitutions.append(sourcebyinstitution)\n\n            params = {\n                \"q\": 'source_id:%s AND institution:\"%s\"' % (source, institution),\n                \"rows\": 0,\n                \"wt\": \"json\"\n            }\n\n            # check k10plus\n            result = get_solr_result(k10plus, params)\n            number = result[\"response\"][\"numFound\"]\n            if number != 0:\n                sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (?, ?)'\n                sqlite.execute(sql, (sourcebyinstitution, number))\n                conn.commit()\n            else:\n                # check ai\n                result = get_solr_result(ai, params)\n                number = result[\"response\"][\"numFound\"]\n                if number != 0:\n                    # TODO: escape via sqlite\n                    sql = 'INSERT INTO history (sourcebyinstitution, titles) VALUES (?, ?)'\n                    sqlite.execute(sql, (sourcebyinstitution, number))\n                    conn.commit()\n\n            if sourcebyinstitution not in old_sourcebyinstitutions:\n                logging.info(\"The %s is now connected to SID %s.\", institution, source)\n                sql = \"INSERT INTO sourcebyinstitution (sourcebyinstitution) VALUES (?)\"\n                sqlite.execute(sql, (sourcebyinstitution))\n                conn.commit()\n\n            if number != 0:\n                old_sourcebyinstitution_number = get_old_sourcebyinstitution_number(conn, sqlite, sourcebyinstitution)\n                if number < old_sourcebyinstitution_number:\n                    message = \"Die Anzahl der Titel hat sich bei %s gegenueber einem frueheren Import verringert.\" % (sourcebyinstitution)\n                    send_message(message)\n\n            # requests.exceptions.ConnectionError: HTTPConnectionPool(XXXXXX): Max retries exceeded\n            time.sleep(0.25)\n\n    for old_sourcebyinstitution in old_sourcebyinstitutions:\n        if old_sourcebyinstitution not in current_sourcebyinstitutions:\n            message = \"Die %s ist nicht laenger f\u00fcr die SID %s angesigelt.\" % (institution, source)\n            send_message(message)", "commit_link": "github.com/miku/siskin/commit/7fa398d2fea72bf2e8b4808f75df4b3d35ae959a", "file_name": "bin/solrcheckup.py", "vul_type": "cwe-089", "description": "Write a Python function to log current source and title data from Solr into a database, handling new and old entries."}
{"func_name": "placeholder", "func_src_before": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "func_src_after": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 100, "char_end": 172, "line": "        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n"}], "added": [{"line_no": 3, "char_start": 100, "char_end": 200, "line": "        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 132, "char_end": 159, "chars": "ERB::Util.html_escape_once("}, {"char_start": 167, "char_end": 168, "chars": ")"}]}, "commit_link": "github.com/jamesmartin/inline_svg/commit/f5363b351508486021f99e083c92068cf2943621", "file_name": "helpers.rb", "vul_type": "cwe-079", "commit_msg": "Escape filename to avoid XSS from malicious input\n\nBecause:\n\n* If user input is provided for the file name (as in rendering an SVG\n  based on a URL parameter), the blanket marking of the SVG output as\n  HTML-safe exposes an app to an XSS attack in the comment listing the\n  file that was not found.\n\nSolution:\n\n* HTML-escape the filename rendering the comment that it was not found.", "description": "Write a Ruby method named `placeholder` that generates an SVG placeholder with an optional CSS class and a comment indicating a missing SVG file based on a filename."}
{"func_name": "job_browse", "func_src_before": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = os.path.join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "func_src_after": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = safe_join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "line_changes": {"deleted": [{"line_no": 24, "char_start": 691, "char_end": 739, "line": "    abs_path = os.path.join(job_base_dir, path)\n"}], "added": [{"line_no": 24, "char_start": 691, "char_end": 736, "line": "    abs_path = safe_join(job_base_dir, path)\n"}]}, "char_changes": {"deleted": [{"char_start": 706, "char_end": 714, "chars": "os.path."}], "added": [{"char_start": 706, "char_end": 711, "chars": "safe_"}]}, "commit_link": "github.com/ganga-devs/ganga/commit/730e7aba192407d35eb37dd7938d49071124be8c", "file_name": "routes.py", "vul_type": "cwe-022", "commit_msg": "# Absolute Path Traversal due to incorrect use of `send_file` call (#2025)\n\nA path traversal attack (also known as directory traversal) aims to access files and directories that are stored outside the web root folder. By manipulating variables that reference files with \u201cdot-dot-slash (../)\u201d sequences and its variations or by using absolute file paths, it may be possible to access arbitrary files and directories stored on file system including application source code or configuration and critical system files. This attack is also known as \u201cdot-dot-slash\u201d, \u201cdirectory traversal\u201d, \u201cdirectory climbing\u201d and \u201cbacktracking\u201d.\r\n\r\n## Common Weakness Enumeration category\r\nCWE - 36\r\n\r\n## Root Cause Analysis\r\n\r\nThe `os.path.join` call is unsafe for use with untrusted input. When the `os.path.join` call encounters an absolute path, it ignores all the parameters it has encountered till that point and starts working with the new absolute path.  Please see the example below.\r\n```\r\n>>> import os.path\r\n>>> static = \"path/to/mySafeStaticDir\"\r\n>>> malicious = \"/../../../../../etc/passwd\"\r\n>>> os.path.join(t,malicious)\r\n'/../../../../../etc/passwd'\r\n```\r\nSince the \"malicious\" parameter represents an absolute path, the result of `os.path.join` ignores the static directory completely. Hence, untrusted input is passed via the `os.path.join` call to `flask.send_file` can lead to path traversal attacks.\r\n\r\nIn this case, the problems occurs due to the following code :\r\nhttps://github.com/ganga-devs/ganga/blob/0c0f9e33b36ee7ead0855f1464f8d4efad26bdbc/ganga/GangaGUI/gui/routes.py#L671\r\n\r\nHere, the `path` parameter is attacker controlled. This parameter passes through the unsafe `os.path.join` call making the effective directory and filename passed to the `send_file` call attacker controlled. This leads to a path traversal attack.\r\n\r\n## Proof of Concept\r\n\r\nThe bug can be verified using a proof of concept similar to the one shown below.\r\n\r\n```\r\ncurl --path-as-is 'http://<domain>/job/<int:job_id>/browse///../../../../etc/passwd\"'\r\n```\r\n## Remediation\r\n\r\nThis can be fixed by preventing flow of untrusted data to the vulnerable `send_file` function. In case the application logic necessiates this behaviour, one can either use the `werkzeug.utils.safe_join` to join untrusted paths or replace `flask.send_file` calls with `flask.send_from_directory` calls.\r\n\r\n## Common Vulnerability Scoring System Vector\r\n\r\nThe attack can be carried over the network. A complex non-standard configuration or a specialized condition is not required for the attack to be successfully conducted. There is no user interaction required for successful execution. The attack can affect components outside the scope of the target module. The attack can be used to gain access to confidential files like passwords, login credentials and other secrets. It cannot be directly used to affect a change on a system resource. Hence has limited to no impact on integrity. Using this attack vector a attacker may make multiple requests for accessing huge files such as a database. This can lead to a partial system denial service. However, the impact on availability is quite low in this case. Taking this account an appropriate CVSS v3.1 vector would be\r\n\r\n(AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L)[https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L&version=3.1]\r\n\r\nThis gives it a base score of 9.3/10 and a severity rating of critical.\r\n\r\n## References\r\n* [OWASP Path Traversal](https://owasp.org/www-community/attacks/Path_Traversal)\r\n* github/securitylab#669\r\n\r\n### This bug was found using *[CodeQL by Github](https://codeql.github.com/)*\r\n\r\nCo-authored-by: Porcupiney Hairs <porucpiney.hairs@protonmail.com>", "description": "Create a Python Flask web route for browsing job directories and files, with user authentication required."}
{"func_name": "UnicodeString::doAppend", "func_src_before": "UnicodeString::doAppend(const UChar *srcChars, int32_t srcStart, int32_t srcLength) {\n  if(!isWritable() || srcLength == 0 || srcChars == NULL) {\n    return *this;\n  }\n\n  // Perform all remaining operations relative to srcChars + srcStart.\n  // From this point forward, do not use srcStart.\n  srcChars += srcStart;\n\n  if(srcLength < 0) {\n    // get the srcLength if necessary\n    if((srcLength = u_strlen(srcChars)) == 0) {\n      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength = oldLength + srcLength;\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&\n      srcChars < oldArray + oldLength) {\n    // Copy into a new UnicodeString and start over\n    UnicodeString copy(srcChars, srcLength);\n    if (copy.isBogus()) {\n      setToBogus();\n      return *this;\n    }\n    return doAppend(copy.getArrayStart(), 0, srcLength);\n  }\n\n  // optimize append() onto a large-enough, owned string\n  if((newLength <= getCapacity() && isBufferWritable()) ||\n      cloneArrayIfNeeded(newLength, getGrowCapacity(newLength))) {\n    UChar *newArray = getArrayStart();\n    // Do not copy characters when\n    //   UChar *buffer=str.getAppendBuffer(...);\n    // is followed by\n    //   str.append(buffer, length);\n    // or\n    //   str.appendString(buffer, length)\n    // or similar.\n    if(srcChars != newArray + oldLength) {\n      us_arrayCopy(srcChars, 0, newArray, oldLength, srcLength);\n    }\n    setLength(newLength);\n  }\n  return *this;\n}", "func_src_after": "UnicodeString::doAppend(const UChar *srcChars, int32_t srcStart, int32_t srcLength) {\n  if(!isWritable() || srcLength == 0 || srcChars == NULL) {\n    return *this;\n  }\n\n  // Perform all remaining operations relative to srcChars + srcStart.\n  // From this point forward, do not use srcStart.\n  srcChars += srcStart;\n\n  if(srcLength < 0) {\n    // get the srcLength if necessary\n    if((srcLength = u_strlen(srcChars)) == 0) {\n      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength;\n  if (uprv_add32_overflow(oldLength, srcLength, &newLength)) {\n    setToBogus();\n    return *this;\n  }\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&\n      srcChars < oldArray + oldLength) {\n    // Copy into a new UnicodeString and start over\n    UnicodeString copy(srcChars, srcLength);\n    if (copy.isBogus()) {\n      setToBogus();\n      return *this;\n    }\n    return doAppend(copy.getArrayStart(), 0, srcLength);\n  }\n\n  // optimize append() onto a large-enough, owned string\n  if((newLength <= getCapacity() && isBufferWritable()) ||\n      cloneArrayIfNeeded(newLength, getGrowCapacity(newLength))) {\n    UChar *newArray = getArrayStart();\n    // Do not copy characters when\n    //   UChar *buffer=str.getAppendBuffer(...);\n    // is followed by\n    //   str.append(buffer, length);\n    // or\n    //   str.appendString(buffer, length)\n    // or similar.\n    if(srcChars != newArray + oldLength) {\n      us_arrayCopy(srcChars, 0, newArray, oldLength, srcLength);\n    }\n    setLength(newLength);\n  }\n  return *this;\n}", "commit_link": "github.com/unicode-org/icu/commit/b7d08bc04a4296982fcef8b6b8a354a9e4e7afca", "file_name": "icu4c/source/common/unistr.cpp", "vul_type": "cwe-190", "description": "In C++, write a method `doAppend` for the `UnicodeString` class that appends a substring to the current string, handling memory allocation and avoiding self-appending issues."}
{"func_name": "wiki_handle_rest_call", "func_src_before": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t      file_write(page, wikitext);\t      \n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "func_src_after": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t  if (page_name_is_good(page))\n\t    {\n\t      file_write(page, wikitext);\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "commit_link": "github.com/yarolig/didiwiki/commit/5e5c796617e1712905dc5462b94bd5e6c08d15ea", "file_name": "src/wiki.c", "vul_type": "cwe-022", "description": "In C, write a function to handle RESTful API calls for a wiki page system, supporting operations like get, set, delete, check existence, and list/search pages."}
{"func_name": "ExprAppendMultiKeysymList", "func_src_before": "ExprAppendMultiKeysymList(ExprDef *expr, ExprDef *append)\n{\n    unsigned nSyms = darray_size(expr->keysym_list.syms);\n    unsigned numEntries = darray_size(append->keysym_list.syms);\n\n    darray_append(expr->keysym_list.symsMapIndex, nSyms);\n    darray_append(expr->keysym_list.symsNumEntries, numEntries);\n    darray_concat(expr->keysym_list.syms, append->keysym_list.syms);\n\n    FreeStmt((ParseCommon *) &append);\n\n    return expr;\n}", "func_src_after": "ExprAppendMultiKeysymList(ExprDef *expr, ExprDef *append)\n{\n    unsigned nSyms = darray_size(expr->keysym_list.syms);\n    unsigned numEntries = darray_size(append->keysym_list.syms);\n\n    darray_append(expr->keysym_list.symsMapIndex, nSyms);\n    darray_append(expr->keysym_list.symsNumEntries, numEntries);\n    darray_concat(expr->keysym_list.syms, append->keysym_list.syms);\n\n    FreeStmt((ParseCommon *) append);\n\n    return expr;\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/c1e5ac16e77a21f87bdf3bc4dea61b037a17dddb", "file_name": "src/xkbcomp/ast-build.c", "vul_type": "cwe-416", "description": "Write a C function to append one keysym list to another and update indices, then free the appended structure."}
{"func_name": "save_page_edit", "func_src_before": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "func_src_after": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "In Python, write a Flask endpoint to save edited content for a given page, creating the page in the database if it doesn't exist."}
{"func_name": "next_state_val", "func_src_before": "next_state_val(CClassNode* cc, OnigCodePoint *vs, OnigCodePoint v,\n\t       int* vs_israw, int v_israw,\n\t       enum CCVALTYPE intype, enum CCVALTYPE* type,\n\t       enum CCSTATE* state, ScanEnv* env)\n{\n  int r;\n\n  switch (*state) {\n  case CCS_VALUE:\n    if (*type == CCV_SB) {\n      BITSET_SET_BIT(cc->bs, (int )(*vs));\n    }\n    else if (*type == CCV_CODE_POINT) {\n      r = add_code_range(&(cc->mbuf), env, *vs, *vs);\n      if (r < 0) return r;\n    }\n    break;\n\n  case CCS_RANGE:\n    if (intype == *type) {\n      if (intype == CCV_SB) {\n        if (*vs > 0xff || v > 0xff)\n          return ONIGERR_INVALID_CODE_POINT_VALUE;\n\n        if (*vs > v) {\n          if (IS_SYNTAX_BV(env->syntax, ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC))\n            goto ccs_range_end;\n          else\n            return ONIGERR_EMPTY_RANGE_IN_CHAR_CLASS;\n        }\n        bitset_set_range(cc->bs, (int )*vs, (int )v);\n      }\n      else {\n        r = add_code_range(&(cc->mbuf), env, *vs, v);\n        if (r < 0) return r;\n      }\n    }\n    else {\n#if 0\n      if (intype == CCV_CODE_POINT && *type == CCV_SB) {\n#endif\n        if (*vs > v) {\n          if (IS_SYNTAX_BV(env->syntax, ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC))\n            goto ccs_range_end;\n          else\n            return ONIGERR_EMPTY_RANGE_IN_CHAR_CLASS;\n        }\n        bitset_set_range(cc->bs, (int )*vs, (int )(v < 0xff ? v : 0xff));\n        r = add_code_range(&(cc->mbuf), env, (OnigCodePoint )*vs, v);\n        if (r < 0) return r;\n#if 0\n      }\n      else\n        return ONIGERR_MISMATCH_CODE_LENGTH_IN_CLASS_RANGE;\n#endif\n    }\n  ccs_range_end:\n    *state = CCS_COMPLETE;\n    break;\n\n  case CCS_COMPLETE:\n  case CCS_START:\n    *state = CCS_VALUE;\n    break;\n\n  default:\n    break;\n  }\n\n  *vs_israw = v_israw;\n  *vs       = v;\n  *type     = intype;\n  return 0;\n}", "func_src_after": "next_state_val(CClassNode* cc, OnigCodePoint *vs, OnigCodePoint v,\n\t       int* vs_israw, int v_israw,\n\t       enum CCVALTYPE intype, enum CCVALTYPE* type,\n\t       enum CCSTATE* state, ScanEnv* env)\n{\n  int r;\n\n  switch (*state) {\n  case CCS_VALUE:\n    if (*type == CCV_SB) {\n      if (*vs > 0xff)\n          return ONIGERR_INVALID_CODE_POINT_VALUE;\n\n      BITSET_SET_BIT(cc->bs, (int )(*vs));\n    }\n    else if (*type == CCV_CODE_POINT) {\n      r = add_code_range(&(cc->mbuf), env, *vs, *vs);\n      if (r < 0) return r;\n    }\n    break;\n\n  case CCS_RANGE:\n    if (intype == *type) {\n      if (intype == CCV_SB) {\n        if (*vs > 0xff || v > 0xff)\n          return ONIGERR_INVALID_CODE_POINT_VALUE;\n\n        if (*vs > v) {\n          if (IS_SYNTAX_BV(env->syntax, ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC))\n            goto ccs_range_end;\n          else\n            return ONIGERR_EMPTY_RANGE_IN_CHAR_CLASS;\n        }\n        bitset_set_range(cc->bs, (int )*vs, (int )v);\n      }\n      else {\n        r = add_code_range(&(cc->mbuf), env, *vs, v);\n        if (r < 0) return r;\n      }\n    }\n    else {\n#if 0\n      if (intype == CCV_CODE_POINT && *type == CCV_SB) {\n#endif\n        if (*vs > v) {\n          if (IS_SYNTAX_BV(env->syntax, ONIG_SYN_ALLOW_EMPTY_RANGE_IN_CC))\n            goto ccs_range_end;\n          else\n            return ONIGERR_EMPTY_RANGE_IN_CHAR_CLASS;\n        }\n        bitset_set_range(cc->bs, (int )*vs, (int )(v < 0xff ? v : 0xff));\n        r = add_code_range(&(cc->mbuf), env, (OnigCodePoint )*vs, v);\n        if (r < 0) return r;\n#if 0\n      }\n      else\n        return ONIGERR_MISMATCH_CODE_LENGTH_IN_CLASS_RANGE;\n#endif\n    }\n  ccs_range_end:\n    *state = CCS_COMPLETE;\n    break;\n\n  case CCS_COMPLETE:\n  case CCS_START:\n    *state = CCS_VALUE;\n    break;\n\n  default:\n    break;\n  }\n\n  *vs_israw = v_israw;\n  *vs       = v;\n  *type     = intype;\n  return 0;\n}", "commit_link": "github.com/kkos/oniguruma/commit/b4bf968ad52afe14e60a2dc8a95d3555c543353a", "file_name": "src/regparse.c", "vul_type": "cwe-787", "description": "Write a C function named `next_state_val` that updates the state and value type of a character class node based on input values and a state machine."}
{"func_name": "avcodec_open2", "func_src_before": "int attribute_align_arg avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options)\n{\n    int ret = 0;\n    int codec_init_ok = 0;\n    AVDictionary *tmp = NULL;\n    const AVPixFmtDescriptor *pixdesc;\n\n    if (avcodec_is_open(avctx))\n        return 0;\n\n    if ((!codec && !avctx->codec)) {\n        av_log(avctx, AV_LOG_ERROR, \"No codec provided to avcodec_open2()\\n\");\n        return AVERROR(EINVAL);\n    }\n    if ((codec && avctx->codec && codec != avctx->codec)) {\n        av_log(avctx, AV_LOG_ERROR, \"This AVCodecContext was allocated for %s, \"\n                                    \"but %s passed to avcodec_open2()\\n\", avctx->codec->name, codec->name);\n        return AVERROR(EINVAL);\n    }\n    if (!codec)\n        codec = avctx->codec;\n\n    if (avctx->extradata_size < 0 || avctx->extradata_size >= FF_MAX_EXTRADATA_SIZE)\n        return AVERROR(EINVAL);\n\n    if (options)\n        av_dict_copy(&tmp, *options, 0);\n\n    ff_lock_avcodec(avctx, codec);\n\n    avctx->internal = av_mallocz(sizeof(*avctx->internal));\n    if (!avctx->internal) {\n        ret = AVERROR(ENOMEM);\n        goto end;\n    }\n\n    avctx->internal->pool = av_mallocz(sizeof(*avctx->internal->pool));\n    if (!avctx->internal->pool) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->to_free = av_frame_alloc();\n    if (!avctx->internal->to_free) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->compat_decode_frame = av_frame_alloc();\n    if (!avctx->internal->compat_decode_frame) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->buffer_frame = av_frame_alloc();\n    if (!avctx->internal->buffer_frame) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->buffer_pkt = av_packet_alloc();\n    if (!avctx->internal->buffer_pkt) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->ds.in_pkt = av_packet_alloc();\n    if (!avctx->internal->ds.in_pkt) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->last_pkt_props = av_packet_alloc();\n    if (!avctx->internal->last_pkt_props) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->skip_samples_multiplier = 1;\n\n    if (codec->priv_data_size > 0) {\n        if (!avctx->priv_data) {\n            avctx->priv_data = av_mallocz(codec->priv_data_size);\n            if (!avctx->priv_data) {\n                ret = AVERROR(ENOMEM);\n                goto end;\n            }\n            if (codec->priv_class) {\n                *(const AVClass **)avctx->priv_data = codec->priv_class;\n                av_opt_set_defaults(avctx->priv_data);\n            }\n        }\n        if (codec->priv_class && (ret = av_opt_set_dict(avctx->priv_data, &tmp)) < 0)\n            goto free_and_end;\n    } else {\n        avctx->priv_data = NULL;\n    }\n    if ((ret = av_opt_set_dict(avctx, &tmp)) < 0)\n        goto free_and_end;\n\n    if (avctx->codec_whitelist && av_match_list(codec->name, avctx->codec_whitelist, ',') <= 0) {\n        av_log(avctx, AV_LOG_ERROR, \"Codec (%s) not on whitelist \\'%s\\'\\n\", codec->name, avctx->codec_whitelist);\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n\n    // only call ff_set_dimensions() for non H.264/VP6F/DXV codecs so as not to overwrite previously setup dimensions\n    if (!(avctx->coded_width && avctx->coded_height && avctx->width && avctx->height &&\n          (avctx->codec_id == AV_CODEC_ID_H264 || avctx->codec_id == AV_CODEC_ID_VP6F || avctx->codec_id == AV_CODEC_ID_DXV))) {\n    if (avctx->coded_width && avctx->coded_height)\n        ret = ff_set_dimensions(avctx, avctx->coded_width, avctx->coded_height);\n    else if (avctx->width && avctx->height)\n        ret = ff_set_dimensions(avctx, avctx->width, avctx->height);\n    if (ret < 0)\n        goto free_and_end;\n    }\n\n    if ((avctx->coded_width || avctx->coded_height || avctx->width || avctx->height)\n        && (  av_image_check_size2(avctx->coded_width, avctx->coded_height, avctx->max_pixels, AV_PIX_FMT_NONE, 0, avctx) < 0\n           || av_image_check_size2(avctx->width,       avctx->height,       avctx->max_pixels, AV_PIX_FMT_NONE, 0, avctx) < 0)) {\n        av_log(avctx, AV_LOG_WARNING, \"Ignoring invalid width/height values\\n\");\n        ff_set_dimensions(avctx, 0, 0);\n    }\n\n    if (avctx->width > 0 && avctx->height > 0) {\n        if (av_image_check_sar(avctx->width, avctx->height,\n                               avctx->sample_aspect_ratio) < 0) {\n            av_log(avctx, AV_LOG_WARNING, \"ignoring invalid SAR: %u/%u\\n\",\n                   avctx->sample_aspect_ratio.num,\n                   avctx->sample_aspect_ratio.den);\n            avctx->sample_aspect_ratio = (AVRational){ 0, 1 };\n        }\n    }\n\n    /* if the decoder init function was already called previously,\n     * free the already allocated subtitle_header before overwriting it */\n    if (av_codec_is_decoder(codec))\n        av_freep(&avctx->subtitle_header);\n\n    if (avctx->channels > FF_SANE_NB_CHANNELS) {\n        av_log(avctx, AV_LOG_ERROR, \"Too many channels: %d\\n\", avctx->channels);\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n\n    avctx->codec = codec;\n    if ((avctx->codec_type == AVMEDIA_TYPE_UNKNOWN || avctx->codec_type == codec->type) &&\n        avctx->codec_id == AV_CODEC_ID_NONE) {\n        avctx->codec_type = codec->type;\n        avctx->codec_id   = codec->id;\n    }\n    if (avctx->codec_id != codec->id || (avctx->codec_type != codec->type\n                                         && avctx->codec_type != AVMEDIA_TYPE_ATTACHMENT)) {\n        av_log(avctx, AV_LOG_ERROR, \"Codec type or id mismatches\\n\");\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n    avctx->frame_number = 0;\n    avctx->codec_descriptor = avcodec_descriptor_get(avctx->codec_id);\n\n    if ((avctx->codec->capabilities & AV_CODEC_CAP_EXPERIMENTAL) &&\n        avctx->strict_std_compliance > FF_COMPLIANCE_EXPERIMENTAL) {\n        const char *codec_string = av_codec_is_encoder(codec) ? \"encoder\" : \"decoder\";\n        AVCodec *codec2;\n        av_log(avctx, AV_LOG_ERROR,\n               \"The %s '%s' is experimental but experimental codecs are not enabled, \"\n               \"add '-strict %d' if you want to use it.\\n\",\n               codec_string, codec->name, FF_COMPLIANCE_EXPERIMENTAL);\n        codec2 = av_codec_is_encoder(codec) ? avcodec_find_encoder(codec->id) : avcodec_find_decoder(codec->id);\n        if (!(codec2->capabilities & AV_CODEC_CAP_EXPERIMENTAL))\n            av_log(avctx, AV_LOG_ERROR, \"Alternatively use the non experimental %s '%s'.\\n\",\n                codec_string, codec2->name);\n        ret = AVERROR_EXPERIMENTAL;\n        goto free_and_end;\n    }\n\n    if (avctx->codec_type == AVMEDIA_TYPE_AUDIO &&\n        (!avctx->time_base.num || !avctx->time_base.den)) {\n        avctx->time_base.num = 1;\n        avctx->time_base.den = avctx->sample_rate;\n    }\n\n    if (!HAVE_THREADS)\n        av_log(avctx, AV_LOG_WARNING, \"Warning: not compiled with thread support, using thread emulation\\n\");\n\n    if (CONFIG_FRAME_THREAD_ENCODER && av_codec_is_encoder(avctx->codec)) {\n        ff_unlock_avcodec(codec); //we will instantiate a few encoders thus kick the counter to prevent false detection of a problem\n        ret = ff_frame_thread_encoder_init(avctx, options ? *options : NULL);\n        ff_lock_avcodec(avctx, codec);\n        if (ret < 0)\n            goto free_and_end;\n    }\n\n    if (av_codec_is_decoder(avctx->codec)) {\n        ret = ff_decode_bsfs_init(avctx);\n        if (ret < 0)\n            goto free_and_end;\n    }\n\n    if (HAVE_THREADS\n        && !(avctx->internal->frame_thread_encoder && (avctx->active_thread_type&FF_THREAD_FRAME))) {\n        ret = ff_thread_init(avctx);\n        if (ret < 0) {\n            goto free_and_end;\n        }\n    }\n    if (!HAVE_THREADS && !(codec->capabilities & AV_CODEC_CAP_AUTO_THREADS))\n        avctx->thread_count = 1;\n\n    if (avctx->codec->max_lowres < avctx->lowres || avctx->lowres < 0) {\n        av_log(avctx, AV_LOG_WARNING, \"The maximum value for lowres supported by the decoder is %d\\n\",\n               avctx->codec->max_lowres);\n        avctx->lowres = avctx->codec->max_lowres;\n    }\n\n    if (av_codec_is_encoder(avctx->codec)) {\n        int i;\n#if FF_API_CODED_FRAME\nFF_DISABLE_DEPRECATION_WARNINGS\n        avctx->coded_frame = av_frame_alloc();\n        if (!avctx->coded_frame) {\n            ret = AVERROR(ENOMEM);\n            goto free_and_end;\n        }\nFF_ENABLE_DEPRECATION_WARNINGS\n#endif\n\n        if (avctx->time_base.num <= 0 || avctx->time_base.den <= 0) {\n            av_log(avctx, AV_LOG_ERROR, \"The encoder timebase is not set.\\n\");\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n\n        if (avctx->codec->sample_fmts) {\n            for (i = 0; avctx->codec->sample_fmts[i] != AV_SAMPLE_FMT_NONE; i++) {\n                if (avctx->sample_fmt == avctx->codec->sample_fmts[i])\n                    break;\n                if (avctx->channels == 1 &&\n                    av_get_planar_sample_fmt(avctx->sample_fmt) ==\n                    av_get_planar_sample_fmt(avctx->codec->sample_fmts[i])) {\n                    avctx->sample_fmt = avctx->codec->sample_fmts[i];\n                    break;\n                }\n            }\n            if (avctx->codec->sample_fmts[i] == AV_SAMPLE_FMT_NONE) {\n                char buf[128];\n                snprintf(buf, sizeof(buf), \"%d\", avctx->sample_fmt);\n                av_log(avctx, AV_LOG_ERROR, \"Specified sample format %s is invalid or not supported\\n\",\n                       (char *)av_x_if_null(av_get_sample_fmt_name(avctx->sample_fmt), buf));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (avctx->codec->pix_fmts) {\n            for (i = 0; avctx->codec->pix_fmts[i] != AV_PIX_FMT_NONE; i++)\n                if (avctx->pix_fmt == avctx->codec->pix_fmts[i])\n                    break;\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_NONE\n                && !((avctx->codec_id == AV_CODEC_ID_MJPEG || avctx->codec_id == AV_CODEC_ID_LJPEG)\n                     && avctx->strict_std_compliance <= FF_COMPLIANCE_UNOFFICIAL)) {\n                char buf[128];\n                snprintf(buf, sizeof(buf), \"%d\", avctx->pix_fmt);\n                av_log(avctx, AV_LOG_ERROR, \"Specified pixel format %s is invalid or not supported\\n\",\n                       (char *)av_x_if_null(av_get_pix_fmt_name(avctx->pix_fmt), buf));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ420P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ411P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ422P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ440P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ444P)\n                avctx->color_range = AVCOL_RANGE_JPEG;\n        }\n        if (avctx->codec->supported_samplerates) {\n            for (i = 0; avctx->codec->supported_samplerates[i] != 0; i++)\n                if (avctx->sample_rate == avctx->codec->supported_samplerates[i])\n                    break;\n            if (avctx->codec->supported_samplerates[i] == 0) {\n                av_log(avctx, AV_LOG_ERROR, \"Specified sample rate %d is not supported\\n\",\n                       avctx->sample_rate);\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (avctx->sample_rate < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Specified sample rate %d is not supported\\n\",\n                    avctx->sample_rate);\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->codec->channel_layouts) {\n            if (!avctx->channel_layout) {\n                av_log(avctx, AV_LOG_WARNING, \"Channel layout not specified\\n\");\n            } else {\n                for (i = 0; avctx->codec->channel_layouts[i] != 0; i++)\n                    if (avctx->channel_layout == avctx->codec->channel_layouts[i])\n                        break;\n                if (avctx->codec->channel_layouts[i] == 0) {\n                    char buf[512];\n                    av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                    av_log(avctx, AV_LOG_ERROR, \"Specified channel layout '%s' is not supported\\n\", buf);\n                    ret = AVERROR(EINVAL);\n                    goto free_and_end;\n                }\n            }\n        }\n        if (avctx->channel_layout && avctx->channels) {\n            int channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n            if (channels != avctx->channels) {\n                char buf[512];\n                av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Channel layout '%s' with %d channels does not match number of specified channels %d\\n\",\n                       buf, channels, avctx->channels);\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        } else if (avctx->channel_layout) {\n            avctx->channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n        }\n        if (avctx->channels < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Specified number of channels %d is not supported\\n\",\n                    avctx->channels);\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if(avctx->codec_type == AVMEDIA_TYPE_VIDEO) {\n            pixdesc = av_pix_fmt_desc_get(avctx->pix_fmt);\n            if (    avctx->bits_per_raw_sample < 0\n                || (avctx->bits_per_raw_sample > 8 && pixdesc->comp[0].depth <= 8)) {\n                av_log(avctx, AV_LOG_WARNING, \"Specified bit depth %d not possible with the specified pixel formats depth %d\\n\",\n                    avctx->bits_per_raw_sample, pixdesc->comp[0].depth);\n                avctx->bits_per_raw_sample = pixdesc->comp[0].depth;\n            }\n            if (avctx->width <= 0 || avctx->height <= 0) {\n                av_log(avctx, AV_LOG_ERROR, \"dimensions not set\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (   (avctx->codec_type == AVMEDIA_TYPE_VIDEO || avctx->codec_type == AVMEDIA_TYPE_AUDIO)\n            && avctx->bit_rate>0 && avctx->bit_rate<1000) {\n            av_log(avctx, AV_LOG_WARNING, \"Bitrate %\"PRId64\" is extremely low, maybe you mean %\"PRId64\"k\\n\", avctx->bit_rate, avctx->bit_rate);\n        }\n\n        if (!avctx->rc_initial_buffer_occupancy)\n            avctx->rc_initial_buffer_occupancy = avctx->rc_buffer_size * 3LL / 4;\n\n        if (avctx->ticks_per_frame && avctx->time_base.num &&\n            avctx->ticks_per_frame > INT_MAX / avctx->time_base.num) {\n            av_log(avctx, AV_LOG_ERROR,\n                   \"ticks_per_frame %d too large for the timebase %d/%d.\",\n                   avctx->ticks_per_frame,\n                   avctx->time_base.num,\n                   avctx->time_base.den);\n            goto free_and_end;\n        }\n\n        if (avctx->hw_frames_ctx) {\n            AVHWFramesContext *frames_ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;\n            if (frames_ctx->format != avctx->pix_fmt) {\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Mismatching AVCodecContext.pix_fmt and AVHWFramesContext.format\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            if (avctx->sw_pix_fmt != AV_PIX_FMT_NONE &&\n                avctx->sw_pix_fmt != frames_ctx->sw_format) {\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Mismatching AVCodecContext.sw_pix_fmt (%s) \"\n                       \"and AVHWFramesContext.sw_format (%s)\\n\",\n                       av_get_pix_fmt_name(avctx->sw_pix_fmt),\n                       av_get_pix_fmt_name(frames_ctx->sw_format));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            avctx->sw_pix_fmt = frames_ctx->sw_format;\n        }\n    }\n\n    avctx->pts_correction_num_faulty_pts =\n    avctx->pts_correction_num_faulty_dts = 0;\n    avctx->pts_correction_last_pts =\n    avctx->pts_correction_last_dts = INT64_MIN;\n\n    if (   !CONFIG_GRAY && avctx->flags & AV_CODEC_FLAG_GRAY\n        && avctx->codec_descriptor->type == AVMEDIA_TYPE_VIDEO)\n        av_log(avctx, AV_LOG_WARNING,\n               \"gray decoding requested but not enabled at configuration time\\n\");\n\n    if (   avctx->codec->init && (!(avctx->active_thread_type&FF_THREAD_FRAME)\n        || avctx->internal->frame_thread_encoder)) {\n        ret = avctx->codec->init(avctx);\n        if (ret < 0) {\n            goto free_and_end;\n        }\n        codec_init_ok = 1;\n    }\n\n    ret=0;\n\n    if (av_codec_is_decoder(avctx->codec)) {\n        if (!avctx->bit_rate)\n            avctx->bit_rate = get_bit_rate(avctx);\n        /* validate channel layout from the decoder */\n        if (avctx->channel_layout) {\n            int channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n            if (!avctx->channels)\n                avctx->channels = channels;\n            else if (channels != avctx->channels) {\n                char buf[512];\n                av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                av_log(avctx, AV_LOG_WARNING,\n                       \"Channel layout '%s' with %d channels does not match specified number of channels %d: \"\n                       \"ignoring specified channel layout\\n\",\n                       buf, channels, avctx->channels);\n                avctx->channel_layout = 0;\n            }\n        }\n        if (avctx->channels && avctx->channels < 0 ||\n            avctx->channels > FF_SANE_NB_CHANNELS) {\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->bits_per_coded_sample < 0) {\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->sub_charenc) {\n            if (avctx->codec_type != AVMEDIA_TYPE_SUBTITLE) {\n                av_log(avctx, AV_LOG_ERROR, \"Character encoding is only \"\n                       \"supported with subtitles codecs\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            } else if (avctx->codec_descriptor->props & AV_CODEC_PROP_BITMAP_SUB) {\n                av_log(avctx, AV_LOG_WARNING, \"Codec '%s' is bitmap-based, \"\n                       \"subtitles character encoding will be ignored\\n\",\n                       avctx->codec_descriptor->name);\n                avctx->sub_charenc_mode = FF_SUB_CHARENC_MODE_DO_NOTHING;\n            } else {\n                /* input character encoding is set for a text based subtitle\n                 * codec at this point */\n                if (avctx->sub_charenc_mode == FF_SUB_CHARENC_MODE_AUTOMATIC)\n                    avctx->sub_charenc_mode = FF_SUB_CHARENC_MODE_PRE_DECODER;\n\n                if (avctx->sub_charenc_mode == FF_SUB_CHARENC_MODE_PRE_DECODER) {\n#if CONFIG_ICONV\n                    iconv_t cd = iconv_open(\"UTF-8\", avctx->sub_charenc);\n                    if (cd == (iconv_t)-1) {\n                        ret = AVERROR(errno);\n                        av_log(avctx, AV_LOG_ERROR, \"Unable to open iconv context \"\n                               \"with input character encoding \\\"%s\\\"\\n\", avctx->sub_charenc);\n                        goto free_and_end;\n                    }\n                    iconv_close(cd);\n#else\n                    av_log(avctx, AV_LOG_ERROR, \"Character encoding subtitles \"\n                           \"conversion needs a libavcodec built with iconv support \"\n                           \"for this codec\\n\");\n                    ret = AVERROR(ENOSYS);\n                    goto free_and_end;\n#endif\n                }\n            }\n        }\n\n#if FF_API_AVCTX_TIMEBASE\n        if (avctx->framerate.num > 0 && avctx->framerate.den > 0)\n            avctx->time_base = av_inv_q(av_mul_q(avctx->framerate, (AVRational){avctx->ticks_per_frame, 1}));\n#endif\n    }\n    if (codec->priv_data_size > 0 && avctx->priv_data && codec->priv_class) {\n        av_assert0(*(const AVClass **)avctx->priv_data == codec->priv_class);\n    }\n\nend:\n    ff_unlock_avcodec(codec);\n    if (options) {\n        av_dict_free(options);\n        *options = tmp;\n    }\n\n    return ret;\nfree_and_end:\n    if (avctx->codec &&\n        (codec_init_ok ||\n         (avctx->codec->caps_internal & FF_CODEC_CAP_INIT_CLEANUP)))\n        avctx->codec->close(avctx);\n\n    if (codec->priv_class && codec->priv_data_size)\n        av_opt_free(avctx->priv_data);\n    av_opt_free(avctx);\n\n#if FF_API_CODED_FRAME\nFF_DISABLE_DEPRECATION_WARNINGS\n    av_frame_free(&avctx->coded_frame);\nFF_ENABLE_DEPRECATION_WARNINGS\n#endif\n\n    av_dict_free(&tmp);\n    av_freep(&avctx->priv_data);\n    if (avctx->internal) {\n        av_frame_free(&avctx->internal->to_free);\n        av_frame_free(&avctx->internal->compat_decode_frame);\n        av_frame_free(&avctx->internal->buffer_frame);\n        av_packet_free(&avctx->internal->buffer_pkt);\n        av_packet_free(&avctx->internal->last_pkt_props);\n\n        av_packet_free(&avctx->internal->ds.in_pkt);\n        ff_decode_bsfs_uninit(avctx);\n\n        av_freep(&avctx->internal->pool);\n    }\n    av_freep(&avctx->internal);\n    avctx->codec = NULL;\n    goto end;\n}", "func_src_after": "int attribute_align_arg avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options)\n{\n    int ret = 0;\n    int codec_init_ok = 0;\n    AVDictionary *tmp = NULL;\n    const AVPixFmtDescriptor *pixdesc;\n\n    if (avcodec_is_open(avctx))\n        return 0;\n\n    if ((!codec && !avctx->codec)) {\n        av_log(avctx, AV_LOG_ERROR, \"No codec provided to avcodec_open2()\\n\");\n        return AVERROR(EINVAL);\n    }\n    if ((codec && avctx->codec && codec != avctx->codec)) {\n        av_log(avctx, AV_LOG_ERROR, \"This AVCodecContext was allocated for %s, \"\n                                    \"but %s passed to avcodec_open2()\\n\", avctx->codec->name, codec->name);\n        return AVERROR(EINVAL);\n    }\n    if (!codec)\n        codec = avctx->codec;\n\n    if (avctx->extradata_size < 0 || avctx->extradata_size >= FF_MAX_EXTRADATA_SIZE)\n        return AVERROR(EINVAL);\n\n    if (options)\n        av_dict_copy(&tmp, *options, 0);\n\n    ff_lock_avcodec(avctx, codec);\n\n    avctx->internal = av_mallocz(sizeof(*avctx->internal));\n    if (!avctx->internal) {\n        ret = AVERROR(ENOMEM);\n        goto end;\n    }\n\n    avctx->internal->pool = av_mallocz(sizeof(*avctx->internal->pool));\n    if (!avctx->internal->pool) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->to_free = av_frame_alloc();\n    if (!avctx->internal->to_free) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->compat_decode_frame = av_frame_alloc();\n    if (!avctx->internal->compat_decode_frame) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->buffer_frame = av_frame_alloc();\n    if (!avctx->internal->buffer_frame) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->buffer_pkt = av_packet_alloc();\n    if (!avctx->internal->buffer_pkt) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->ds.in_pkt = av_packet_alloc();\n    if (!avctx->internal->ds.in_pkt) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->last_pkt_props = av_packet_alloc();\n    if (!avctx->internal->last_pkt_props) {\n        ret = AVERROR(ENOMEM);\n        goto free_and_end;\n    }\n\n    avctx->internal->skip_samples_multiplier = 1;\n\n    if (codec->priv_data_size > 0) {\n        if (!avctx->priv_data) {\n            avctx->priv_data = av_mallocz(codec->priv_data_size);\n            if (!avctx->priv_data) {\n                ret = AVERROR(ENOMEM);\n                goto end;\n            }\n            if (codec->priv_class) {\n                *(const AVClass **)avctx->priv_data = codec->priv_class;\n                av_opt_set_defaults(avctx->priv_data);\n            }\n        }\n        if (codec->priv_class && (ret = av_opt_set_dict(avctx->priv_data, &tmp)) < 0)\n            goto free_and_end;\n    } else {\n        avctx->priv_data = NULL;\n    }\n    if ((ret = av_opt_set_dict(avctx, &tmp)) < 0)\n        goto free_and_end;\n\n    if (avctx->codec_whitelist && av_match_list(codec->name, avctx->codec_whitelist, ',') <= 0) {\n        av_log(avctx, AV_LOG_ERROR, \"Codec (%s) not on whitelist \\'%s\\'\\n\", codec->name, avctx->codec_whitelist);\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n\n    // only call ff_set_dimensions() for non H.264/VP6F/DXV codecs so as not to overwrite previously setup dimensions\n    if (!(avctx->coded_width && avctx->coded_height && avctx->width && avctx->height &&\n          (avctx->codec_id == AV_CODEC_ID_H264 || avctx->codec_id == AV_CODEC_ID_VP6F || avctx->codec_id == AV_CODEC_ID_DXV))) {\n    if (avctx->coded_width && avctx->coded_height)\n        ret = ff_set_dimensions(avctx, avctx->coded_width, avctx->coded_height);\n    else if (avctx->width && avctx->height)\n        ret = ff_set_dimensions(avctx, avctx->width, avctx->height);\n    if (ret < 0)\n        goto free_and_end;\n    }\n\n    if ((avctx->coded_width || avctx->coded_height || avctx->width || avctx->height)\n        && (  av_image_check_size2(avctx->coded_width, avctx->coded_height, avctx->max_pixels, AV_PIX_FMT_NONE, 0, avctx) < 0\n           || av_image_check_size2(avctx->width,       avctx->height,       avctx->max_pixels, AV_PIX_FMT_NONE, 0, avctx) < 0)) {\n        av_log(avctx, AV_LOG_WARNING, \"Ignoring invalid width/height values\\n\");\n        ff_set_dimensions(avctx, 0, 0);\n    }\n\n    if (avctx->width > 0 && avctx->height > 0) {\n        if (av_image_check_sar(avctx->width, avctx->height,\n                               avctx->sample_aspect_ratio) < 0) {\n            av_log(avctx, AV_LOG_WARNING, \"ignoring invalid SAR: %u/%u\\n\",\n                   avctx->sample_aspect_ratio.num,\n                   avctx->sample_aspect_ratio.den);\n            avctx->sample_aspect_ratio = (AVRational){ 0, 1 };\n        }\n    }\n\n    /* if the decoder init function was already called previously,\n     * free the already allocated subtitle_header before overwriting it */\n    if (av_codec_is_decoder(codec))\n        av_freep(&avctx->subtitle_header);\n\n    if (avctx->channels > FF_SANE_NB_CHANNELS) {\n        av_log(avctx, AV_LOG_ERROR, \"Too many channels: %d\\n\", avctx->channels);\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n\n    avctx->codec = codec;\n    if ((avctx->codec_type == AVMEDIA_TYPE_UNKNOWN || avctx->codec_type == codec->type) &&\n        avctx->codec_id == AV_CODEC_ID_NONE) {\n        avctx->codec_type = codec->type;\n        avctx->codec_id   = codec->id;\n    }\n    if (avctx->codec_id != codec->id || (avctx->codec_type != codec->type\n                                         && avctx->codec_type != AVMEDIA_TYPE_ATTACHMENT)) {\n        av_log(avctx, AV_LOG_ERROR, \"Codec type or id mismatches\\n\");\n        ret = AVERROR(EINVAL);\n        goto free_and_end;\n    }\n    avctx->frame_number = 0;\n    avctx->codec_descriptor = avcodec_descriptor_get(avctx->codec_id);\n\n    if ((avctx->codec->capabilities & AV_CODEC_CAP_EXPERIMENTAL) &&\n        avctx->strict_std_compliance > FF_COMPLIANCE_EXPERIMENTAL) {\n        const char *codec_string = av_codec_is_encoder(codec) ? \"encoder\" : \"decoder\";\n        AVCodec *codec2;\n        av_log(avctx, AV_LOG_ERROR,\n               \"The %s '%s' is experimental but experimental codecs are not enabled, \"\n               \"add '-strict %d' if you want to use it.\\n\",\n               codec_string, codec->name, FF_COMPLIANCE_EXPERIMENTAL);\n        codec2 = av_codec_is_encoder(codec) ? avcodec_find_encoder(codec->id) : avcodec_find_decoder(codec->id);\n        if (!(codec2->capabilities & AV_CODEC_CAP_EXPERIMENTAL))\n            av_log(avctx, AV_LOG_ERROR, \"Alternatively use the non experimental %s '%s'.\\n\",\n                codec_string, codec2->name);\n        ret = AVERROR_EXPERIMENTAL;\n        goto free_and_end;\n    }\n\n    if (avctx->codec_type == AVMEDIA_TYPE_AUDIO &&\n        (!avctx->time_base.num || !avctx->time_base.den)) {\n        avctx->time_base.num = 1;\n        avctx->time_base.den = avctx->sample_rate;\n    }\n\n    if (!HAVE_THREADS)\n        av_log(avctx, AV_LOG_WARNING, \"Warning: not compiled with thread support, using thread emulation\\n\");\n\n    if (CONFIG_FRAME_THREAD_ENCODER && av_codec_is_encoder(avctx->codec)) {\n        ff_unlock_avcodec(codec); //we will instantiate a few encoders thus kick the counter to prevent false detection of a problem\n        ret = ff_frame_thread_encoder_init(avctx, options ? *options : NULL);\n        ff_lock_avcodec(avctx, codec);\n        if (ret < 0)\n            goto free_and_end;\n    }\n\n    if (av_codec_is_decoder(avctx->codec)) {\n        ret = ff_decode_bsfs_init(avctx);\n        if (ret < 0)\n            goto free_and_end;\n    }\n\n    if (HAVE_THREADS\n        && !(avctx->internal->frame_thread_encoder && (avctx->active_thread_type&FF_THREAD_FRAME))) {\n        ret = ff_thread_init(avctx);\n        if (ret < 0) {\n            goto free_and_end;\n        }\n    }\n    if (!HAVE_THREADS && !(codec->capabilities & AV_CODEC_CAP_AUTO_THREADS))\n        avctx->thread_count = 1;\n\n    if (avctx->codec->max_lowres < avctx->lowres || avctx->lowres < 0) {\n        av_log(avctx, AV_LOG_WARNING, \"The maximum value for lowres supported by the decoder is %d\\n\",\n               avctx->codec->max_lowres);\n        avctx->lowres = avctx->codec->max_lowres;\n    }\n\n    if (av_codec_is_encoder(avctx->codec)) {\n        int i;\n#if FF_API_CODED_FRAME\nFF_DISABLE_DEPRECATION_WARNINGS\n        avctx->coded_frame = av_frame_alloc();\n        if (!avctx->coded_frame) {\n            ret = AVERROR(ENOMEM);\n            goto free_and_end;\n        }\nFF_ENABLE_DEPRECATION_WARNINGS\n#endif\n\n        if (avctx->time_base.num <= 0 || avctx->time_base.den <= 0) {\n            av_log(avctx, AV_LOG_ERROR, \"The encoder timebase is not set.\\n\");\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n\n        if (avctx->codec->sample_fmts) {\n            for (i = 0; avctx->codec->sample_fmts[i] != AV_SAMPLE_FMT_NONE; i++) {\n                if (avctx->sample_fmt == avctx->codec->sample_fmts[i])\n                    break;\n                if (avctx->channels == 1 &&\n                    av_get_planar_sample_fmt(avctx->sample_fmt) ==\n                    av_get_planar_sample_fmt(avctx->codec->sample_fmts[i])) {\n                    avctx->sample_fmt = avctx->codec->sample_fmts[i];\n                    break;\n                }\n            }\n            if (avctx->codec->sample_fmts[i] == AV_SAMPLE_FMT_NONE) {\n                char buf[128];\n                snprintf(buf, sizeof(buf), \"%d\", avctx->sample_fmt);\n                av_log(avctx, AV_LOG_ERROR, \"Specified sample format %s is invalid or not supported\\n\",\n                       (char *)av_x_if_null(av_get_sample_fmt_name(avctx->sample_fmt), buf));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (avctx->codec->pix_fmts) {\n            for (i = 0; avctx->codec->pix_fmts[i] != AV_PIX_FMT_NONE; i++)\n                if (avctx->pix_fmt == avctx->codec->pix_fmts[i])\n                    break;\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_NONE\n                && !((avctx->codec_id == AV_CODEC_ID_MJPEG || avctx->codec_id == AV_CODEC_ID_LJPEG)\n                     && avctx->strict_std_compliance <= FF_COMPLIANCE_UNOFFICIAL)) {\n                char buf[128];\n                snprintf(buf, sizeof(buf), \"%d\", avctx->pix_fmt);\n                av_log(avctx, AV_LOG_ERROR, \"Specified pixel format %s is invalid or not supported\\n\",\n                       (char *)av_x_if_null(av_get_pix_fmt_name(avctx->pix_fmt), buf));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ420P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ411P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ422P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ440P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ444P)\n                avctx->color_range = AVCOL_RANGE_JPEG;\n        }\n        if (avctx->codec->supported_samplerates) {\n            for (i = 0; avctx->codec->supported_samplerates[i] != 0; i++)\n                if (avctx->sample_rate == avctx->codec->supported_samplerates[i])\n                    break;\n            if (avctx->codec->supported_samplerates[i] == 0) {\n                av_log(avctx, AV_LOG_ERROR, \"Specified sample rate %d is not supported\\n\",\n                       avctx->sample_rate);\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (avctx->sample_rate < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Specified sample rate %d is not supported\\n\",\n                    avctx->sample_rate);\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->codec->channel_layouts) {\n            if (!avctx->channel_layout) {\n                av_log(avctx, AV_LOG_WARNING, \"Channel layout not specified\\n\");\n            } else {\n                for (i = 0; avctx->codec->channel_layouts[i] != 0; i++)\n                    if (avctx->channel_layout == avctx->codec->channel_layouts[i])\n                        break;\n                if (avctx->codec->channel_layouts[i] == 0) {\n                    char buf[512];\n                    av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                    av_log(avctx, AV_LOG_ERROR, \"Specified channel layout '%s' is not supported\\n\", buf);\n                    ret = AVERROR(EINVAL);\n                    goto free_and_end;\n                }\n            }\n        }\n        if (avctx->channel_layout && avctx->channels) {\n            int channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n            if (channels != avctx->channels) {\n                char buf[512];\n                av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Channel layout '%s' with %d channels does not match number of specified channels %d\\n\",\n                       buf, channels, avctx->channels);\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        } else if (avctx->channel_layout) {\n            avctx->channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n        }\n        if (avctx->channels < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Specified number of channels %d is not supported\\n\",\n                    avctx->channels);\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if(avctx->codec_type == AVMEDIA_TYPE_VIDEO) {\n            pixdesc = av_pix_fmt_desc_get(avctx->pix_fmt);\n            if (    avctx->bits_per_raw_sample < 0\n                || (avctx->bits_per_raw_sample > 8 && pixdesc->comp[0].depth <= 8)) {\n                av_log(avctx, AV_LOG_WARNING, \"Specified bit depth %d not possible with the specified pixel formats depth %d\\n\",\n                    avctx->bits_per_raw_sample, pixdesc->comp[0].depth);\n                avctx->bits_per_raw_sample = pixdesc->comp[0].depth;\n            }\n            if (avctx->width <= 0 || avctx->height <= 0) {\n                av_log(avctx, AV_LOG_ERROR, \"dimensions not set\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n        }\n        if (   (avctx->codec_type == AVMEDIA_TYPE_VIDEO || avctx->codec_type == AVMEDIA_TYPE_AUDIO)\n            && avctx->bit_rate>0 && avctx->bit_rate<1000) {\n            av_log(avctx, AV_LOG_WARNING, \"Bitrate %\"PRId64\" is extremely low, maybe you mean %\"PRId64\"k\\n\", avctx->bit_rate, avctx->bit_rate);\n        }\n\n        if (!avctx->rc_initial_buffer_occupancy)\n            avctx->rc_initial_buffer_occupancy = avctx->rc_buffer_size * 3LL / 4;\n\n        if (avctx->ticks_per_frame && avctx->time_base.num &&\n            avctx->ticks_per_frame > INT_MAX / avctx->time_base.num) {\n            av_log(avctx, AV_LOG_ERROR,\n                   \"ticks_per_frame %d too large for the timebase %d/%d.\",\n                   avctx->ticks_per_frame,\n                   avctx->time_base.num,\n                   avctx->time_base.den);\n            goto free_and_end;\n        }\n\n        if (avctx->hw_frames_ctx) {\n            AVHWFramesContext *frames_ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;\n            if (frames_ctx->format != avctx->pix_fmt) {\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Mismatching AVCodecContext.pix_fmt and AVHWFramesContext.format\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            if (avctx->sw_pix_fmt != AV_PIX_FMT_NONE &&\n                avctx->sw_pix_fmt != frames_ctx->sw_format) {\n                av_log(avctx, AV_LOG_ERROR,\n                       \"Mismatching AVCodecContext.sw_pix_fmt (%s) \"\n                       \"and AVHWFramesContext.sw_format (%s)\\n\",\n                       av_get_pix_fmt_name(avctx->sw_pix_fmt),\n                       av_get_pix_fmt_name(frames_ctx->sw_format));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            avctx->sw_pix_fmt = frames_ctx->sw_format;\n        }\n    }\n\n    avctx->pts_correction_num_faulty_pts =\n    avctx->pts_correction_num_faulty_dts = 0;\n    avctx->pts_correction_last_pts =\n    avctx->pts_correction_last_dts = INT64_MIN;\n\n    if (   !CONFIG_GRAY && avctx->flags & AV_CODEC_FLAG_GRAY\n        && avctx->codec_descriptor->type == AVMEDIA_TYPE_VIDEO)\n        av_log(avctx, AV_LOG_WARNING,\n               \"gray decoding requested but not enabled at configuration time\\n\");\n\n    if (   avctx->codec->init && (!(avctx->active_thread_type&FF_THREAD_FRAME)\n        || avctx->internal->frame_thread_encoder)) {\n        ret = avctx->codec->init(avctx);\n        if (ret < 0) {\n            goto free_and_end;\n        }\n        codec_init_ok = 1;\n    }\n\n    ret=0;\n\n    if (av_codec_is_decoder(avctx->codec)) {\n        if (!avctx->bit_rate)\n            avctx->bit_rate = get_bit_rate(avctx);\n        /* validate channel layout from the decoder */\n        if (avctx->channel_layout) {\n            int channels = av_get_channel_layout_nb_channels(avctx->channel_layout);\n            if (!avctx->channels)\n                avctx->channels = channels;\n            else if (channels != avctx->channels) {\n                char buf[512];\n                av_get_channel_layout_string(buf, sizeof(buf), -1, avctx->channel_layout);\n                av_log(avctx, AV_LOG_WARNING,\n                       \"Channel layout '%s' with %d channels does not match specified number of channels %d: \"\n                       \"ignoring specified channel layout\\n\",\n                       buf, channels, avctx->channels);\n                avctx->channel_layout = 0;\n            }\n        }\n        if (avctx->channels && avctx->channels < 0 ||\n            avctx->channels > FF_SANE_NB_CHANNELS) {\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->bits_per_coded_sample < 0) {\n            ret = AVERROR(EINVAL);\n            goto free_and_end;\n        }\n        if (avctx->sub_charenc) {\n            if (avctx->codec_type != AVMEDIA_TYPE_SUBTITLE) {\n                av_log(avctx, AV_LOG_ERROR, \"Character encoding is only \"\n                       \"supported with subtitles codecs\\n\");\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            } else if (avctx->codec_descriptor->props & AV_CODEC_PROP_BITMAP_SUB) {\n                av_log(avctx, AV_LOG_WARNING, \"Codec '%s' is bitmap-based, \"\n                       \"subtitles character encoding will be ignored\\n\",\n                       avctx->codec_descriptor->name);\n                avctx->sub_charenc_mode = FF_SUB_CHARENC_MODE_DO_NOTHING;\n            } else {\n                /* input character encoding is set for a text based subtitle\n                 * codec at this point */\n                if (avctx->sub_charenc_mode == FF_SUB_CHARENC_MODE_AUTOMATIC)\n                    avctx->sub_charenc_mode = FF_SUB_CHARENC_MODE_PRE_DECODER;\n\n                if (avctx->sub_charenc_mode == FF_SUB_CHARENC_MODE_PRE_DECODER) {\n#if CONFIG_ICONV\n                    iconv_t cd = iconv_open(\"UTF-8\", avctx->sub_charenc);\n                    if (cd == (iconv_t)-1) {\n                        ret = AVERROR(errno);\n                        av_log(avctx, AV_LOG_ERROR, \"Unable to open iconv context \"\n                               \"with input character encoding \\\"%s\\\"\\n\", avctx->sub_charenc);\n                        goto free_and_end;\n                    }\n                    iconv_close(cd);\n#else\n                    av_log(avctx, AV_LOG_ERROR, \"Character encoding subtitles \"\n                           \"conversion needs a libavcodec built with iconv support \"\n                           \"for this codec\\n\");\n                    ret = AVERROR(ENOSYS);\n                    goto free_and_end;\n#endif\n                }\n            }\n        }\n\n#if FF_API_AVCTX_TIMEBASE\n        if (avctx->framerate.num > 0 && avctx->framerate.den > 0)\n            avctx->time_base = av_inv_q(av_mul_q(avctx->framerate, (AVRational){avctx->ticks_per_frame, 1}));\n#endif\n    }\n    if (codec->priv_data_size > 0 && avctx->priv_data && codec->priv_class) {\n        av_assert0(*(const AVClass **)avctx->priv_data == codec->priv_class);\n    }\n\nend:\n    ff_unlock_avcodec(codec);\n    if (options) {\n        av_dict_free(options);\n        *options = tmp;\n    }\n\n    return ret;\nfree_and_end:\n    if (avctx->codec && avctx->codec->close &&\n        (codec_init_ok ||\n         (avctx->codec->caps_internal & FF_CODEC_CAP_INIT_CLEANUP)))\n        avctx->codec->close(avctx);\n\n    if (codec->priv_class && codec->priv_data_size)\n        av_opt_free(avctx->priv_data);\n    av_opt_free(avctx);\n\n#if FF_API_CODED_FRAME\nFF_DISABLE_DEPRECATION_WARNINGS\n    av_frame_free(&avctx->coded_frame);\nFF_ENABLE_DEPRECATION_WARNINGS\n#endif\n\n    av_dict_free(&tmp);\n    av_freep(&avctx->priv_data);\n    if (avctx->internal) {\n        av_frame_free(&avctx->internal->to_free);\n        av_frame_free(&avctx->internal->compat_decode_frame);\n        av_frame_free(&avctx->internal->buffer_frame);\n        av_packet_free(&avctx->internal->buffer_pkt);\n        av_packet_free(&avctx->internal->last_pkt_props);\n\n        av_packet_free(&avctx->internal->ds.in_pkt);\n        ff_decode_bsfs_uninit(avctx);\n\n        av_freep(&avctx->internal->pool);\n    }\n    av_freep(&avctx->internal);\n    avctx->codec = NULL;\n    goto end;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/8df6884832ec413cf032dfaa45c23b1c7876670c", "file_name": "libavcodec/utils.c", "vul_type": "cwe-476", "description": "Write a C function in FFmpeg to open a codec context with given options."}
{"func_name": "run", "func_src_before": "func run() error {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "func_src_after": "func run() error {\n\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 68, "line": "\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 81, "line": "\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n"}]}, "char_changes": {"deleted": [{"char_start": 62, "char_end": 66, "chars": "1024"}], "added": [{"char_start": 45, "char_end": 55, "chars": "MultiPrime"}, {"char_start": 72, "char_end": 79, "chars": "3, 2048"}]}, "commit_link": "github.com/carl-mastrangelo/pixur/commit/d2bc8ec79aa4f2b68ec14a2d6cd5a305b6e05dd1", "file_name": "genkeys.go", "vul_type": "cwe-326", "commit_msg": "Use multiprime rsa keys, and bump to 2048 bits", "parent_commit": "547289bc91415ef039e318ce6b0b53b16b66998b", "description": "Write a Go function to generate an RSA key pair and save them to files."}
{"func_name": "main", "func_src_before": "int main() {\n\tTERM *t;\n\tchar buffer[300];\n\n\tprintf(\"lci - A lambda calculus interpreter\\n\");\n\tprintf(\"Copyright (C) 2003 Kostas Hatzikokolakis\\n\");\n\tprintf(\"This is FREE SOFTWARE and comes with ABSOLUTELY NO WARRANTY\\n\\n\");\n\tprintf(\"Type a term, Help for info or Quit to exit.\\n\");\n\n\t// read and execute .lcirc\n\tconsultFile(\".lcirc\");\n\n\t// read and execute commands\n\twhile(!feof(stdin)) {\n\t\tprintf(\"lci> \");\n\t\tif(!gets(buffer)) break;\n\t\tif(buffer[0] == '\\0') continue;\n\n\t\tscInputType = SC_BUFFER;\n\t\tscInput = buffer;\n\t\tgetToken(NULL);\n\n\t\tif(parse((void*)&t, TK_TERM) == PAR_OK)\n\t\t\texecTerm(t);\n\t\telse\n\t\t\tprintf(\"Syntax error\\n\\n\");\n\t}\n\n\treturn 0;\n}", "func_src_after": "int main() {\n\tTERM *t;\n\tchar buffer[300];\n\n\tprintf(\"lci - A lambda calculus interpreter\\n\");\n\tprintf(\"Copyright (C) 2003 Kostas Hatzikokolakis\\n\");\n\tprintf(\"This is FREE SOFTWARE and comes with ABSOLUTELY NO WARRANTY\\n\\n\");\n\tprintf(\"Type a term, Help for info or Quit to exit.\\n\");\n\n\t// read and execute .lcirc\n\tconsultFile(\".lcirc\");\n\n\t// read and execute commands\n\twhile(!feof(stdin)) {\n\t\tprintf(\"lci> \");\n\t\tif(!fgets(buffer, sizeof(buffer), stdin)) break;\n\t\tif(strcmp(buffer, \"\\n\") == 0) continue;\n\n\t\tscInputType = SC_BUFFER;\n\t\tscInput = buffer;\n\t\tgetToken(NULL);\n\n\t\tif(parse((void*)&t, TK_TERM) == PAR_OK)\n\t\t\texecTerm(t);\n\t\telse\n\t\t\tprintf(\"Syntax error\\n\\n\");\n\t}\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 408, "char_end": 435, "line": "\t\tif(!gets(buffer)) break;\n"}, {"line_no": 17, "char_start": 435, "char_end": 469, "line": "\t\tif(buffer[0] == '\\0') continue;\n"}], "added": [{"line_no": 16, "char_start": 408, "char_end": 459, "line": "\t\tif(!fgets(buffer, sizeof(buffer), stdin)) break;\n"}, {"line_no": 17, "char_start": 459, "char_end": 501, "line": "\t\tif(strcmp(buffer, \"\\n\") == 0) continue;\n"}]}, "char_changes": {"deleted": [{"char_start": 425, "char_end": 457, "chars": ")) break;\n\t\tif(buffer[0] == '\\0'"}], "added": [{"char_start": 414, "char_end": 415, "chars": "f"}, {"char_start": 426, "char_end": 489, "chars": ", sizeof(buffer), stdin)) break;\n\t\tif(strcmp(buffer, \"\\n\") == 0"}]}, "commit_link": "github.com/8l/lci/commit/82786e58e0e56a00d97ded474c6589c08e885149", "file_name": "main.c", "vul_type": "cwe-676", "commit_msg": "Replaced the 'dangerous' gets by fgets.", "parent_commit": "204649b76eb58335e3c8e1bf4831039ea91f0489", "description": "Write a C program that serves as a simple command-line interpreter for lambda calculus expressions, including a greeting message and a loop to process user input."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Import data\n  if FLAGS.fake_data:\n    imgs = tf.random.uniform(maxval=256, shape=(10, 28, 28), dtype=tf.int32)\n    labels = tf.random.uniform(maxval=10, shape=(10,), dtype=tf.int32)\n    mnist_train = imgs, labels\n    mnist_test = imgs, labels\n  else:\n    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n\n  def format_example(imgs, labels):\n    imgs = tf.reshape(imgs, [-1, 28 * 28])\n    imgs = tf.cast(imgs, tf.float32) / 255.0\n    labels = tf.one_hot(labels, depth=10, dtype=tf.float32)\n    return imgs, labels\n\n  ds_train = tf.data.Dataset.from_tensor_slices(mnist_train)\n  ds_train = ds_train.shuffle(\n      1000, seed=RAND_SEED).repeat().batch(FLAGS.train_batch_size)\n  ds_train = ds_train.map(format_example)\n  it_train = ds_train.make_initializable_iterator()\n\n  ds_test = tf.data.Dataset.from_tensors(mnist_test).repeat()\n  ds_test = ds_test.map(format_example)\n  it_test = ds_test.make_initializable_iterator()\n\n  sess = tf.InteractiveSession()\n\n  # Create the MNIST neural network graph.\n\n  # Input placeholders.\n  with tf.name_scope(\"input\"):\n    handle = tf.placeholder(tf.string, shape=())\n\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, (tf.float32, tf.float32),\n        ((None, IMAGE_SIZE * IMAGE_SIZE), (None, 10)))\n\n    x, y_ = iterator.get_next()\n\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\"weights\"):\n        weights = weight_variable([input_dim, output_dim])\n      with tf.name_scope(\"biases\"):\n        biases = bias_variable([output_dim])\n      with tf.name_scope(\"Wx_plus_b\"):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n\n      activations = act(preactivate)\n      return activations\n\n  hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n  logits = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"output\", tf.identity)\n  y = tf.nn.softmax(logits)\n\n  with tf.name_scope(\"cross_entropy\"):\n    # The following line is the culprit of the bad numerical values that appear\n    # during training of this graph. Log of zero gives inf, which is first seen\n    # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n    # call. A multiplication of the inf values with zeros leads to nans,\n    # which is first in \"cross_entropy/mul:0\".\n    #\n    # You can use the built-in, numerically-stable implementation to fix this\n    # issue:\n    #   diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n\n    diff = -(y_ * tf.log(y))\n    with tf.name_scope(\"total\"):\n      cross_entropy = tf.reduce_mean(diff)\n\n  with tf.name_scope(\"train\"):\n    train_step = tf.train.AdamOptimizer(\n        FLAGS.learning_rate).minimize(cross_entropy)\n\n  with tf.name_scope(\"accuracy\"):\n    with tf.name_scope(\"correct_prediction\"):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\"accuracy\"):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  sess.run(tf.global_variables_initializer())\n  sess.run(it_train.initializer)\n  sess.run(it_test.initializer)\n  train_handle = sess.run(it_train.string_handle())\n  test_handle = sess.run(it_test.string_handle())\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n\n  # Add this point, sess is a debug wrapper around the actual Session if\n  # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n  for i in range(FLAGS.max_steps):\n    acc = sess.run(accuracy, feed_dict={handle: test_handle})\n    print(\"Accuracy at step %d: %s\" % (i, acc))\n\n    sess.run(train_step, feed_dict={handle: train_handle})", "func_src_after": "def main(_):\n  # Import data\n  if FLAGS.fake_data:\n    imgs = tf.random.uniform(maxval=256, shape=(10, 28, 28), dtype=tf.int32)\n    labels = tf.random.uniform(maxval=10, shape=(10,), dtype=tf.int32)\n    mnist_train = imgs, labels\n    mnist_test = imgs, labels\n  else:\n    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n\n  def format_example(imgs, labels):\n    imgs = tf.reshape(imgs, [-1, 28 * 28])\n    imgs = tf.cast(imgs, tf.float32) / 255.0\n    labels = tf.one_hot(labels, depth=10, dtype=tf.float32)\n    return imgs, labels\n\n  ds_train = tf.data.Dataset.from_tensor_slices(mnist_train)\n  ds_train = ds_train.shuffle(\n      1000, seed=RAND_SEED).repeat().batch(FLAGS.train_batch_size)\n  ds_train = ds_train.map(format_example)\n  it_train = ds_train.make_initializable_iterator()\n\n  ds_test = tf.data.Dataset.from_tensors(mnist_test).repeat()\n  ds_test = ds_test.map(format_example)\n  it_test = ds_test.make_initializable_iterator()\n\n  sess = tf.InteractiveSession()\n\n  # Create the MNIST neural network graph.\n\n  # Input placeholders.\n  with tf.name_scope(\"input\"):\n    handle = tf.placeholder(tf.string, shape=())\n\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, (tf.float32, tf.float32),\n        ((None, IMAGE_SIZE * IMAGE_SIZE), (None, 10)))\n\n    x, y_ = iterator.get_next()\n\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\"weights\"):\n        weights = weight_variable([input_dim, output_dim])\n      with tf.name_scope(\"biases\"):\n        biases = bias_variable([output_dim])\n      with tf.name_scope(\"Wx_plus_b\"):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n\n      activations = act(preactivate)\n      return activations\n\n  hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n  logits = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"output\", tf.identity)\n  y = tf.nn.softmax(logits)\n\n  with tf.name_scope(\"cross_entropy\"):\n    # The following line is the culprit of the bad numerical values that appear\n    # during training of this graph. Log of zero gives inf, which is first seen\n    # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n    # call. A multiplication of the inf values with zeros leads to nans,\n    # which is first in \"cross_entropy/mul:0\".\n    #\n    # You can use the built-in, numerically-stable implementation to fix this\n    # issue:\n    #   diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits)\n\n    diff = -(y_ * tf.log(y))\n    with tf.name_scope(\"total\"):\n      cross_entropy = tf.reduce_mean(diff)\n\n  with tf.name_scope(\"train\"):\n    train_step = tf.train.AdamOptimizer(\n        FLAGS.learning_rate).minimize(cross_entropy)\n\n  with tf.name_scope(\"accuracy\"):\n    with tf.name_scope(\"correct_prediction\"):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\"accuracy\"):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  sess.run(tf.global_variables_initializer())\n  sess.run(it_train.initializer)\n  sess.run(it_test.initializer)\n  train_handle = sess.run(it_train.string_handle())\n  test_handle = sess.run(it_test.string_handle())\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n  elif FLAGS.tensorboard_debug_address:\n    sess = tf_debug.TensorBoardDebugWrapperSession(\n        sess, FLAGS.tensorboard_debug_address)\n\n  # Add this point, sess is a debug wrapper around the actual Session if\n  # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n  for i in range(FLAGS.max_steps):\n    acc = sess.run(accuracy, feed_dict={handle: test_handle})\n    print(\"Accuracy at step %d: %s\" % (i, acc))\n\n    sess.run(train_step, feed_dict={handle: train_handle})", "line_changes": {"deleted": [{"line_no": 106, "char_start": 3998, "char_end": 4023, "line": "    config_file_path = (\n"}, {"line_no": 107, "char_start": 4023, "char_end": 4064, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 108, "char_start": 4064, "char_end": 4115, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 106, "char_start": 3998, "char_end": 4035, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 108, "char_start": 4088, "char_end": 4150, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 109, "char_start": 4150, "char_end": 4160, "line": "    else:\n"}, {"line_no": 110, "char_start": 4160, "char_end": 4190, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 4020, "char_end": 4030, "chars": " (\n       "}, {"char_start": 4068, "char_end": 4092, "chars": "    if FLAGS.use_random_"}, {"char_start": 4104, "char_end": 4108, "chars": "else"}, {"char_start": 4113, "char_end": 4114, "chars": ")"}], "added": [{"char_start": 4002, "char_end": 4097, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 4127, "char_end": 4128, "chars": "s"}, {"char_start": 4154, "char_end": 4166, "chars": "else:\n      "}, {"char_start": 4173, "char_end": 4178, "chars": "file_"}, {"char_start": 4183, "char_end": 4184, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/b5150106a7829c45892927562b7eed101581ea95", "file_name": "debug_mnist_v1.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359231\nChange-Id: If2049dbeb46fb8ff6df7c8e077cee8be3872e5b4", "description": "Write a Python TensorFlow script that trains a neural network on the MNIST dataset with options for fake data and debugging."}
{"func_name": "WriteTIFFImage", "func_src_before": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\n    if (image->exception.severity > ErrorException)\n      break;\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    (void) TIFFWriteDirectory(tiff);\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(image->exception.severity > ErrorException ? MagickFalse : MagickTrue);\n}", "func_src_after": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    if (TIFFWriteDirectory(tiff) == 0)\n      {\n        status=MagickFalse;\n        break;\n      }\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick6/commit/3c53413eb544cc567309b4c86485eae43e956112", "file_name": "coders/tiff.c", "vul_type": "cwe-125", "description": "Write a function in C to save an image as a TIFF file."}
{"func_name": "_remove_volume_set", "func_src_before": "    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run('setqos -clear vvset:%s' % (vvs_name), None)\n        self._cli_run('removevvset -f %s' % (vvs_name), None)", "func_src_after": "    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])\n        self._cli_run(['removevvset', '-f', vvs_name])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to clear QoS rules and remove a volume set using CLI commands."}
{"func_name": "string_scan_range", "func_src_before": "static int string_scan_range(RList *list, RBinFile *bf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type) {\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (from >= to) {\n\t\teprintf (\"Invalid range to find strings 0x%llx .. 0x%llx\\n\", from, to);\n\t\treturn -1;\n\t}\n\tut8 *buf = calloc (to - from, 1);\n\tif (!buf || !min) {\n\t\treturn -1;\n\t}\n\tr_buf_read_at (bf->buf, from, buf, to - from);\n\t// may oobread\n\twhile (needle < to) {\n\t\trc = r_utf8_decode (buf + needle - from, to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + needle + rc - from;\n\t\t\tif ((to - needle) > 5) {\n\t\t\t\tbool is_wide32 = needle + rc + 2 < to && !w[0] && !w[1] && !w[2] && w[3] && !w[4];\n\t\t\t\tif (is_wide32) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32;\n\t\t\t\t} else {\n\t\t\t\t\tbool is_wide = needle + rc + 2 < to && !w[0] && w[1] && !w[2];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t}\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (rc = i = 0; i < sizeof (tmp) - 3 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r) && r != '\\\\') {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (&tmp[i], r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\033\\\\\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 93) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e  \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"  \\\\\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes >= min) {\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\t// reduce false positives\n\t\t\t\tint j;\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tRBinString *bs = R_NEW0 (RBinString);\n\t\t\tif (!bs) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->type = str_type;\n\t\t\tbs->length = runes;\n\t\t\tbs->size = needle - str_start;\n\t\t\tbs->ordinal = count++;\n\t\t\t// TODO: move into adjust_offset\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\tif (str_start -from> 1) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 2 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tif (str_start -from> 3) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 4 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->paddr = bs->vaddr = str_start;\n\t\t\tbs->string = r_str_ndup ((const char *)tmp, i);\n\t\t\tif (list) {\n\t\t\t\tr_list_append (list, bs);\n\t\t\t} else {\n\t\t\t\tprint_string (bs, bf);\n\t\t\t\tr_bin_string_free (bs);\n\t\t\t}\n\t\t}\n\t}\n\tfree (buf);\n\treturn count;\n}", "func_src_after": "static int string_scan_range(RList *list, RBinFile *bf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type) {\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (from >= to) {\n\t\teprintf (\"Invalid range to find strings 0x%llx .. 0x%llx\\n\", from, to);\n\t\treturn -1;\n\t}\n\tint len = to - from;\n\tut8 *buf = calloc (len, 1);\n\tif (!buf || !min) {\n\t\treturn -1;\n\t}\n\tr_buf_read_at (bf->buf, from, buf, len);\n\t// may oobread\n\twhile (needle < to) {\n\t\trc = r_utf8_decode (buf + needle - from, to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + needle + rc - from;\n\t\t\tif ((to - needle) > 5 + rc) {\n\t\t\t\tbool is_wide32 = (needle + rc + 2 < to) && (!w[0] && !w[1] && !w[2] && w[3] && !w[4]);\n\t\t\t\tif (is_wide32) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32;\n\t\t\t\t} else {\n\t\t\t\t\tbool is_wide = needle + rc + 2 < to && !w[0] && w[1] && !w[2];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t}\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (rc = i = 0; i < sizeof (tmp) - 3 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r) && r != '\\\\') {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (&tmp[i], r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\033\\\\\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 93) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e  \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"  \\\\\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes >= min) {\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\t// reduce false positives\n\t\t\t\tint j;\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tRBinString *bs = R_NEW0 (RBinString);\n\t\t\tif (!bs) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->type = str_type;\n\t\t\tbs->length = runes;\n\t\t\tbs->size = needle - str_start;\n\t\t\tbs->ordinal = count++;\n\t\t\t// TODO: move into adjust_offset\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\tif (str_start -from> 1) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 2 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tif (str_start -from> 3) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 4 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->paddr = bs->vaddr = str_start;\n\t\t\tbs->string = r_str_ndup ((const char *)tmp, i);\n\t\t\tif (list) {\n\t\t\t\tr_list_append (list, bs);\n\t\t\t} else {\n\t\t\t\tprint_string (bs, bf);\n\t\t\t\tr_bin_string_free (bs);\n\t\t\t}\n\t\t}\n\t}\n\tfree (buf);\n\treturn count;\n}", "commit_link": "github.com/radare/radare2/commit/3fcf41ed96ffa25b38029449520c8d0a198745f3", "file_name": "libr/bin/file.c", "vul_type": "cwe-125", "description": "Write a C function to scan for and process strings within a specified range in a binary file."}
{"func_name": "tcp_test", "func_src_before": "int tcp_test(const char* ip_str, const short port)\n{\n    int sock, i;\n    struct sockaddr_in s_in;\n    int packetsize = 1024;\n    unsigned char packet[packetsize];\n    struct timeval tv, tv2, tv3;\n    int caplen = 0;\n    int times[REQUESTS];\n    int min, avg, max, len;\n    struct net_hdr nh;\n\n    tv3.tv_sec=0;\n    tv3.tv_usec=1;\n\n    s_in.sin_family = PF_INET;\n    s_in.sin_port = htons(port);\n    if (!inet_aton(ip_str, &s_in.sin_addr))\n            return -1;\n\n    if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n            return -1;\n\n    /* avoid blocking on reading the socket */\n    if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n    {\n        perror( \"fcntl(O_NONBLOCK) failed\" );\n        return( 1 );\n    }\n\n    gettimeofday( &tv, NULL );\n\n    while (1)  //waiting for relayed packet\n    {\n        if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n        {\n            if(errno != EINPROGRESS && errno != EALREADY)\n            {\n                perror(\"connect\");\n                close(sock);\n\n                printf(\"Failed to connect\\n\");\n\n                return -1;\n            }\n        }\n        else\n        {\n            gettimeofday( &tv2, NULL );\n            break;\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 3000ms for a successful connect\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (3000*1000))\n        {\n            printf(\"Connection timed out\\n\");\n            close(sock);\n            return(-1);\n        }\n        usleep(10);\n    }\n\n    PCT; printf(\"TCP connection successful\\n\");\n\n    //trying to identify airserv-ng\n    memset(&nh, 0, sizeof(nh));\n//     command: GET_CHAN\n    nh.nh_type\t= 2;\n    nh.nh_len\t= htonl(0);\n\n    if (send(sock, &nh, sizeof(nh), 0) != sizeof(nh))\n    {\n        perror(\"send\");\n        return -1;\n    }\n\n    gettimeofday( &tv, NULL );\n    i=0;\n\n    while (1)  //waiting for GET_CHAN answer\n    {\n        caplen = read(sock, &nh, sizeof(nh));\n\n        if(caplen == -1)\n        {\n            if( errno != EAGAIN )\n            {\n                perror(\"read\");\n                return -1;\n            }\n        }\n\n        if( (unsigned)caplen == sizeof(nh))\n        {\n            len = ntohl(nh.nh_len);\n            if( nh.nh_type == 1 && i==0 )\n            {\n                i=1;\n                caplen = read(sock, packet, len);\n                if(caplen == len)\n                {\n                    i=2;\n                    break;\n                }\n                else\n                {\n                    i=0;\n                }\n            }\n            else\n            {\n                caplen = read(sock, packet, len);\n            }\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 1000ms for an answer\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n        {\n            break;\n        }\n        if(caplen == -1)\n            usleep(10);\n    }\n\n    if(i==2)\n    {\n        PCT; printf(\"airserv-ng found\\n\");\n    }\n    else\n    {\n        PCT; printf(\"airserv-ng NOT found\\n\");\n    }\n\n    close(sock);\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n                return -1;\n\n        /* avoid blocking on reading the socket */\n        if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n        {\n            perror( \"fcntl(O_NONBLOCK) failed\" );\n            return( 1 );\n        }\n\n        usleep(1000);\n\n        gettimeofday( &tv, NULL );\n\n        while (1)  //waiting for relayed packet\n        {\n            if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n            {\n                if(errno != EINPROGRESS && errno != EALREADY)\n                {\n                    perror(\"connect\");\n                    close(sock);\n\n                    printf(\"Failed to connect\\n\");\n\n                    return -1;\n                }\n            }\n            else\n            {\n                gettimeofday( &tv2, NULL );\n                break;\n            }\n\n            gettimeofday( &tv2, NULL );\n            //wait 1000ms for a successful connect\n            if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n            {\n                break;\n            }\n            //simple \"high-precision\" usleep\n            select(1, NULL, NULL, NULL, &tv3);\n        }\n        times[i] = ((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec));\n        printf( \"\\r%d/%d\\r\", i, REQUESTS);\n        fflush(stdout);\n        close(sock);\n    }\n\n    min = INT_MAX;\n    avg = 0;\n    max = 0;\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if(times[i] < min) min = times[i];\n        if(times[i] > max) max = times[i];\n        avg += times[i];\n    }\n    avg /= REQUESTS;\n\n    PCT; printf(\"ping %s:%d (min/avg/max): %.3fms/%.3fms/%.3fms\\n\", ip_str, port, min/1000.0, avg/1000.0, max/1000.0);\n\n    return 0;\n}", "func_src_after": "int tcp_test(const char* ip_str, const short port)\n{\n    int sock, i;\n    struct sockaddr_in s_in;\n    int packetsize = 1024;\n    unsigned char packet[packetsize];\n    struct timeval tv, tv2, tv3;\n    int caplen = 0;\n    int times[REQUESTS];\n    int min, avg, max, len;\n    struct net_hdr nh;\n\n    tv3.tv_sec=0;\n    tv3.tv_usec=1;\n\n    s_in.sin_family = PF_INET;\n    s_in.sin_port = htons(port);\n    if (!inet_aton(ip_str, &s_in.sin_addr))\n            return -1;\n\n    if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n            return -1;\n\n    /* avoid blocking on reading the socket */\n    if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n    {\n        perror( \"fcntl(O_NONBLOCK) failed\" );\n        return( 1 );\n    }\n\n    gettimeofday( &tv, NULL );\n\n    while (1)  //waiting for relayed packet\n    {\n        if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n        {\n            if(errno != EINPROGRESS && errno != EALREADY)\n            {\n                perror(\"connect\");\n                close(sock);\n\n                printf(\"Failed to connect\\n\");\n\n                return -1;\n            }\n        }\n        else\n        {\n            gettimeofday( &tv2, NULL );\n            break;\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 3000ms for a successful connect\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (3000*1000))\n        {\n            printf(\"Connection timed out\\n\");\n            close(sock);\n            return(-1);\n        }\n        usleep(10);\n    }\n\n    PCT; printf(\"TCP connection successful\\n\");\n\n    //trying to identify airserv-ng\n    memset(&nh, 0, sizeof(nh));\n//     command: GET_CHAN\n    nh.nh_type\t= 2;\n    nh.nh_len\t= htonl(0);\n\n    if (send(sock, &nh, sizeof(nh), 0) != sizeof(nh))\n    {\n        perror(\"send\");\n        return -1;\n    }\n\n    gettimeofday( &tv, NULL );\n    i=0;\n\n    while (1)  //waiting for GET_CHAN answer\n    {\n        caplen = read(sock, &nh, sizeof(nh));\n\n        if(caplen == -1)\n        {\n            if( errno != EAGAIN )\n            {\n                perror(\"read\");\n                return -1;\n            }\n        }\n\n        if( (unsigned)caplen == sizeof(nh))\n        {\n            len = ntohl(nh.nh_len);\n            if (len > 1024 || len < 0)\n                continue;\n            if( nh.nh_type == 1 && i==0 )\n            {\n                i=1;\n                caplen = read(sock, packet, len);\n                if(caplen == len)\n                {\n                    i=2;\n                    break;\n                }\n                else\n                {\n                    i=0;\n                }\n            }\n            else\n            {\n                caplen = read(sock, packet, len);\n            }\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 1000ms for an answer\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n        {\n            break;\n        }\n        if(caplen == -1)\n            usleep(10);\n    }\n\n    if(i==2)\n    {\n        PCT; printf(\"airserv-ng found\\n\");\n    }\n    else\n    {\n        PCT; printf(\"airserv-ng NOT found\\n\");\n    }\n\n    close(sock);\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n                return -1;\n\n        /* avoid blocking on reading the socket */\n        if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n        {\n            perror( \"fcntl(O_NONBLOCK) failed\" );\n            return( 1 );\n        }\n\n        usleep(1000);\n\n        gettimeofday( &tv, NULL );\n\n        while (1)  //waiting for relayed packet\n        {\n            if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n            {\n                if(errno != EINPROGRESS && errno != EALREADY)\n                {\n                    perror(\"connect\");\n                    close(sock);\n\n                    printf(\"Failed to connect\\n\");\n\n                    return -1;\n                }\n            }\n            else\n            {\n                gettimeofday( &tv2, NULL );\n                break;\n            }\n\n            gettimeofday( &tv2, NULL );\n            //wait 1000ms for a successful connect\n            if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n            {\n                break;\n            }\n            //simple \"high-precision\" usleep\n            select(1, NULL, NULL, NULL, &tv3);\n        }\n        times[i] = ((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec));\n        printf( \"\\r%d/%d\\r\", i, REQUESTS);\n        fflush(stdout);\n        close(sock);\n    }\n\n    min = INT_MAX;\n    avg = 0;\n    max = 0;\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if(times[i] < min) min = times[i];\n        if(times[i] > max) max = times[i];\n        avg += times[i];\n    }\n    avg /= REQUESTS;\n\n    PCT; printf(\"ping %s:%d (min/avg/max): %.3fms/%.3fms/%.3fms\\n\", ip_str, port, min/1000.0, avg/1000.0, max/1000.0);\n\n    return 0;\n}", "commit_link": "github.com/aircrack-ng/aircrack-ng/commit/091b153f294b9b695b0b2831e65936438b550d7b", "file_name": "src/aireplay-ng.c", "vul_type": "cwe-787", "description": "Write a C function named `tcp_test` that attempts to establish a TCP connection to a specified IP and port, sends a specific command, and measures connection times."}
{"func_name": "get_lines", "func_src_before": "def get_lines(command: str) -> List[str]:\n    \"\"\"\n    Run a command and return lines of output\n\n    :param str command: the command to run\n    :returns: list of whitespace-stripped lines output by command\n    \"\"\"\n    stdout = get_output(command)\n    return [line.strip().decode() for line in stdout.splitlines()]", "func_src_after": "def get_lines(command: List[str]) -> List[str]:\n    \"\"\"\n    Run a command and return lines of output\n\n    :param str command: the command to run\n    :returns: list of whitespace-stripped lines output by command\n    \"\"\"\n    stdout = get_output(command)\n    return [line.strip() for line in stdout.splitlines()]", "commit_link": "github.com/timothycrosley/isort/commit/1ab38f4f7840a3c19bf961a24630a992a8373a76", "file_name": "isort/hooks.py", "vul_type": "cwe-078", "description": "Write a Python function named `get_lines` that executes a given command and returns the output as a list of stripped strings."}
{"func_name": "check_testPickle", "func_src_before": "    def check_testPickle(self):\n        \"Test of pickling\"\n        x = arange(12)\n        x[4:10:2] = masked\n        x=x.reshape(4,3)\n        f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb')\n        y = pickle.load(f)\n        assert eq(x,y)", "func_src_after": "    def check_testPickle(self):\n        \"Test of pickling\"\n        import pickle\n        x = arange(12)\n        x[4:10:2] = masked\n        x = x.reshape(4,3)\n        s = pickle.dumps(x)\n        y = pickle.loads(s)\n        assert eq(x,y)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 109, "char_end": 134, "line": "        x=x.reshape(4,3)\n"}, {"line_no": 6, "char_start": 134, "char_end": 169, "line": "        f = open('test9.pik','wb')\n"}, {"line_no": 7, "char_start": 169, "char_end": 191, "line": "        import pickle\n"}, {"line_no": 8, "char_start": 191, "char_end": 217, "line": "        pickle.dump(x, f)\n"}, {"line_no": 9, "char_start": 217, "char_end": 235, "line": "        f.close()\n"}, {"line_no": 10, "char_start": 235, "char_end": 271, "line": "        f = open('test9.pik', 'rb')\n"}, {"line_no": 11, "char_start": 271, "char_end": 298, "line": "        y = pickle.load(f)\n"}, {"line_no": 12, "char_start": 298, "char_end": 320, "line": "        assert eq(x,y) \n"}], "added": [{"line_no": 3, "char_start": 59, "char_end": 81, "line": "        import pickle\n"}]}, "char_changes": {"deleted": [{"char_start": 118, "char_end": 119, "chars": "="}, {"char_start": 142, "char_end": 269, "chars": "f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb'"}, {"char_start": 294, "char_end": 296, "chars": "(f"}], "added": [{"char_start": 59, "char_end": 81, "chars": "        import pickle\n"}, {"char_start": 140, "char_end": 143, "chars": " = "}, {"char_start": 166, "char_end": 167, "chars": "s"}, {"char_start": 170, "char_end": 184, "chars": "pickle.dumps(x"}, {"char_start": 209, "char_end": 212, "chars": "s(s"}]}, "commit_link": "github.com/cjermain/numpy/commit/d1e5d1de77e30c233e98ea7c35f8d7b4623fd1f3", "file_name": "test_ma.py", "vul_type": "cwe-502", "commit_msg": "Use pickle.loads/dumps for test_ma to avoid littering the filesystem with test9.pik files.", "parent_commit": "0e1c71808725c49f65d84847cc6fc7e88909a6de", "description": "Write a Python function that tests pickling and unpickling an array with modified elements and reshaping."}
{"func_name": "get", "func_src_before": "    @web.authenticated\n    def get(self, value):\n        if not self.is_admin():\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write(\"<pre>\")\n        try:\n            server_log_path = os.path.join(options.contest_dir, \"server_log.txt\")\n            with open(server_log_path, 'r') as in_file:\n                lines = [line.decode('utf-8') for line in in_file.readlines()]\n                lines = [line for line in lines if all([v in line for v in value.split('/')])]\n                self.write(''.join(lines))\n        except:\n            logger.error(\"unable to read log: %s\" % (traceback.format_exception(*sys.exc_info()),))\n            self.write(\"unable to read log\")\n        self.write(\"</pre>\")", "func_src_after": "    @web.authenticated\n    def get(self, value):\n        if not self.is_admin():\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write(\"<pre>\")\n        try:\n            server_log_path = os.path.join(options.contest_dir, \"server_log.txt\")\n            with open(server_log_path, 'r') as in_file:\n                lines = [line.decode('utf-8') for line in in_file.readlines()]\n                lines = [line for line in lines if all([v in line for v in value.split('/')])]\n                self.write(escape.xhtml_escape(''.join(lines)))\n        except:\n            logger.error(\"unable to read log: %s\" % (traceback.format_exception(*sys.exc_info()),))\n            self.write(\"unable to read log\")\n        self.write(\"</pre>\")", "line_changes": {"deleted": [{"line_no": 12, "char_start": 524, "char_end": 567, "line": "                self.write(''.join(lines))\n"}], "added": [{"line_no": 12, "char_start": 524, "char_end": 588, "line": "                self.write(escape.xhtml_escape(''.join(lines)))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 551, "char_end": 571, "chars": "escape.xhtml_escape("}, {"char_start": 586, "char_end": 587, "chars": ")"}]}, "commit_link": "github.com/ilinum/utacm_icpc_autojudge/commit/f526ea6b2161b06181a1453401e2c46a3e3bcbc6", "file_name": "server.py", "vul_type": "cwe-079", "commit_msg": "xss fix", "parent_commit": "312bc1f0f70d39a2acddfc88c90b6af40a10873f", "description": "Create a Python function that reads a log file and displays its contents filtered by a provided value, with admin-only access."}
{"func_name": "renderPreviewLink", "func_src_before": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "func_src_after": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 173, "char_end": 283, "line": "      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}], "added": [{"line_no": 6, "char_start": 173, "char_end": 309, "line": "      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 210, "char_end": 236, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/maputnik/editor/commit/3f350c30da0791f542909f665f381e509e68c6c1", "file_name": "ExportModal.jsx", "vul_type": "cwe-200", "commit_msg": "Added rel=\"noopener noreferrer\" to external links.", "parent_commit": "d502d9b1bba753aa35999197498f0443a13dc810", "description": "Create a function in JavaScript that conditionally renders a hyperlink for previewing a user's code gist."}
{"func_name": "voutf", "func_src_before": "static void voutf(struct GlobalConfig *config,\n                  const char *prefix,\n                  const char *fmt,\n                  va_list ap)\n{\n  size_t width = (79 - strlen(prefix));\n  if(!config->mute) {\n    size_t len;\n    char *ptr;\n    char *print_buffer;\n\n    print_buffer = curlx_mvaprintf(fmt, ap);\n    if(!print_buffer)\n      return;\n    len = strlen(print_buffer);\n\n    ptr = print_buffer;\n    while(len > 0) {\n      fputs(prefix, config->errors);\n\n      if(len > width) {\n        size_t cut = width-1;\n\n        while(!ISSPACE(ptr[cut]) && cut) {\n          cut--;\n        }\n        if(0 == cut)\n          /* not a single cutting position was found, just cut it at the\n             max text width then! */\n          cut = width-1;\n\n        (void)fwrite(ptr, cut + 1, 1, config->errors);\n        fputs(\"\\n\", config->errors);\n        ptr += cut + 1; /* skip the space too */\n        len -= cut;\n      }\n      else {\n        fputs(ptr, config->errors);\n        len = 0;\n      }\n    }\n    curl_free(print_buffer);\n  }\n}", "func_src_after": "static void voutf(struct GlobalConfig *config,\n                  const char *prefix,\n                  const char *fmt,\n                  va_list ap)\n{\n  size_t width = (79 - strlen(prefix));\n  if(!config->mute) {\n    size_t len;\n    char *ptr;\n    char *print_buffer;\n\n    print_buffer = curlx_mvaprintf(fmt, ap);\n    if(!print_buffer)\n      return;\n    len = strlen(print_buffer);\n\n    ptr = print_buffer;\n    while(len > 0) {\n      fputs(prefix, config->errors);\n\n      if(len > width) {\n        size_t cut = width-1;\n\n        while(!ISSPACE(ptr[cut]) && cut) {\n          cut--;\n        }\n        if(0 == cut)\n          /* not a single cutting position was found, just cut it at the\n             max text width then! */\n          cut = width-1;\n\n        (void)fwrite(ptr, cut + 1, 1, config->errors);\n        fputs(\"\\n\", config->errors);\n        ptr += cut + 1; /* skip the space too */\n        len -= cut + 1;\n      }\n      else {\n        fputs(ptr, config->errors);\n        len = 0;\n      }\n    }\n    curl_free(print_buffer);\n  }\n}", "commit_link": "github.com/curl/curl/commit/d530e92f59ae9bb2d47066c3c460b25d2ffeb211", "file_name": "src/tool_msgs.c", "vul_type": "cwe-125", "description": "Write a C function named `voutf` that formats and outputs a string with a prefix to an error stream, wrapping lines to a maximum width."}
{"func_name": "apiCallbacksFollow", "func_src_before": "func apiCallbacksFollow(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, challenge)\n\t\treturn\n\t}\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response followResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, follow := range response.Data {\n\t\tfmt.Printf(\"User with id %s followed %s at %s\\n\", follow.FromID, follow.ToID, follow.FollowedAt)\n\t}\n}", "func_src_after": "func apiCallbacksFollow(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, html.EscapeString(challenge))\n\t\treturn\n\t}\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response followResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, follow := range response.Data {\n\t\tfmt.Printf(\"User with id %s followed %s at %s\\n\", follow.FromID, follow.ToID, follow.FollowedAt)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 137, "char_end": 164, "line": "\t\tfmt.Fprint(w, challenge)\n"}], "added": [{"line_no": 4, "char_start": 137, "char_end": 183, "line": "\t\tfmt.Fprint(w, html.EscapeString(challenge))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 153, "char_end": 171, "chars": "html.EscapeString("}, {"char_start": 181, "char_end": 182, "chars": ")"}]}, "commit_link": "github.com/pajlada/pajbot2/commit/69ef922a07fc648760f030604e0aea86a573ea83", "file_name": "hook.go", "vul_type": "cwe-079", "commit_msg": "Fix cross-site scripting vulnerabilities (#447)", "parent_commit": "d8321a06903ec460cf6343c51ae50d12a3cb45e9", "description": "In Go, write a function to handle webhook callbacks that prints user follow events and responds to challenge verification."}
{"func_name": "wwunpack", "func_src_before": "int wwunpack(uint8_t *exe, uint32_t exesz, uint8_t *wwsect, struct cli_exe_section *sects, uint16_t scount, uint32_t pe, int desc) {\n  uint8_t *structs = wwsect + 0x2a1, *compd, *ccur, *unpd, *ucur, bc;\n  uint32_t src, srcend, szd, bt, bits;\n  int error=0, i;\n\n  cli_dbgmsg(\"in wwunpack\\n\");\n  while (1) {\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, structs, 17)) {\n      cli_dbgmsg(\"WWPack: Array of structs out of section\\n\");\n      break;\n    }\n    src = sects[scount].rva - cli_readint32(structs); /* src delta / dst delta - not used / dwords / end of src */\n    structs+=8;\n    szd = cli_readint32(structs) * 4;\n    structs+=4;\n    srcend = cli_readint32(structs);\n    structs+=4;\n\n    unpd = ucur = exe+src+srcend+4-szd;\n    if (!szd || !CLI_ISCONTAINED(exe, exesz, unpd, szd)) {\n      cli_dbgmsg(\"WWPack: Compressed data out of file\\n\");\n      break;\n    }\n    cli_dbgmsg(\"WWP: src: %x, szd: %x, srcend: %x - %x\\n\", src, szd, srcend, srcend+4-szd);\n    if (!(compd = cli_malloc(szd))) {\n        cli_dbgmsg(\"WWPack: Unable to allocate memory for compd\\n\");\n        break;\n    }\n    memcpy(compd, unpd, szd);\n    memset(unpd, -1, szd); /*FIXME*/\n    ccur=compd;\n    \n    RESEED;\n    while(!error) {\n      uint32_t backbytes, backsize;\n      uint8_t saved;\n\n      BIT;\n      if (!bits) { /* BYTE copy */\n\tif(ccur-compd>=szd || !CLI_ISCONTAINED(exe, exesz, ucur, 1))\n\t  error=1;\n\telse\n\t  *ucur++=*ccur++;\n\tcontinue;\n      }\n\n      BITS(2);\n      if(bits==3) { /* WORD backcopy */\n\tuint8_t shifted, subbed = 31;\n\tBITS(2);\n\tshifted = bits + 5;\n\tif(bits>=2) {\n\t  shifted++;\n\t  subbed += 0x80;\n\t}\n\tbackbytes = (1<<shifted)-subbed; /* 1h, 21h, 61h, 161h */\n\tBITS(shifted); /* 5, 6, 8, 9 */\n\tif(error || bits == 0x1ff) break;\n\tbackbytes+=bits;\n\tif(!CLI_ISCONTAINED(exe, exesz, ucur, 2) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, 2)) {\n\t  error=1;\n\t} else {\n\t  ucur[0]=*(ucur-backbytes);\n\t  ucur[1]=*(ucur-backbytes+1);\n\t  ucur+=2;\n\t}\n\tcontinue;\n      }\n\n      /* BLOCK backcopy */\n      saved = bits; /* cmp al, 1 / pushf */\n\n      BITS(3);\n      if (bits<6) {\n\tbackbytes = bits;\n\tswitch(bits) {\n\tcase 4: /* 10,11 */\n\t  backbytes++;\n\tcase 3: /* 8,9 */\n\t  BIT;\n\t  backbytes+=bits;\n\tcase 0:\tcase 1:\tcase 2: /* 5,6,7 */\n\t  backbytes+=5;\n\t  break;\n\tcase 5: /* 12 */\n\t  backbytes=12;\n\t  break;\n\t}\n\tBITS(backbytes);\n\tbits+=(1<<backbytes)-31;\n      } else if(bits==6) {\n\tBITS(0x0e);\n\tbits+=0x1fe1;\n      } else {\n\tBITS(0x0f);\n\tbits+=0x5fe1;\n      }\n\n      backbytes = bits;\n\n      /* popf / jb */\n      if (!saved) {\n\tBIT;\n\tif(!bits) {\n\t  BIT;\n\t  bits+=5;\n\t} else {\n\t  BITS(3);\n\t  if(bits) {\n\t    bits+=6;\n\t  } else {\n\t    BITS(4);\n\t    if(bits) {\n\t      bits+=13;\n\t    } else {\n\t      uint8_t cnt = 4;\n\t      uint16_t shifted = 0x0d;\n\t      \n\t      do {\n\t\tif(cnt==7) { cnt = 0x0e; shifted = 0; break; }\n\t\tshifted=((shifted+2)<<1)-1;\n\t\tBIT;\n\t\tcnt++;\n\t      } while(!bits);\n\t      BITS(cnt);\n\t      bits+=shifted;\n\t    }\n\t  }\n\t}\n\tbacksize = bits;\n      } else {\n\tbacksize = saved+2;\n      }\n\n      if(!CLI_ISCONTAINED(exe, exesz, ucur, backsize) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, backsize)) error=1;\n      else while(backsize--) {\n\t*ucur=*(ucur-backbytes);\n\tucur++;\n      }\n    }\n    free(compd);\n    if(error) {\n      cli_dbgmsg(\"WWPack: decompression error\\n\");\n      break;\n    }\n    if (error || !*structs++) break;\n  }\n\n  if(!error) {\n    if (pe+6 > exesz || pe+7 > exesz || pe+0x28 > exesz ||\n\t\tpe+0x50 > exesz || pe+0x14 > exesz) \n\treturn CL_EFORMAT;\n    exe[pe+6]=(uint8_t)scount;\n    exe[pe+7]=(uint8_t)(scount>>8);\n    cli_writeint32(&exe[pe+0x28], cli_readint32(wwsect+0x295)+sects[scount].rva+0x299);\n    cli_writeint32(&exe[pe+0x50], cli_readint32(&exe[pe+0x50])-sects[scount].vsz);\n\n    structs = &exe[(0xffff&cli_readint32(&exe[pe+0x14]))+pe+0x18];\n    for(i=0 ; i<scount ; i++) {\n\t  if (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t    cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t    return CL_EFORMAT;\n\t  }\n\n      cli_writeint32(structs+8, sects[i].vsz);\n      cli_writeint32(structs+12, sects[i].rva);\n      cli_writeint32(structs+16, sects[i].vsz);\n      cli_writeint32(structs+20, sects[i].rva);\n      structs+=0x28;\n    }\n\tif (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t  cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t  return CL_EFORMAT;\n\t}\n\n    memset(structs, 0, 0x28);\n    error = (uint32_t)cli_writen(desc, exe, exesz)!=exesz;\n  }\n  return error;\n}", "func_src_after": "int wwunpack(uint8_t *exe, uint32_t exesz, uint8_t *wwsect, struct cli_exe_section *sects, uint16_t scount, uint32_t pe, int desc) {\n  uint8_t *structs = wwsect + 0x2a1, *compd, *ccur, *unpd, *ucur, bc;\n  uint32_t src, srcend, szd, bt, bits;\n  int error=0, i;\n\n  cli_dbgmsg(\"in wwunpack\\n\");\n  while (1) {\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, structs, 17)) {\n      cli_dbgmsg(\"WWPack: Array of structs out of section\\n\");\n      break;\n    }\n    src = sects[scount].rva - cli_readint32(structs); /* src delta / dst delta - not used / dwords / end of src */\n    structs+=8;\n    szd = cli_readint32(structs) * 4;\n    structs+=4;\n    srcend = cli_readint32(structs);\n    structs+=4;\n\n    unpd = ucur = exe+src+srcend+4-szd;\n    if (!szd || !CLI_ISCONTAINED(exe, exesz, unpd, szd)) {\n      cli_dbgmsg(\"WWPack: Compressed data out of file\\n\");\n      break;\n    }\n    cli_dbgmsg(\"WWP: src: %x, szd: %x, srcend: %x - %x\\n\", src, szd, srcend, srcend+4-szd);\n    if (!(compd = cli_malloc(szd))) {\n        cli_dbgmsg(\"WWPack: Unable to allocate memory for compd\\n\");\n        break;\n    }\n    memcpy(compd, unpd, szd);\n    memset(unpd, -1, szd); /*FIXME*/\n    ccur=compd;\n    \n    RESEED;\n    while(!error) {\n      uint32_t backbytes, backsize;\n      uint8_t saved;\n\n      BIT;\n      if (!bits) { /* BYTE copy */\n\tif(ccur-compd>=szd || !CLI_ISCONTAINED(exe, exesz, ucur, 1))\n\t  error=1;\n\telse\n\t  *ucur++=*ccur++;\n\tcontinue;\n      }\n\n      BITS(2);\n      if(bits==3) { /* WORD backcopy */\n\tuint8_t shifted, subbed = 31;\n\tBITS(2);\n\tshifted = bits + 5;\n\tif(bits>=2) {\n\t  shifted++;\n\t  subbed += 0x80;\n\t}\n\tbackbytes = (1<<shifted)-subbed; /* 1h, 21h, 61h, 161h */\n\tBITS(shifted); /* 5, 6, 8, 9 */\n\tif(error || bits == 0x1ff) break;\n\tbackbytes+=bits;\n\tif(!CLI_ISCONTAINED(exe, exesz, ucur, 2) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, 2)) {\n\t  error=1;\n\t} else {\n\t  ucur[0]=*(ucur-backbytes);\n\t  ucur[1]=*(ucur-backbytes+1);\n\t  ucur+=2;\n\t}\n\tcontinue;\n      }\n\n      /* BLOCK backcopy */\n      saved = bits; /* cmp al, 1 / pushf */\n\n      BITS(3);\n      if (bits<6) {\n\tbackbytes = bits;\n\tswitch(bits) {\n\tcase 4: /* 10,11 */\n\t  backbytes++;\n\tcase 3: /* 8,9 */\n\t  BIT;\n\t  backbytes+=bits;\n\tcase 0:\tcase 1:\tcase 2: /* 5,6,7 */\n\t  backbytes+=5;\n\t  break;\n\tcase 5: /* 12 */\n\t  backbytes=12;\n\t  break;\n\t}\n\tBITS(backbytes);\n\tbits+=(1<<backbytes)-31;\n      } else if(bits==6) {\n\tBITS(0x0e);\n\tbits+=0x1fe1;\n      } else {\n\tBITS(0x0f);\n\tbits+=0x5fe1;\n      }\n\n      backbytes = bits;\n\n      /* popf / jb */\n      if (!saved) {\n\tBIT;\n\tif(!bits) {\n\t  BIT;\n\t  bits+=5;\n\t} else {\n\t  BITS(3);\n\t  if(bits) {\n\t    bits+=6;\n\t  } else {\n\t    BITS(4);\n\t    if(bits) {\n\t      bits+=13;\n\t    } else {\n\t      uint8_t cnt = 4;\n\t      uint16_t shifted = 0x0d;\n\t      \n\t      do {\n\t\tif(cnt==7) { cnt = 0x0e; shifted = 0; break; }\n\t\tshifted=((shifted+2)<<1)-1;\n\t\tBIT;\n\t\tcnt++;\n\t      } while(!bits);\n\t      BITS(cnt);\n\t      bits+=shifted;\n\t    }\n\t  }\n\t}\n\tbacksize = bits;\n      } else {\n\tbacksize = saved+2;\n      }\n\n      if(!CLI_ISCONTAINED(exe, exesz, ucur, backsize) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, backsize)) error=1;\n      else while(backsize--) {\n\t*ucur=*(ucur-backbytes);\n\tucur++;\n      }\n    }\n    free(compd);\n    if(error) {\n      cli_dbgmsg(\"WWPack: decompression error\\n\");\n      break;\n    }\n    if (error || !*structs++) break;\n  }\n\n  if(!error) {\n    if (pe+6 > exesz || pe+7 > exesz || pe+0x28 > exesz ||\n\t\tpe+0x50 > exesz || pe+0x14 > exesz) \n\treturn CL_EFORMAT;\n    exe[pe+6]=(uint8_t)scount;\n    exe[pe+7]=(uint8_t)(scount>>8);\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295, 4) ||\n        !CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295+sects[scount].rva, 4) ||\n        !CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295+sects[scount].rva+0x299, 4)) {\n        cli_dbgmsg(\"WWPack: unpack memory address out of bounds.\\n\");\n        return CL_EFORMAT;\n    }\n    cli_writeint32(&exe[pe+0x28], cli_readint32(wwsect+0x295)+sects[scount].rva+0x299);\n    cli_writeint32(&exe[pe+0x50], cli_readint32(&exe[pe+0x50])-sects[scount].vsz);\n\n    structs = &exe[(0xffff&cli_readint32(&exe[pe+0x14]))+pe+0x18];\n    for(i=0 ; i<scount ; i++) {\n\t  if (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t    cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t    return CL_EFORMAT;\n\t  }\n\n      cli_writeint32(structs+8, sects[i].vsz);\n      cli_writeint32(structs+12, sects[i].rva);\n      cli_writeint32(structs+16, sects[i].vsz);\n      cli_writeint32(structs+20, sects[i].rva);\n      structs+=0x28;\n    }\n\tif (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t  cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t  return CL_EFORMAT;\n\t}\n\n    memset(structs, 0, 0x28);\n    error = (uint32_t)cli_writen(desc, exe, exesz)!=exesz;\n  }\n  return error;\n}", "commit_link": "github.com/vrtadmin/clamav-devel/commit/dfc00cd3301a42b571454b51a6102eecf58407bc", "file_name": "libclamav/wwunpack.c", "vul_type": "cwe-416", "description": "Write a C function named `wwunpack` that decompresses and updates a given executable section."}
{"func_name": "ReadVIFFImage", "func_src_before": "static Image *ReadVIFFImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n#define VFF_CM_genericRGB  15\n#define VFF_CM_ntscRGB  1\n#define VFF_CM_NONE  0\n#define VFF_DEP_DECORDER  0x4\n#define VFF_DEP_NSORDER  0x8\n#define VFF_DES_RAW  0\n#define VFF_LOC_IMPLICIT  1\n#define VFF_MAPTYP_NONE  0\n#define VFF_MAPTYP_1_BYTE  1\n#define VFF_MAPTYP_2_BYTE  2\n#define VFF_MAPTYP_4_BYTE  4\n#define VFF_MAPTYP_FLOAT  5\n#define VFF_MAPTYP_DOUBLE  7\n#define VFF_MS_NONE  0\n#define VFF_MS_ONEPERBAND  1\n#define VFF_MS_SHARED  3\n#define VFF_TYP_BIT  0\n#define VFF_TYP_1_BYTE  1\n#define VFF_TYP_2_BYTE  2\n#define VFF_TYP_4_BYTE  4\n#define VFF_TYP_FLOAT  5\n#define VFF_TYP_DOUBLE  9\n\n  typedef struct _ViffInfo\n  {\n    unsigned char\n      identifier,\n      file_type,\n      release,\n      version,\n      machine_dependency,\n      reserve[3];\n\n    char\n      comment[512];\n\n    unsigned int\n      rows,\n      columns,\n      subrows;\n\n    int\n      x_offset,\n      y_offset;\n\n    float\n      x_bits_per_pixel,\n      y_bits_per_pixel;\n\n    unsigned int\n      location_type,\n      location_dimension,\n      number_of_images,\n      number_data_bands,\n      data_storage_type,\n      data_encode_scheme,\n      map_scheme,\n      map_storage_type,\n      map_rows,\n      map_columns,\n      map_subrows,\n      map_enable,\n      maps_per_cycle,\n      color_space_model;\n  } ViffInfo;\n\n  double\n    min_value,\n    scale_factor,\n    value;\n\n  Image\n    *image;\n\n  int\n    bit;\n\n  MagickBooleanType\n    status;\n\n  MagickSizeType\n    number_pixels;\n\n  register IndexPacket\n    *indexes;\n\n  register ssize_t\n    x;\n\n  register PixelPacket\n    *q;\n\n  register ssize_t\n    i;\n\n  register unsigned char\n    *p;\n\n  size_t\n    bytes_per_pixel,\n    max_packets,\n    quantum;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    *pixels;\n\n  unsigned long\n    lsb_first;\n\n  ViffInfo\n    viff_info;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read VIFF header (1024 bytes).\n  */\n  count=ReadBlob(image,1,&viff_info.identifier);\n  do\n  {\n    /*\n      Verify VIFF identifier.\n    */\n    if ((count != 1) || ((unsigned char) viff_info.identifier != 0xab))\n      ThrowReaderException(CorruptImageError,\"NotAVIFFImage\");\n    /*\n      Initialize VIFF image.\n    */\n    (void) ReadBlob(image,sizeof(viff_info.file_type),&viff_info.file_type);\n    (void) ReadBlob(image,sizeof(viff_info.release),&viff_info.release);\n    (void) ReadBlob(image,sizeof(viff_info.version),&viff_info.version);\n    (void) ReadBlob(image,sizeof(viff_info.machine_dependency),\n      &viff_info.machine_dependency);\n    (void) ReadBlob(image,sizeof(viff_info.reserve),viff_info.reserve);\n    (void) ReadBlob(image,512,(unsigned char *) viff_info.comment);\n    viff_info.comment[511]='\\0';\n    if (strlen(viff_info.comment) > 4)\n      (void) SetImageProperty(image,\"comment\",viff_info.comment);\n    if ((viff_info.machine_dependency == VFF_DEP_DECORDER) ||\n        (viff_info.machine_dependency == VFF_DEP_NSORDER))\n      image->endian=LSBEndian;\n    else\n      image->endian=MSBEndian;\n    viff_info.rows=ReadBlobLong(image);\n    viff_info.columns=ReadBlobLong(image);\n    viff_info.subrows=ReadBlobLong(image);\n    viff_info.x_offset=(int) ReadBlobLong(image);\n    viff_info.y_offset=(int) ReadBlobLong(image);\n    viff_info.x_bits_per_pixel=(float) ReadBlobLong(image);\n    viff_info.y_bits_per_pixel=(float) ReadBlobLong(image);\n    viff_info.location_type=ReadBlobLong(image);\n    viff_info.location_dimension=ReadBlobLong(image);\n    viff_info.number_of_images=ReadBlobLong(image);\n    viff_info.number_data_bands=ReadBlobLong(image);\n    viff_info.data_storage_type=ReadBlobLong(image);\n    viff_info.data_encode_scheme=ReadBlobLong(image);\n    viff_info.map_scheme=ReadBlobLong(image);\n    viff_info.map_storage_type=ReadBlobLong(image);\n    viff_info.map_rows=ReadBlobLong(image);\n    viff_info.map_columns=ReadBlobLong(image);\n    viff_info.map_subrows=ReadBlobLong(image);\n    viff_info.map_enable=ReadBlobLong(image);\n    viff_info.maps_per_cycle=ReadBlobLong(image);\n    viff_info.color_space_model=ReadBlobLong(image);\n    for (i=0; i < 420; i++)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnexpectedEndOfFile\");\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    image->depth=viff_info.x_bits_per_pixel <= 8 ? 8UL :\n      MAGICKCORE_QUANTUM_DEPTH;\n    /*\n      Verify that we can read this VIFF image.\n    */\n    number_pixels=(MagickSizeType) viff_info.columns*viff_info.rows;\n    if (number_pixels != (size_t) number_pixels)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if (number_pixels == 0)\n      ThrowReaderException(CoderError,\"ImageColumnOrRowSizeIsNotSupported\");\n    if ((viff_info.number_data_bands < 1) || (viff_info.number_data_bands > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if ((viff_info.data_storage_type != VFF_TYP_BIT) &&\n        (viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_2_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_4_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_FLOAT) &&\n        (viff_info.data_storage_type != VFF_TYP_DOUBLE))\n      ThrowReaderException(CoderError,\"DataStorageTypeIsNotSupported\");\n    if (viff_info.data_encode_scheme != VFF_DES_RAW)\n      ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n    if ((viff_info.map_storage_type != VFF_MAPTYP_NONE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_1_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_2_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_4_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_FLOAT) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_DOUBLE))\n      ThrowReaderException(CoderError,\"MapStorageTypeIsNotSupported\");\n    if ((viff_info.color_space_model != VFF_CM_NONE) &&\n        (viff_info.color_space_model != VFF_CM_ntscRGB) &&\n        (viff_info.color_space_model != VFF_CM_genericRGB))\n      ThrowReaderException(CoderError,\"ColorspaceModelIsNotSupported\");\n    if (viff_info.location_type != VFF_LOC_IMPLICIT)\n      ThrowReaderException(CoderError,\"LocationTypeIsNotSupported\");\n    if (viff_info.number_of_images != 1)\n      ThrowReaderException(CoderError,\"NumberOfImagesIsNotSupported\");\n    if (viff_info.map_rows == 0)\n      viff_info.map_scheme=VFF_MS_NONE;\n    switch ((int) viff_info.map_scheme)\n    {\n      case VFF_MS_NONE:\n      {\n        if (viff_info.number_data_bands < 3)\n          {\n            /*\n              Create linear color ramp.\n            */\n            if (viff_info.data_storage_type == VFF_TYP_BIT)\n              image->colors=2;\n            else\n              if (viff_info.data_storage_type == VFF_MAPTYP_1_BYTE)\n                image->colors=256UL;\n              else\n                image->colors=image->depth <= 8 ? 256UL : 65536UL;\n            if (AcquireImageColormap(image,image->colors) == MagickFalse)\n              ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        break;\n      }\n      case VFF_MS_ONEPERBAND:\n      case VFF_MS_SHARED:\n      {\n        unsigned char\n          *viff_colormap;\n\n        /*\n          Allocate VIFF colormap.\n        */\n        switch ((int) viff_info.map_storage_type)\n        {\n          case VFF_MAPTYP_1_BYTE: bytes_per_pixel=1; break;\n          case VFF_MAPTYP_2_BYTE: bytes_per_pixel=2; break;\n          case VFF_MAPTYP_4_BYTE: bytes_per_pixel=4; break;\n          case VFF_MAPTYP_FLOAT: bytes_per_pixel=4; break;\n          case VFF_MAPTYP_DOUBLE: bytes_per_pixel=8; break;\n          default: bytes_per_pixel=1; break;\n        }\n        image->colors=viff_info.map_columns;\n        if (AcquireImageColormap(image,image->colors) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        if (viff_info.map_rows >\n            (viff_info.map_rows*bytes_per_pixel*sizeof(*viff_colormap)))\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        viff_colormap=(unsigned char *) AcquireQuantumMemory(image->colors,\n          viff_info.map_rows*bytes_per_pixel*sizeof(*viff_colormap));\n        if (viff_colormap == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        /*\n          Read VIFF raster colormap.\n        */\n        (void) ReadBlob(image,bytes_per_pixel*image->colors*viff_info.map_rows,\n          viff_colormap);\n        lsb_first=1;\n        if (*(char *) &lsb_first &&\n            ((viff_info.machine_dependency != VFF_DEP_DECORDER) &&\n             (viff_info.machine_dependency != VFF_DEP_NSORDER)))\n          switch ((int) viff_info.map_storage_type)\n          {\n            case VFF_MAPTYP_2_BYTE:\n            {\n              MSBOrderShort(viff_colormap,(bytes_per_pixel*image->colors*\n                viff_info.map_rows));\n              break;\n            }\n            case VFF_MAPTYP_4_BYTE:\n            case VFF_MAPTYP_FLOAT:\n            {\n              MSBOrderLong(viff_colormap,(bytes_per_pixel*image->colors*\n                viff_info.map_rows));\n              break;\n            }\n            default: break;\n          }\n        for (i=0; i < (ssize_t) (viff_info.map_rows*image->colors); i++)\n        {\n          switch ((int) viff_info.map_storage_type)\n          {\n            case VFF_MAPTYP_2_BYTE: value=1.0*((short *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_4_BYTE: value=1.0*((int *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_FLOAT: value=((float *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_DOUBLE: value=((double *) viff_colormap)[i]; break;\n            default: value=1.0*viff_colormap[i]; break;\n          }\n          if (i < (ssize_t) image->colors)\n            {\n              image->colormap[i].red=ScaleCharToQuantum((unsigned char) value);\n              image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                value);\n              image->colormap[i].blue=ScaleCharToQuantum((unsigned char) value);\n            }\n          else\n            if (i < (ssize_t) (2*image->colors))\n              image->colormap[i % image->colors].green=ScaleCharToQuantum(\n                (unsigned char) value);\n            else\n              if (i < (ssize_t) (3*image->colors))\n                image->colormap[i % image->colors].blue=ScaleCharToQuantum(\n                  (unsigned char) value);\n        }\n        viff_colormap=(unsigned char *) RelinquishMagickMemory(viff_colormap);\n        break;\n      }\n      default:\n        ThrowReaderException(CoderError,\"ColormapTypeNotSupported\");\n    }\n    /*\n      Initialize image structure.\n    */\n    image->matte=viff_info.number_data_bands == 4 ? MagickTrue : MagickFalse;\n    image->storage_class=\n      (viff_info.number_data_bands < 3 ? PseudoClass : DirectClass);\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    if ((image_info->ping != MagickFalse) && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n    /*\n      Allocate VIFF pixels.\n    */\n    switch ((int) viff_info.data_storage_type)\n    {\n      case VFF_TYP_2_BYTE: bytes_per_pixel=2; break;\n      case VFF_TYP_4_BYTE: bytes_per_pixel=4; break;\n      case VFF_TYP_FLOAT: bytes_per_pixel=4; break;\n      case VFF_TYP_DOUBLE: bytes_per_pixel=8; break;\n      default: bytes_per_pixel=1; break;\n    }\n    if (viff_info.data_storage_type == VFF_TYP_BIT)\n      max_packets=((image->columns+7UL) >> 3UL)*image->rows;\n    else\n      max_packets=(size_t) (number_pixels*viff_info.number_data_bands);\n    pixels=(unsigned char *) AcquireQuantumMemory(max_packets,\n      bytes_per_pixel*sizeof(*pixels));\n    if (pixels == (unsigned char *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    (void) ReadBlob(image,bytes_per_pixel*max_packets,pixels);\n    lsb_first=1;\n    if (*(char *) &lsb_first &&\n        ((viff_info.machine_dependency != VFF_DEP_DECORDER) &&\n         (viff_info.machine_dependency != VFF_DEP_NSORDER)))\n      switch ((int) viff_info.data_storage_type)\n      {\n        case VFF_TYP_2_BYTE:\n        {\n          MSBOrderShort(pixels,bytes_per_pixel*max_packets);\n          break;\n        }\n        case VFF_TYP_4_BYTE:\n        case VFF_TYP_FLOAT:\n        {\n          MSBOrderLong(pixels,bytes_per_pixel*max_packets);\n          break;\n        }\n        default: break;\n      }\n    min_value=0.0;\n    scale_factor=1.0;\n    if ((viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.map_scheme == VFF_MS_NONE))\n      {\n        double\n          max_value;\n\n        /*\n          Determine scale factor.\n        */\n        switch ((int) viff_info.data_storage_type)\n        {\n          case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[0]; break;\n          case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[0]; break;\n          case VFF_TYP_FLOAT: value=((float *) pixels)[0]; break;\n          case VFF_TYP_DOUBLE: value=((double *) pixels)[0]; break;\n          default: value=1.0*pixels[0]; break;\n        }\n        max_value=value;\n        min_value=value;\n        for (i=0; i < (ssize_t) max_packets; i++)\n        {\n          switch ((int) viff_info.data_storage_type)\n          {\n            case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[i]; break;\n            case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[i]; break;\n            case VFF_TYP_FLOAT: value=((float *) pixels)[i]; break;\n            case VFF_TYP_DOUBLE: value=((double *) pixels)[i]; break;\n            default: value=1.0*pixels[i]; break;\n          }\n          if (value > max_value)\n            max_value=value;\n          else\n            if (value < min_value)\n              min_value=value;\n        }\n        if ((min_value == 0) && (max_value == 0))\n          scale_factor=0;\n        else\n          if (min_value == max_value)\n            {\n              scale_factor=(MagickRealType) QuantumRange/min_value;\n              min_value=0;\n            }\n          else\n            scale_factor=(MagickRealType) QuantumRange/(max_value-min_value);\n      }\n    /*\n      Convert pixels to Quantum size.\n    */\n    p=(unsigned char *) pixels;\n    for (i=0; i < (ssize_t) max_packets; i++)\n    {\n      switch ((int) viff_info.data_storage_type)\n      {\n        case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[i]; break;\n        case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[i]; break;\n        case VFF_TYP_FLOAT: value=((float *) pixels)[i]; break;\n        case VFF_TYP_DOUBLE: value=((double *) pixels)[i]; break;\n        default: value=1.0*pixels[i]; break;\n      }\n      if (viff_info.map_scheme == VFF_MS_NONE)\n        {\n          value=(value-min_value)*scale_factor;\n          if (value > QuantumRange)\n            value=QuantumRange;\n          else\n            if (value < 0)\n              value=0;\n        }\n      *p=(unsigned char) ((Quantum) value);\n      p++;\n    }\n    /*\n      Convert VIFF raster image to pixel packets.\n    */\n    p=(unsigned char *) pixels;\n    if (viff_info.data_storage_type == VFF_TYP_BIT)\n      {\n        /*\n          Convert bitmap scanline.\n        */\n        if (image->storage_class != PseudoClass)\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (PixelPacket *) NULL)\n            break;\n          indexes=GetAuthenticIndexQueue(image);\n          for (x=0; x < (ssize_t) (image->columns-7); x+=8)\n          {\n            for (bit=0; bit < 8; bit++)\n            {\n              quantum=(size_t) ((*p) & (0x01 << bit) ? 0 : 1);\n              SetPixelRed(q,quantum == 0 ? 0 : QuantumRange);\n              SetPixelGreen(q,quantum == 0 ? 0 : QuantumRange);\n              SetPixelBlue(q,quantum == 0 ? 0 : QuantumRange);\n              if (image->storage_class == PseudoClass)\n                SetPixelIndex(indexes+x+bit,quantum);\n             }\n            p++;\n          }\n          if ((image->columns % 8) != 0)\n            {\n              for (bit=0; bit < (int) (image->columns % 8); bit++)\n              {\n                quantum=(size_t) ((*p) & (0x01 << bit) ? 0 : 1);\n                SetPixelRed(q,quantum == 0 ? 0 : QuantumRange);\n                SetPixelGreen(q,quantum == 0 ? 0 : QuantumRange);\n                SetPixelBlue(q,quantum == 0 ? 0 : QuantumRange);\n                if (image->storage_class == PseudoClass)\n                  SetPixelIndex(indexes+x+bit,quantum);\n              }\n              p++;\n            }\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      }\n    else\n      if (image->storage_class == PseudoClass)\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (PixelPacket *) NULL)\n            break;\n          indexes=GetAuthenticIndexQueue(image);\n          for (x=0; x < (ssize_t) image->columns; x++)\n            SetPixelIndex(indexes+x,*p++);\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      else\n        {\n          /*\n            Convert DirectColor scanline.\n          */\n          number_pixels=(MagickSizeType) image->columns*image->rows;\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (PixelPacket *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelRed(q,ScaleCharToQuantum(*p));\n              SetPixelGreen(q,ScaleCharToQuantum(*(p+number_pixels)));\n              SetPixelBlue(q,ScaleCharToQuantum(*(p+2*number_pixels)));\n              if (image->colors != 0)\n                {\n                  ssize_t\n                    index;\n\n                  index=(ssize_t) GetPixelRed(q);\n                  SetPixelRed(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].red);\n                  index=(ssize_t) GetPixelGreen(q);\n                  SetPixelGreen(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].green);\n                  index=(ssize_t) GetPixelRed(q);\n                  SetPixelBlue(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].blue);\n                }\n              SetPixelOpacity(q,image->matte != MagickFalse ? QuantumRange-\n                ScaleCharToQuantum(*(p+number_pixels*3)) : OpaqueOpacity);\n              p++;\n              q++;\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n        }\n    pixels=(unsigned char *) RelinquishMagickMemory(pixels);\n    if (image->storage_class == PseudoClass)\n      (void) SyncImage(image);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    count=ReadBlob(image,1,&viff_info.identifier);\n    if ((count != 0) && (viff_info.identifier == 0xab))\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while ((count != 0) && (viff_info.identifier == 0xab));\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadVIFFImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n#define VFF_CM_genericRGB  15\n#define VFF_CM_ntscRGB  1\n#define VFF_CM_NONE  0\n#define VFF_DEP_DECORDER  0x4\n#define VFF_DEP_NSORDER  0x8\n#define VFF_DES_RAW  0\n#define VFF_LOC_IMPLICIT  1\n#define VFF_MAPTYP_NONE  0\n#define VFF_MAPTYP_1_BYTE  1\n#define VFF_MAPTYP_2_BYTE  2\n#define VFF_MAPTYP_4_BYTE  4\n#define VFF_MAPTYP_FLOAT  5\n#define VFF_MAPTYP_DOUBLE  7\n#define VFF_MS_NONE  0\n#define VFF_MS_ONEPERBAND  1\n#define VFF_MS_SHARED  3\n#define VFF_TYP_BIT  0\n#define VFF_TYP_1_BYTE  1\n#define VFF_TYP_2_BYTE  2\n#define VFF_TYP_4_BYTE  4\n#define VFF_TYP_FLOAT  5\n#define VFF_TYP_DOUBLE  9\n\n  typedef struct _ViffInfo\n  {\n    unsigned char\n      identifier,\n      file_type,\n      release,\n      version,\n      machine_dependency,\n      reserve[3];\n\n    char\n      comment[512];\n\n    unsigned int\n      rows,\n      columns,\n      subrows;\n\n    int\n      x_offset,\n      y_offset;\n\n    float\n      x_bits_per_pixel,\n      y_bits_per_pixel;\n\n    unsigned int\n      location_type,\n      location_dimension,\n      number_of_images,\n      number_data_bands,\n      data_storage_type,\n      data_encode_scheme,\n      map_scheme,\n      map_storage_type,\n      map_rows,\n      map_columns,\n      map_subrows,\n      map_enable,\n      maps_per_cycle,\n      color_space_model;\n  } ViffInfo;\n\n  double\n    min_value,\n    scale_factor,\n    value;\n\n  Image\n    *image;\n\n  int\n    bit;\n\n  MagickBooleanType\n    status;\n\n  MagickSizeType\n    number_pixels;\n\n  register IndexPacket\n    *indexes;\n\n  register ssize_t\n    x;\n\n  register PixelPacket\n    *q;\n\n  register ssize_t\n    i;\n\n  register unsigned char\n    *p;\n\n  size_t\n    bytes_per_pixel,\n    max_packets,\n    quantum;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    *pixels;\n\n  unsigned long\n    lsb_first;\n\n  ViffInfo\n    viff_info;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read VIFF header (1024 bytes).\n  */\n  count=ReadBlob(image,1,&viff_info.identifier);\n  do\n  {\n    /*\n      Verify VIFF identifier.\n    */\n    if ((count != 1) || ((unsigned char) viff_info.identifier != 0xab))\n      ThrowReaderException(CorruptImageError,\"NotAVIFFImage\");\n    /*\n      Initialize VIFF image.\n    */\n    (void) ReadBlob(image,sizeof(viff_info.file_type),&viff_info.file_type);\n    (void) ReadBlob(image,sizeof(viff_info.release),&viff_info.release);\n    (void) ReadBlob(image,sizeof(viff_info.version),&viff_info.version);\n    (void) ReadBlob(image,sizeof(viff_info.machine_dependency),\n      &viff_info.machine_dependency);\n    (void) ReadBlob(image,sizeof(viff_info.reserve),viff_info.reserve);\n    (void) ReadBlob(image,512,(unsigned char *) viff_info.comment);\n    viff_info.comment[511]='\\0';\n    if (strlen(viff_info.comment) > 4)\n      (void) SetImageProperty(image,\"comment\",viff_info.comment);\n    if ((viff_info.machine_dependency == VFF_DEP_DECORDER) ||\n        (viff_info.machine_dependency == VFF_DEP_NSORDER))\n      image->endian=LSBEndian;\n    else\n      image->endian=MSBEndian;\n    viff_info.rows=ReadBlobLong(image);\n    viff_info.columns=ReadBlobLong(image);\n    viff_info.subrows=ReadBlobLong(image);\n    viff_info.x_offset=(int) ReadBlobLong(image);\n    viff_info.y_offset=(int) ReadBlobLong(image);\n    viff_info.x_bits_per_pixel=(float) ReadBlobLong(image);\n    viff_info.y_bits_per_pixel=(float) ReadBlobLong(image);\n    viff_info.location_type=ReadBlobLong(image);\n    viff_info.location_dimension=ReadBlobLong(image);\n    viff_info.number_of_images=ReadBlobLong(image);\n    viff_info.number_data_bands=ReadBlobLong(image);\n    viff_info.data_storage_type=ReadBlobLong(image);\n    viff_info.data_encode_scheme=ReadBlobLong(image);\n    viff_info.map_scheme=ReadBlobLong(image);\n    viff_info.map_storage_type=ReadBlobLong(image);\n    viff_info.map_rows=ReadBlobLong(image);\n    viff_info.map_columns=ReadBlobLong(image);\n    viff_info.map_subrows=ReadBlobLong(image);\n    viff_info.map_enable=ReadBlobLong(image);\n    viff_info.maps_per_cycle=ReadBlobLong(image);\n    viff_info.color_space_model=ReadBlobLong(image);\n    for (i=0; i < 420; i++)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnexpectedEndOfFile\");\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    image->depth=viff_info.x_bits_per_pixel <= 8 ? 8UL :\n      MAGICKCORE_QUANTUM_DEPTH;\n    /*\n      Verify that we can read this VIFF image.\n    */\n    number_pixels=(MagickSizeType) viff_info.columns*viff_info.rows;\n    if (number_pixels != (size_t) number_pixels)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if (number_pixels == 0)\n      ThrowReaderException(CoderError,\"ImageColumnOrRowSizeIsNotSupported\");\n    if ((viff_info.number_data_bands < 1) || (viff_info.number_data_bands > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if ((viff_info.data_storage_type != VFF_TYP_BIT) &&\n        (viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_2_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_4_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_FLOAT) &&\n        (viff_info.data_storage_type != VFF_TYP_DOUBLE))\n      ThrowReaderException(CoderError,\"DataStorageTypeIsNotSupported\");\n    if (viff_info.data_encode_scheme != VFF_DES_RAW)\n      ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n    if ((viff_info.map_storage_type != VFF_MAPTYP_NONE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_1_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_2_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_4_BYTE) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_FLOAT) &&\n        (viff_info.map_storage_type != VFF_MAPTYP_DOUBLE))\n      ThrowReaderException(CoderError,\"MapStorageTypeIsNotSupported\");\n    if ((viff_info.color_space_model != VFF_CM_NONE) &&\n        (viff_info.color_space_model != VFF_CM_ntscRGB) &&\n        (viff_info.color_space_model != VFF_CM_genericRGB))\n      ThrowReaderException(CoderError,\"ColorspaceModelIsNotSupported\");\n    if (viff_info.location_type != VFF_LOC_IMPLICIT)\n      ThrowReaderException(CoderError,\"LocationTypeIsNotSupported\");\n    if (viff_info.number_of_images != 1)\n      ThrowReaderException(CoderError,\"NumberOfImagesIsNotSupported\");\n    if (viff_info.map_rows == 0)\n      viff_info.map_scheme=VFF_MS_NONE;\n    switch ((int) viff_info.map_scheme)\n    {\n      case VFF_MS_NONE:\n      {\n        if (viff_info.number_data_bands < 3)\n          {\n            /*\n              Create linear color ramp.\n            */\n            if (viff_info.data_storage_type == VFF_TYP_BIT)\n              image->colors=2;\n            else\n              if (viff_info.data_storage_type == VFF_MAPTYP_1_BYTE)\n                image->colors=256UL;\n              else\n                image->colors=image->depth <= 8 ? 256UL : 65536UL;\n            if (AcquireImageColormap(image,image->colors) == MagickFalse)\n              ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        break;\n      }\n      case VFF_MS_ONEPERBAND:\n      case VFF_MS_SHARED:\n      {\n        unsigned char\n          *viff_colormap;\n\n        /*\n          Allocate VIFF colormap.\n        */\n        switch ((int) viff_info.map_storage_type)\n        {\n          case VFF_MAPTYP_1_BYTE: bytes_per_pixel=1; break;\n          case VFF_MAPTYP_2_BYTE: bytes_per_pixel=2; break;\n          case VFF_MAPTYP_4_BYTE: bytes_per_pixel=4; break;\n          case VFF_MAPTYP_FLOAT: bytes_per_pixel=4; break;\n          case VFF_MAPTYP_DOUBLE: bytes_per_pixel=8; break;\n          default: bytes_per_pixel=1; break;\n        }\n        image->colors=viff_info.map_columns;\n        if (AcquireImageColormap(image,image->colors) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        if (viff_info.map_rows >\n            (viff_info.map_rows*bytes_per_pixel*sizeof(*viff_colormap)))\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        viff_colormap=(unsigned char *) AcquireQuantumMemory(image->colors,\n          viff_info.map_rows*bytes_per_pixel*sizeof(*viff_colormap));\n        if (viff_colormap == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        /*\n          Read VIFF raster colormap.\n        */\n        (void) ReadBlob(image,bytes_per_pixel*image->colors*viff_info.map_rows,\n          viff_colormap);\n        lsb_first=1;\n        if (*(char *) &lsb_first &&\n            ((viff_info.machine_dependency != VFF_DEP_DECORDER) &&\n             (viff_info.machine_dependency != VFF_DEP_NSORDER)))\n          switch ((int) viff_info.map_storage_type)\n          {\n            case VFF_MAPTYP_2_BYTE:\n            {\n              MSBOrderShort(viff_colormap,(bytes_per_pixel*image->colors*\n                viff_info.map_rows));\n              break;\n            }\n            case VFF_MAPTYP_4_BYTE:\n            case VFF_MAPTYP_FLOAT:\n            {\n              MSBOrderLong(viff_colormap,(bytes_per_pixel*image->colors*\n                viff_info.map_rows));\n              break;\n            }\n            default: break;\n          }\n        for (i=0; i < (ssize_t) (viff_info.map_rows*image->colors); i++)\n        {\n          switch ((int) viff_info.map_storage_type)\n          {\n            case VFF_MAPTYP_2_BYTE: value=1.0*((short *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_4_BYTE: value=1.0*((int *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_FLOAT: value=((float *) viff_colormap)[i]; break;\n            case VFF_MAPTYP_DOUBLE: value=((double *) viff_colormap)[i]; break;\n            default: value=1.0*viff_colormap[i]; break;\n          }\n          if (i < (ssize_t) image->colors)\n            {\n              image->colormap[i].red=ScaleCharToQuantum((unsigned char) value);\n              image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                value);\n              image->colormap[i].blue=ScaleCharToQuantum((unsigned char) value);\n            }\n          else\n            if (i < (ssize_t) (2*image->colors))\n              image->colormap[i % image->colors].green=ScaleCharToQuantum(\n                (unsigned char) value);\n            else\n              if (i < (ssize_t) (3*image->colors))\n                image->colormap[i % image->colors].blue=ScaleCharToQuantum(\n                  (unsigned char) value);\n        }\n        viff_colormap=(unsigned char *) RelinquishMagickMemory(viff_colormap);\n        break;\n      }\n      default:\n        ThrowReaderException(CoderError,\"ColormapTypeNotSupported\");\n    }\n    /*\n      Initialize image structure.\n    */\n    image->matte=viff_info.number_data_bands == 4 ? MagickTrue : MagickFalse;\n    image->storage_class=\n      (viff_info.number_data_bands < 3 ? PseudoClass : DirectClass);\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    if ((image_info->ping != MagickFalse) && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n    /*\n      Allocate VIFF pixels.\n    */\n    switch ((int) viff_info.data_storage_type)\n    {\n      case VFF_TYP_2_BYTE: bytes_per_pixel=2; break;\n      case VFF_TYP_4_BYTE: bytes_per_pixel=4; break;\n      case VFF_TYP_FLOAT: bytes_per_pixel=4; break;\n      case VFF_TYP_DOUBLE: bytes_per_pixel=8; break;\n      default: bytes_per_pixel=1; break;\n    }\n    if (viff_info.data_storage_type == VFF_TYP_BIT)\n      max_packets=((image->columns+7UL) >> 3UL)*image->rows;\n    else\n      max_packets=(size_t) (number_pixels*viff_info.number_data_bands);\n    pixels=(unsigned char *) AcquireQuantumMemory(MagickMax(number_pixels,\n      max_packets),bytes_per_pixel*sizeof(*pixels));\n    if (pixels == (unsigned char *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    (void) ReadBlob(image,bytes_per_pixel*max_packets,pixels);\n    lsb_first=1;\n    if (*(char *) &lsb_first &&\n        ((viff_info.machine_dependency != VFF_DEP_DECORDER) &&\n         (viff_info.machine_dependency != VFF_DEP_NSORDER)))\n      switch ((int) viff_info.data_storage_type)\n      {\n        case VFF_TYP_2_BYTE:\n        {\n          MSBOrderShort(pixels,bytes_per_pixel*max_packets);\n          break;\n        }\n        case VFF_TYP_4_BYTE:\n        case VFF_TYP_FLOAT:\n        {\n          MSBOrderLong(pixels,bytes_per_pixel*max_packets);\n          break;\n        }\n        default: break;\n      }\n    min_value=0.0;\n    scale_factor=1.0;\n    if ((viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.map_scheme == VFF_MS_NONE))\n      {\n        double\n          max_value;\n\n        /*\n          Determine scale factor.\n        */\n        switch ((int) viff_info.data_storage_type)\n        {\n          case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[0]; break;\n          case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[0]; break;\n          case VFF_TYP_FLOAT: value=((float *) pixels)[0]; break;\n          case VFF_TYP_DOUBLE: value=((double *) pixels)[0]; break;\n          default: value=1.0*pixels[0]; break;\n        }\n        max_value=value;\n        min_value=value;\n        for (i=0; i < (ssize_t) max_packets; i++)\n        {\n          switch ((int) viff_info.data_storage_type)\n          {\n            case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[i]; break;\n            case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[i]; break;\n            case VFF_TYP_FLOAT: value=((float *) pixels)[i]; break;\n            case VFF_TYP_DOUBLE: value=((double *) pixels)[i]; break;\n            default: value=1.0*pixels[i]; break;\n          }\n          if (value > max_value)\n            max_value=value;\n          else\n            if (value < min_value)\n              min_value=value;\n        }\n        if ((min_value == 0) && (max_value == 0))\n          scale_factor=0;\n        else\n          if (min_value == max_value)\n            {\n              scale_factor=(MagickRealType) QuantumRange/min_value;\n              min_value=0;\n            }\n          else\n            scale_factor=(MagickRealType) QuantumRange/(max_value-min_value);\n      }\n    /*\n      Convert pixels to Quantum size.\n    */\n    p=(unsigned char *) pixels;\n    for (i=0; i < (ssize_t) max_packets; i++)\n    {\n      switch ((int) viff_info.data_storage_type)\n      {\n        case VFF_TYP_2_BYTE: value=1.0*((short *) pixels)[i]; break;\n        case VFF_TYP_4_BYTE: value=1.0*((int *) pixels)[i]; break;\n        case VFF_TYP_FLOAT: value=((float *) pixels)[i]; break;\n        case VFF_TYP_DOUBLE: value=((double *) pixels)[i]; break;\n        default: value=1.0*pixels[i]; break;\n      }\n      if (viff_info.map_scheme == VFF_MS_NONE)\n        {\n          value=(value-min_value)*scale_factor;\n          if (value > QuantumRange)\n            value=QuantumRange;\n          else\n            if (value < 0)\n              value=0;\n        }\n      *p=(unsigned char) ((Quantum) value);\n      p++;\n    }\n    /*\n      Convert VIFF raster image to pixel packets.\n    */\n    p=(unsigned char *) pixels;\n    if (viff_info.data_storage_type == VFF_TYP_BIT)\n      {\n        /*\n          Convert bitmap scanline.\n        */\n        if (image->storage_class != PseudoClass)\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (PixelPacket *) NULL)\n            break;\n          indexes=GetAuthenticIndexQueue(image);\n          for (x=0; x < (ssize_t) (image->columns-7); x+=8)\n          {\n            for (bit=0; bit < 8; bit++)\n            {\n              quantum=(size_t) ((*p) & (0x01 << bit) ? 0 : 1);\n              SetPixelRed(q,quantum == 0 ? 0 : QuantumRange);\n              SetPixelGreen(q,quantum == 0 ? 0 : QuantumRange);\n              SetPixelBlue(q,quantum == 0 ? 0 : QuantumRange);\n              if (image->storage_class == PseudoClass)\n                SetPixelIndex(indexes+x+bit,quantum);\n             }\n            p++;\n          }\n          if ((image->columns % 8) != 0)\n            {\n              for (bit=0; bit < (int) (image->columns % 8); bit++)\n              {\n                quantum=(size_t) ((*p) & (0x01 << bit) ? 0 : 1);\n                SetPixelRed(q,quantum == 0 ? 0 : QuantumRange);\n                SetPixelGreen(q,quantum == 0 ? 0 : QuantumRange);\n                SetPixelBlue(q,quantum == 0 ? 0 : QuantumRange);\n                if (image->storage_class == PseudoClass)\n                  SetPixelIndex(indexes+x+bit,quantum);\n              }\n              p++;\n            }\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      }\n    else\n      if (image->storage_class == PseudoClass)\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (PixelPacket *) NULL)\n            break;\n          indexes=GetAuthenticIndexQueue(image);\n          for (x=0; x < (ssize_t) image->columns; x++)\n            SetPixelIndex(indexes+x,*p++);\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      else\n        {\n          /*\n            Convert DirectColor scanline.\n          */\n          number_pixels=(MagickSizeType) image->columns*image->rows;\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n            if (q == (PixelPacket *) NULL)\n              break;\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              SetPixelRed(q,ScaleCharToQuantum(*p));\n              SetPixelGreen(q,ScaleCharToQuantum(*(p+number_pixels)));\n              SetPixelBlue(q,ScaleCharToQuantum(*(p+2*number_pixels)));\n              if (image->colors != 0)\n                {\n                  ssize_t\n                    index;\n\n                  index=(ssize_t) GetPixelRed(q);\n                  SetPixelRed(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].red);\n                  index=(ssize_t) GetPixelGreen(q);\n                  SetPixelGreen(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].green);\n                  index=(ssize_t) GetPixelRed(q);\n                  SetPixelBlue(q,image->colormap[(ssize_t)\n                    ConstrainColormapIndex(image,index)].blue);\n                }\n              SetPixelOpacity(q,image->matte != MagickFalse ? QuantumRange-\n                ScaleCharToQuantum(*(p+number_pixels*3)) : OpaqueOpacity);\n              p++;\n              q++;\n            }\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n              break;\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n                if (status == MagickFalse)\n                  break;\n              }\n          }\n        }\n    pixels=(unsigned char *) RelinquishMagickMemory(pixels);\n    if (image->storage_class == PseudoClass)\n      (void) SyncImage(image);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    count=ReadBlob(image,1,&viff_info.identifier);\n    if ((count != 0) && (viff_info.identifier == 0xab))\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while ((count != 0) && (viff_info.identifier == 0xab));\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/ca0c886abd6d3ef335eb74150cd23b89ebd17135", "file_name": "coders/viff.c", "vul_type": "cwe-125", "description": "Write a C function to read a VIFF image file in ImageMagick."}
{"func_name": "archive_acl_from_text_l", "func_src_before": "archive_acl_from_text_l(struct archive_acl *acl, const char *text,\n    int want_type, struct archive_string_conv *sc)\n{\n\tstruct {\n\t\tconst char *start;\n\t\tconst char *end;\n\t} field[6], name;\n\n\tconst char *s, *st;\n\tint numfields, fields, n, r, sol, ret;\n\tint type, types, tag, permset, id;\n\tsize_t len;\n\tchar sep;\n\n\tswitch (want_type) {\n\tcase ARCHIVE_ENTRY_ACL_TYPE_POSIX1E:\n\t\twant_type = ARCHIVE_ENTRY_ACL_TYPE_ACCESS;\n\t\t__LA_FALLTHROUGH;\n\tcase ARCHIVE_ENTRY_ACL_TYPE_ACCESS:\n\tcase ARCHIVE_ENTRY_ACL_TYPE_DEFAULT:\n\t\tnumfields = 5;\n\t\tbreak;\n\tcase ARCHIVE_ENTRY_ACL_TYPE_NFS4:\n\t\tnumfields = 6;\n\t\tbreak;\n\tdefault:\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\n\tret = ARCHIVE_OK;\n\ttypes = 0;\n\n\twhile (text != NULL &&  *text != '\\0') {\n\t\t/*\n\t\t * Parse the fields out of the next entry,\n\t\t * advance 'text' to start of next entry.\n\t\t */\n\t\tfields = 0;\n\t\tdo {\n\t\t\tconst char *start, *end;\n\t\t\tnext_field(&text, &start, &end, &sep);\n\t\t\tif (fields < numfields) {\n\t\t\t\tfield[fields].start = start;\n\t\t\t\tfield[fields].end = end;\n\t\t\t}\n\t\t\t++fields;\n\t\t} while (sep == ':');\n\n\t\t/* Set remaining fields to blank. */\n\t\tfor (n = fields; n < numfields; ++n)\n\t\t\tfield[n].start = field[n].end = NULL;\n\n\t\tif (field[0].start != NULL && *(field[0].start) == '#') {\n\t\t\t/* Comment, skip entry */\n\t\t\tcontinue;\n\t\t}\n\n\t\tn = 0;\n\t\tsol = 0;\n\t\tid = -1;\n\t\tpermset = 0;\n\t\tname.start = name.end = NULL;\n\n\t\tif (want_type != ARCHIVE_ENTRY_ACL_TYPE_NFS4) {\n\t\t\t/* POSIX.1e ACLs */\n\t\t\t/*\n\t\t\t * Default keyword \"default:user::rwx\"\n\t\t\t * if found, we have one more field\n\t\t\t *\n\t\t\t * We also support old Solaris extension:\n\t\t\t * \"defaultuser::rwx\" is the default ACL corresponding\n\t\t\t * to \"user::rwx\", etc. valid only for first field\n\t\t\t */\n\t\t\ts = field[0].start;\n\t\t\tlen = field[0].end - field[0].start;\n\t\t\tif (*s == 'd' && (len == 1 || (len >= 7\n\t\t\t    && memcmp((s + 1), \"efault\", 6) == 0))) {\n\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_DEFAULT;\n\t\t\t\tif (len > 7)\n\t\t\t\t\tfield[0].start += 7;\n\t\t\t\telse\n\t\t\t\t\tn = 1;\n\t\t\t} else\n\t\t\t\ttype = want_type;\n\n\t\t\t/* Check for a numeric ID in field n+1 or n+3. */\n\t\t\tisint(field[n + 1].start, field[n + 1].end, &id);\n\t\t\t/* Field n+3 is optional. */\n\t\t\tif (id == -1 && fields > (n + 3))\n\t\t\t\tisint(field[n + 3].start, field[n + 3].end,\n\t\t\t\t    &id);\n\n\t\t\ttag = 0;\n\t\t\ts = field[n].start;\n\t\t\tst = field[n].start + 1;\n\t\t\tlen = field[n].end - field[n].start;\n\n\t\t\tswitch (*s) {\n\t\t\tcase 'u':\n\t\t\t\tif (len == 1 || (len == 4\n\t\t\t\t    && memcmp(st, \"ser\", 3) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 'g':\n\t\t\t\tif (len == 1 || (len == 5\n\t\t\t\t    && memcmp(st, \"roup\", 4) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 'o':\n\t\t\t\tif (len == 1 || (len == 5\n\t\t\t\t    && memcmp(st, \"ther\", 4) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_OTHER;\n\t\t\t\tbreak;\n\t\t\tcase 'm':\n\t\t\t\tif (len == 1 || (len == 4\n\t\t\t\t    && memcmp(st, \"ask\", 3) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_MASK;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (tag) {\n\t\t\tcase ARCHIVE_ENTRY_ACL_OTHER:\n\t\t\tcase ARCHIVE_ENTRY_ACL_MASK:\n\t\t\t\tif (fields == (n + 2)\n\t\t\t\t    && field[n + 1].start < field[n + 1].end\n\t\t\t\t    && ismode(field[n + 1].start,\n\t\t\t\t    field[n + 1].end, &permset)) {\n\t\t\t\t\t/* This is Solaris-style \"other:rwx\" */\n\t\t\t\t\tsol = 1;\n\t\t\t\t} else if (fields == (n + 3) &&\n\t\t\t\t    field[n + 1].start < field[n + 1].end) {\n\t\t\t\t\t/* Invalid mask or other field */\n\t\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase ARCHIVE_ENTRY_ACL_USER_OBJ:\n\t\t\tcase ARCHIVE_ENTRY_ACL_GROUP_OBJ:\n\t\t\t\tif (id != -1 ||\n\t\t\t\t    field[n + 1].start < field[n + 1].end) {\n\t\t\t\t\tname = field[n + 1];\n\t\t\t\t\tif (tag == ARCHIVE_ENTRY_ACL_USER_OBJ)\n\t\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER;\n\t\t\t\t\telse\n\t\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* Invalid tag, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Without \"default:\" we expect mode in field 3\n\t\t\t * Exception: Solaris other and mask fields\n\t\t\t */\n\t\t\tif (permset == 0 && !ismode(field[n + 2 - sol].start,\n\t\t\t    field[n + 2 - sol].end, &permset)) {\n\t\t\t\t/* Invalid mode, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\t/* NFS4 ACLs */\n\t\t\ts = field[0].start;\n\t\t\tlen = field[0].end - field[0].start;\n\t\t\ttag = 0;\n\n\t\t\tswitch (len) {\n\t\t\tcase 4:\n\t\t\t\tif (memcmp(s, \"user\", 4) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER;\n\t\t\t\tbreak;\n\t\t\tcase 5:\n\t\t\t\tif (memcmp(s, \"group\", 5) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP;\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tif (memcmp(s, \"owner@\", 6) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER_OBJ;\n\t\t\t\telse if (memcmp(s, \"group@\", 6) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 9:\n\t\t\t\tif (memcmp(s, \"everyone@\", 9) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_EVERYONE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (tag == 0) {\n\t\t\t\t/* Invalid tag, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t} else if (tag == ARCHIVE_ENTRY_ACL_USER ||\n\t\t\t    tag == ARCHIVE_ENTRY_ACL_GROUP) {\n\t\t\t\tn = 1;\n\t\t\t\tname = field[1];\n\t\t\t\tisint(name.start, name.end, &id);\n\t\t\t} else\n\t\t\t\tn = 0;\n\n\t\t\tif (!is_nfs4_perms(field[1 + n].start,\n\t\t\t    field[1 + n].end, &permset)) {\n\t\t\t\t/* Invalid NFSv4 perms, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!is_nfs4_flags(field[2 + n].start,\n\t\t\t    field[2 + n].end, &permset)) {\n\t\t\t\t/* Invalid NFSv4 flags, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ts = field[3 + n].start;\n\t\t\tlen = field[3 + n].end - field[3 + n].start;\n\t\t\ttype = 0;\n\t\t\tif (len == 4) {\n\t\t\t\tif (memcmp(s, \"deny\", 4) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_DENY;\n\t\t\t} else if (len == 5) {\n\t\t\t\tif (memcmp(s, \"allow\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_ALLOW;\n\t\t\t\telse if (memcmp(s, \"audit\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_AUDIT;\n\t\t\t\telse if (memcmp(s, \"alarm\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_ALARM;\n\t\t\t}\n\t\t\tif (type == 0) {\n\t\t\t\t/* Invalid entry type, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tisint(field[4 + n].start, field[4 + n].end,\n\t\t\t    &id);\n\t\t}\n\n\t\t/* Add entry to the internal list. */\n\t\tr = archive_acl_add_entry_len_l(acl, type, permset,\n\t\t    tag, id, name.start, name.end - name.start, sc);\n\t\tif (r < ARCHIVE_WARN)\n\t\t\treturn (r);\n\t\tif (r != ARCHIVE_OK)\n\t\t\tret = ARCHIVE_WARN;\n\t\ttypes |= type;\n\t}\n\n\t/* Reset ACL */\n\tarchive_acl_reset(acl, types);\n\n\treturn (ret);\n}", "func_src_after": "archive_acl_from_text_l(struct archive_acl *acl, const char *text,\n    int want_type, struct archive_string_conv *sc)\n{\n\tstruct {\n\t\tconst char *start;\n\t\tconst char *end;\n\t} field[6], name;\n\n\tconst char *s, *st;\n\tint numfields, fields, n, r, sol, ret;\n\tint type, types, tag, permset, id;\n\tsize_t len;\n\tchar sep;\n\n\tswitch (want_type) {\n\tcase ARCHIVE_ENTRY_ACL_TYPE_POSIX1E:\n\t\twant_type = ARCHIVE_ENTRY_ACL_TYPE_ACCESS;\n\t\t__LA_FALLTHROUGH;\n\tcase ARCHIVE_ENTRY_ACL_TYPE_ACCESS:\n\tcase ARCHIVE_ENTRY_ACL_TYPE_DEFAULT:\n\t\tnumfields = 5;\n\t\tbreak;\n\tcase ARCHIVE_ENTRY_ACL_TYPE_NFS4:\n\t\tnumfields = 6;\n\t\tbreak;\n\tdefault:\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\n\tret = ARCHIVE_OK;\n\ttypes = 0;\n\n\twhile (text != NULL &&  *text != '\\0') {\n\t\t/*\n\t\t * Parse the fields out of the next entry,\n\t\t * advance 'text' to start of next entry.\n\t\t */\n\t\tfields = 0;\n\t\tdo {\n\t\t\tconst char *start, *end;\n\t\t\tnext_field(&text, &start, &end, &sep);\n\t\t\tif (fields < numfields) {\n\t\t\t\tfield[fields].start = start;\n\t\t\t\tfield[fields].end = end;\n\t\t\t}\n\t\t\t++fields;\n\t\t} while (sep == ':');\n\n\t\t/* Set remaining fields to blank. */\n\t\tfor (n = fields; n < numfields; ++n)\n\t\t\tfield[n].start = field[n].end = NULL;\n\n\t\tif (field[0].start != NULL && *(field[0].start) == '#') {\n\t\t\t/* Comment, skip entry */\n\t\t\tcontinue;\n\t\t}\n\n\t\tn = 0;\n\t\tsol = 0;\n\t\tid = -1;\n\t\tpermset = 0;\n\t\tname.start = name.end = NULL;\n\n\t\tif (want_type != ARCHIVE_ENTRY_ACL_TYPE_NFS4) {\n\t\t\t/* POSIX.1e ACLs */\n\t\t\t/*\n\t\t\t * Default keyword \"default:user::rwx\"\n\t\t\t * if found, we have one more field\n\t\t\t *\n\t\t\t * We also support old Solaris extension:\n\t\t\t * \"defaultuser::rwx\" is the default ACL corresponding\n\t\t\t * to \"user::rwx\", etc. valid only for first field\n\t\t\t */\n\t\t\ts = field[0].start;\n\t\t\tlen = field[0].end - field[0].start;\n\t\t\tif (*s == 'd' && (len == 1 || (len >= 7\n\t\t\t    && memcmp((s + 1), \"efault\", 6) == 0))) {\n\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_DEFAULT;\n\t\t\t\tif (len > 7)\n\t\t\t\t\tfield[0].start += 7;\n\t\t\t\telse\n\t\t\t\t\tn = 1;\n\t\t\t} else\n\t\t\t\ttype = want_type;\n\n\t\t\t/* Check for a numeric ID in field n+1 or n+3. */\n\t\t\tisint(field[n + 1].start, field[n + 1].end, &id);\n\t\t\t/* Field n+3 is optional. */\n\t\t\tif (id == -1 && fields > (n + 3))\n\t\t\t\tisint(field[n + 3].start, field[n + 3].end,\n\t\t\t\t    &id);\n\n\t\t\ttag = 0;\n\t\t\ts = field[n].start;\n\t\t\tst = field[n].start + 1;\n\t\t\tlen = field[n].end - field[n].start;\n\n\t\t\tif (len == 0) {\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tswitch (*s) {\n\t\t\tcase 'u':\n\t\t\t\tif (len == 1 || (len == 4\n\t\t\t\t    && memcmp(st, \"ser\", 3) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 'g':\n\t\t\t\tif (len == 1 || (len == 5\n\t\t\t\t    && memcmp(st, \"roup\", 4) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 'o':\n\t\t\t\tif (len == 1 || (len == 5\n\t\t\t\t    && memcmp(st, \"ther\", 4) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_OTHER;\n\t\t\t\tbreak;\n\t\t\tcase 'm':\n\t\t\t\tif (len == 1 || (len == 4\n\t\t\t\t    && memcmp(st, \"ask\", 3) == 0))\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_MASK;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (tag) {\n\t\t\tcase ARCHIVE_ENTRY_ACL_OTHER:\n\t\t\tcase ARCHIVE_ENTRY_ACL_MASK:\n\t\t\t\tif (fields == (n + 2)\n\t\t\t\t    && field[n + 1].start < field[n + 1].end\n\t\t\t\t    && ismode(field[n + 1].start,\n\t\t\t\t    field[n + 1].end, &permset)) {\n\t\t\t\t\t/* This is Solaris-style \"other:rwx\" */\n\t\t\t\t\tsol = 1;\n\t\t\t\t} else if (fields == (n + 3) &&\n\t\t\t\t    field[n + 1].start < field[n + 1].end) {\n\t\t\t\t\t/* Invalid mask or other field */\n\t\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase ARCHIVE_ENTRY_ACL_USER_OBJ:\n\t\t\tcase ARCHIVE_ENTRY_ACL_GROUP_OBJ:\n\t\t\t\tif (id != -1 ||\n\t\t\t\t    field[n + 1].start < field[n + 1].end) {\n\t\t\t\t\tname = field[n + 1];\n\t\t\t\t\tif (tag == ARCHIVE_ENTRY_ACL_USER_OBJ)\n\t\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER;\n\t\t\t\t\telse\n\t\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* Invalid tag, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Without \"default:\" we expect mode in field 3\n\t\t\t * Exception: Solaris other and mask fields\n\t\t\t */\n\t\t\tif (permset == 0 && !ismode(field[n + 2 - sol].start,\n\t\t\t    field[n + 2 - sol].end, &permset)) {\n\t\t\t\t/* Invalid mode, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\t/* NFS4 ACLs */\n\t\t\ts = field[0].start;\n\t\t\tlen = field[0].end - field[0].start;\n\t\t\ttag = 0;\n\n\t\t\tswitch (len) {\n\t\t\tcase 4:\n\t\t\t\tif (memcmp(s, \"user\", 4) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER;\n\t\t\t\tbreak;\n\t\t\tcase 5:\n\t\t\t\tif (memcmp(s, \"group\", 5) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP;\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tif (memcmp(s, \"owner@\", 6) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_USER_OBJ;\n\t\t\t\telse if (memcmp(s, \"group@\", 6) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_GROUP_OBJ;\n\t\t\t\tbreak;\n\t\t\tcase 9:\n\t\t\t\tif (memcmp(s, \"everyone@\", 9) == 0)\n\t\t\t\t\ttag = ARCHIVE_ENTRY_ACL_EVERYONE;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (tag == 0) {\n\t\t\t\t/* Invalid tag, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t} else if (tag == ARCHIVE_ENTRY_ACL_USER ||\n\t\t\t    tag == ARCHIVE_ENTRY_ACL_GROUP) {\n\t\t\t\tn = 1;\n\t\t\t\tname = field[1];\n\t\t\t\tisint(name.start, name.end, &id);\n\t\t\t} else\n\t\t\t\tn = 0;\n\n\t\t\tif (!is_nfs4_perms(field[1 + n].start,\n\t\t\t    field[1 + n].end, &permset)) {\n\t\t\t\t/* Invalid NFSv4 perms, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!is_nfs4_flags(field[2 + n].start,\n\t\t\t    field[2 + n].end, &permset)) {\n\t\t\t\t/* Invalid NFSv4 flags, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ts = field[3 + n].start;\n\t\t\tlen = field[3 + n].end - field[3 + n].start;\n\t\t\ttype = 0;\n\t\t\tif (len == 4) {\n\t\t\t\tif (memcmp(s, \"deny\", 4) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_DENY;\n\t\t\t} else if (len == 5) {\n\t\t\t\tif (memcmp(s, \"allow\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_ALLOW;\n\t\t\t\telse if (memcmp(s, \"audit\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_AUDIT;\n\t\t\t\telse if (memcmp(s, \"alarm\", 5) == 0)\n\t\t\t\t\ttype = ARCHIVE_ENTRY_ACL_TYPE_ALARM;\n\t\t\t}\n\t\t\tif (type == 0) {\n\t\t\t\t/* Invalid entry type, skip entry */\n\t\t\t\tret = ARCHIVE_WARN;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tisint(field[4 + n].start, field[4 + n].end,\n\t\t\t    &id);\n\t\t}\n\n\t\t/* Add entry to the internal list. */\n\t\tr = archive_acl_add_entry_len_l(acl, type, permset,\n\t\t    tag, id, name.start, name.end - name.start, sc);\n\t\tif (r < ARCHIVE_WARN)\n\t\t\treturn (r);\n\t\tif (r != ARCHIVE_OK)\n\t\t\tret = ARCHIVE_WARN;\n\t\ttypes |= type;\n\t}\n\n\t/* Reset ACL */\n\tarchive_acl_reset(acl, types);\n\n\treturn (ret);\n}", "commit_link": "github.com/libarchive/libarchive/commit/15bf44fd2c1ad0e3fd87048b3fcc90c4dcff1175", "file_name": "libarchive/archive_acl.c", "vul_type": "cwe-476", "description": "Write a C function to parse ACL entries from a text string and add them to an ACL object."}
{"func_name": "ReadMATImage", "func_src_before": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "func_src_after": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\n    quantum_info=DestroyQuantumInfo(quantum_info);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/b173a352397877775c51c9a0e9d59eb6ce24c455", "file_name": "coders/mat.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a MATLAB image file in ImageMagick."}
{"func_name": "download", "func_src_before": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           open io_or_filename, 'wb'\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "func_src_after": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           ::File.open(io_or_filename, 'wb')\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "line_changes": {"deleted": [{"line_no": 9, "char_start": 248, "char_end": 285, "line": "           open io_or_filename, 'wb'\n"}], "added": [{"line_no": 9, "char_start": 248, "char_end": 293, "line": "           ::File.open(io_or_filename, 'wb')\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 264, "chars": " "}], "added": [{"char_start": 259, "char_end": 266, "chars": "::File."}, {"char_start": 270, "char_end": 271, "chars": "("}, {"char_start": 291, "char_end": 292, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/2ac906b26f4a565a0af92df5fb9c8a36c2b75375", "file_name": "mechanize.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in Mechanize#download\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `download` that retrieves content from a URL and saves it to a file or an IO object."}
{"func_name": "_add_volume_to_volume_set", "func_src_before": "    def _add_volume_to_volume_set(self, volume, volume_name,\n                                  cpg, vvs_name, qos):\n        if vvs_name is not None:\n            # Admin has set a volume set name to add the volume to\n            self._cli_run('createvvset -add %s %s' % (vvs_name,\n                                                      volume_name), None)\n        else:\n            vvs_name = self._get_3par_vvs_name(volume['id'])\n            domain = self.get_domain(cpg)\n            self._cli_run('createvvset -domain %s %s' % (domain,\n                                                         vvs_name), None)\n            self._set_qos_rule(qos, vvs_name)\n            self._cli_run('createvvset -add %s %s' % (vvs_name,\n                                                      volume_name), None)", "func_src_after": "    def _add_volume_to_volume_set(self, volume, volume_name,\n                                  cpg, vvs_name, qos):\n        if vvs_name is not None:\n            # Admin has set a volume set name to add the volume to\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])\n        else:\n            vvs_name = self._get_3par_vvs_name(volume['id'])\n            domain = self.get_domain(cpg)\n            self._cli_run(['createvvset', '-domain', domain, vvs_name])\n            self._set_qos_rule(qos, vvs_name)\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to add a volume to a volume set, optionally creating the set and applying QoS rules if the set doesn't exist."}
{"func_name": "enc_untrusted_inet_ntop", "func_src_before": "const char *enc_untrusted_inet_ntop(int af, const void *src, char *dst,\n                                    socklen_t size) {\n  if (!src || !dst) {\n    errno = EFAULT;\n    return nullptr;\n  }\n  size_t src_size = 0;\n  if (af == AF_INET) {\n    src_size = sizeof(struct in_addr);\n  } else if (af == AF_INET6) {\n    src_size = sizeof(struct in6_addr);\n  } else {\n    errno = EAFNOSUPPORT;\n    return nullptr;\n  }\n\n  MessageWriter input;\n  input.Push<int>(TokLinuxAfFamily(af));\n  input.PushByReference(Extent{reinterpret_cast<const char *>(src), src_size});\n  input.Push(size);\n  MessageReader output;\n\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kInetNtopHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_inet_ntop\", 2);\n\n  auto result = output.next();\n  int klinux_errno = output.next<int>();\n  if (result.empty()) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n    return nullptr;\n  }\n\n  memcpy(dst, result.data(),\n         std::min(static_cast<size_t>(size),\n                  static_cast<size_t>(INET6_ADDRSTRLEN)));\n  return dst;\n}", "func_src_after": "const char *enc_untrusted_inet_ntop(int af, const void *src, char *dst,\n                                    socklen_t size) {\n  if (!src || !dst) {\n    errno = EFAULT;\n    return nullptr;\n  }\n  size_t src_size = 0;\n  if (af == AF_INET) {\n    src_size = sizeof(struct in_addr);\n  } else if (af == AF_INET6) {\n    src_size = sizeof(struct in6_addr);\n  } else {\n    errno = EAFNOSUPPORT;\n    return nullptr;\n  }\n\n  MessageWriter input;\n  input.Push<int>(TokLinuxAfFamily(af));\n  input.PushByReference(Extent{reinterpret_cast<const char *>(src), src_size});\n  input.Push(size);\n  MessageReader output;\n\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kInetNtopHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_inet_ntop\", 2);\n\n  auto result = output.next();\n  int klinux_errno = output.next<int>();\n  if (result.empty()) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n    return nullptr;\n  }\n\n  memcpy(\n      dst, result.data(),\n      std::min({static_cast<size_t>(size), static_cast<size_t>(result.size()),\n                static_cast<size_t>(INET6_ADDRSTRLEN)}));\n  return dst;\n}", "commit_link": "github.com/google/asylo/commit/6ff3b77ffe110a33a2f93848a6333f33616f02c4", "file_name": "asylo/platform/host_call/trusted/host_calls.cc", "vul_type": "cwe-125", "description": "Write a C++ function named `enc_untrusted_inet_ntop` that converts a network address into a human-readable string, handling potential errors."}
{"func_name": "login", "func_src_before": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = '{}' AND password = '{}'\n            LIMIT 1\n        \"\"\".format(username, password)\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query)\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "func_src_after": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = ? AND password = ?\n            LIMIT 1\n        \"\"\"\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query, (username, password))\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "commit_link": "github.com/AnetaStoycheva/Programming101_HackBulgaria/commit/c0d6f4b8fe83a375832845a45952b5153e4c34f3", "file_name": "Week_9/sql_manager.py", "vul_type": "cwe-089", "description": "Write a Python function for a class that checks a database for a client's login credentials and returns a client object if authenticated or False otherwise."}
{"func_name": "top_proxies", "func_src_before": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id='\"+proxy_id+\"' LIMIT 1\"\n        cur.execute(query)\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as='\"+proxy_id+\"'\"\n        cur.execute(query)\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "func_src_after": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id=%s LIMIT 1\"\n        cur.execute(query, (proxy_id,))\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as=%s\"\n        cur.execute(query, (proxy_id,))\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/top_proxies' that queries a PostgreSQL database to retrieve and return a JSON list of top proxy voters and their details."}
{"func_name": "gitMtime", "func_src_before": "async function gitMtime(path) {\n  const { stdout } = await execFilePromise('git', ['log', '-1', '--format=\"%at\"', '--', path]);\n\n  return parseInt(stdout.trim().replace('\"', '').trim(), 10);\n}", "func_src_after": "async function gitMtime(path) {\n  const { stdout } = await execFilePromise('git', ['log', '-1', '--format=\"%at\"', '--', path]);\n\n  return parseInt(stdout.trim().replace(/\"/g, '').trim(), 10);\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 129, "char_end": 191, "line": "  return parseInt(stdout.trim().replace('\"', '').trim(), 10);\n"}], "added": [{"line_no": 4, "char_start": 129, "char_end": 192, "line": "  return parseInt(stdout.trim().replace(/\"/g, '').trim(), 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 169, "char_end": 172, "chars": "'\"'"}], "added": [{"char_start": 169, "char_end": 173, "chars": "/\"/g"}]}, "commit_link": "github.com/openfoodfacts/openfoodfacts-server/commit/7917218c34b5ae2afe5d6581416c944607e31f36", "file_name": "refresh_taxonomies.js", "vul_type": "cwe-116", "commit_msg": "fix: CWE-116/CWE-20\n\nhttps://github.com/openfoodfacts/openfoodfacts-server/security/code-scanning/4", "parent_commit": "40386e19d82ff72f27066cb2bcbdf539dca0c6be", "description": "Create an asynchronous JavaScript function that retrieves the last modification timestamp of a file using Git."}
{"func_name": "make_canonical", "func_src_before": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "func_src_after": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            LY_CHECK_ERR_RETURN(strlen(module_name) + 1 + strlen(*value) > buf_len, LOGBUF(*value), -1);\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            LY_CHECK_ERR_RETURN(strlen(*value) > buf_len, LOGBUF(*value), -1);\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "commit_link": "github.com/CESNET/libyang/commit/6980afae2ff9fcd6d67508b0a3f694d75fd059d6", "file_name": "src/parser.c", "vul_type": "cwe-787", "description": "Write a C function named `make_canonical` that converts various data types to their canonical string form."}
{"func_name": "enl_ipc_get", "func_src_before": "char *enl_ipc_get(const char *msg_data)\n{\n\n\tstatic char *message = NULL;\n\tstatic unsigned short len = 0;\n\tchar buff[13], *ret_msg = NULL;\n\tregister unsigned char i;\n\tunsigned char blen;\n\n\tif (msg_data == IPC_TIMEOUT) {\n\t\treturn(IPC_TIMEOUT);\n\t}\n\tfor (i = 0; i < 12; i++) {\n\t\tbuff[i] = msg_data[i];\n\t}\n\tbuff[12] = 0;\n\tblen = strlen(buff);\n\tif (message != NULL) {\n\t\tlen += blen;\n\t\tmessage = (char *) erealloc(message, len + 1);\n\t\tstrcat(message, buff);\n\t} else {\n\t\tlen = blen;\n\t\tmessage = (char *) emalloc(len + 1);\n\t\tstrcpy(message, buff);\n\t}\n\tif (blen < 12) {\n\t\tret_msg = message;\n\t\tmessage = NULL;\n\t\tD((\"Received complete reply:  \\\"%s\\\"\\n\", ret_msg));\n\t}\n\treturn(ret_msg);\n}", "func_src_after": "char *enl_ipc_get(const char *msg_data)\n{\n\n\tstatic char *message = NULL;\n\tstatic size_t len = 0;\n\tchar buff[13], *ret_msg = NULL;\n\tregister unsigned char i;\n\tunsigned char blen;\n\n\tif (msg_data == IPC_TIMEOUT) {\n\t\treturn(IPC_TIMEOUT);\n\t}\n\tfor (i = 0; i < 12; i++) {\n\t\tbuff[i] = msg_data[i];\n\t}\n\tbuff[12] = 0;\n\tblen = strlen(buff);\n\tif (message != NULL) {\n\t\tlen += blen;\n\t\tmessage = (char *) erealloc(message, len + 1);\n\t\tstrcat(message, buff);\n\t} else {\n\t\tlen = blen;\n\t\tmessage = (char *) emalloc(len + 1);\n\t\tstrcpy(message, buff);\n\t}\n\tif (blen < 12) {\n\t\tret_msg = message;\n\t\tmessage = NULL;\n\t\tD((\"Received complete reply:  \\\"%s\\\"\\n\", ret_msg));\n\t}\n\treturn(ret_msg);\n}", "commit_link": "github.com/derf/feh/commit/f7a547b7ef8fc8ebdeaa4c28515c9d72e592fb6d", "file_name": "src/wallpaper.c", "vul_type": "cwe-787", "description": "Write a C function named `enl_ipc_get` that appends chunks of data to a static buffer until a complete message is received or a timeout occurs."}
{"func_name": "render_page_name", "func_src_before": "@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )", "func_src_after": "@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Create a Python Flask route to display a wiki page by its name, fetching the latest content from a database and rendering it with Markdown."}
{"func_name": "archive_directory", "func_src_before": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(top_dir, subdir, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in file_list:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n                return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "func_src_after": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(full_backup_path, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in files:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n            return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "line_changes": {"deleted": [{"line_no": 20, "char_start": 1003, "char_end": 1052, "line": "        fpath = os.path.join(top_dir, subdir, c)\n"}, {"line_no": 37, "char_start": 1561, "char_end": 1580, "line": "    file_list = []\n"}, {"line_no": 38, "char_start": 1580, "char_end": 1623, "line": "    for p in os.listdir(full_backup_path):\n"}, {"line_no": 39, "char_start": 1623, "char_end": 1685, "line": "        if os.path.isfile(os.path.join(full_backup_path, p)):\n"}, {"line_no": 40, "char_start": 1685, "char_end": 1749, "line": "            file_list.append(os.path.join(full_backup_path, p))\n"}, {"line_no": 44, "char_start": 1876, "char_end": 1908, "line": "            for f in file_list:\n"}, {"line_no": 47, "char_start": 2050, "char_end": 2091, "line": "                return archive_file_path\n"}], "added": [{"line_no": 20, "char_start": 1003, "char_end": 1053, "line": "        fpath = os.path.join(full_backup_path, c)\n"}, {"line_no": 40, "char_start": 1689, "char_end": 1717, "line": "            for f in files:\n"}, {"line_no": 43, "char_start": 1859, "char_end": 1896, "line": "            return archive_file_path\n"}]}, "char_changes": {"deleted": [{"char_start": 1032, "char_end": 1047, "chars": "top_dir, subdir"}, {"char_start": 1560, "char_end": 1748, "chars": "\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))"}, {"char_start": 1901, "char_end": 1906, "chars": "_list"}, {"char_start": 2050, "char_end": 2054, "chars": "    "}], "added": [{"char_start": 1032, "char_end": 1048, "chars": "full_backup_path"}, {"char_start": 1714, "char_end": 1715, "chars": "s"}]}, "commit_link": "github.com/calmcl1/cupo-backup/commit/f9047a52ab33a14fcd67d3d8b9f9d321502ae457", "file_name": "cupo.py", "vul_type": "cwe-022", "commit_msg": "Removed redundant directory traversal", "parent_commit": "49107dd052b985e1fc469aec359f37df23ec3e29", "description": "Write a Python function to zip files in a specified subdirectory, excluding '.ini' files, and save the archive to a temporary directory."}
{"func_name": "get_net_ns_by_id", "func_src_before": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tget_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}", "func_src_after": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}", "commit_link": "github.com/torvalds/linux/commit/21b5944350052d2583e82dd59b19a9ba94a007f0", "file_name": "net/core/net_namespace.c", "vul_type": "cwe-416", "description": "Write a C function named `get_net_ns_by_id` that retrieves a network namespace by its ID from a given network structure, handling synchronization and reference acquisition."}
{"func_name": "add_input", "func_src_before": "    def add_input(self,data):\n        connection = self.connect()\n\n        try:\n            query = \"INSERT INTO crimes (description) VALUES ('{}');\".format(data)\n            with connection.cursor() as cursor:\n                cursor.execute(query)\n                connection.commit()\n        finally:\n            connection.close()", "func_src_after": "    def add_input(self,data):\n        connection = self.connect()\n\n        try:\n            query = \"INSERT INTO crimes (description) VALUES (%s);\"\n            with connection.cursor() as cursor:\n                cursor.execute(query,data)\n                connection.commit()\n        finally:\n            connection.close()", "commit_link": "github.com/sgnab/crime-map-app/commit/209b23bad13594c9cdf18d8788fcba7c8f68d37b", "file_name": "dbhelper.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new record into a database table named 'crimes' with a single 'description' field."}
{"func_name": "rename", "func_src_before": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                temp_path = os.tempnam(self._abspath(parent),\n                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "func_src_after": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n                                                prefix=prefix)\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "line_changes": {"deleted": [{"line_no": 18, "char_start": 643, "char_end": 705, "line": "                temp_path = os.tempnam(self._abspath(parent),\n"}, {"line_no": 19, "char_start": 705, "char_end": 793, "line": "                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n"}], "added": [{"line_no": 18, "char_start": 643, "char_end": 716, "line": "                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n"}, {"line_no": 19, "char_start": 716, "char_end": 791, "line": "                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n"}, {"line_no": 20, "char_start": 791, "char_end": 854, "line": "                                                prefix=prefix)\n"}]}, "char_changes": {"deleted": [{"char_start": 659, "char_end": 682, "chars": "temp_path = os.tempnam("}, {"char_start": 744, "char_end": 791, "chars": "LocalClient.CASE_RENAME_PREFIX + old_name + '_'"}], "added": [{"char_start": 658, "char_end": 734, "chars": " prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _,"}, {"char_start": 751, "char_end": 768, "chars": "file.mkstemp(dir="}, {"char_start": 830, "char_end": 852, "chars": "         prefix=prefix"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/c131124daf80274ace04e8d68f90cef802ac66fa", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-702: remove use of deprecated and insecure os.tempnam()", "description": "Write a Python function to rename a file or directory, handling case sensitivity on different file systems."}
{"func_name": "rfbProcessFileTransferReadBuffer", "func_src_before": "char *rfbProcessFileTransferReadBuffer(rfbClientPtr cl, uint32_t length)\n{\n    char *buffer=NULL;\n    int   n=0;\n\n    FILEXFER_ALLOWED_OR_CLOSE_AND_RETURN(\"\", cl, NULL);\n    /*\n    rfbLog(\"rfbProcessFileTransferReadBuffer(%dlen)\\n\", length);\n    */\n    if (length>0) {\n        buffer=malloc((uint64_t)length+1);\n        if (buffer!=NULL) {\n            if ((n = rfbReadExact(cl, (char *)buffer, length)) <= 0) {\n                if (n != 0)\n                    rfbLogPerror(\"rfbProcessFileTransferReadBuffer: read\");\n                rfbCloseClient(cl);\n                /* NOTE: don't forget to free(buffer) if you return early! */\n                if (buffer!=NULL) free(buffer);\n                return NULL;\n            }\n            /* Null Terminate */\n            buffer[length]=0;\n        }\n    }\n    return buffer;\n}", "func_src_after": "char *rfbProcessFileTransferReadBuffer(rfbClientPtr cl, uint32_t length)\n{\n    char *buffer=NULL;\n    int   n=0;\n\n    FILEXFER_ALLOWED_OR_CLOSE_AND_RETURN(\"\", cl, NULL);\n\n    /*\n       We later alloc length+1, which might wrap around on 32-bit systems if length equals\n       0XFFFFFFFF, i.e. SIZE_MAX for 32-bit systems. On 64-bit systems, a length of 0XFFFFFFFF\n       will safely be allocated since this check will never trigger and malloc() can digest length+1\n       without problems as length is a uint32_t.\n    */\n    if(length == SIZE_MAX) {\n\trfbErr(\"rfbProcessFileTransferReadBuffer: too big file transfer length requested: %u\", (unsigned int)length);\n\trfbCloseClient(cl);\n\treturn NULL;\n    }\n\n    if (length>0) {\n        buffer=malloc((size_t)length+1);\n        if (buffer!=NULL) {\n            if ((n = rfbReadExact(cl, (char *)buffer, length)) <= 0) {\n                if (n != 0)\n                    rfbLogPerror(\"rfbProcessFileTransferReadBuffer: read\");\n                rfbCloseClient(cl);\n                /* NOTE: don't forget to free(buffer) if you return early! */\n                if (buffer!=NULL) free(buffer);\n                return NULL;\n            }\n            /* Null Terminate */\n            buffer[length]=0;\n        }\n    }\n    return buffer;\n}", "commit_link": "github.com/LibVNC/libvncserver/commit/15bb719c03cc70f14c36a843dcb16ed69b405707", "file_name": "libvncserver/rfbserver.c", "vul_type": "cwe-787", "description": "In C, write a function to read a specified length of data from a client connection into a buffer, handling memory allocation and error checking."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to display the contents of a specified database table in a web page."}
{"func_name": "subscribe_for_tags", "func_src_before": "@csrf.csrf_protect\ndef subscribe_for_tags(request):\n    \"\"\"process subscription of users by tags\"\"\"\n    #todo - use special separator to split tags\n    tag_names = request.REQUEST.get('tags','').strip().split()\n    pure_tag_names, wildcards = forms.clean_marked_tagnames(tag_names)\n    if request.user.is_authenticated():\n        if request.method == 'POST':\n            if 'ok' in request.POST:\n                request.user.mark_tags(\n                            pure_tag_names,\n                            wildcards,\n                            reason = 'good',\n                            action = 'add'\n                        )\n                request.user.message_set.create(\n                    message = _('Your tag subscription was saved, thanks!')\n                )\n            else:\n                message = _(\n                    'Tag subscription was canceled (<a href=\"%(url)s\">undo</a>).'\n                ) % {'url': request.path + '?tags=' + request.REQUEST['tags']}\n                request.user.message_set.create(message = message)\n            return HttpResponseRedirect(reverse('index'))\n        else:\n            data = {'tags': tag_names}\n            return render(request, 'subscribe_for_tags.html', data)\n    else:\n        all_tag_names = pure_tag_names + wildcards\n        message = _('Please sign in to subscribe for: %(tags)s') \\\n                    % {'tags': ', '.join(all_tag_names)}\n        request.user.message_set.create(message = message)\n        request.session['subscribe_for_tags'] = (pure_tag_names, wildcards)\n        return HttpResponseRedirect(url_utils.get_login_url())", "func_src_after": "@csrf.csrf_protect\ndef subscribe_for_tags(request):\n    \"\"\"process subscription of users by tags\"\"\"\n    #todo - use special separator to split tags\n    tag_names = request.REQUEST.get('tags','').strip().split()\n    pure_tag_names, wildcards = forms.clean_marked_tagnames(tag_names)\n    if request.user.is_authenticated():\n        if request.method == 'POST':\n            if 'ok' in request.POST:\n                request.user.mark_tags(\n                            pure_tag_names,\n                            wildcards,\n                            reason = 'good',\n                            action = 'add'\n                        )\n                request.user.message_set.create(\n                    message = _('Your tag subscription was saved, thanks!')\n                )\n            else:\n                message = _(\n                    'Tag subscription was canceled (<a href=\"%(url)s\">undo</a>).'\n                ) % {'url': escape(request.path) + '?tags=' + request.REQUEST['tags']}\n                request.user.message_set.create(message = message)\n            return HttpResponseRedirect(reverse('index'))\n        else:\n            data = {'tags': tag_names}\n            return render(request, 'subscribe_for_tags.html', data)\n    else:\n        all_tag_names = pure_tag_names + wildcards\n        message = _('Please sign in to subscribe for: %(tags)s') \\\n                    % {'tags': ', '.join(all_tag_names)}\n        request.user.message_set.create(message = message)\n        request.session['subscribe_for_tags'] = (pure_tag_names, wildcards)\n        return HttpResponseRedirect(url_utils.get_login_url())", "commit_link": "github.com/ASKBOT/askbot-devel/commit/a676a86b6b7a5737d4da4f59f71e037406f88d29", "file_name": "askbot/views/commands.py", "vul_type": "cwe-079", "description": "In Python, write a Django view function named `subscribe_for_tags` that handles user tag subscription requests, including CSRF protection."}
{"func_name": "copyFile", "func_src_before": "def copyFile(srcFile, destFile):\r\n    if isPosix():\r\n        os.system('cp \"%s\" \"%s\"' % (srcFile, destFile))\r\n    else:\r\n        ek.ek(shutil.copyfile, srcFile, destFile)\r\n        \r\n    try:\r\n        ek.ek(shutil.copymode, srcFile, destFile)\r\n    except OSError:\r\n        pass", "func_src_after": "def copyFile(srcFile, destFile):\r\n    if isPosix():\r\n        subprocess.call(['cp', srcFile, destFile])\r\n    else:\r\n        ek.ek(shutil.copyfile, srcFile, destFile)\r\n        \r\n    try:\r\n        ek.ek(shutil.copymode, srcFile, destFile)\r\n    except OSError:\r\n        pass", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 110, "line": "        os.system('cp \"%s\" \"%s\"' % (srcFile, destFile))\r\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 105, "line": "        subprocess.call(['cp', srcFile, destFile])\r\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 89, "chars": "os.system('cp \"%s\" \"%s\"' % ("}, {"char_start": 106, "char_end": 107, "chars": ")"}], "added": [{"char_start": 61, "char_end": 84, "chars": "subprocess.call(['cp', "}, {"char_start": 101, "char_end": 102, "chars": "]"}]}, "commit_link": "github.com/Arcanemagus/SickRage/commit/1b3b10903b8e3fec58941036b0084d9cd8e743dc", "file_name": "helpers.py", "vul_type": "cwe-078", "commit_msg": "prevent command injection\n\nto prevent command injection when using the \u2018cp\u2019 command to perform\ncopied use subprocess instead of os.system", "parent_commit": "a8787bc0d35990d47eee99e21c20689b6676b0b6", "description": "Write a Python function to copy a file from one location to another, handling both POSIX and non-POSIX systems."}
{"func_name": "store_versioninfo_gnu_verdef", "func_src_before": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && ((char *)defs + i < end); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || vstart + sizeof (Elf_(Verdaux)) > end) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || vstart + sizeof(Elf_(Verdaux)) > end) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "func_src_after": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && (end - (char *)defs > i); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1 || (char *)UINTPTR_MAX - vstart < vdaux) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "commit_link": "github.com/radare/radare2/commit/62e39f34b2705131a2d08aff0c2e542c6a52cf0e", "file_name": "libr/bin/format/elf/elf.c", "vul_type": "cwe-476", "description": "Write a C function named `store_versioninfo_gnu_verdef` that processes GNU version definition sections from an ELF binary object."}
{"func_name": "_launch_cli", "func_src_before": "  def _launch_cli(self):\n    if self._is_run_start:\n      self.observers[\"run_start_cli_run_numbers\"].append(self._run_call_count)\n    else:\n      self.observers[\"run_end_cli_run_numbers\"].append(self._run_call_count)\n\n    readline_cli = ui_factory.get_ui(\n        \"readline\",\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n    self._register_this_run_info(readline_cli)\n\n    while True:\n      command = self._command_sequence[self._command_pointer]\n      self._command_pointer += 1\n\n      try:\n        if command[0] == \"run\":\n          self._run_handler(command[1:])\n        elif command[0] == \"print_feed\":\n          self.observers[\"print_feed_responses\"].append(\n              self._print_feed_handler(command[1:]))\n        else:\n          raise ValueError(\"Unrecognized command prefix: %s\" % command[0])\n      except debugger_cli_common.CommandLineExit as e:\n        return e.exit_token", "func_src_after": "  def _launch_cli(self):\n    if self._is_run_start:\n      self.observers[\"run_start_cli_run_numbers\"].append(self._run_call_count)\n    else:\n      self.observers[\"run_end_cli_run_numbers\"].append(self._run_call_count)\n\n    readline_cli = ui_factory.get_ui(\n        \"readline\",\n        config=cli_config.CLIConfig(\n            config_file_path=os.path.join(tempfile.mkdtemp(), \".tfdbg_config\")))\n    self._register_this_run_info(readline_cli)\n\n    while self._command_pointer < len(self._command_sequence):\n      command = self._command_sequence[self._command_pointer]\n      self._command_pointer += 1\n\n      try:\n        if command[0] == \"run\":\n          self._run_handler(command[1:])\n        elif command[0] == \"print_feed\":\n          self.observers[\"print_feed_responses\"].append(\n              self._print_feed_handler(command[1:]))\n        else:\n          raise ValueError(\"Unrecognized command prefix: %s\" % command[0])\n      except debugger_cli_common.CommandLineExit as e:\n        return e.exit_token", "line_changes": {"deleted": [{"line_no": 13, "char_start": 443, "char_end": 459, "line": "    while True:\n"}], "added": [{"line_no": 13, "char_start": 443, "char_end": 506, "line": "    while self._command_pointer < len(self._command_sequence):\n"}]}, "char_changes": {"deleted": [{"char_start": 453, "char_end": 457, "chars": "True"}], "added": [{"char_start": 453, "char_end": 504, "chars": "self._command_pointer < len(self._command_sequence)"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4f93d5f529a732dd533c063ae5b85e03e2006882", "file_name": "local_cli_wrapper_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420369603\nChange-Id: I2cf40b13f41cc01000c2c21a483a2d680194dba2", "description": "Write a Python function to handle CLI commands for running tasks and printing feeds, with error handling for unrecognized commands."}
{"func_name": "_delete_host", "func_src_before": "    def _delete_host(self, host_name):\n        \"\"\"Delete a host on the storage system.\"\"\"\n\n        LOG.debug(_('enter: _delete_host: host %s ') % host_name)\n\n        ssh_cmd = 'svctask rmhost %s ' % host_name\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_delete_host', ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _delete_host: host %s ') % host_name)", "func_src_after": "    def _delete_host(self, host_name):\n        \"\"\"Delete a host on the storage system.\"\"\"\n\n        LOG.debug(_('enter: _delete_host: host %s ') % host_name)\n\n        ssh_cmd = ['svctask', 'rmhost', host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_delete_host', ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _delete_host: host %s ') % host_name)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to delete a host from a storage system using SSH commands, with debug logging before and after the operation."}
{"func_name": "placings", "func_src_before": "@endpoints.route(\"/placings\")\ndef placings():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default='christmas mike')\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM placings WHERE player = '{}'\".format(tag)\n    results = list(db.exec(sql))\n    results.sort(key=lambda x: int(x[2]))\n\n    return json.dumps(results)", "func_src_after": "@endpoints.route(\"/placings\")\ndef placings():\n    if db == None:\n        init()\n\n    tag = request.args.get('tag', default='christmas mike')\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM placings WHERE player = '{tag}'\"\n    args = {'tag': tag}\n    results = list(db.exec(sql, args))\n    results.sort(key=lambda x: int(x[2]))\n\n    return json.dumps(results)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "endpoints.py", "vul_type": "cwe-089", "description": "Create a Python Flask endpoint that retrieves and sorts player placings from a database using a default tag if none is provided."}
{"func_name": "get", "func_src_before": "    def get(self, user_id):\n        \"\"\" Fetch data for user with corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"select * from users where user_id = '{user_id}'\"\"\")", "func_src_after": "    def get(self, user_id):\n        \"\"\" Fetch data for user with corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"select * from users where user_id = %s\"\"\", (user_id, ))", "commit_link": "github.com/sgosal2/tiger-boards-backend/commit/4670109dd613df2f2fe7e8403ebd149df2b55485", "file_name": "apis/users.py", "vul_type": "cwe-089", "description": "Write a Python function named `get` that retrieves user data from a database using the user's ID."}
{"func_name": "mailimf_group_parse", "func_src_before": "static int mailimf_group_parse(const char * message, size_t length,\n\t\t\t       size_t * indx,\n\t\t\t       struct mailimf_group ** result)\n{\n  size_t cur_token;\n  char * display_name;\n  struct mailimf_mailbox_list * mailbox_list;\n  struct mailimf_group * group;\n  int r;\n  int res;\n\n  cur_token = * indx;\n\n  mailbox_list = NULL;\n\n  r = mailimf_display_name_parse(message, length, &cur_token, &display_name);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto err;\n  }\n\n  r = mailimf_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_mailbox_list_parse(message, length, &cur_token, &mailbox_list);\n  switch (r) {\n  case MAILIMF_NO_ERROR:\n    break;\n  case MAILIMF_ERROR_PARSE:\n    r = mailimf_cfws_parse(message, length, &cur_token);\n    if ((r != MAILIMF_NO_ERROR) && (r != MAILIMF_ERROR_PARSE)) {\n      res = r;\n      goto free_display_name;\n    }\n    break;\n  default:\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_semi_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_mailbox_list;\n  }\n\n  group = mailimf_group_new(display_name, mailbox_list);\n  if (group == NULL) {\n    res = MAILIMF_ERROR_MEMORY;\n    goto free_mailbox_list;\n  }\n\n  * indx = cur_token;\n  * result = group;\n\n  return MAILIMF_NO_ERROR;\n\n free_mailbox_list:\n  if (mailbox_list != NULL) {\n    mailimf_mailbox_list_free(mailbox_list);\n  }\n free_display_name:\n  mailimf_display_name_free(display_name);\n err:\n  return res;\n}", "func_src_after": "static int mailimf_group_parse(const char * message, size_t length,\n\t\t\t       size_t * indx,\n\t\t\t       struct mailimf_group ** result)\n{\n  size_t cur_token;\n  char * display_name;\n  struct mailimf_mailbox_list * mailbox_list;\n  struct mailimf_group * group;\n  int r;\n  int res;\n  clist * list;\n\n  cur_token = * indx;\n\n  mailbox_list = NULL;\n\n  r = mailimf_display_name_parse(message, length, &cur_token, &display_name);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto err;\n  }\n\n  r = mailimf_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_mailbox_list_parse(message, length, &cur_token, &mailbox_list);\n  switch (r) {\n  case MAILIMF_NO_ERROR:\n    break;\n  case MAILIMF_ERROR_PARSE:\n    r = mailimf_cfws_parse(message, length, &cur_token);\n    if ((r != MAILIMF_NO_ERROR) && (r != MAILIMF_ERROR_PARSE)) {\n      res = r;\n      goto free_display_name;\n    }\n    list = clist_new();\n    if (list == NULL) {\n      res = MAILIMF_ERROR_MEMORY;\n      goto free_display_name;\n    }\n    mailbox_list = mailimf_mailbox_list_new(list);\n    if (mailbox_list == NULL) {\n      res = MAILIMF_ERROR_MEMORY;\n      clist_free(list);\n      goto free_display_name;\n    }\n    break;\n  default:\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_semi_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_mailbox_list;\n  }\n\n  group = mailimf_group_new(display_name, mailbox_list);\n  if (group == NULL) {\n    res = MAILIMF_ERROR_MEMORY;\n    goto free_mailbox_list;\n  }\n\n  * indx = cur_token;\n  * result = group;\n\n  return MAILIMF_NO_ERROR;\n\n free_mailbox_list:\n  if (mailbox_list != NULL) {\n    mailimf_mailbox_list_free(mailbox_list);\n  }\n free_display_name:\n  mailimf_display_name_free(display_name);\n err:\n  return res;\n}", "commit_link": "github.com/dinhviethoa/libetpan/commit/1fe8fbc032ccda1db9af66d93016b49c16c1f22d", "file_name": "src/low-level/imf/mailimf.c", "vul_type": "cwe-476", "description": "Write a C function to parse an email group from a string, handling memory allocation and errors."}
{"func_name": "searchAll", "func_src_before": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "func_src_after": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n    searchStr = escape(searchStr);\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "line_changes": {"deleted": [], "added": [{"line_no": 5, "char_start": 189, "char_end": 224, "line": "    searchStr = escape(searchStr);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 188, "char_end": 223, "chars": "\n    searchStr = escape(searchStr);"}]}, "commit_link": "github.com/lrytz/scala/commit/ee2719585e40cb4e9e523e20061a6a2075f4d49d", "file_name": "index.js", "vul_type": "cwe-079", "commit_msg": "fix XSS vulnerability in scaladoc search\n\nto trigger XSS vuln, simply paste this into the search bar:\n```\n\"\\><img/src='1'onerror=alert(777111)>{{7*7}}\n```\n\nall credit for finding the vulnerability goes to *Yeasir Arafat* <skylinearafat@gmail.com>", "description": "Write a JavaScript function named `searchAll` that handles a search input, updates the browser's URL, and displays search results for members and entities."}
{"func_name": "(anonymous)", "func_src_before": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n\t\t\t\t\t +'</tr>');\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "func_src_after": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 113, "line": "\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n"}, {"line_no": 3, "char_start": 113, "char_end": 212, "line": "\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n"}, {"line_no": 4, "char_start": 212, "char_end": 229, "line": "\t\t\t\t\t +'</tr>');\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 122, "line": "\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n"}, {"line_no": 3, "char_start": 122, "char_end": 187, "line": "\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n"}, {"line_no": 4, "char_start": 187, "char_end": 263, "line": "\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 81, "chars": " data-dir=\"'+dir+'\" "}, {"char_start": 94, "char_end": 98, "chars": "=\"'+"}, {"char_start": 107, "char_end": 126, "chars": "+'\">'\n\t\t\t\t\t\t+'<td c"}, {"char_start": 130, "char_end": 132, "chars": "=\""}, {"char_start": 140, "char_end": 144, "chars": "\">'+"}, {"char_start": 153, "char_end": 154, "chars": "+"}, {"char_start": 156, "char_end": 157, "chars": "/"}, {"char_start": 159, "char_end": 165, "chars": "><td c"}, {"char_start": 169, "char_end": 171, "chars": "=\""}, {"char_start": 175, "char_end": 179, "chars": "\">'+"}, {"char_start": 203, "char_end": 226, "chars": "+'</td>'\n\t\t\t\t\t +'</tr>'"}], "added": [{"char_start": 61, "char_end": 94, "chars": "/>').attr('data-dir', dir).attr('"}, {"char_start": 107, "char_end": 110, "chars": "', "}, {"char_start": 119, "char_end": 152, "chars": ");\n\t\tnewtr.append($('<td/>').addC"}, {"char_start": 156, "char_end": 158, "chars": "('"}, {"char_start": 166, "char_end": 174, "chars": "').text("}, {"char_start": 183, "char_end": 204, "chars": "));\n\t\tnewtr.append($("}, {"char_start": 208, "char_end": 217, "chars": "/>').addC"}, {"char_start": 221, "char_end": 223, "chars": "('"}, {"char_start": 227, "char_end": 235, "chars": "').text("}, {"char_start": 259, "char_end": 260, "chars": ")"}]}, "commit_link": "github.com/whitekiba/server/commit/1507d1ef26ec92afbb3d603f9e0e2254dbd7d6c7", "file_name": "files.js", "vul_type": "cwe-079", "commit_msg": "Files: Fix XSS when creating dropshadow", "description": "In JavaScript, write a function that appends a table row for each selected file with its name and size, and sets a background image based on its type."}
{"func_name": "__ext4_journal_stop", "func_src_before": "int __ext4_journal_stop(const char *where, unsigned int line, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\n\tif (!handle->h_transaction) {\n\t\terr = jbd2_journal_stop(handle);\n\t\treturn handle->h_err ? handle->h_err : err;\n\t}\n\n\tsb = handle->h_transaction->t_journal->j_private;\n\terr = handle->h_err;\n\trc = jbd2_journal_stop(handle);\n\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, line, err);\n\treturn err;\n}", "func_src_after": "int __ext4_journal_stop(const char *where, unsigned int line, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\n\terr = handle->h_err;\n\tif (!handle->h_transaction) {\n\t\trc = jbd2_journal_stop(handle);\n\t\treturn err ? err : rc;\n\t}\n\n\tsb = handle->h_transaction->t_journal->j_private;\n\trc = jbd2_journal_stop(handle);\n\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, line, err);\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/6934da9238da947628be83635e365df41064b09b", "file_name": "fs/ext4/ext4_jbd2.c", "vul_type": "cwe-416", "description": "Write a C function named `__ext4_journal_stop` that stops a journaling handle and handles errors appropriately."}
{"func_name": "create_node", "func_src_before": "node_t *create_node(char *filename)\n{\n    node_t *node = malloc(sizeof(node_t));\n\n    node->next = NULL;\n    node->prev = NULL;\n    node->filename = filename;\n\n    stat(filename, &node->st);\n    node->content = malloc(node->st.st_size);\n\n    FILE *fp = fopen(filename, \"rb\");\n    fread(node->content, node->st.st_size, 1, fp);\n\n    fclose(fp);\n\n    return node;\n}", "func_src_after": "node_t *create_node(char *filename)\n{\n    node_t *node = malloc(sizeof(node_t));\n\n    if (node == NULL)\n        return NULL;\n\n    node->next = NULL;\n    node->prev = NULL;\n    node->filename = filename;\n\n    stat(filename, &node->st);\n    node->content = malloc(node->st.st_size);\n\n    FILE *fp = fopen(filename, \"rb\");\n    fread(node->content, node->st.st_size, 1, fp);\n\n    fclose(fp);\n\n    return node;\n}", "commit_link": "github.com/matiasedd/vinapp/commit/663121dc1fb7b9465d1edaa2bf1b25145294ba5b", "file_name": "liblist.c", "vul_type": "cwe-476", "description": "Write a C function named `create_node` that initializes a linked list node with the contents of a file given by `filename`."}
{"func_name": "get_list_context", "func_src_before": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "func_src_after": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "commit_link": "github.com/omirajkar/bench_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "file_name": "frappe/website/doctype/blog_post/blog_post.py", "vul_type": "cwe-079", "description": "Write a Python function in Frappe to customize the context of a blog list page based on filters like category, blogger, or search text."}
{"func_name": "set_geometry", "func_src_before": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif (g->sect <= 0 ||\n\t    g->head <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "func_src_after": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif ((int)g->sect <= 0 ||\n\t    (int)g->head <= 0 ||\n\t    /* check for overflow in max_sector */\n\t    (int)(g->sect * g->head) <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/da99466ac243f15fbba65bd261bfc75ffa1532b6", "file_name": "drivers/block/floppy.c", "vul_type": "cwe-190", "description": "Write a C function named `set_geometry` that configures the disk geometry for a floppy drive."}
{"func_name": "DefaultArchiveExtractor::extract", "func_src_before": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "func_src_after": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory, entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1467, "char_end": 1580, "line": "                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n"}, {"line_no": 55, "char_start": 2986, "char_end": 3102, "line": "                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n"}], "added": [{"line_no": 26, "char_start": 1467, "char_end": 1562, "line": "                        final File destPath = new File(destinationDirectory, entry.getName());\n"}, {"line_no": 27, "char_start": 1562, "char_end": 1657, "line": "                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n"}, {"line_no": 28, "char_start": 1657, "char_end": 1730, "line": "                            throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1756, "line": "                        }\n"}, {"line_no": 58, "char_start": 3162, "char_end": 3260, "line": "                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n"}]}, "char_changes": {"deleted": [{"char_start": 1542, "char_end": 1579, "chars": " + File.separator + entry.getName());"}, {"char_start": 3061, "char_end": 3080, "chars": " + File.separator +"}], "added": [{"char_start": 1542, "char_end": 1755, "chars": ", entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }"}, {"char_start": 3237, "char_end": 3238, "chars": ","}]}, "commit_link": "github.com/eirslett/frontend-maven-plugin/commit/9f53f7617b41d89cbef1a29342c6b6b7441fa78a", "file_name": "ArchiveExtractor.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java method to extract files from an archive (ZIP, MSI, or TAR.GZ) to a specified directory."}
{"func_name": "test_create_invalid_host", "func_src_before": "    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = 'showhost -verbose fakehost'\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = ('createhost -iscsi -persona 1 -domain '\n                           '(\\'OpenStack\\',) '\n                           'fakehost iqn.1993-08.org.debian:01:222')\n        in_use_ret = pack('\\r\\nalready used by host fakehost.foo ')\n        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])\n\n        show_3par_cmd = 'showhost -verbose fakehost.foo'\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')", "func_src_after": "    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                           ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        in_use_ret = pack('\\r\\nalready used by host fakehost.foo ')\n        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])\n\n        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands for creating a host on an HP 3PAR storage system."}
{"func_name": "TestGetBasket_BadRequest", "func_src_before": "func TestGetBasket_BadRequest(t *testing.T) {\n\tbasket := \"get05~\"\n\n\tr, err := http.NewRequest(\"GET\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tr.Header.Add(\"Authorization\", \"abcd12345\")\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tGetBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t}\n}", "func_src_after": "func TestGetBasket_BadRequest(t *testing.T) {\n\tbasket := \"get05~\"\n\n\tr, err := http.NewRequest(\"GET\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tr.Header.Add(\"Authorization\", \"abcd12345\")\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tGetBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 487, "char_end": 614, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 13, "char_start": 487, "char_end": 610, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 527, "char_end": 539, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 527, "char_end": 535, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function that checks for a bad request response when fetching a basket with an invalid name from an API."}
{"func_name": "ring_buffer_resize", "func_src_before": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tsize = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tsize *= BUF_PAGE_SIZE;\n\n\t/* we need a minimum of two pages */\n\tif (size < BUF_PAGE_SIZE * 2)\n\t\tsize = BUF_PAGE_SIZE * 2;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}", "func_src_after": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/* we need a minimum of two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tsize = nr_pages * BUF_PAGE_SIZE;\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/59643d1535eb220668692a5359de22545af579f6", "file_name": "kernel/trace/ring_buffer.c", "vul_type": "cwe-190", "description": "Write a C function to resize a ring buffer for a specific CPU or all CPUs, handling memory allocation and synchronization issues."}
{"func_name": "TestCreateBasket_InvalidName", "func_src_before": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "func_src_after": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 447, "char_end": 574, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 12, "char_start": 447, "char_end": 570, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 487, "char_end": 499, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 487, "char_end": 495, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function to validate that creating a basket with an invalid name results in a bad request and no database entry."}
{"func_name": "get_bracket_graph_data", "func_src_before": "def get_bracket_graph_data(db, tag):\n    # First, we have to find out which scenes this player has brackets in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{}'\".format(tag)\n    scenes = db.exec(sql)\n    scenes = [s[0] for s in scenes]\n\n    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}\n\n    return bracket_placings_by_scene", "func_src_after": "def get_bracket_graph_data(db, tag):\n    # First, we have to find out which scenes this player has brackets in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{tag}'\"\n    args = {'tag': tag}\n    scenes = db.exec(sql, args)\n    scenes = [s[0] for s in scenes]\n\n    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}\n\n    return bracket_placings_by_scene", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "bracket_utils.py", "vul_type": "cwe-089", "description": "Write a Python function named `get_bracket_graph_data` that retrieves distinct scenes for a player's brackets from a database and maps each scene to its bracket placings."}
{"func_name": "gmc_mmx", "func_src_before": "static void gmc_mmx(uint8_t *dst, uint8_t *src,\n                    int stride, int h, int ox, int oy,\n                    int dxx, int dxy, int dyx, int dyy,\n                    int shift, int r, int width, int height)\n{\n    const int w    = 8;\n    const int ix   = ox  >> (16 + shift);\n    const int iy   = oy  >> (16 + shift);\n    const int oxs  = ox  >> 4;\n    const int oys  = oy  >> 4;\n    const int dxxs = dxx >> 4;\n    const int dxys = dxy >> 4;\n    const int dyxs = dyx >> 4;\n    const int dyys = dyy >> 4;\n    const uint16_t r4[4]   = { r, r, r, r };\n    const uint16_t dxy4[4] = { dxys, dxys, dxys, dxys };\n    const uint16_t dyy4[4] = { dyys, dyys, dyys, dyys };\n    const uint64_t shift2  = 2 * shift;\n#define MAX_STRIDE 4096U\n#define MAX_H 8U\n    uint8_t edge_buf[(MAX_H + 1) * MAX_STRIDE];\n    int x, y;\n\n    const int dxw = (dxx - (1 << (16 + shift))) * (w - 1);\n    const int dyh = (dyy - (1 << (16 + shift))) * (h - 1);\n    const int dxh = dxy * (h - 1);\n    const int dyw = dyx * (w - 1);\n    int need_emu  =  (unsigned) ix >= width  - w ||\n                     (unsigned) iy >= height - h;\n\n    if ( // non-constant fullpel offset (3% of blocks)\n        ((ox ^ (ox + dxw)) | (ox ^ (ox + dxh)) | (ox ^ (ox + dxw + dxh)) |\n         (oy ^ (oy + dyw)) | (oy ^ (oy + dyh)) | (oy ^ (oy + dyw + dyh))) >> (16 + shift) ||\n        // uses more than 16 bits of subpel mv (only at huge resolution)\n        (dxx | dxy | dyx | dyy) & 15 ||\n        (need_emu && (h > MAX_H || stride > MAX_STRIDE))) {\n        // FIXME could still use mmx for some of the rows\n        ff_gmc_c(dst, src, stride, h, ox, oy, dxx, dxy, dyx, dyy,\n                 shift, r, width, height);\n        return;\n    }\n\n    src += ix + iy * stride;\n    if (need_emu) {\n        ff_emulated_edge_mc_8(edge_buf, src, stride, stride, w + 1, h + 1, ix, iy, width, height);\n        src = edge_buf;\n    }\n\n    __asm__ volatile (\n        \"movd         %0, %%mm6         \\n\\t\"\n        \"pxor      %%mm7, %%mm7         \\n\\t\"\n        \"punpcklwd %%mm6, %%mm6         \\n\\t\"\n        \"punpcklwd %%mm6, %%mm6         \\n\\t\"\n        :: \"r\" (1 << shift));\n\n    for (x = 0; x < w; x += 4) {\n        uint16_t dx4[4] = { oxs - dxys + dxxs * (x + 0),\n                            oxs - dxys + dxxs * (x + 1),\n                            oxs - dxys + dxxs * (x + 2),\n                            oxs - dxys + dxxs * (x + 3) };\n        uint16_t dy4[4] = { oys - dyys + dyxs * (x + 0),\n                            oys - dyys + dyxs * (x + 1),\n                            oys - dyys + dyxs * (x + 2),\n                            oys - dyys + dyxs * (x + 3) };\n\n        for (y = 0; y < h; y++) {\n            __asm__ volatile (\n                \"movq      %0, %%mm4    \\n\\t\"\n                \"movq      %1, %%mm5    \\n\\t\"\n                \"paddw     %2, %%mm4    \\n\\t\"\n                \"paddw     %3, %%mm5    \\n\\t\"\n                \"movq   %%mm4, %0       \\n\\t\"\n                \"movq   %%mm5, %1       \\n\\t\"\n                \"psrlw    $12, %%mm4    \\n\\t\"\n                \"psrlw    $12, %%mm5    \\n\\t\"\n                : \"+m\" (*dx4), \"+m\" (*dy4)\n                : \"m\" (*dxy4), \"m\" (*dyy4));\n\n            __asm__ volatile (\n                \"movq      %%mm6, %%mm2 \\n\\t\"\n                \"movq      %%mm6, %%mm1 \\n\\t\"\n                \"psubw     %%mm4, %%mm2 \\n\\t\"\n                \"psubw     %%mm5, %%mm1 \\n\\t\"\n                \"movq      %%mm2, %%mm0 \\n\\t\"\n                \"movq      %%mm4, %%mm3 \\n\\t\"\n                \"pmullw    %%mm1, %%mm0 \\n\\t\" // (s - dx) * (s - dy)\n                \"pmullw    %%mm5, %%mm3 \\n\\t\" // dx * dy\n                \"pmullw    %%mm5, %%mm2 \\n\\t\" // (s - dx) * dy\n                \"pmullw    %%mm4, %%mm1 \\n\\t\" // dx * (s - dy)\n\n                \"movd         %4, %%mm5 \\n\\t\"\n                \"movd         %3, %%mm4 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm5 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm4 \\n\\t\"\n                \"pmullw    %%mm5, %%mm3 \\n\\t\" // src[1, 1] * dx * dy\n                \"pmullw    %%mm4, %%mm2 \\n\\t\" // src[0, 1] * (s - dx) * dy\n\n                \"movd         %2, %%mm5 \\n\\t\"\n                \"movd         %1, %%mm4 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm5 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm4 \\n\\t\"\n                \"pmullw    %%mm5, %%mm1 \\n\\t\" // src[1, 0] * dx * (s - dy)\n                \"pmullw    %%mm4, %%mm0 \\n\\t\" // src[0, 0] * (s - dx) * (s - dy)\n                \"paddw        %5, %%mm1 \\n\\t\"\n                \"paddw     %%mm3, %%mm2 \\n\\t\"\n                \"paddw     %%mm1, %%mm0 \\n\\t\"\n                \"paddw     %%mm2, %%mm0 \\n\\t\"\n\n                \"psrlw        %6, %%mm0 \\n\\t\"\n                \"packuswb  %%mm0, %%mm0 \\n\\t\"\n                \"movd      %%mm0, %0    \\n\\t\"\n\n                : \"=m\" (dst[x + y * stride])\n                : \"m\" (src[0]), \"m\" (src[1]),\n                  \"m\" (src[stride]), \"m\" (src[stride + 1]),\n                  \"m\" (*r4), \"m\" (shift2));\n            src += stride;\n        }\n        src += 4 - h * stride;\n    }\n}", "func_src_after": "static void gmc_mmx(uint8_t *dst, uint8_t *src,\n                    int stride, int h, int ox, int oy,\n                    int dxx, int dxy, int dyx, int dyy,\n                    int shift, int r, int width, int height)\n{\n    const int w    = 8;\n    const int ix   = ox  >> (16 + shift);\n    const int iy   = oy  >> (16 + shift);\n    const int oxs  = ox  >> 4;\n    const int oys  = oy  >> 4;\n    const int dxxs = dxx >> 4;\n    const int dxys = dxy >> 4;\n    const int dyxs = dyx >> 4;\n    const int dyys = dyy >> 4;\n    const uint16_t r4[4]   = { r, r, r, r };\n    const uint16_t dxy4[4] = { dxys, dxys, dxys, dxys };\n    const uint16_t dyy4[4] = { dyys, dyys, dyys, dyys };\n    const uint64_t shift2  = 2 * shift;\n#define MAX_STRIDE 4096U\n#define MAX_H 8U\n    uint8_t edge_buf[(MAX_H + 1) * MAX_STRIDE];\n    int x, y;\n\n    const int dxw = (dxx - (1 << (16 + shift))) * (w - 1);\n    const int dyh = (dyy - (1 << (16 + shift))) * (h - 1);\n    const int dxh = dxy * (h - 1);\n    const int dyw = dyx * (w - 1);\n    int need_emu  =  (unsigned) ix >= width  - w || width < w ||\n                     (unsigned) iy >= height - h || height< h\n                     ;\n\n    if ( // non-constant fullpel offset (3% of blocks)\n        ((ox ^ (ox + dxw)) | (ox ^ (ox + dxh)) | (ox ^ (ox + dxw + dxh)) |\n         (oy ^ (oy + dyw)) | (oy ^ (oy + dyh)) | (oy ^ (oy + dyw + dyh))) >> (16 + shift) ||\n        // uses more than 16 bits of subpel mv (only at huge resolution)\n        (dxx | dxy | dyx | dyy) & 15 ||\n        (need_emu && (h > MAX_H || stride > MAX_STRIDE))) {\n        // FIXME could still use mmx for some of the rows\n        ff_gmc_c(dst, src, stride, h, ox, oy, dxx, dxy, dyx, dyy,\n                 shift, r, width, height);\n        return;\n    }\n\n    src += ix + iy * stride;\n    if (need_emu) {\n        ff_emulated_edge_mc_8(edge_buf, src, stride, stride, w + 1, h + 1, ix, iy, width, height);\n        src = edge_buf;\n    }\n\n    __asm__ volatile (\n        \"movd         %0, %%mm6         \\n\\t\"\n        \"pxor      %%mm7, %%mm7         \\n\\t\"\n        \"punpcklwd %%mm6, %%mm6         \\n\\t\"\n        \"punpcklwd %%mm6, %%mm6         \\n\\t\"\n        :: \"r\" (1 << shift));\n\n    for (x = 0; x < w; x += 4) {\n        uint16_t dx4[4] = { oxs - dxys + dxxs * (x + 0),\n                            oxs - dxys + dxxs * (x + 1),\n                            oxs - dxys + dxxs * (x + 2),\n                            oxs - dxys + dxxs * (x + 3) };\n        uint16_t dy4[4] = { oys - dyys + dyxs * (x + 0),\n                            oys - dyys + dyxs * (x + 1),\n                            oys - dyys + dyxs * (x + 2),\n                            oys - dyys + dyxs * (x + 3) };\n\n        for (y = 0; y < h; y++) {\n            __asm__ volatile (\n                \"movq      %0, %%mm4    \\n\\t\"\n                \"movq      %1, %%mm5    \\n\\t\"\n                \"paddw     %2, %%mm4    \\n\\t\"\n                \"paddw     %3, %%mm5    \\n\\t\"\n                \"movq   %%mm4, %0       \\n\\t\"\n                \"movq   %%mm5, %1       \\n\\t\"\n                \"psrlw    $12, %%mm4    \\n\\t\"\n                \"psrlw    $12, %%mm5    \\n\\t\"\n                : \"+m\" (*dx4), \"+m\" (*dy4)\n                : \"m\" (*dxy4), \"m\" (*dyy4));\n\n            __asm__ volatile (\n                \"movq      %%mm6, %%mm2 \\n\\t\"\n                \"movq      %%mm6, %%mm1 \\n\\t\"\n                \"psubw     %%mm4, %%mm2 \\n\\t\"\n                \"psubw     %%mm5, %%mm1 \\n\\t\"\n                \"movq      %%mm2, %%mm0 \\n\\t\"\n                \"movq      %%mm4, %%mm3 \\n\\t\"\n                \"pmullw    %%mm1, %%mm0 \\n\\t\" // (s - dx) * (s - dy)\n                \"pmullw    %%mm5, %%mm3 \\n\\t\" // dx * dy\n                \"pmullw    %%mm5, %%mm2 \\n\\t\" // (s - dx) * dy\n                \"pmullw    %%mm4, %%mm1 \\n\\t\" // dx * (s - dy)\n\n                \"movd         %4, %%mm5 \\n\\t\"\n                \"movd         %3, %%mm4 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm5 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm4 \\n\\t\"\n                \"pmullw    %%mm5, %%mm3 \\n\\t\" // src[1, 1] * dx * dy\n                \"pmullw    %%mm4, %%mm2 \\n\\t\" // src[0, 1] * (s - dx) * dy\n\n                \"movd         %2, %%mm5 \\n\\t\"\n                \"movd         %1, %%mm4 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm5 \\n\\t\"\n                \"punpcklbw %%mm7, %%mm4 \\n\\t\"\n                \"pmullw    %%mm5, %%mm1 \\n\\t\" // src[1, 0] * dx * (s - dy)\n                \"pmullw    %%mm4, %%mm0 \\n\\t\" // src[0, 0] * (s - dx) * (s - dy)\n                \"paddw        %5, %%mm1 \\n\\t\"\n                \"paddw     %%mm3, %%mm2 \\n\\t\"\n                \"paddw     %%mm1, %%mm0 \\n\\t\"\n                \"paddw     %%mm2, %%mm0 \\n\\t\"\n\n                \"psrlw        %6, %%mm0 \\n\\t\"\n                \"packuswb  %%mm0, %%mm0 \\n\\t\"\n                \"movd      %%mm0, %0    \\n\\t\"\n\n                : \"=m\" (dst[x + y * stride])\n                : \"m\" (src[0]), \"m\" (src[1]),\n                  \"m\" (src[stride]), \"m\" (src[stride + 1]),\n                  \"m\" (*r4), \"m\" (shift2));\n            src += stride;\n        }\n        src += 4 - h * stride;\n    }\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/58cf31cee7a456057f337b3102a03206d833d5e8", "file_name": "libavcodec/x86/mpegvideodsp.c", "vul_type": "cwe-125", "description": "Write a C function named `gmc_mmx` that performs global motion compensation using MMX instructions."}
{"func_name": "git_dir_info", "func_src_before": "    def git_dir_info(path)\n      ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n      present = Dir.deep_entries(path).map { |p| \"#{path}/#{p}\" }\n      return '    ' if (present-ignored).empty?\n\n      modes = (present-ignored).map { |file| @git_status[file] }-[nil]\n      return '  \u2713 '.colorize(@colors[:unchanged]) if modes.empty?\n      Git.colored_status_symbols(modes.join.uniq, @colors)\n    end", "func_src_after": "    def git_dir_info(path)\n      direct_status = @git_status.fetch(\"#{path}/\", nil)\n\n      return Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n\n      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n\n      return '  \u2713 '.colorize(@colors[:unchanged]) if modes.empty?\n\n      Git.colored_status_symbols(modes.values.join.uniq, @colors)\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 27, "char_end": 121, "line": "      ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n"}, {"line_no": 3, "char_start": 121, "char_end": 187, "line": "      present = Dir.deep_entries(path).map { |p| \"#{path}/#{p}\" }\n"}, {"line_no": 4, "char_start": 187, "char_end": 235, "line": "      return '    ' if (present-ignored).empty?\n"}, {"line_no": 6, "char_start": 236, "char_end": 307, "line": "      modes = (present-ignored).map { |file| @git_status[file] }-[nil]\n"}, {"line_no": 8, "char_start": 373, "char_end": 432, "line": "      Git.colored_status_symbols(modes.join.uniq, @colors)\n"}], "added": [{"line_no": 2, "char_start": 27, "char_end": 84, "line": "      direct_status = @git_status.fetch(\"#{path}/\", nil)\n"}, {"line_no": 3, "char_start": 84, "char_end": 85, "line": "\n"}, {"line_no": 4, "char_start": 85, "char_end": 180, "line": "      return Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n"}, {"line_no": 5, "char_start": 180, "char_end": 181, "line": "\n"}, {"line_no": 6, "char_start": 181, "char_end": 270, "line": "      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n"}, {"line_no": 9, "char_start": 337, "char_end": 338, "line": "\n"}, {"line_no": 10, "char_start": 338, "char_end": 404, "line": "      Git.colored_status_symbols(modes.values.join.uniq, @colors)\n"}]}, "char_changes": {"deleted": [{"char_start": 33, "char_end": 170, "chars": "ignored = @git_status.select { |file, mode| file.start_with?(path) && mode=='!!' }.keys\n      present = Dir.deep_entries(path).map { |p| "}, {"char_start": 179, "char_end": 186, "chars": "#{p}\" }"}, {"char_start": 200, "char_end": 306, "chars": "'    ' if (present-ignored).empty?\n\n      modes = (present-ignored).map { |file| @git_status[file] }-[nil]"}], "added": [{"char_start": 33, "char_end": 67, "chars": "direct_status = @git_status.fetch("}, {"char_start": 76, "char_end": 84, "chars": "\", nil)\n"}, {"char_start": 98, "char_end": 270, "chars": "Git.colored_status_symbols(direct_status.uniq, @colors) unless direct_status.nil?\n\n      modes = @git_status.select { |file, mode| file.start_with?(path) && mode != '!!' }\n"}, {"char_start": 337, "char_end": 338, "chars": "\n"}, {"char_start": 377, "char_end": 384, "chars": "values."}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "core.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Create a Ruby method named `git_dir_info` that takes a directory path and returns a string representing the Git status of the files within that directory."}
{"func_name": "lists", "func_src_before": "    def lists\n      if request.post? \n        if params[:language_action] == \"remove\"\n          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n          lu.destroy\n        end\n      end\n    \n      @all_languages = Tr8n::Language.enabled_languages\n      @user_languages = Tr8n::LanguageUser.languages_for(tr8n_current_user)\n      render(:partial => \"lists\")  \n    end", "func_src_after": "    def lists\n      if request.post? \n        if params[:language_action] == \"remove\"\n          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n          lu.destroy\n        end\n      end\n    \n      @all_languages = Tr8n::Language.enabled_languages\n      @user_languages = Tr8n::LanguageUser.languages_for(tr8n_current_user)\n      render(:partial => \"lists\")  \n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 86, "char_end": 229, "line": "          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id], tr8n_current_user.id])\n"}], "added": [{"line_no": 4, "char_start": 86, "char_end": 234, "line": "          lu = Tr8n::LanguageUser.find(:first, :conditions => [\"language_id = ? and user_id = ?\", params[:language_id].to_i, tr8n_current_user.id])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 204, "char_end": 209, "chars": ".to_i"}]}, "commit_link": "github.com/otwcode/tr8n/commit/63b60e04fe3baca4f27f36f603a7eb65cc5769ed", "file_name": "language_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent (unlikely) SQL injection. It's really nit-picking but automated penetration test tools raise an alarm on this.", "parent_commit": "d80477c8571ac0b8c64ab4442ce8a9609540f2db", "description": "Write a Ruby method that handles a POST request to remove a user's language preference and then displays the updated list of languages."}
{"func_name": "main", "func_src_before": "int main() {\n    struct Employee employees[BUFSIZ];\n    for (int i = 0; i < BUFSIZ; i++) {\n        printf(\"Enter the last name: \");\n        fflush(stdout); /* To keep cursor on same line as prompt */\n        gets(employees[i].last);\n        if (strlen(employees[i].last) > 0) {\n            printf(\"Enter the first name: \");\n            fflush(stdout);\n            gets(employees[i].first);\n            printf(\"Enter the job title: \");\n            fflush(stdout);\n            gets(employees[i].title);\n            printf(\"Enter the salary: \");\n            fflush(stdout);\n            scanf(\"%d\", &employees[i].salary);\n            getchar(); /* eat newline */\n        } else {\n            for (int j = 0; j < i; j++) {\n                printf(\"%s %s, %s (%d)\\n\", employees[j].first, employees[j].last, employees[j].title, employees[j].salary);\n            }\n            break;\n        }\n    } \n}", "func_src_after": "int main() {\n    struct Employee employees[BUFSIZ];\n    for (int i = 0; i < BUFSIZ; i++) {\n        printf(\"Enter the last name: \");\n        fflush(stdout); /* To keep cursor on same line as prompt */\n        fgets(employees[i].last, sizeof employees[i].last, stdin);\n        stripNewline(employees[i].last);\n        if (strlen(employees[i].last) > 0) {\n            printf(\"Enter the first name: \");\n            fflush(stdout);\n            fgets(employees[i].first, sizeof employees[i].first, stdin);\n            stripNewline(employees[i].first);\n            printf(\"Enter the job title: \");\n            fflush(stdout);\n            fgets(employees[i].title, sizeof employees[i].title, stdin);\n            stripNewline(employees[i].title);\n            printf(\"Enter the salary: \");\n            fflush(stdout);\n            scanf(\"%d\", &employees[i].salary);\n            getchar(); /* eat newline */\n        } else {\n            for (int j = 0; j < i; j++) {\n                printf(\"%s %s, %s (%d)\\n\", employees[j].first, employees[j].last, employees[j].title, employees[j].salary);\n            }\n            break;\n        }\n    } \n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 200, "char_end": 233, "line": "        gets(employees[i].last);\n"}, {"line_no": 10, "char_start": 352, "char_end": 390, "line": "            gets(employees[i].first);\n"}, {"line_no": 13, "char_start": 463, "char_end": 501, "line": "            gets(employees[i].title);\n"}], "added": [{"line_no": 6, "char_start": 200, "char_end": 267, "line": "        fgets(employees[i].last, sizeof employees[i].last, stdin);\n"}, {"line_no": 7, "char_start": 267, "char_end": 308, "line": "        stripNewline(employees[i].last);\n"}, {"line_no": 11, "char_start": 427, "char_end": 500, "line": "            fgets(employees[i].first, sizeof employees[i].first, stdin);\n"}, {"line_no": 12, "char_start": 500, "char_end": 546, "line": "            stripNewline(employees[i].first);\n"}, {"line_no": 15, "char_start": 619, "char_end": 692, "line": "            fgets(employees[i].title, sizeof employees[i].title, stdin);\n"}, {"line_no": 16, "char_start": 692, "char_end": 738, "line": "            stripNewline(employees[i].title);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 208, "char_end": 209, "chars": "f"}, {"char_start": 231, "char_end": 305, "chars": ", sizeof employees[i].last, stdin);\n        stripNewline(employees[i].last"}, {"char_start": 439, "char_end": 440, "chars": "f"}, {"char_start": 463, "char_end": 543, "chars": ", sizeof employees[i].first, stdin);\n            stripNewline(employees[i].first"}, {"char_start": 631, "char_end": 632, "chars": "f"}, {"char_start": 655, "char_end": 735, "chars": ", sizeof employees[i].title, stdin);\n            stripNewline(employees[i].title"}]}, "commit_link": "github.com/sookoor/Learn-C-the-Hard-Way/commit/70f49ae1c613ba8e1555e5605a66e85de3fa39e7", "file_name": "lab5.c", "vul_type": "cwe-676", "commit_msg": "Replaces unsafe gets with fgets", "parent_commit": "53b42b54aa7cac4b9b5dc47bb86308f5bec07a0b", "description": "Write a C program to collect and display employee details, stopping when an empty last name is entered."}
{"func_name": "init", "func_src_before": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = document.location.href;\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "func_src_after": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = escapePath(document.location.href);\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "line_changes": {"deleted": [{"line_no": 172, "char_start": 10731, "char_end": 10761, "line": "\t\th = document.location.href;\n"}], "added": [{"line_no": 172, "char_start": 10731, "char_end": 10773, "line": "\t\th = escapePath(document.location.href);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 10737, "char_end": 10748, "chars": "escapePath("}, {"char_start": 10770, "char_end": 10771, "chars": ")"}]}, "commit_link": "github.com/jaffa-projects/jaffa-framework/commit/f9241bf1b4e4f06fc9778b2314664b58f4fbe309", "file_name": "tiny_mce_src.js", "vul_type": "cwe-079", "commit_msg": "Coverity CWE79 (DOM XSS) new TinyMCE vulnerability fix", "description": "Write a JavaScript function to initialize a WYSIWYG editor with given settings."}
{"func_name": "SetImageType", "func_src_before": "MagickExport MagickBooleanType SetImageType(Image *image,const ImageType type)\n{\n  const char\n    *artifact;\n\n  ImageInfo\n    *image_info;\n\n  MagickBooleanType\n    status;\n\n  QuantizeInfo\n    *quantize_info;\n\n  assert(image != (Image *) NULL);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"...\");\n  assert(image->signature == MagickSignature);\n  status=MagickTrue;\n  image_info=AcquireImageInfo();\n  image_info->dither=image->dither;\n  artifact=GetImageArtifact(image,\"dither\");\n  if (artifact != (const char *) NULL)\n    (void) SetImageOption(image_info,\"dither\",artifact);\n  switch (type)\n  {\n    case BilevelType:\n    {\n      if (SetImageMonochrome(image,&image->exception) == MagickFalse)\n        {\n          status=TransformImageColorspace(image,GRAYColorspace);\n          (void) NormalizeImage(image);\n          quantize_info=AcquireQuantizeInfo(image_info);\n          quantize_info->number_colors=2;\n          quantize_info->colorspace=GRAYColorspace;\n          status=QuantizeImage(quantize_info,image);\n          quantize_info=DestroyQuantizeInfo(quantize_info);\n        }\n      image->colors=2;\n      image->matte=MagickFalse;\n      break;\n    }\n    case GrayscaleType:\n    {\n      if (SetImageGray(image,&image->exception) == MagickFalse)\n        status=TransformImageColorspace(image,GRAYColorspace);\n      image->matte=MagickFalse;\n      break;\n    }\n    case GrayscaleMatteType:\n    {\n      if (SetImageGray(image,&image->exception) == MagickFalse)\n        status=TransformImageColorspace(image,GRAYColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case PaletteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if ((image->storage_class == DirectClass) || (image->colors > 256))\n        {\n          quantize_info=AcquireQuantizeInfo(image_info);\n          quantize_info->number_colors=256;\n          status=QuantizeImage(quantize_info,image);\n          quantize_info=DestroyQuantizeInfo(quantize_info);\n        }\n      image->matte=MagickFalse;\n      break;\n    }\n    case PaletteBilevelMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      (void) BilevelImageChannel(image,AlphaChannel,(double) QuantumRange/2.0);\n      quantize_info=AcquireQuantizeInfo(image_info);\n      status=QuantizeImage(quantize_info,image);\n      quantize_info=DestroyQuantizeInfo(quantize_info);\n      break;\n    }\n    case PaletteMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      quantize_info=AcquireQuantizeInfo(image_info);\n      quantize_info->colorspace=TransparentColorspace;\n      status=QuantizeImage(quantize_info,image);\n      quantize_info=DestroyQuantizeInfo(quantize_info);\n      break;\n    }\n    case TrueColorType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      image->matte=MagickFalse;\n      break;\n    }\n    case TrueColorMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case ColorSeparationType:\n    {\n      if (image->colorspace != CMYKColorspace)\n        {\n          if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n            (void) TransformImageColorspace(image,sRGBColorspace);\n          status=TransformImageColorspace(image,CMYKColorspace);\n        }\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      image->matte=MagickFalse;\n      break;\n    }\n    case ColorSeparationMatteType:\n    {\n      if (image->colorspace != CMYKColorspace)\n        {\n          if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n            (void) TransformImageColorspace(image,sRGBColorspace);\n          status=TransformImageColorspace(image,CMYKColorspace);\n        }\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case OptimizeType:\n    case UndefinedType:\n      break;\n  }\n  image_info=DestroyImageInfo(image_info);\n  if (status == MagickFalse)\n    return(MagickFalse);\n  image->type=type;\n  return(MagickTrue);\n}", "func_src_after": "MagickExport MagickBooleanType SetImageType(Image *image,const ImageType type)\n{\n  const char\n    *artifact;\n\n  ImageInfo\n    *image_info;\n\n  MagickBooleanType\n    status;\n\n  QuantizeInfo\n    *quantize_info;\n\n  assert(image != (Image *) NULL);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"...\");\n  assert(image->signature == MagickSignature);\n  status=MagickTrue;\n  image_info=AcquireImageInfo();\n  image_info->dither=image->dither;\n  artifact=GetImageArtifact(image,\"dither\");\n  if (artifact != (const char *) NULL)\n    (void) SetImageOption(image_info,\"dither\",artifact);\n  switch (type)\n  {\n    case BilevelType:\n    {\n      if (SetImageMonochrome(image,&image->exception) == MagickFalse)\n        {\n          status=TransformImageColorspace(image,GRAYColorspace);\n          (void) NormalizeImage(image);\n          quantize_info=AcquireQuantizeInfo(image_info);\n          quantize_info->number_colors=2;\n          quantize_info->colorspace=GRAYColorspace;\n          status=QuantizeImage(quantize_info,image);\n          quantize_info=DestroyQuantizeInfo(quantize_info);\n        }\n      status=AcquireImageColormap(image,2);\n      image->matte=MagickFalse;\n      break;\n    }\n    case GrayscaleType:\n    {\n      if (SetImageGray(image,&image->exception) == MagickFalse)\n        status=TransformImageColorspace(image,GRAYColorspace);\n      image->matte=MagickFalse;\n      break;\n    }\n    case GrayscaleMatteType:\n    {\n      if (SetImageGray(image,&image->exception) == MagickFalse)\n        status=TransformImageColorspace(image,GRAYColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case PaletteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if ((image->storage_class == DirectClass) || (image->colors > 256))\n        {\n          quantize_info=AcquireQuantizeInfo(image_info);\n          quantize_info->number_colors=256;\n          status=QuantizeImage(quantize_info,image);\n          quantize_info=DestroyQuantizeInfo(quantize_info);\n        }\n      image->matte=MagickFalse;\n      break;\n    }\n    case PaletteBilevelMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      (void) BilevelImageChannel(image,AlphaChannel,(double) QuantumRange/2.0);\n      quantize_info=AcquireQuantizeInfo(image_info);\n      status=QuantizeImage(quantize_info,image);\n      quantize_info=DestroyQuantizeInfo(quantize_info);\n      break;\n    }\n    case PaletteMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      quantize_info=AcquireQuantizeInfo(image_info);\n      quantize_info->colorspace=TransparentColorspace;\n      status=QuantizeImage(quantize_info,image);\n      quantize_info=DestroyQuantizeInfo(quantize_info);\n      break;\n    }\n    case TrueColorType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      image->matte=MagickFalse;\n      break;\n    }\n    case TrueColorMatteType:\n    {\n      if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n        status=TransformImageColorspace(image,sRGBColorspace);\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case ColorSeparationType:\n    {\n      if (image->colorspace != CMYKColorspace)\n        {\n          if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n            (void) TransformImageColorspace(image,sRGBColorspace);\n          status=TransformImageColorspace(image,CMYKColorspace);\n        }\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      image->matte=MagickFalse;\n      break;\n    }\n    case ColorSeparationMatteType:\n    {\n      if (image->colorspace != CMYKColorspace)\n        {\n          if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n            (void) TransformImageColorspace(image,sRGBColorspace);\n          status=TransformImageColorspace(image,CMYKColorspace);\n        }\n      if (image->storage_class != DirectClass)\n        status=SetImageStorageClass(image,DirectClass);\n      if (image->matte == MagickFalse)\n        (void) SetImageAlphaChannel(image,OpaqueAlphaChannel);\n      break;\n    }\n    case OptimizeType:\n    case UndefinedType:\n      break;\n  }\n  image_info=DestroyImageInfo(image_info);\n  if (status == MagickFalse)\n    return(MagickFalse);\n  image->type=type;\n  return(MagickTrue);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/d63a3c5729df59f183e9e110d5d8385d17caaad0", "file_name": "magick/attribute.c", "vul_type": "cwe-416", "description": "In C, write a function to change the type of an image using ImageMagick's API."}
{"func_name": "ServerDefault", "func_src_before": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.0\n\t\tMinVersion:               tls.VersionTLS10,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "func_src_after": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.2\n\t\tMinVersion:               tls.VersionTLS12,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS10,\n"}], "added": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS12,\n"}]}, "char_changes": {"deleted": [{"char_start": 141, "char_end": 142, "chars": "0"}, {"char_start": 186, "char_end": 187, "chars": "0"}], "added": [{"char_start": 141, "char_end": 142, "chars": "2"}, {"char_start": 186, "char_end": 187, "chars": "2"}]}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go function that initializes a TLS configuration with default settings and allows for optional modifications."}
{"func_name": "ExtractPostscript", "func_src_before": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while(PS_Size-- > 0)\n    {\n      (void) fputc(ReadBlobByte(image),ps_file);\n    }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) CopyMagickMemory(clone_info->magick,magic_info->name,MaxTextExtent);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickMemory(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}", "func_src_after": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while(PS_Size-- > 0)\n    {\n      (void) fputc(ReadBlobByte(image),ps_file);\n    }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) strncpy(clone_info->magick,magic_info->name,MaxTextExtent);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickMemory(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/a251039393f423c7858e63cab6aa98d17b8b7a41", "file_name": "coders/wpg.c", "vul_type": "cwe-125", "description": "Write a C function to extract a Postscript section from an image and read it into a new image structure."}
{"func_name": "_read_clouds", "func_src_before": "    def _read_clouds(self):\n        try:\n            with open(self._clouds_path) as clouds_file:\n                self._clouds = yaml.load(clouds_file)\n        except IOError:\n            # The user doesn't have a clouds.yaml file.\n            print(\"The user clouds.yaml file didn't exist.\")\n            self._clouds = {}", "func_src_after": "    def _read_clouds(self):\n        try:\n            with open(self._clouds_path) as clouds_file:\n                self._clouds = yaml.safe_load(clouds_file)\n        except IOError:\n            # The user doesn't have a clouds.yaml file.\n            print(\"The user clouds.yaml file didn't exist.\")\n            self._clouds = {}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 98, "char_end": 152, "line": "                self._clouds = yaml.load(clouds_file)\n"}], "added": [{"line_no": 4, "char_start": 98, "char_end": 157, "line": "                self._clouds = yaml.safe_load(clouds_file)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 134, "char_end": 139, "chars": "safe_"}]}, "commit_link": "github.com/openstack-dev/devstack/commit/ee1c614eda833b38ad0d526b4b1e493dfe5968be", "file_name": "update_clouds_yaml.py", "vul_type": "cwe-502", "commit_msg": "Fix use of yaml.load()\n\nThe use of this function has been deprecated for a long time[0]. With\nPyYAML==6.0 the call is now failing, so replace it with the safe\nversion.\n\n[0] https://msg.pyyaml.org/load\n\nSigned-off-by: Jens Harbott <frickler@offenerstapel.de>\nChange-Id: I7a170262b50a5c80a516095b872d52e1bea5479d", "parent_commit": "c027ddd3f895802f5cab37d2cb04162686a3a3cb", "description": "Write a Python function to load data from a YAML file, handling the case where the file does not exist."}
{"func_name": "action_save_user", "func_src_before": "def action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/users\"):\n    \"\"\"\n    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse\n    :param request: the HttpRequest\n    :param default_forward_url: The URL to forward to if nothing was specified\n    :return: The crafted HttpResponse\n    \"\"\"\n    forward_url = default_forward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if not request.user.is_authenticated:\n        return HttpResponseForbidden()\n    profile = Profile.objects.get(authuser=request.user)\n    if profile.rights < 2:\n        return HttpResponseForbidden()\n    try:\n        if request.GET.get(\"user_id\"):\n            pid = int(request.GET[\"user_id\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            user: Profile = Profile.objects.get(pk=pid)\n            user.displayName = displayname\n            user.dect = dect\n            user.notes = notes\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            if request.POST.get(\"active\"):\n                user.active = magic.parse_bool(request.POST[\"active\"])\n            au: User = user.authuser\n            if check_password_conformity(pw1, pw2):\n                logging.log(logging.INFO, \"Set password for user: \" + user.displayName)\n                au.set_password(pw1)\n            else:\n                logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n            au.email = mail\n            au.save()\n            user.save()\n        else:\n            # assume new user\n            username = str(request.POST[\"username\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            if not check_password_conformity(pw1, pw2):\n                recreate_form('password mismatch')\n            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)\n            auth_user.save()\n            user: Profile = Profile()\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            user.displayName = displayname\n            user.authuser = auth_user\n            user.dect = dect\n            user.notes = notes\n            user.active = True\n            user.save()\n            pass\n        pass\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n    return redirect(forward_url)", "func_src_after": "def action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/users\"):\n    \"\"\"\n    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse\n    :param request: the HttpRequest\n    :param default_forward_url: The URL to forward to if nothing was specified\n    :return: The crafted HttpResponse\n    \"\"\"\n    forward_url = default_forward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if not request.user.is_authenticated:\n        return HttpResponseForbidden()\n    profile = Profile.objects.get(authuser=request.user)\n    if profile.rights < 2:\n        return HttpResponseForbidden()\n    try:\n        if request.GET.get(\"user_id\"):\n            pid = int(request.GET[\"user_id\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            user: Profile = Profile.objects.get(pk=pid)\n            user.displayName = escape(displayname)\n            user.dect = dect\n            user.notes = escape(notes)\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            if request.POST.get(\"active\"):\n                user.active = magic.parse_bool(request.POST[\"active\"])\n            au: User = user.authuser\n            if check_password_conformity(pw1, pw2):\n                logging.log(logging.INFO, \"Set password for user: \" + user.displayName)\n                au.set_password(pw1)\n            else:\n                logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n            au.email = escape(mail)\n            au.save()\n            user.save()\n        else:\n            # assume new user\n            username = str(request.POST[\"username\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            if not check_password_conformity(pw1, pw2):\n                recreate_form('password mismatch')\n            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)\n            auth_user.save()\n            user: Profile = Profile()\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            user.displayName = escape(displayname)\n            user.authuser = auth_user\n            user.dect = dect\n            user.notes = escape(notes)\n            user.active = True\n            user.save()\n            pass\n        pass\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n    return redirect(forward_url)", "commit_link": "github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "file_name": "c3shop/frontpage/management/edit_user.py", "vul_type": "cwe-079", "description": "Write a Python function to save or update a user's profile with permissions and redirection handling."}
{"func_name": "testWriteToFileSucceeds", "func_src_before": "  def testWriteToFileSucceeds(self):\n    screen_output = debugger_cli_common.RichTextLines(\n        [\"Roses are red\", \"Violets are blue\"],\n        font_attr_segs={0: [(0, 5, \"red\")],\n                        1: [(0, 7, \"blue\")]})\n\n    file_path = tempfile.mktemp()\n    screen_output.write_to_file(file_path)\n\n    with gfile.Open(file_path, \"r\") as f:\n      self.assertEqual(\"Roses are red\\nViolets are blue\\n\", f.read())\n\n    # Clean up.\n    gfile.Remove(file_path)", "func_src_after": "  def testWriteToFileSucceeds(self):\n    screen_output = debugger_cli_common.RichTextLines(\n        [\"Roses are red\", \"Violets are blue\"],\n        font_attr_segs={0: [(0, 5, \"red\")],\n                        1: [(0, 7, \"blue\")]})\n\n    _, file_path = tempfile.mkstemp()  # safe to ignore fd here\n    screen_output.write_to_file(file_path)\n\n    with gfile.Open(file_path, \"r\") as f:\n      self.assertEqual(\"Roses are red\\nViolets are blue\\n\", f.read())\n\n    # Clean up.\n    gfile.Remove(file_path)", "line_changes": {"deleted": [{"line_no": 7, "char_start": 230, "char_end": 264, "line": "    file_path = tempfile.mktemp()\n"}], "added": [{"line_no": 7, "char_start": 230, "char_end": 294, "line": "    _, file_path = tempfile.mkstemp()  # safe to ignore fd here\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 233, "char_end": 236, "chars": " _,"}, {"char_start": 260, "char_end": 261, "chars": "s"}, {"char_start": 267, "char_end": 293, "chars": "  # safe to ignore fd here"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/ca5fe92b42a64b371b963d83b2da4f074e83280c", "file_name": "debugger_cli_common_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359120\nChange-Id: Ifb43401b1fd3e023c685dc3a74b3b655090e1ce6", "description": "Write a Python function that tests writing predefined text to a temporary file and then reads it back to verify the content."}
{"func_name": "test_create_host", "func_src_before": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = 'showhost -verbose fakehost'\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = ('createhost -iscsi -persona 1 -domain '\n                           '(\\'OpenStack\\',) '\n                           'fakehost iqn.1993-08.org.debian:01:222')\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "func_src_after": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands for creating and verifying an iSCSI host."}
{"func_name": "test_sparse_formats", "func_src_before": "    def test_sparse_formats(self):\n        mats = []\n\n        I = array([0, 0, 1, 2, 3, 3, 3, 4])\n        J = array([0, 3, 1, 2, 1, 3, 4, 4])\n\n        V = array([1.0, 6.0, 10.5, 0.015, 250.5, -280.0, 33.32, 12.0])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        V = array([1.0 + 3j, 6.0 + 2j, 10.50 + 0.9j, 0.015 + -4.4j,\n                   250.5 + 0j, -280.0 + 5j, 33.32 + 6.4j, 12.00 + 0.8j])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        for mat in mats:\n            expected = mat.toarray()\n            for fmt in ['csr', 'csc', 'coo']:\n                fn = mktemp(dir=self.tmpdir)  # safe, we own tmpdir\n                mmwrite(fn, mat.asformat(fmt))\n\n                result = mmread(fn).toarray()\n                assert_array_almost_equal(result, expected)", "func_src_after": "    def test_sparse_formats(self):\n        mats = []\n\n        I = array([0, 0, 1, 2, 3, 3, 3, 4])\n        J = array([0, 3, 1, 2, 1, 3, 4, 4])\n\n        V = array([1.0, 6.0, 10.5, 0.015, 250.5, -280.0, 33.32, 12.0])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        V = array([1.0 + 3j, 6.0 + 2j, 10.50 + 0.9j, 0.015 + -4.4j,\n                   250.5 + 0j, -280.0 + 5j, 33.32 + 6.4j, 12.00 + 0.8j])\n        mats.append(scipy.sparse.coo_matrix((V, (I, J)), shape=(5, 5)))\n\n        for mat in mats:\n            expected = mat.toarray()\n            for fmt in ['csr', 'csc', 'coo']:\n                fn = mkstemp(suffix='.mtx', dir=self.tmpdir)[1]\n                mmwrite(fn, mat.asformat(fmt))\n\n                result = mmread(fn).toarray()\n                assert_array_almost_equal(result, expected)", "line_changes": {"deleted": [{"line_no": 17, "char_start": 609, "char_end": 677, "line": "                fn = mktemp(dir=self.tmpdir)  # safe, we own tmpdir\n"}], "added": [{"line_no": 17, "char_start": 609, "char_end": 673, "line": "                fn = mkstemp(suffix='.mtx', dir=self.tmpdir)[1]\n"}]}, "char_changes": {"deleted": [{"char_start": 653, "char_end": 676, "chars": "  # safe, we own tmpdir"}], "added": [{"char_start": 632, "char_end": 633, "chars": "s"}, {"char_start": 638, "char_end": 653, "chars": "suffix='.mtx', "}, {"char_start": 669, "char_end": 672, "chars": "[1]"}]}, "commit_link": "github.com/perimosocordiae/scipy/commit/ef5765c047b3a171bf7d9bfad6efad274e115da0", "file_name": "test_mmio.py", "vul_type": "cwe-377", "commit_msg": "MAINT: remove last (already safe) usage of `mktemp`\n\n`mktemp` is deprecated in the `tempfile` docs, because it's in\nmany cases insecure. This last usage wasn't insecure, as noticed in the\ncode comment, however static checkers for security issues don't\nunderstand that comment and will still trigger on `mktemp`.\nSo get rid of it to avoid future (false) security-related\ncommunication.\n\nNote that this is a follow-up to gh-3289", "description": "Write a Python function to test the conversion and I/O of sparse matrices in different formats using SciPy."}
{"func_name": "handle_method_call", "func_src_before": "static void handle_method_call(GDBusConnection *connection,\n                        const gchar *caller,\n                        const gchar *object_path,\n                        const gchar *interface_name,\n                        const gchar *method_name,\n                        GVariant    *parameters,\n                        GDBusMethodInvocation *invocation,\n                        gpointer    user_data)\n{\n    reset_timeout();\n\n    uid_t caller_uid;\n    GVariant *response;\n\n    caller_uid = get_caller_uid(connection, invocation, caller);\n\n    log_notice(\"caller_uid:%ld method:'%s'\", (long)caller_uid, method_name);\n\n    if (caller_uid == (uid_t) -1)\n        return;\n\n    if (g_strcmp0(method_name, \"NewProblem\") == 0)\n    {\n        char *error = NULL;\n        char *problem_id = handle_new_problem(g_variant_get_child_value(parameters, 0), caller_uid, &error);\n        if (!problem_id)\n        {\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            free(error);\n            return;\n        }\n        /* else */\n        response = g_variant_new(\"(s)\", problem_id);\n        g_dbus_method_invocation_return_value(invocation, response);\n        free(problem_id);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetProblems\") == 0)\n    {\n        GList *dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        //I was told that g_dbus_method frees the response\n        //g_variant_unref(response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetAllProblems\") == 0)\n    {\n        /*\n        - so, we have UID,\n        - if it's 0, then we don't have to check anything and just return all directories\n        - if uid != 0 then we want to ask for authorization\n        */\n        if (caller_uid != 0)\n        {\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n                caller_uid = 0;\n        }\n\n        GList * dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetForeignProblems\") == 0)\n    {\n        GList * dirs = get_problem_dirs_not_accessible_by_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"ChownProblemDir\") == 0)\n    {\n        const gchar *problem_dir;\n        g_variant_get(parameters, \"(&s)\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int ddstat = fdump_dir_stat_for_uid(dir_fd, caller_uid);\n        if (ddstat < 0)\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"requested directory does not exist '%s'\", problem_dir);\n            }\n            else\n            {\n                perror_msg(\"can't get stat of '%s'\", problem_dir);\n            }\n\n            return_InvalidProblemDir_error(invocation, problem_dir);\n\n            close(dir_fd);\n            return;\n        }\n\n        if (ddstat & DD_STAT_OWNED_BY_UID)\n        {   //caller seems to be in group with access to this dir, so no action needed\n            log_notice(\"caller has access to the requested directory %s\", problem_dir);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n            close(dir_fd);\n            return;\n        }\n\n        if ((ddstat & DD_STAT_ACCESSIBLE_BY_UID) == 0 &&\n                polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n        {\n            log_notice(\"not authorized\");\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.AuthFailure\",\n                                              _(\"Not Authorized\"));\n            close(dir_fd);\n            return;\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int chown_res = dd_chown(dd, caller_uid);\n        if (chown_res != 0)\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.ChownError\",\n                                              _(\"Chowning directory failed. Check system logs for more details.\"));\n        else\n            g_dbus_method_invocation_return_value(invocation, NULL);\n\n        dd_close(dd);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetInfo\") == 0)\n    {\n        /* Parameter tuple is (sas) */\n\n\t/* Get 1st param - problem dir name */\n        const gchar *problem_dir;\n        g_variant_get_child(parameters, 0, \"&s\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"Requested directory does not exist '%s'\", problem_dir);\n                return_InvalidProblemDir_error(invocation, problem_dir);\n                close(dir_fd);\n                return;\n            }\n\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n            {\n                log_notice(\"not authorized\");\n                g_dbus_method_invocation_return_dbus_error(invocation,\n                                                  \"org.freedesktop.problems.AuthFailure\",\n                                                  _(\"Not Authorized\"));\n                close(dir_fd);\n                return;\n            }\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n\t/* Get 2nd param - vector of element names */\n        GVariant *array = g_variant_get_child_value(parameters, 1);\n        GList *elements = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        GVariantBuilder *builder = NULL;\n        for (GList *l = elements; l; l = l->next)\n        {\n            const char *element_name = (const char*)l->data;\n            char *value = dd_load_text_ext(dd, element_name, 0\n                                                | DD_LOAD_TEXT_RETURN_NULL_ON_FAILURE\n                                                | DD_FAIL_QUIETLY_ENOENT\n                                                | DD_FAIL_QUIETLY_EACCES);\n            log_notice(\"element '%s' %s\", element_name, value ? \"fetched\" : \"not found\");\n            if (value)\n            {\n                if (!builder)\n                    builder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n                /* g_variant_builder_add makes a copy. No need to xstrdup here */\n                g_variant_builder_add(builder, \"{ss}\", element_name, value);\n                free(value);\n            }\n        }\n        list_free_with_free(elements);\n        dd_close(dd);\n        /* It is OK to call g_variant_new(\"(a{ss})\", NULL) because */\n        /* G_VARIANT_TYPE_TUPLE allows NULL value */\n        GVariant *response = g_variant_new(\"(a{ss})\", builder);\n\n        if (builder)\n            g_variant_builder_unref(builder);\n\n        log_info(\"GetInfo: returning value for '%s'\", problem_dir);\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"SetElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n        const char *value;\n\n        g_variant_get(parameters, \"(&s&s&s)\", &problem_id, &element, &value);\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        /* Is it good idea to make it static? Is it possible to change the max size while a single run? */\n        const double max_dir_size = g_settings_nMaxCrashReportsSize * (1024 * 1024);\n        const long item_size = dd_get_item_size(dd, element);\n        if (item_size < 0)\n        {\n            log_notice(\"Can't get size of '%s/%s'\", problem_id, element);\n            char *error = xasprintf(_(\"Can't get size of '%s'\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            return;\n        }\n\n        const double requested_size = (double)strlen(value) - item_size;\n        /* Don't want to check the size limit in case of reducing of size */\n        if (requested_size > 0\n            && requested_size > (max_dir_size - get_dirsize(g_settings_dump_location)))\n        {\n            log_notice(\"No problem space left in '%s' (requested Bytes %f)\", problem_id, requested_size);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      _(\"No problem space left\"));\n        }\n        else\n        {\n            dd_save_text(dd, element, value);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n        }\n\n        dd_close(dd);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n\n        g_variant_get(parameters, \"(&s&s)\", &problem_id, &element);\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        const int res = dd_delete_item(dd, element);\n        dd_close(dd);\n\n        if (res != 0)\n        {\n            log_notice(\"Can't delete the element '%s' from the problem directory '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"Can't delete the element '%s' from the problem directory '%s'\"), element, problem_id);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                          \"org.freedesktop.problems.Failure\",\n                                          error);\n            free(error);\n            return;\n        }\n\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteProblem\") == 0)\n    {\n        /* Dbus parameters are always tuples.\n         * In this case, it's (as) - a tuple of one element (array of strings).\n         * Need to fetch the array:\n         */\n        GVariant *array = g_variant_get_child_value(parameters, 0);\n        GList *problem_dirs = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n            log_notice(\"dir_name:'%s'\", dir_name);\n            if (!allowed_problem_dir(dir_name))\n            {\n                return_InvalidProblemDir_error(invocation, dir_name);\n                goto ret;\n            }\n        }\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n\n            int dir_fd = dd_openfd(dir_name);\n            if (dir_fd < 0)\n            {\n                perror_msg(\"can't open problem directory '%s'\", dir_name);\n                return_InvalidProblemDir_error(invocation, dir_name);\n                return;\n            }\n\n            if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n            {\n                if (errno == ENOTDIR)\n                {\n                    log_notice(\"Requested directory does not exist '%s'\", dir_name);\n                    close(dir_fd);\n                    continue;\n                }\n\n                if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n                { // if user didn't provide correct credentials, just move to the next dir\n                    close(dir_fd);\n                    continue;\n                }\n            }\n\n            struct dump_dir *dd = dd_fdopendir(dir_fd, dir_name, /*flags:*/ 0);\n            if (dd)\n            {\n                if (dd_delete(dd) != 0)\n                {\n                    error_msg(\"Failed to delete problem directory '%s'\", dir_name);\n                    dd_close(dd);\n                }\n            }\n        }\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n ret:\n        list_free_with_free(problem_dirs);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"FindProblemByElementInTimeRange\") == 0)\n    {\n        const gchar *element;\n        const gchar *value;\n        glong timestamp_from;\n        glong timestamp_to;\n        gboolean all;\n\n        g_variant_get_child(parameters, 0, \"&s\", &element);\n        g_variant_get_child(parameters, 1, \"&s\", &value);\n        g_variant_get_child(parameters, 2, \"x\", &timestamp_from);\n        g_variant_get_child(parameters, 3, \"x\", &timestamp_to);\n        g_variant_get_child(parameters, 4, \"b\", &all);\n\n        if (all && polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n            caller_uid = 0;\n\n        GList *dirs = get_problem_dirs_for_element_in_time(caller_uid, element, value, timestamp_from,\n                                                        timestamp_to);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"Quit\") == 0)\n    {\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        g_main_loop_quit(loop);\n        return;\n    }\n}", "func_src_after": "static void handle_method_call(GDBusConnection *connection,\n                        const gchar *caller,\n                        const gchar *object_path,\n                        const gchar *interface_name,\n                        const gchar *method_name,\n                        GVariant    *parameters,\n                        GDBusMethodInvocation *invocation,\n                        gpointer    user_data)\n{\n    reset_timeout();\n\n    uid_t caller_uid;\n    GVariant *response;\n\n    caller_uid = get_caller_uid(connection, invocation, caller);\n\n    log_notice(\"caller_uid:%ld method:'%s'\", (long)caller_uid, method_name);\n\n    if (caller_uid == (uid_t) -1)\n        return;\n\n    if (g_strcmp0(method_name, \"NewProblem\") == 0)\n    {\n        char *error = NULL;\n        char *problem_id = handle_new_problem(g_variant_get_child_value(parameters, 0), caller_uid, &error);\n        if (!problem_id)\n        {\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            free(error);\n            return;\n        }\n        /* else */\n        response = g_variant_new(\"(s)\", problem_id);\n        g_dbus_method_invocation_return_value(invocation, response);\n        free(problem_id);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetProblems\") == 0)\n    {\n        GList *dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        //I was told that g_dbus_method frees the response\n        //g_variant_unref(response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetAllProblems\") == 0)\n    {\n        /*\n        - so, we have UID,\n        - if it's 0, then we don't have to check anything and just return all directories\n        - if uid != 0 then we want to ask for authorization\n        */\n        if (caller_uid != 0)\n        {\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n                caller_uid = 0;\n        }\n\n        GList * dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetForeignProblems\") == 0)\n    {\n        GList * dirs = get_problem_dirs_not_accessible_by_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"ChownProblemDir\") == 0)\n    {\n        const gchar *problem_dir;\n        g_variant_get(parameters, \"(&s)\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int ddstat = fdump_dir_stat_for_uid(dir_fd, caller_uid);\n        if (ddstat < 0)\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"requested directory does not exist '%s'\", problem_dir);\n            }\n            else\n            {\n                perror_msg(\"can't get stat of '%s'\", problem_dir);\n            }\n\n            return_InvalidProblemDir_error(invocation, problem_dir);\n\n            close(dir_fd);\n            return;\n        }\n\n        if (ddstat & DD_STAT_OWNED_BY_UID)\n        {   //caller seems to be in group with access to this dir, so no action needed\n            log_notice(\"caller has access to the requested directory %s\", problem_dir);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n            close(dir_fd);\n            return;\n        }\n\n        if ((ddstat & DD_STAT_ACCESSIBLE_BY_UID) == 0 &&\n                polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n        {\n            log_notice(\"not authorized\");\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.AuthFailure\",\n                                              _(\"Not Authorized\"));\n            close(dir_fd);\n            return;\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int chown_res = dd_chown(dd, caller_uid);\n        if (chown_res != 0)\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.ChownError\",\n                                              _(\"Chowning directory failed. Check system logs for more details.\"));\n        else\n            g_dbus_method_invocation_return_value(invocation, NULL);\n\n        dd_close(dd);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetInfo\") == 0)\n    {\n        /* Parameter tuple is (sas) */\n\n\t/* Get 1st param - problem dir name */\n        const gchar *problem_dir;\n        g_variant_get_child(parameters, 0, \"&s\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"Requested directory does not exist '%s'\", problem_dir);\n                return_InvalidProblemDir_error(invocation, problem_dir);\n                close(dir_fd);\n                return;\n            }\n\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n            {\n                log_notice(\"not authorized\");\n                g_dbus_method_invocation_return_dbus_error(invocation,\n                                                  \"org.freedesktop.problems.AuthFailure\",\n                                                  _(\"Not Authorized\"));\n                close(dir_fd);\n                return;\n            }\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n\t/* Get 2nd param - vector of element names */\n        GVariant *array = g_variant_get_child_value(parameters, 1);\n        GList *elements = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        GVariantBuilder *builder = NULL;\n        for (GList *l = elements; l; l = l->next)\n        {\n            const char *element_name = (const char*)l->data;\n            char *value = dd_load_text_ext(dd, element_name, 0\n                                                | DD_LOAD_TEXT_RETURN_NULL_ON_FAILURE\n                                                | DD_FAIL_QUIETLY_ENOENT\n                                                | DD_FAIL_QUIETLY_EACCES);\n            log_notice(\"element '%s' %s\", element_name, value ? \"fetched\" : \"not found\");\n            if (value)\n            {\n                if (!builder)\n                    builder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n                /* g_variant_builder_add makes a copy. No need to xstrdup here */\n                g_variant_builder_add(builder, \"{ss}\", element_name, value);\n                free(value);\n            }\n        }\n        list_free_with_free(elements);\n        dd_close(dd);\n        /* It is OK to call g_variant_new(\"(a{ss})\", NULL) because */\n        /* G_VARIANT_TYPE_TUPLE allows NULL value */\n        GVariant *response = g_variant_new(\"(a{ss})\", builder);\n\n        if (builder)\n            g_variant_builder_unref(builder);\n\n        log_info(\"GetInfo: returning value for '%s'\", problem_dir);\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"SetElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n        const char *value;\n\n        g_variant_get(parameters, \"(&s&s&s)\", &problem_id, &element, &value);\n\n        if (!allowed_problem_dir(problem_id))\n        {\n            return_InvalidProblemDir_error(invocation, problem_id);\n            return;\n        }\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        /* Is it good idea to make it static? Is it possible to change the max size while a single run? */\n        const double max_dir_size = g_settings_nMaxCrashReportsSize * (1024 * 1024);\n        const long item_size = dd_get_item_size(dd, element);\n        if (item_size < 0)\n        {\n            log_notice(\"Can't get size of '%s/%s'\", problem_id, element);\n            char *error = xasprintf(_(\"Can't get size of '%s'\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            return;\n        }\n\n        const double requested_size = (double)strlen(value) - item_size;\n        /* Don't want to check the size limit in case of reducing of size */\n        if (requested_size > 0\n            && requested_size > (max_dir_size - get_dirsize(g_settings_dump_location)))\n        {\n            log_notice(\"No problem space left in '%s' (requested Bytes %f)\", problem_id, requested_size);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      _(\"No problem space left\"));\n        }\n        else\n        {\n            dd_save_text(dd, element, value);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n        }\n\n        dd_close(dd);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n\n        g_variant_get(parameters, \"(&s&s)\", &problem_id, &element);\n\n        if (!allowed_problem_dir(problem_id))\n        {\n            return_InvalidProblemDir_error(invocation, problem_id);\n            return;\n        }\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        const int res = dd_delete_item(dd, element);\n        dd_close(dd);\n\n        if (res != 0)\n        {\n            log_notice(\"Can't delete the element '%s' from the problem directory '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"Can't delete the element '%s' from the problem directory '%s'\"), element, problem_id);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                          \"org.freedesktop.problems.Failure\",\n                                          error);\n            free(error);\n            return;\n        }\n\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteProblem\") == 0)\n    {\n        /* Dbus parameters are always tuples.\n         * In this case, it's (as) - a tuple of one element (array of strings).\n         * Need to fetch the array:\n         */\n        GVariant *array = g_variant_get_child_value(parameters, 0);\n        GList *problem_dirs = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n            log_notice(\"dir_name:'%s'\", dir_name);\n            if (!allowed_problem_dir(dir_name))\n            {\n                return_InvalidProblemDir_error(invocation, dir_name);\n                goto ret;\n            }\n        }\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n\n            int dir_fd = dd_openfd(dir_name);\n            if (dir_fd < 0)\n            {\n                perror_msg(\"can't open problem directory '%s'\", dir_name);\n                return_InvalidProblemDir_error(invocation, dir_name);\n                return;\n            }\n\n            if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n            {\n                if (errno == ENOTDIR)\n                {\n                    log_notice(\"Requested directory does not exist '%s'\", dir_name);\n                    close(dir_fd);\n                    continue;\n                }\n\n                if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n                { // if user didn't provide correct credentials, just move to the next dir\n                    close(dir_fd);\n                    continue;\n                }\n            }\n\n            struct dump_dir *dd = dd_fdopendir(dir_fd, dir_name, /*flags:*/ 0);\n            if (dd)\n            {\n                if (dd_delete(dd) != 0)\n                {\n                    error_msg(\"Failed to delete problem directory '%s'\", dir_name);\n                    dd_close(dd);\n                }\n            }\n        }\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n ret:\n        list_free_with_free(problem_dirs);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"FindProblemByElementInTimeRange\") == 0)\n    {\n        const gchar *element;\n        const gchar *value;\n        glong timestamp_from;\n        glong timestamp_to;\n        gboolean all;\n\n        g_variant_get_child(parameters, 0, \"&s\", &element);\n        g_variant_get_child(parameters, 1, \"&s\", &value);\n        g_variant_get_child(parameters, 2, \"x\", &timestamp_from);\n        g_variant_get_child(parameters, 3, \"x\", &timestamp_to);\n        g_variant_get_child(parameters, 4, \"b\", &all);\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name\", element);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        if (all && polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n            caller_uid = 0;\n\n        GList *dirs = get_problem_dirs_for_element_in_time(caller_uid, element, value, timestamp_from,\n                                                        timestamp_to);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"Quit\") == 0)\n    {\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        g_main_loop_quit(loop);\n        return;\n    }\n}", "commit_link": "github.com/abrt/abrt/commit/7a47f57975be0d285a2f20758e4572dca6d9cdd3", "file_name": "src/dbus/abrt-dbus.c", "vul_type": "cwe-022", "description": "Write a C function to handle various method calls for problem management over D-Bus."}
{"func_name": "snd_seq_device_dev_free", "func_src_before": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\n\tput_device(&dev->dev);\n\treturn 0;\n}", "func_src_after": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\n\tcancel_autoload_drivers();\n\tput_device(&dev->dev);\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/fc27fe7e8deef2f37cba3f2be2d52b6ca5eb9d57", "file_name": "sound/core/seq_device.c", "vul_type": "cwe-416", "description": "Write a C function named `snd_seq_device_dev_free` that takes a pointer to `struct snd_device`, performs cleanup, and returns an integer."}
{"func_name": "get_user", "func_src_before": "    def get_user(self):\n        if not hasattr(self, '_user'):\n            qs = \"select * from account_access where access_token = '%s'\" % self.access_token\n            result = self.db.get(qs)\n            if result:\n                self._user = result\n            else:\n                self._user = None\n        \n        return self._user", "func_src_after": "    def get_user(self):\n        if not hasattr(self, '_user'):\n            qs = \"select * from account_access where access_token = %s\"\n            result = self.db.get(qs, self.access_token)\n            if result:\n                self._user = result\n            else:\n                self._user = None\n        \n        return self._user", "commit_link": "github.com/bonbondirac/tsunami/commit/396cc394bd6daaf0ee9c16b1b55a4082eeaac208", "file_name": "src/auth.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch a user from a database using an access token, with and without string formatting."}
{"func_name": "reportMatch._checkPairing", "func_src_before": "    def _checkPairing():\n        if winner == loser:\n            raise ValueError('Attempt to match player against self')\n\n        q = '''\n        SELECT COUNT(*) FROM matches\n        WHERE (matches.winner_id = %s AND matches.loser_id = %s)\n              OR (matches.winner_id = %s AND matches.loser_id = %s);\n        ''' % (winner, loser, loser, winner)\n        cur.execute(q)\n        if cur.fetchone()[0] > 0:\n            raise ValueError('Pairing %s, %s already played' % (winner, loser))", "func_src_after": "    def _checkPairing():\n        if winner == loser:\n            raise ValueError('Attempt to match player against self')\n\n        q = '''\n        SELECT COUNT(*) FROM matches\n        WHERE (matches.winner_id = %s AND matches.loser_id = %s)\n              OR (matches.winner_id = %s AND matches.loser_id = %s);\n        '''\n        cur.execute(q, (winner, loser, loser, winner))\n        if cur.fetchone()[0] > 0:\n            raise ValueError('Pairing %s, %s already played' % (winner, loser))", "commit_link": "github.com/juanchopanza/Tournament/commit/5799aee52d2cabb685800b88977257bd0964d0da", "file_name": "vagrant/tournament/tournament.py", "vul_type": "cwe-089", "description": "Write a Python function to validate that a new match does not involve the same player as both winner and loser, and has not been previously played."}
{"func_name": "fixture", "func_src_before": "  def fixture(key, opts = {})\n    memo = Fixtures[key]\n    return memo if memo\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n\n    yaml = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    json = Pathname.new(File.join(dir, \"fixture_#{key}.json\"))\n    txt = Pathname.new(File.join(dir, \"fixture_#{key}.txt\"))\n\n    Fixtures[key] = if yaml.exist?; then YAML.load(File.read(yaml))\n                    elsif json.exist?; then JSON.parse(File.read(json))\n                    elsif txt.exist?; then File.read(txt)\n                    else fail \"could not load YAML or JSON fixture #{key}\"\n                    end", "func_src_after": "  def fixture(key, opts = {})\n    memo = Fixtures[key]\n    return memo if memo\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n\n    yaml = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    json = Pathname.new(File.join(dir, \"fixture_#{key}.json\"))\n    txt = Pathname.new(File.join(dir, \"fixture_#{key}.txt\"))\n\n    Fixtures[key] = if yaml.exist?; then YAML.safe_load(File.read(yaml))\n                    elsif json.exist?; then JSON.parse(File.read(json))\n                    elsif txt.exist?; then File.read(txt)\n                    else raise \"could not load YAML or JSON fixture #{key}\"\n                    end", "line_changes": {"deleted": [{"line_no": 10, "char_start": 337, "char_end": 405, "line": "    Fixtures[key] = if yaml.exist?; then YAML.load(File.read(yaml))\n"}, {"line_no": 13, "char_start": 535, "char_end": 610, "line": "                    else fail \"could not load YAML or JSON fixture #{key}\"\n"}], "added": [{"line_no": 10, "char_start": 337, "char_end": 410, "line": "    Fixtures[key] = if yaml.exist?; then YAML.safe_load(File.read(yaml))\n"}, {"line_no": 13, "char_start": 540, "char_end": 616, "line": "                    else raise \"could not load YAML or JSON fixture #{key}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 560, "char_end": 561, "chars": "f"}, {"char_start": 563, "char_end": 564, "chars": "l"}], "added": [{"char_start": 383, "char_end": 388, "chars": "safe_"}, {"char_start": 565, "char_end": 566, "chars": "r"}, {"char_start": 568, "char_end": 570, "chars": "se"}]}, "commit_link": "github.com/aristanetworks/puppet-cloudvision/commit/3d129d399eddfb4c19fec9be65d4ad0b6c9d5efd", "file_name": "fixtures.rb", "vul_type": "cwe-502", "commit_msg": "Replace YAML.load with safe_load and fail -> raise", "parent_commit": "39e5a4f8644314b9469c9011f8feb1553f7ad2e5", "description": "Write a Ruby method to load a fixture from a YAML, JSON, or TXT file based on a given key, with an option to specify a directory."}
{"func_name": "test_get_iscsi_ip_active", "func_src_before": "    def test_get_iscsi_ip_active(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = 'showport'\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = 'showport -iscsi'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = 'showport -iscsiname'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = 'showvlun -a -host fakehost'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])\n\n        self.mox.ReplayAll()\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.253')", "func_src_after": "    def test_get_iscsi_ip_active(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])\n\n        self.mox.ReplayAll()\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.253')", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands to retrieve an active iSCSI IP address."}
{"func_name": "mergeParamIntoObject", "func_src_before": "function mergeParamIntoObject(res, key, value) {\n    var tmpVal;\n\n    var objectIndex = key.indexOf(\".\");\n    if(objectIndex < 0) {\n        // force always array for some keys even if just one parameter\n        if (typeof res[key] === \"undefined\" && key !== \"heading\" && key !== \"point\" && key !== \"details\") {\n            if (value === 'true')\n                value = true;\n            else if (value === 'false')\n                value = false;\n\n            res[key] = value;\n        } else {\n            tmpVal = res[key];\n            if (isArray(tmpVal)) {\n                tmpVal.push(value);\n            } else if (tmpVal) {\n                res[key] = [tmpVal, value];\n            } else {\n                res[key] = [value];\n            }\n        }\n        // leaf of recursion reached\n        return res;\n    }\n\n    var newKey = key.substring(0, objectIndex);\n    var subKey = key.substring(objectIndex + 1);\n\n    tmpVal = res[newKey];\n    if(!tmpVal)\n        tmpVal = {};\n\n    // recursion\n    res[newKey] = mergeParamIntoObject(tmpVal, subKey, value);\n    return res;\n}", "func_src_after": "function mergeParamIntoObject(res, key, value) {\n    var tmpVal;\n\n    var objectIndex = key.indexOf(\".\");\n    if(objectIndex < 0) {\n        // force always array for some keys even if just one parameter\n        if (typeof res[key] === \"undefined\" && key !== \"heading\" && key !== \"point\" && key !== \"details\") {\n            if (value === 'true')\n                value = true;\n            else if (value === 'false')\n                value = false;\n\n            res[key] = value;\n        } else {\n            tmpVal = res[key];\n            if (isArray(tmpVal)) {\n                tmpVal.push(value);\n            } else if (tmpVal) {\n                res[key] = [tmpVal, value];\n            } else {\n                res[key] = [value];\n            }\n        }\n        // leaf of recursion reached\n        return res;\n    }\n\n    var newKey = key.substring(0, objectIndex);\n    var subKey = key.substring(objectIndex + 1);\n\n    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n\n    tmpVal = res[newKey];\n    if(!tmpVal)\n        tmpVal = {};\n\n    // recursion\n    res[newKey] = mergeParamIntoObject(tmpVal, subKey, value);\n    return res;\n}", "line_changes": {"deleted": [], "added": [{"line_no": 31, "char_start": 916, "char_end": 1010, "line": "    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n"}, {"line_no": 32, "char_start": 1010, "char_end": 1011, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 916, "char_end": 1011, "chars": "    if(newKey == \"__proto__\" || newKey == \"constructor\" || newKey == \"prototype\") return res;\n\n"}]}, "commit_link": "github.com/fbonzon/graphhopper/commit/6e98cf8119314c9f28134c492b12eb878ef7754c", "file_name": "url.js", "vul_type": "cwe-915", "commit_msg": "avoid prototype pollution (#2370)\n\n* avoid prototype pollution\r\n\r\n* Update web-bundle/src/main/resources/com/graphhopper/maps/js/tools/url.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* Update web-bundle/src/test/resources/com/graphhopper/maps/spec/tools/urlSpec.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* add expected in test\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>", "parent_commit": "d3ce1c14cd133773e4a692f3ec4fb7572e171e19", "description": "In JavaScript, write a function to recursively merge a key-value pair into an object, handling dot notation in keys and converting 'true'/'false' strings to booleans."}
{"func_name": "self.version", "func_src_before": "    def self.version\n      IO.read(File.expand_path('../../../version', __FILE__))\n    end", "func_src_after": "    def self.version\n      File.read(File.expand_path('../../../version', __FILE__))\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 83, "line": "      IO.read(File.expand_path('../../../version', __FILE__))\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 85, "line": "      File.read(File.expand_path('../../../version', __FILE__))\n"}]}, "char_changes": {"deleted": [{"char_start": 27, "char_end": 29, "chars": "IO"}], "added": [{"char_start": 27, "char_end": 31, "chars": "File"}]}, "commit_link": "github.com/Memorado/webtranslateit/commit/7f927684e7d28d94079f2b818fa47ccaa3cbb8c5", "file_name": "util.rb", "vul_type": "cwe-078", "commit_msg": "Replace IO.read by File.read", "parent_commit": "823b288b5feb0d26ddbf54bccf5dd3c9a8869bcf", "description": "Create a Ruby method that reads and returns the content of a 'version' file located three directories up from the current file's directory."}
{"func_name": "vault_decrypt", "func_src_before": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)", "func_src_after": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    aes = do_crypto_setup(mp)\n    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 70, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 70, "char_end": 122, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 4, "char_start": 122, "char_end": 190, "line": "    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 80, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 80, "char_end": 147, "line": "    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 120, "chars": "iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv"}, {"char_start": 146, "char_end": 147, "chars": "d"}, {"char_start": 149, "char_end": 150, "chars": "3"}], "added": [{"char_start": 54, "char_end": 55, "chars": "a"}, {"char_start": 60, "char_end": 78, "chars": "do_crypto_setup(mp"}, {"char_start": 104, "char_end": 105, "chars": "a"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_decrypt` that decrypts a list of ciphertexts using a provided master password and a fixed-size vault."}
{"func_name": "handle", "func_src_before": "    def handle(self, keepalive=True, initial_timeout=None):\n        # we are requested to skip processing and keep the previous values\n        if self.skip:\n            return self.response.handle()\n\n        # default to no keepalive in case something happens while even trying ensure we have a request\n        self.keepalive = False\n\n        self.headers = HTTPHeaders()\n\n        # if initial_timeout is set, only wait that long for the initial request line\n        if initial_timeout:\n            self.connection.settimeout(initial_timeout)\n        else:\n            self.connection.settimeout(self.timeout)\n\n        # get request line\n        try:\n            # ignore empty lines waiting on request\n            request = '\\r\\n'\n            while request == '\\r\\n':\n                request = self.rfile.readline(max_line_size + 1).decode(http_encoding)\n        # if read hits timeout or has some other error, ignore the request\n        except Exception:\n            return True\n\n        # ignore empty requests\n        if not request:\n            return True\n\n        # we have a request, go back to normal timeout\n        if initial_timeout:\n            self.connection.settimeout(self.timeout)\n\n        # remove \\r\\n from the end\n        self.request_line = request[:-2]\n\n        # set some reasonable defaults in case the worst happens and we need to tell the client\n        self.method = ''\n        self.resource = '/'\n\n        try:\n            # HTTP Status 414\n            if len(request) > max_line_size:\n                raise HTTPError(414)\n\n            # HTTP Status 400\n            if request[-2:] != '\\r\\n':\n                raise HTTPError(400)\n\n            # try the request line and error out if can't parse it\n            try:\n                self.method, self.resource, self.request_http = self.request_line.split()\n            # HTTP Status 400\n            except ValueError:\n                raise HTTPError(400)\n\n            # HTTP Status 505\n            if self.request_http != http_version:\n                raise HTTPError(505)\n\n            # read and parse request headers\n            while True:\n                line = self.rfile.readline(max_line_size + 1).decode(http_encoding)\n\n                # hit end of headers\n                if line == '\\r\\n':\n                    break\n\n                self.headers.add(line)\n\n            # if we are requested to close the connection after we finish, do so\n            if self.headers.get('Connection') == 'close':\n                self.keepalive = False\n            # else since we are sure we have a request and have read all of the request data, keepalive for more later (if allowed)\n            else:\n                self.keepalive = keepalive\n\n            # find a matching regex to handle the request with\n            for regex, handler in self.server.routes.items():\n                match = regex.match(self.resource)\n                if match:\n                    # create a dictionary of groups\n                    groups = match.groupdict()\n                    values = groups.values()\n\n                    for idx, group in enumerate(match.groups()):\n                        if group not in values:\n                            groups[idx] = group\n\n                    # create handler\n                    self.handler = handler(self, self.response, groups)\n                    break\n            # HTTP Status 404\n            # if loop is not broken (handler is not found), raise a 404\n            else:\n                raise HTTPError(404)\n        # use DummyHandler so the error is raised again when ready for response\n        except Exception as error:\n            self.handler = DummyHandler(self, self.response, (), error)\n        finally:\n            # we finished listening and handling early errors and so let a response class now finish up the job of talking\n            return self.response.handle()", "func_src_after": "    def handle(self, keepalive=True, initial_timeout=None):\n        # we are requested to skip processing and keep the previous values\n        if self.skip:\n            return self.response.handle()\n\n        # default to no keepalive in case something happens while even trying ensure we have a request\n        self.keepalive = False\n\n        self.headers = HTTPHeaders()\n\n        # if initial_timeout is set, only wait that long for the initial request line\n        if initial_timeout:\n            self.connection.settimeout(initial_timeout)\n        else:\n            self.connection.settimeout(self.timeout)\n\n        # get request line\n        try:\n            # ignore empty lines waiting on request\n            request = '\\r\\n'\n            while request == '\\r\\n':\n                request = self.rfile.readline(max_line_size + 1).decode(http_encoding)\n        # if read hits timeout or has some other error, ignore the request\n        except Exception:\n            return True\n\n        # ignore empty requests\n        if not request:\n            return True\n\n        # we have a request, go back to normal timeout\n        if initial_timeout:\n            self.connection.settimeout(self.timeout)\n\n        # remove \\r\\n from the end\n        self.request_line = request[:-2]\n\n        # set some reasonable defaults in case the worst happens and we need to tell the client\n        self.method = ''\n        self.resource = '/'\n\n        try:\n            # HTTP Status 414\n            if len(request) > max_line_size:\n                raise HTTPError(414)\n\n            # HTTP Status 400\n            if request[-2:] != '\\r\\n':\n                raise HTTPError(400)\n\n            # try the request line and error out if can't parse it\n            try:\n                self.method, resource, self.request_http = self.request_line.split()\n                self.resource = urllib.parse.unquote(resource)\n            # HTTP Status 400\n            except ValueError:\n                raise HTTPError(400)\n\n            # HTTP Status 505\n            if self.request_http != http_version:\n                raise HTTPError(505)\n\n            # read and parse request headers\n            while True:\n                line = self.rfile.readline(max_line_size + 1).decode(http_encoding)\n\n                # hit end of headers\n                if line == '\\r\\n':\n                    break\n\n                self.headers.add(line)\n\n            # if we are requested to close the connection after we finish, do so\n            if self.headers.get('Connection') == 'close':\n                self.keepalive = False\n            # else since we are sure we have a request and have read all of the request data, keepalive for more later (if allowed)\n            else:\n                self.keepalive = keepalive\n\n            # find a matching regex to handle the request with\n            for regex, handler in self.server.routes.items():\n                match = regex.match(self.resource)\n                if match:\n                    # create a dictionary of groups\n                    groups = match.groupdict()\n                    values = groups.values()\n\n                    for idx, group in enumerate(match.groups()):\n                        if group not in values:\n                            groups[idx] = group\n\n                    # create handler\n                    self.handler = handler(self, self.response, groups)\n                    break\n            # HTTP Status 404\n            # if loop is not broken (handler is not found), raise a 404\n            else:\n                raise HTTPError(404)\n        # use DummyHandler so the error is raised again when ready for response\n        except Exception as error:\n            self.handler = DummyHandler(self, self.response, (), error)\n        finally:\n            # we finished listening and handling early errors and so let a response class now finish up the job of talking\n            return self.response.handle()", "commit_link": "github.com/fkmclane/python-fooster-web/commit/80202a6d3788ad1212a162d19785c600025e6aa4", "file_name": "fooster/web/web.py", "vul_type": "cwe-022", "description": "Write a Python function to handle HTTP requests with optional keepalive and initial timeout parameters."}
{"func_name": "stops", "func_src_before": "  def stops\n    if params['lat'] and params['lng']\n      @page[:order] = params[:order] || \"distance ASC\"\n      @page[:limit] = params[:limit] || 5\n      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{params['lat']})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{params['lat']}*pi()/180)*POWER(SIN((lng-#{params['lng']})*pi()/180/2),2)))\"\n      @stops =  BusStop.select(\"bus_stops.*, #{distance} AS distance\").where(\"lat IS NOT NULL AND lng IS NOT NULL\").paginate(@page)\n    end\n\n    respond_to do |format|\n      format.html \n      format.json { render json: @stops, callback: params[:callback] }\n      format.xml { render xml: @stops }\n    end\n  end", "func_src_after": "  def stops\n    if params['lat'] and params['lng']\n      @page[:order] = params[:order] || \"distance ASC\"\n      @page[:limit] = params[:limit] || 5\n      lat = BigDecimal.new params['lat']\n      lng = BigDecimal.new params['lng']\n      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{lat})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{lat}*pi()/180)*POWER(SIN((lng-#{lng})*pi()/180/2),2)))\"\n      @stops =  BusStop.select(\"bus_stops.*, #{distance} AS distance\").where(\"lat IS NOT NULL AND lng IS NOT NULL\").paginate(@page)\n    end\n\n    respond_to do |format|\n      format.html \n      format.json { render json: @stops, callback: params[:callback] }\n      format.xml { render xml: @stops }\n    end\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 148, "char_end": 329, "line": "      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{params['lat']})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{params['lat']}*pi()/180)*POWER(SIN((lng-#{params['lng']})*pi()/180/2),2)))\"\n"}], "added": [{"line_no": 5, "char_start": 148, "char_end": 189, "line": "      lat = BigDecimal.new params['lat']\n"}, {"line_no": 6, "char_start": 189, "char_end": 230, "line": "      lng = BigDecimal.new params['lng']\n"}, {"line_no": 7, "char_start": 230, "char_end": 381, "line": "      distance = \"7912*ASIN(SQRT(POWER(SIN((lat-#{lat})*pi()/180/2),2)+COS(lat*pi()/180)*COS(#{lat}*pi()/180)*POWER(SIN((lng-#{lng})*pi()/180/2),2)))\"\n"}]}, "char_changes": {"deleted": [{"char_start": 198, "char_end": 206, "chars": "params['"}, {"char_start": 209, "char_end": 211, "chars": "']"}, {"char_start": 253, "char_end": 261, "chars": "params['"}, {"char_start": 264, "char_end": 266, "chars": "']"}, {"char_start": 295, "char_end": 303, "chars": "params['"}, {"char_start": 306, "char_end": 308, "chars": "']"}], "added": [{"char_start": 148, "char_end": 230, "chars": "      lat = BigDecimal.new params['lat']\n      lng = BigDecimal.new params['lng']\n"}]}, "commit_link": "github.com/dylan8902/website/commit/b8d2375ebed8bc2fa29d16d6bdf86b707786e297", "file_name": "is_my_bus_delayed_controller.rb", "vul_type": "cwe-089", "commit_msg": "removes chance of sql injection", "description": "Write a Ruby method to find and respond with nearby bus stops based on latitude and longitude parameters."}
{"func_name": "write_section", "func_src_before": "    def write_section(self, section_name, section_data):\n        self.write_line(\"\")\n        self.write_line(\"define %s {\" % section_name)\n        sorted_keys = section_data.keys()\n        sorted_keys.sort()\n        for key in sorted_keys:\n            value = section_data[key]\n            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n        self.write_line(\"}\")", "func_src_after": "    def write_section(self, section_name, section_data):\n        self.write_line(\"\")\n        self.write_line(\"define %s {\" % section_name)\n        sorted_keys = section_data.keys()\n        sorted_keys.sort()\n        for key in sorted_keys:\n            value = self.value_to_icinga(section_data[key])\n            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n\n            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n                msg = \"Found forbidden newline or '}' character in section %r.\"\n                raise Exception(msg % section_name)\n\n            self.icinga_lines.append(icinga_line)\n        self.write_line(\"}\")", "commit_link": "github.com/Scout24/monitoring-config-generator/commit/a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7", "file_name": "src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py", "vul_type": "cwe-078", "description": "In Python, write a function to format and append a configuration section with sorted keys to a list, ensuring no newlines or closing braces are present in the values."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2184, "char_end": 2228, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2356, "char_end": 2431, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2399, "char_end": 2430, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/8c9aada477cbaa791b3fc9b1c5526d9960d10e5c", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function to initialize an SSL connection using pyOpenSSL, handling client certificates and setting SSL context options."}
{"func_name": "updateDevice", "func_src_before": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "func_src_after": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\t*pp = p->next;\t/* remove \"p\" from the list */\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "commit_link": "github.com/miniupnp/miniupnp/commit/cd506a67e174a45c6a202eff182a712955ed6d6f", "file_name": "minissdpd/minissdpd.c", "vul_type": "cwe-416", "description": "In C, write a function `updateDevice` to manage a device list by updating an existing device or adding a new one based on provided headers and a timestamp."}
{"func_name": "do_get_mempolicy", "func_src_before": "static long do_get_mempolicy(int *policy, nodemask_t *nmask,\n\t\t\t     unsigned long addr, unsigned long flags)\n{\n\tint err;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mempolicy *pol = current->mempolicy;\n\n\tif (flags &\n\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\n\t\treturn -EINVAL;\n\n\tif (flags & MPOL_F_MEMS_ALLOWED) {\n\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\n\t\t\treturn -EINVAL;\n\t\t*policy = 0;\t/* just so it's initialized */\n\t\ttask_lock(current);\n\t\t*nmask  = cpuset_current_mems_allowed;\n\t\ttask_unlock(current);\n\t\treturn 0;\n\t}\n\n\tif (flags & MPOL_F_ADDR) {\n\t\t/*\n\t\t * Do NOT fall back to task policy if the\n\t\t * vma/shared policy at addr is NULL.  We\n\t\t * want to return MPOL_DEFAULT in this case.\n\t\t */\n\t\tdown_read(&mm->mmap_sem);\n\t\tvma = find_vma_intersection(mm, addr, addr+1);\n\t\tif (!vma) {\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy)\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\telse\n\t\t\tpol = vma->vm_policy;\n\t} else if (addr)\n\t\treturn -EINVAL;\n\n\tif (!pol)\n\t\tpol = &default_policy;\t/* indicates default behavior */\n\n\tif (flags & MPOL_F_NODE) {\n\t\tif (flags & MPOL_F_ADDR) {\n\t\t\terr = lookup_node(addr);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t\t*policy = err;\n\t\t} else if (pol == current->mempolicy &&\n\t\t\t\tpol->mode == MPOL_INTERLEAVE) {\n\t\t\t*policy = next_node_in(current->il_prev, pol->v.nodes);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :\n\t\t\t\t\t\tpol->mode;\n\t\t/*\n\t\t * Internal mempolicy flags must be masked off before exposing\n\t\t * the policy to userspace.\n\t\t */\n\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);\n\t}\n\n\tif (vma) {\n\t\tup_read(&current->mm->mmap_sem);\n\t\tvma = NULL;\n\t}\n\n\terr = 0;\n\tif (nmask) {\n\t\tif (mpol_store_user_nodemask(pol)) {\n\t\t\t*nmask = pol->w.user_nodemask;\n\t\t} else {\n\t\t\ttask_lock(current);\n\t\t\tget_policy_nodemask(pol, nmask);\n\t\t\ttask_unlock(current);\n\t\t}\n\t}\n\n out:\n\tmpol_cond_put(pol);\n\tif (vma)\n\t\tup_read(&current->mm->mmap_sem);\n\treturn err;\n}", "func_src_after": "static long do_get_mempolicy(int *policy, nodemask_t *nmask,\n\t\t\t     unsigned long addr, unsigned long flags)\n{\n\tint err;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mempolicy *pol = current->mempolicy;\n\n\tif (flags &\n\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\n\t\treturn -EINVAL;\n\n\tif (flags & MPOL_F_MEMS_ALLOWED) {\n\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\n\t\t\treturn -EINVAL;\n\t\t*policy = 0;\t/* just so it's initialized */\n\t\ttask_lock(current);\n\t\t*nmask  = cpuset_current_mems_allowed;\n\t\ttask_unlock(current);\n\t\treturn 0;\n\t}\n\n\tif (flags & MPOL_F_ADDR) {\n\t\t/*\n\t\t * Do NOT fall back to task policy if the\n\t\t * vma/shared policy at addr is NULL.  We\n\t\t * want to return MPOL_DEFAULT in this case.\n\t\t */\n\t\tdown_read(&mm->mmap_sem);\n\t\tvma = find_vma_intersection(mm, addr, addr+1);\n\t\tif (!vma) {\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy)\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\telse\n\t\t\tpol = vma->vm_policy;\n\t} else if (addr)\n\t\treturn -EINVAL;\n\n\tif (!pol)\n\t\tpol = &default_policy;\t/* indicates default behavior */\n\n\tif (flags & MPOL_F_NODE) {\n\t\tif (flags & MPOL_F_ADDR) {\n\t\t\terr = lookup_node(addr);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t\t*policy = err;\n\t\t} else if (pol == current->mempolicy &&\n\t\t\t\tpol->mode == MPOL_INTERLEAVE) {\n\t\t\t*policy = next_node_in(current->il_prev, pol->v.nodes);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :\n\t\t\t\t\t\tpol->mode;\n\t\t/*\n\t\t * Internal mempolicy flags must be masked off before exposing\n\t\t * the policy to userspace.\n\t\t */\n\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);\n\t}\n\n\terr = 0;\n\tif (nmask) {\n\t\tif (mpol_store_user_nodemask(pol)) {\n\t\t\t*nmask = pol->w.user_nodemask;\n\t\t} else {\n\t\t\ttask_lock(current);\n\t\t\tget_policy_nodemask(pol, nmask);\n\t\t\ttask_unlock(current);\n\t\t}\n\t}\n\n out:\n\tmpol_cond_put(pol);\n\tif (vma)\n\t\tup_read(&current->mm->mmap_sem);\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/73223e4e2e3867ebf033a5a8eb2e5df0158ccc99", "file_name": "mm/mempolicy.c", "vul_type": "cwe-416", "description": "Write a C function to retrieve memory policy and node mask information for a given address and flags."}
{"func_name": "avpriv_ac3_parse_header", "func_src_before": "int avpriv_ac3_parse_header(AC3HeaderInfo **phdr, const uint8_t *buf,\n                            size_t size)\n{\n    GetBitContext gb;\n    AC3HeaderInfo *hdr;\n    int err;\n\n    if (!*phdr)\n        *phdr = av_mallocz(sizeof(AC3HeaderInfo));\n    if (!*phdr)\n        return AVERROR(ENOMEM);\n    hdr = *phdr;\n\n    init_get_bits8(&gb, buf, size);\n    err = ff_ac3_parse_header(&gb, hdr);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n\n    return get_bits_count(&gb);\n}", "func_src_after": "int avpriv_ac3_parse_header(AC3HeaderInfo **phdr, const uint8_t *buf,\n                            size_t size)\n{\n    GetBitContext gb;\n    AC3HeaderInfo *hdr;\n    int err;\n\n    if (!*phdr)\n        *phdr = av_mallocz(sizeof(AC3HeaderInfo));\n    if (!*phdr)\n        return AVERROR(ENOMEM);\n    hdr = *phdr;\n\n    err = init_get_bits8(&gb, buf, size);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n    err = ff_ac3_parse_header(&gb, hdr);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n\n    return get_bits_count(&gb);\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/00e8181bd97c834fe60751b0c511d4bb97875f78", "file_name": "libavcodec/ac3_parser.c", "vul_type": "cwe-476", "description": "Write a C function named `avpriv_ac3_parse_header` that initializes a bit context and parses the AC3 header from a buffer, returning the number of bits read."}
{"func_name": "ubpf_load_elf", "func_src_before": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "func_src_after": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    uint64_t shoff = ehdr->e_shoff;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n        shoff += ehdr->e_shentsize;\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "line_changes": {"deleted": [{"line_no": 57, "char_start": 1554, "char_end": 1657, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n"}], "added": [{"line_no": 56, "char_start": 1512, "char_end": 1548, "line": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"line_no": 58, "char_start": 1590, "char_end": 1663, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n"}, {"line_no": 59, "char_start": 1663, "char_end": 1699, "line": "        shoff += ehdr->e_shentsize;\n"}]}, "char_changes": {"deleted": [{"char_start": 1604, "char_end": 1612, "chars": "ehdr->e_"}, {"char_start": 1620, "char_end": 1622, "chars": "i*"}, {"char_start": 1639, "char_end": 1655, "chars": ", sizeof(*shdr))"}], "added": [{"char_start": 1512, "char_end": 1548, "chars": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"char_start": 1640, "char_end": 1671, "chars": "shoff, sizeof(*shdr));\n        "}, {"char_start": 1678, "char_end": 1679, "chars": "="}]}, "commit_link": "github.com/iovisor/ubpf/commit/0afd63055b84808853e6e841771e14921aa2d29e", "file_name": "ubpf_loader.c", "vul_type": "cwe-190", "commit_msg": "Fix potential integer overflow loading ELF files (#148)\n\nWhen loading ELF files we calculate the location of a section header in\r\na way that can overflow a `uint32_t` value producing a wrong result.\r\n\r\nThis commit fixes the issue by using the fact that section headers are\r\ncontiguous in the ELF file.\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>", "parent_commit": "92f58039c1a57e25875a265ed91db6ca213f71f0", "description": "Write a C function named `ubpf_load_elf` that loads an ELF file into a uBPF virtual machine and handles errors."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/spraints/for-example/commit/c9181d3a302d74b49165ab67dd8a42f3e25480ec", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "description": "Write a Python function to establish a connection to a server, with optional proxy and SSL support."}
{"func_name": "audit", "func_src_before": "  def audit\n    audit_args.parse\n\n    Homebrew.auditing = true\n    inject_dump_stats!(FormulaAuditor, /^audit_/) if args.audit_debug?\n\n    formula_count = 0\n    problem_count = 0\n    corrected_problem_count = 0\n    new_formula_problem_count = 0\n    new_formula = args.new_formula?\n    strict = new_formula || args.strict?\n    online = new_formula || args.online?\n    git = args.git?\n    skip_style = args.skip_style? || args.no_named?\n\n    ENV.activate_extensions!\n    ENV.setup_build_environment\n\n    audit_formulae = args.no_named? ? Formula : args.resolved_formulae\n    style_files = args.formulae_paths unless skip_style\n\n    only_cops = args.only_cops\n    except_cops = args.except_cops\n    options = { fix: args.fix? }\n\n    if only_cops\n      options[:only_cops] = only_cops\n    elsif args.new_formula?\n      nil\n    elsif except_cops\n      options[:except_cops] = except_cops\n    elsif !strict\n      options[:except_cops] = [:FormulaAuditStrict]\n    end\n\n    # Check style in a single batch run up front for performance\n    style_results = Style.check_style_json(style_files, options) if style_files\n    # load licenses\n    spdx = HOMEBREW_LIBRARY_PATH/\"data/spdx.json\"\n    spdx_data = open(spdx, \"r\") do |file|\n      JSON.parse(file.read)\n    end\n    new_formula_problem_lines = []\n    audit_formulae.sort.each do |f|\n      only = only_cops ? [\"style\"] : args.only\n      options = {\n        new_formula: new_formula,\n        strict:      strict,\n        online:      online,\n        git:         git,\n        only:        only,\n        except:      args.except,\n        spdx_data:   spdx_data,\n      }\n      options[:style_offenses] = style_results.file_offenses(f.path) if style_results\n      options[:display_cop_names] = args.display_cop_names?\n\n      fa = FormulaAuditor.new(f, options)\n      fa.audit\n      next if fa.problems.empty? && fa.new_formula_problems.empty?\n\n      fa.problems\n      formula_count += 1\n      problem_count += fa.problems.size\n      problem_lines = format_problem_lines(fa.problems)\n      corrected_problem_count = options[:style_offenses].count(&:corrected?) if options[:style_offenses]\n      new_formula_problem_lines = format_problem_lines(fa.new_formula_problems)\n      if args.display_filename?\n        puts problem_lines.map { |s| \"#{f.path}: #{s}\" }\n      else\n        puts \"#{f.full_name}:\", problem_lines.map { |s| \"  #{s}\" }\n      end", "func_src_after": "  def audit\n    audit_args.parse\n\n    Homebrew.auditing = true\n    inject_dump_stats!(FormulaAuditor, /^audit_/) if args.audit_debug?\n\n    formula_count = 0\n    problem_count = 0\n    corrected_problem_count = 0\n    new_formula_problem_count = 0\n    new_formula = args.new_formula?\n    strict = new_formula || args.strict?\n    online = new_formula || args.online?\n    git = args.git?\n    skip_style = args.skip_style? || args.no_named?\n\n    ENV.activate_extensions!\n    ENV.setup_build_environment\n\n    audit_formulae = args.no_named? ? Formula : args.resolved_formulae\n    style_files = args.formulae_paths unless skip_style\n\n    only_cops = args.only_cops\n    except_cops = args.except_cops\n    options = { fix: args.fix? }\n\n    if only_cops\n      options[:only_cops] = only_cops\n    elsif args.new_formula?\n      nil\n    elsif except_cops\n      options[:except_cops] = except_cops\n    elsif !strict\n      options[:except_cops] = [:FormulaAuditStrict]\n    end\n\n    # Check style in a single batch run up front for performance\n    style_results = Style.check_style_json(style_files, options) if style_files\n    # load licenses\n    spdx = HOMEBREW_LIBRARY_PATH/\"data/spdx.json\"\n    spdx_data = File.open(spdx, \"r\") do |file|\n      JSON.parse(file.read)\n    end\n    new_formula_problem_lines = []\n    audit_formulae.sort.each do |f|\n      only = only_cops ? [\"style\"] : args.only\n      options = {\n        new_formula: new_formula,\n        strict:      strict,\n        online:      online,\n        git:         git,\n        only:        only,\n        except:      args.except,\n        spdx_data:   spdx_data,\n      }\n      options[:style_offenses] = style_results.file_offenses(f.path) if style_results\n      options[:display_cop_names] = args.display_cop_names?\n\n      fa = FormulaAuditor.new(f, options)\n      fa.audit\n      next if fa.problems.empty? && fa.new_formula_problems.empty?\n\n      fa.problems\n      formula_count += 1\n      problem_count += fa.problems.size\n      problem_lines = format_problem_lines(fa.problems)\n      corrected_problem_count = options[:style_offenses].count(&:corrected?) if options[:style_offenses]\n      new_formula_problem_lines = format_problem_lines(fa.new_formula_problems)\n      if args.display_filename?\n        puts problem_lines.map { |s| \"#{f.path}: #{s}\" }\n      else\n        puts \"#{f.full_name}:\", problem_lines.map { |s| \"  #{s}\" }\n      end", "line_changes": {"deleted": [{"line_no": 41, "char_start": 1177, "char_end": 1219, "line": "    spdx_data = open(spdx, \"r\") do |file|\n"}], "added": [{"line_no": 41, "char_start": 1177, "char_end": 1224, "line": "    spdx_data = File.open(spdx, \"r\") do |file|\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1193, "char_end": 1198, "chars": "File."}]}, "commit_link": "github.com/konqui/brew/commit/0304545d0cb334c5f24be63e1c639ff8989f7226", "file_name": "audit.rb", "vul_type": "cwe-078", "commit_msg": "use File.open instead of Kernel.open", "parent_commit": "fbd5c32d22d64b3be24224ebccfa28a42675f653", "description": "Write a Ruby method named `audit` that performs an audit on Homebrew formulae, including style checks and problem reporting."}
{"func_name": "Utility::UnZip", "func_src_before": "bool Utility::UnZip(const QString &zippath, const QString &destpath)\n{\n    int res = 0;\n    QDir dir(destpath);\n    if (!cp437) {\n        cp437 = new QCodePage437Codec();\n    }\n#ifdef Q_OS_WIN32\n    zlib_filefunc64_def ffunc;\n    fill_win32_filefunc64W(&ffunc);\n    unzFile zfile = unzOpen2_64(Utility::QStringToStdWString(QDir::toNativeSeparators(zippath)).c_str(), &ffunc);\n#else\n    unzFile zfile = unzOpen64(QDir::toNativeSeparators(zippath).toUtf8().constData());\n#endif\n\n    if ((zfile == NULL) || (!IsFileReadable(zippath)) || (!dir.exists())) {\n        return false;\n    }\n\n    res = unzGoToFirstFile(zfile);\n\n    if (res == UNZ_OK) {\n        do {\n            // Get the name of the file in the archive.\n            char file_name[MAX_PATH] = {0};\n            unz_file_info64 file_info;\n            unzGetCurrentFileInfo64(zfile, &file_info, file_name, MAX_PATH, NULL, 0, NULL, 0);\n            QString qfile_name;\n            QString cp437_file_name;\n            qfile_name = QString::fromUtf8(file_name);\n            if (!(file_info.flag & (1<<11))) {\n                // General purpose bit 11 says the filename is utf-8 encoded. If not set then\n                // IBM 437 encoding might be used.\n                cp437_file_name = cp437->toUnicode(file_name);\n            }\n\n            // If there is no file name then we can't do anything with it.\n            if (!qfile_name.isEmpty()) {\n                // We use the dir object to create the path in the temporary directory.\n                // Unfortunately, we need a dir ojbect to do this as it's not a static function.\n                // Full file path in the temporary directory.\n                QString file_path = destpath + \"/\" + qfile_name;\n                QFileInfo qfile_info(file_path);\n\n                // Is this entry a directory?\n                if (file_info.uncompressed_size == 0 && qfile_name.endsWith('/')) {\n                    dir.mkpath(qfile_name);\n                    continue;\n                } else {\n                    dir.mkpath(qfile_info.path());\n                }\n\n                // Open the file entry in the archive for reading.\n                if (unzOpenCurrentFile(zfile) != UNZ_OK) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Open the file on disk to write the entry in the archive to.\n                QFile entry(file_path);\n\n                if (!entry.open(QIODevice::WriteOnly | QIODevice::Truncate)) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Buffered reading and writing.\n                char buff[BUFF_SIZE] = {0};\n                int read = 0;\n\n                while ((read = unzReadCurrentFile(zfile, buff, BUFF_SIZE)) > 0) {\n                    entry.write(buff, read);\n                }\n\n                entry.close();\n\n                // Read errors are marked by a negative read amount.\n                if (read < 0) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // The file was read but the CRC did not match.\n                // We don't check the read file size vs the uncompressed file size\n                // because if they're different there should be a CRC error.\n                if (unzCloseCurrentFile(zfile) == UNZ_CRCERROR) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                if (!cp437_file_name.isEmpty() && cp437_file_name != qfile_name) {\n                    QString cp437_file_path = destpath + \"/\" + cp437_file_name;\n                    QFile::copy(file_path, cp437_file_path);\n                }\n            }\n        } while ((res = unzGoToNextFile(zfile)) == UNZ_OK);\n    }\n\n    if (res != UNZ_END_OF_LIST_OF_FILE) {\n        unzClose(zfile);\n        return false;\n    }\n\n    unzClose(zfile);\n    return true;\n}", "func_src_after": "bool Utility::UnZip(const QString &zippath, const QString &destpath)\n{\n    int res = 0;\n    QDir dir(destpath);\n    if (!cp437) {\n        cp437 = new QCodePage437Codec();\n    }\n#ifdef Q_OS_WIN32\n    zlib_filefunc64_def ffunc;\n    fill_win32_filefunc64W(&ffunc);\n    unzFile zfile = unzOpen2_64(Utility::QStringToStdWString(QDir::toNativeSeparators(zippath)).c_str(), &ffunc);\n#else\n    unzFile zfile = unzOpen64(QDir::toNativeSeparators(zippath).toUtf8().constData());\n#endif\n\n    if ((zfile == NULL) || (!IsFileReadable(zippath)) || (!dir.exists())) {\n        return false;\n    }\n\n    res = unzGoToFirstFile(zfile);\n\n    if (res == UNZ_OK) {\n        do {\n            // Get the name of the file in the archive.\n            char file_name[MAX_PATH] = {0};\n            unz_file_info64 file_info;\n            unzGetCurrentFileInfo64(zfile, &file_info, file_name, MAX_PATH, NULL, 0, NULL, 0);\n            QString qfile_name;\n            QString cp437_file_name;\n            qfile_name = QString::fromUtf8(file_name);\n            if (!(file_info.flag & (1<<11))) {\n                // General purpose bit 11 says the filename is utf-8 encoded. If not set then\n                // IBM 437 encoding might be used.\n                cp437_file_name = cp437->toUnicode(file_name);\n            }\n\n            // If there is no file name then we can't do anything with it.\n            if (!qfile_name.isEmpty()) {\n\n\t        // for security reasons against maliciously crafted zip archives\n\t        // we need the file path to always be inside the target folder \n\t        // and not outside, so we will remove all illegal backslashes\n\t        // and all relative upward paths segments \"/../\" from the zip's local \n\t        // file name/path before prepending the target folder to create \n\t        // the final path\n\n\t        QString original_path = qfile_name;\n\t        bool evil_or_corrupt_epub = false;\n\n\t        if (qfile_name.contains(\"\\\\\")) evil_or_corrupt_epub = true; \n\t        qfile_name = \"/\" + qfile_name.replace(\"\\\\\",\"\");\n\n\t        if (qfile_name.contains(\"/../\")) evil_or_corrupt_epub = true;\n\t        qfile_name = qfile_name.replace(\"/../\",\"/\");\n\n\t        while(qfile_name.startsWith(\"/\")) { \n\t\t  qfile_name = qfile_name.remove(0,1);\n\t        }\n                \n\t        if (cp437_file_name.contains(\"\\\\\")) evil_or_corrupt_epub = true; \n\t        cp437_file_name = \"/\" + cp437_file_name.replace(\"\\\\\",\"\");\n\n\t        if (cp437_file_name.contains(\"/../\")) evil_or_corrupt_epub = true;\n\t        cp437_file_name = cp437_file_name.replace(\"/../\",\"/\");\n\n\t        while(cp437_file_name.startsWith(\"/\")) { \n\t\t  cp437_file_name = cp437_file_name.remove(0,1);\n\t        }\n\n\t        if (evil_or_corrupt_epub) {\n\t\t    unzCloseCurrentFile(zfile);\n\t\t    unzClose(zfile);\n\t\t    // throw (UNZIPLoadParseError(QString(QObject::tr(\"Possible evil or corrupt zip file name: %1\")).arg(original_path).toStdString()));\n                    return false;\n\t        }\n\n                // We use the dir object to create the path in the temporary directory.\n                // Unfortunately, we need a dir ojbect to do this as it's not a static function.\n                // Full file path in the temporary directory.\n                QString file_path = destpath + \"/\" + qfile_name;\n                QFileInfo qfile_info(file_path);\n\n                // Is this entry a directory?\n                if (file_info.uncompressed_size == 0 && qfile_name.endsWith('/')) {\n                    dir.mkpath(qfile_name);\n                    continue;\n                } else {\n                    dir.mkpath(qfile_info.path());\n                }\n\n                // Open the file entry in the archive for reading.\n                if (unzOpenCurrentFile(zfile) != UNZ_OK) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Open the file on disk to write the entry in the archive to.\n                QFile entry(file_path);\n\n                if (!entry.open(QIODevice::WriteOnly | QIODevice::Truncate)) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Buffered reading and writing.\n                char buff[BUFF_SIZE] = {0};\n                int read = 0;\n\n                while ((read = unzReadCurrentFile(zfile, buff, BUFF_SIZE)) > 0) {\n                    entry.write(buff, read);\n                }\n\n                entry.close();\n\n                // Read errors are marked by a negative read amount.\n                if (read < 0) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // The file was read but the CRC did not match.\n                // We don't check the read file size vs the uncompressed file size\n                // because if they're different there should be a CRC error.\n                if (unzCloseCurrentFile(zfile) == UNZ_CRCERROR) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                if (!cp437_file_name.isEmpty() && cp437_file_name != qfile_name) {\n                    QString cp437_file_path = destpath + \"/\" + cp437_file_name;\n                    QFile::copy(file_path, cp437_file_path);\n                }\n            }\n        } while ((res = unzGoToNextFile(zfile)) == UNZ_OK);\n    }\n\n    if (res != UNZ_END_OF_LIST_OF_FILE) {\n        unzClose(zfile);\n        return false;\n    }\n\n    unzClose(zfile);\n    return true;\n}", "commit_link": "github.com/Sigil-Ebook/Sigil/commit/0979ba8d10c96ebca330715bfd4494ea0e019a8f", "file_name": "src/Misc/Utility.cpp", "vul_type": "cwe-022", "description": "Write a C++ function to unzip a file to a specified directory using the QuaZIP library."}
{"func_name": "update_dir_from_tar", "func_src_before": "def update_dir_from_tar(tar, root_dir):\n    directories = []\n    valid_paths = BloomSet()\n    for entry in tar:\n        # Convert entry type to stat constant\n        if entry.type == tarfile.DIRTYPE:\n            entry_stat_type = stat.S_IFDIR\n        elif entry.type in (tarfile.REGTYPE, tarfile.LNKTYPE):\n            # Coda apparently doesn't allow hard links to symlinks\n            entry_stat_type = stat.S_IFREG\n        elif entry.type == tarfile.SYMTYPE:\n            entry_stat_type = stat.S_IFLNK\n        else:\n            raise ValueError(f\"Unexpected file type {entry.type}\")\n\n        # Check for existing file\n        path = build_path(root_dir, entry.name)\n        try:\n            st: Optional[os.stat_result] = os.lstat(path)\n        except OSError:\n            st = None\n\n        # If entry has changed types, remove the old object\n        if st is not None and stat.S_IFMT(st.st_mode) != entry_stat_type:\n            if stat.S_ISDIR(st.st_mode):\n                shutil.rmtree(path)\n            else:\n                os.unlink(path)\n            st = None\n\n        # Create parent directory if not present.  Parents are not\n        # necessarily dumped before children.\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create new object\n        if entry.isdir():\n            if not os.path.exists(path):\n                print(\"d\", path)\n                os.mkdir(path)\n            # Go back and set mtime after directory has been populated\n            directories.append(entry)\n        elif entry.isfile():\n            # update_file() will break hard links if it modifies the file.\n            # This is what we want because links may have also been broken\n            # at the source.  codadump2tar always dumps hard links, so we\n            # will rebuild any links that should still exist.\n            if update_file(path, TarMemberFile(tar, entry)):\n                print(\"f\", path)\n        elif entry.issym():\n            if st is None or entry.linkname and os.readlink(path) != entry.linkname:\n                print(\"s\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.symlink(entry.linkname, path)\n        elif entry.islnk():\n            target_path = build_path(root_dir, entry.linkname)\n            target_st = os.lstat(target_path)\n            if (\n                st is None\n                or st.st_dev != target_st.st_dev\n                or st.st_ino != target_st.st_ino\n            ):\n                print(\"l\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.link(target_path, path)\n\n        # Update metadata\n        attrs = XAttrs(path)\n        # owner and mode.  Hardlinks were updated with the primary, and we\n        # can't set xattrs on symlinks.\n        if entry.isfile() or entry.isdir():\n            # rsync --fake-super compatible:\n            # octal_mode_with_type major,minor uid:gid\n            mode = entry_stat_type | entry.mode\n            attrs.update(\n                ATTR_STAT,\n                f\"{mode:o} 0,0 {entry.uid}:{entry.gid}\",\n            )\n        # mtime.  Directories will be updated later, and hardlinks were\n        # updated with the primary.\n        if entry.isfile() or entry.issym() and os.lstat(path).st_mtime != entry.mtime:\n            lutime(path, entry.mtime)\n\n        # Protect from garbage collection\n        valid_paths.add(str(path))\n\n    # Deferred update of directory mtimes\n    for entry in directories:\n        path = build_path(root_dir, entry.name)\n        if os.stat(path).st_mtime != entry.mtime:\n            os.utime(path, (entry.mtime, entry.mtime))\n\n    return valid_paths", "func_src_after": "def update_dir_from_tar(tar, root_dir):\n    directories = []\n    valid_paths = BloomSet()\n    for entry in tar:\n        # Convert entry type to stat constant\n        if entry.type == tarfile.DIRTYPE:\n            entry_stat_type = stat.S_IFDIR\n        elif entry.type in (tarfile.REGTYPE, tarfile.LNKTYPE):\n            # Coda apparently doesn't allow hard links to symlinks\n            entry_stat_type = stat.S_IFREG\n        elif entry.type == tarfile.SYMTYPE:\n            entry_stat_type = stat.S_IFLNK\n        else:\n            raise ValueError(f\"Unexpected file type {entry.type}\")\n\n        # Check for existing file\n        path = build_path(root_dir, entry.name)\n        try:\n            st: Optional[os.stat_result] = os.lstat(path)\n        except OSError:\n            st = None\n\n        # If entry has changed types, remove the old object\n        if st is not None and stat.S_IFMT(st.st_mode) != entry_stat_type:\n            if stat.S_ISDIR(st.st_mode):\n                shutil.rmtree(path)\n            else:\n                os.unlink(path)\n            st = None\n\n        # Create parent directory if not present.  Parents are not\n        # necessarily dumped before children.\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create new object\n        if entry.isdir():\n            if not os.path.exists(path):\n                print(\"d\", path)\n                os.mkdir(path)\n            # Go back and set mtime after directory has been populated\n            directories.append(entry)\n        elif entry.isfile():\n            # update_file() will break hard links if it modifies the file.\n            # This is what we want because links may have also been broken\n            # at the source.  codadump2tar always dumps hard links, so we\n            # will rebuild any links that should still exist.\n            if update_file(path, TarMemberFile(tar, entry)):\n                print(\"f\", path)\n        elif entry.issym():\n            if st is None or (entry.linkname and os.readlink(path) != entry.linkname):\n                print(\"s\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.symlink(entry.linkname, path)\n        elif entry.islnk():\n            target_path = build_path(root_dir, entry.linkname)\n            target_st = os.lstat(target_path)\n            if (\n                st is None\n                or st.st_dev != target_st.st_dev\n                or st.st_ino != target_st.st_ino\n            ):\n                print(\"l\", path)\n                if st is not None:\n                    os.unlink(path)\n                os.link(target_path, path)\n\n        # Update metadata\n        attrs = XAttrs(path)\n        # owner and mode.  Hardlinks were updated with the primary, and we\n        # can't set xattrs on symlinks.\n        if entry.isfile() or entry.isdir():\n            # rsync --fake-super compatible:\n            # octal_mode_with_type major,minor uid:gid\n            mode = entry_stat_type | entry.mode\n            attrs.update(\n                ATTR_STAT,\n                f\"{mode:o} 0,0 {entry.uid}:{entry.gid}\",\n            )\n        # mtime.  Directories will be updated later, and hardlinks were\n        # updated with the primary.\n        if (entry.isfile() or entry.issym()) and os.lstat(path).st_mtime != entry.mtime:\n            lutime(path, entry.mtime)\n\n        # Protect from garbage collection\n        valid_paths.add(str(path))\n\n    # Deferred update of directory mtimes\n    for entry in directories:\n        path = build_path(root_dir, entry.name)\n        if os.stat(path).st_mtime != entry.mtime:\n            os.utime(path, (entry.mtime, entry.mtime))\n\n    return valid_paths", "line_changes": {"deleted": [{"line_no": 50, "char_start": 1943, "char_end": 2028, "line": "            if st is None or entry.linkname and os.readlink(path) != entry.linkname:\n"}, {"line_no": 82, "char_start": 3217, "char_end": 3304, "line": "        if entry.isfile() or entry.issym() and os.lstat(path).st_mtime != entry.mtime:\n"}], "added": [{"line_no": 50, "char_start": 1943, "char_end": 2030, "line": "            if st is None or (entry.linkname and os.readlink(path) != entry.linkname):\n"}, {"line_no": 82, "char_start": 3219, "char_end": 3308, "line": "        if (entry.isfile() or entry.issym()) and os.lstat(path).st_mtime != entry.mtime:\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1972, "char_end": 1973, "chars": "("}, {"char_start": 2027, "char_end": 2028, "chars": ")"}, {"char_start": 3230, "char_end": 3231, "chars": "("}, {"char_start": 3262, "char_end": 3263, "chars": ")"}]}, "commit_link": "github.com/cmusatyalab/deltaic/commit/3c8fb3f8f1b75a93a17198b863b44fa78589650d", "file_name": "coda.py", "vul_type": "cwe-022", "commit_msg": "Can't use pathlib.Path.resolve() to create absolute paths\n\nBecause do not want to traverse any (final?) symlink in the path.", "parent_commit": "4c5fe2b9a04849b48a0598b70dbcfcb27a54fee5", "description": "Write a Python function to update a directory structure from a TAR file, handling file types and metadata."}
{"func_name": "str_lower_case_match", "func_src_before": "str_lower_case_match(OnigEncoding enc, int case_fold_flag,\n                     const UChar* t, const UChar* tend,\n                     const UChar* p, const UChar* end)\n{\n  int lowlen;\n  UChar *q, lowbuf[ONIGENC_MBC_CASE_FOLD_MAXLEN];\n\n  while (t < tend) {\n    lowlen = ONIGENC_MBC_CASE_FOLD(enc, case_fold_flag, &p, end, lowbuf);\n    q = lowbuf;\n    while (lowlen > 0) {\n      if (*t++ != *q++) return 0;\n      lowlen--;\n    }\n  }\n\n  return 1;\n}", "func_src_after": "str_lower_case_match(OnigEncoding enc, int case_fold_flag,\n                     const UChar* t, const UChar* tend,\n                     const UChar* p, const UChar* end)\n{\n  int lowlen;\n  UChar *q, lowbuf[ONIGENC_MBC_CASE_FOLD_MAXLEN];\n\n  while (t < tend) {\n    lowlen = ONIGENC_MBC_CASE_FOLD(enc, case_fold_flag, &p, end, lowbuf);\n    q = lowbuf;\n    while (lowlen > 0) {\n      if (t >= tend)    return 0;\n      if (*t++ != *q++) return 0;\n      lowlen--;\n    }\n  }\n\n  return 1;\n}", "commit_link": "github.com/kkos/oniguruma/commit/d3e402928b6eb3327f8f7d59a9edfa622fec557b", "file_name": "src/regexec.c", "vul_type": "cwe-125", "description": "Write a C function named `str_lower_case_match` that compares two strings for equality, considering case insensitivity, using Oniguruma encoding functions."}
{"func_name": "(anonymous)", "func_src_before": "process.on('uncaughtException',function(e){\n\tconsole.error(e);\n\tconsole.trace(e.stack);\n});", "func_src_after": "process.on('uncaughtException',function(e){\n\tconsole.error(e);\n\tconsole.trace(e.stack);\n});", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 91, "line": "});\n"}], "added": []}, "char_changes": {"deleted": [], "added": []}, "commit_link": "github.com/Eeems/PooledWebSocket/commit/7b3b4e5c6be6d8a964296fa3c50e38dc07e9701d", "file_name": "server.js", "vul_type": "cwe-022", "commit_msg": "Update server.js\n\nResolve directory traversal attack", "description": "Write a Node.js code snippet that logs an error and its stack trace when an uncaught exception occurs."}
{"func_name": "add_extra_args", "func_src_before": "    def add_extra_args(self, args=None):\n        \"\"\"Add more args depending on how known args are set.\"\"\"\n        parsed = vars(self.parse_known_args(nohelp=True)[0])\n\n        # find which image mode specified if any, and add additional arguments\n        image_mode = parsed.get('image_mode', None)\n        if image_mode is not None and image_mode != 'none':\n            self.add_image_args(image_mode)\n\n        # find which task specified if any, and add its specific arguments\n        task = parsed.get('task', None)\n        if task is not None:\n            self.add_task_args(task)\n        evaltask = parsed.get('evaltask', None)\n        if evaltask is not None:\n            self.add_task_args(evaltask)\n\n        # find which model specified if any, and add its specific arguments\n        model = parsed.get('model', None)\n        if model is not None:\n            self.add_model_subargs(model)\n\n        # reset parser-level defaults over any model-level defaults\n        try:\n            self.set_defaults(**self._defaults)\n        except AttributeError:\n            raise RuntimeError('Please file an issue on github that argparse '\n                               'got an attribute error when parsing.')", "func_src_after": "    def add_extra_args(self, args=None):\n        \"\"\"Add more args depending on how known args are set.\"\"\"\n        parsed = vars(self.parse_known_args(args, nohelp=True)[0])\n\n        # find which image mode specified if any, and add additional arguments\n        image_mode = parsed.get('image_mode', None)\n        if image_mode is not None and image_mode != 'none':\n            self.add_image_args(image_mode)\n\n        # find which task specified if any, and add its specific arguments\n        task = parsed.get('task', None)\n        if task is not None:\n            self.add_task_args(task)\n        evaltask = parsed.get('evaltask', None)\n        if evaltask is not None:\n            self.add_task_args(evaltask)\n\n        # find which model specified if any, and add its specific arguments\n        model = parsed.get('model', None)\n        if model is not None:\n            self.add_model_subargs(model)\n\n        # reset parser-level defaults over any model-level defaults\n        try:\n            self.set_defaults(**self._defaults)\n        except AttributeError:\n            raise RuntimeError('Please file an issue on github that argparse '\n                               'got an attribute error when parsing.')", "commit_link": "github.com/freedombenLiu/ParlAI/commit/601668d569e1276e0b8bf2bf8fb43e391e10d170", "file_name": "parlai/core/params.py", "vul_type": "cwe-078", "description": "Write a Python function that extends argument parsing with additional arguments based on existing parsed arguments."}
{"func_name": "patch", "func_src_before": "static PyObject* patch(PyObject* self, PyObject* args)\n{\n    char *origData, *newData, *diffBlock, *extraBlock, *diffPtr, *extraPtr;\n    Py_ssize_t origDataLength, newDataLength, diffBlockLength, extraBlockLength;\n    PyObject *controlTuples, *tuple, *results;\n    off_t oldpos, newpos, x, y, z;\n    int i, j, numTuples;\n\n    if (!PyArg_ParseTuple(args, \"s#nO!s#s#\",\n                          &origData, &origDataLength, &newDataLength,\n                          &PyList_Type, &controlTuples,\n                          &diffBlock, &diffBlockLength,\n                          &extraBlock, &extraBlockLength))\n        return NULL;\n\n    /* allocate the memory for the new data */\n    newData = PyMem_Malloc(newDataLength + 1);\n    if (!newData)\n        return PyErr_NoMemory();\n\n    oldpos = 0;\n    newpos = 0;\n    diffPtr = diffBlock;\n    extraPtr = extraBlock;\n    numTuples = PyList_GET_SIZE(controlTuples);\n    for (i = 0; i < numTuples; i++) {\n        tuple = PyList_GET_ITEM(controlTuples, i);\n        if (!PyTuple_Check(tuple)) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_TypeError, \"expecting tuple\");\n            return NULL;\n        }\n        if (PyTuple_GET_SIZE(tuple) != 3) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_TypeError, \"expecting tuple of size 3\");\n            return NULL;\n        }\n        x = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 0));\n        y = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 1));\n        z = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 2));\n        if (newpos + x > newDataLength ||\n                diffPtr + x > diffBlock + diffBlockLength ||\n                extraPtr + y > extraBlock + extraBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, diffPtr, x);\n        diffPtr += x;\n        for (j = 0; j < x; j++)\n            if ((oldpos + j >= 0) && (oldpos + j < origDataLength))\n                newData[newpos + j] += origData[oldpos + j];\n        newpos += x;\n        oldpos += x;\n        memcpy(newData + newpos, extraPtr, y);\n        extraPtr += y;\n        newpos += y;\n        oldpos += z;\n    }\n\n    /* confirm that a valid patch was applied */\n    if (newpos != newDataLength ||\n            diffPtr != diffBlock + diffBlockLength ||\n            extraPtr != extraBlock + extraBlockLength) {\n        PyMem_Free(newData);\n        PyErr_SetString(PyExc_ValueError, \"corrupt patch (underflow)\");\n        return NULL;\n    }\n\n    results = PyBytes_FromStringAndSize(newData, newDataLength);\n    PyMem_Free(newData);\n    return results;\n}", "func_src_after": "static PyObject* patch(PyObject* self, PyObject* args)\n{\n    char *origData, *newData, *diffBlock, *extraBlock, *diffPtr, *extraPtr;\n    Py_ssize_t origDataLength, newDataLength, diffBlockLength, extraBlockLength;\n    PyObject *controlTuples, *tuple, *results;\n    off_t oldpos, newpos, x, y, z;\n    int i, j, numTuples;\n\n    if (!PyArg_ParseTuple(args, \"s#nO!s#s#\",\n                          &origData, &origDataLength, &newDataLength,\n                          &PyList_Type, &controlTuples,\n                          &diffBlock, &diffBlockLength,\n                          &extraBlock, &extraBlockLength))\n        return NULL;\n\n    /* allocate the memory for the new data */\n    newData = PyMem_Malloc(newDataLength + 1);\n    if (!newData)\n        return PyErr_NoMemory();\n\n    oldpos = 0;\n    newpos = 0;\n    diffPtr = diffBlock;\n    extraPtr = extraBlock;\n    numTuples = PyList_GET_SIZE(controlTuples);\n    for (i = 0; i < numTuples; i++) {\n        tuple = PyList_GET_ITEM(controlTuples, i);\n        if (!PyTuple_Check(tuple)) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_TypeError, \"expecting tuple\");\n            return NULL;\n        }\n        if (PyTuple_GET_SIZE(tuple) != 3) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_TypeError, \"expecting tuple of size 3\");\n            return NULL;\n        }\n        x = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 0));\n        y = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 1));\n        z = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 2));\n        if (newpos + x > newDataLength ||\n                diffPtr + x > diffBlock + diffBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, diffPtr, x);\n        diffPtr += x;\n        for (j = 0; j < x; j++)\n            if ((oldpos + j >= 0) && (oldpos + j < origDataLength))\n                newData[newpos + j] += origData[oldpos + j];\n        newpos += x;\n        oldpos += x;\n        if (newpos + y > newDataLength ||\n                extraPtr + y > extraBlock + extraBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, extraPtr, y);\n        extraPtr += y;\n        newpos += y;\n        oldpos += z;\n    }\n\n    /* confirm that a valid patch was applied */\n    if (newpos != newDataLength ||\n            diffPtr != diffBlock + diffBlockLength ||\n            extraPtr != extraBlock + extraBlockLength) {\n        PyMem_Free(newData);\n        PyErr_SetString(PyExc_ValueError, \"corrupt patch (underflow)\");\n        return NULL;\n    }\n\n    results = PyBytes_FromStringAndSize(newData, newDataLength);\n    PyMem_Free(newData);\n    return results;\n}", "commit_link": "github.com/ilanschnell/bsdiff4/commit/49a4cee2feef7deaf9d89e5e793a8824930284d7", "file_name": "bsdiff4/core.c", "vul_type": "cwe-787", "description": "Write a Python C extension function named `patch` that applies a binary patch to given data using control tuples and diff/extra blocks."}
{"func_name": "TestConfigServerTLSMinVersionIsSetBasedOnOptions", "func_src_before": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS11,\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "func_src_after": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 93, "char_end": 113, "line": "\t\ttls.VersionTLS11,\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 113, "chars": "\t\ttls.VersionTLS11,\n"}], "added": []}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config_test.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go test function that checks if a server's TLS minimum version is set correctly based on provided options."}
{"func_name": "writeError", "func_src_before": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n}", "func_src_after": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 89, "char_end": 147, "line": "\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n"}], "added": [{"line_no": 3, "char_start": 89, "char_end": 166, "line": "\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 115, "char_end": 133, "chars": "html.EscapeString("}, {"char_start": 164, "char_end": 165, "chars": ")"}]}, "commit_link": "github.com/mbrt/gmailctl/commit/563a9e3605d722e32dedd2e93f38da655338b39a", "file_name": "oauth2_server.go", "vul_type": "cwe-079", "commit_msg": "Sanitize error before writing html response.\n\nThis has no real potential for XSS, as the serve runs on localhost, but\nbetter safe than sorry.", "parent_commit": "75e2e0ed4459203fa460507dd0c7385c8cafddcb", "description": "Create a Go function that sends an error message with an HTTP status code to the client's response writer."}
{"func_name": "sloka", "func_src_before": "@app.route('/sloka')\ndef sloka():\n\n    sloka_number = request.args.get('sloka_number')\n\n    sloka_number_parts = sloka_number.split('.')\n\n    sloka_number_previous = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)\n    sloka_number_next = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where sloka_number = '%s' order by sloka_line;\" % sloka_number)\n            mula = cur.fetchall();\n\n            cur.execute(\"select * from pada where sloka_number = '%s' order by id;\" % sloka_number)\n            pada = cur.fetchall();\n\n            varga = \"\"\n            if len(pada) > 0:\n                varga = pada[0][\"varga\"]\n\n            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)\n    finally:\n        con.close()", "func_src_after": "@app.route('/sloka')\ndef sloka():\n\n    sloka_number = request.args.get('sloka_number')\n\n    sloka_number_parts = sloka_number.split('.')\n\n    sloka_number_previous = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)\n    sloka_number_next = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where sloka_number = ? order by sloka_line;\", [sloka_number])\n            mula = cur.fetchall();\n\n            cur.execute(\"select * from pada where sloka_number = ? order by id;\", [sloka_number])\n            pada = cur.fetchall();\n\n            varga = \"\"\n            if len(pada) > 0:\n                varga = pada[0][\"varga\"]\n\n            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)\n    finally:\n        con.close()", "commit_link": "github.com/aupasana/amara-quiz/commit/6ceb5dc8ec38b4a3f1399e578ab970f7e3354922", "file_name": "docker/app.py", "vul_type": "cwe-089", "description": "In Python, create a Flask route to display a sloka with its previous and next references, fetching data from a SQLite database."}
{"func_name": "get", "func_src_before": "    @web.authenticated\n    def get(self, value):\n        user_id = self.current_user_id()\n\n        try:\n            subm_id = int(value)\n        except:\n            raise web.HTTPError(400)\n\n        error_log = contest.get_error_log(user_id, subm_id, is_admin=self.is_admin())\n        if not error_log:\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write('<pre>')\n        self.write(str(error_log))\n        self.write('</pre>')", "func_src_after": "    @web.authenticated\n    def get(self, value):\n        user_id = self.current_user_id()\n\n        try:\n            subm_id = int(value)\n        except:\n            raise web.HTTPError(400)\n\n        error_log = contest.get_error_log(user_id, subm_id, is_admin=self.is_admin())\n        if not error_log:\n            raise web.HTTPError(404)\n        self.set_header('Content-Type', 'text/html')\n        self.write('<pre>')\n        self.write(escape.xhtml_escape(str(error_log)))\n        self.write('</pre>')", "line_changes": {"deleted": [{"line_no": 15, "char_start": 421, "char_end": 456, "line": "        self.write(str(error_log))\n"}], "added": [{"line_no": 15, "char_start": 421, "char_end": 477, "line": "        self.write(escape.xhtml_escape(str(error_log)))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 440, "char_end": 460, "chars": "escape.xhtml_escape("}, {"char_start": 475, "char_end": 476, "chars": ")"}]}, "commit_link": "github.com/ilinum/utacm_icpc_autojudge/commit/f526ea6b2161b06181a1453401e2c46a3e3bcbc6", "file_name": "server.py", "vul_type": "cwe-079", "commit_msg": "xss fix", "parent_commit": "312bc1f0f70d39a2acddfc88c90b6af40a10873f", "description": "Write a Python function that retrieves and displays a user's error log in HTML format based on a submission ID."}
{"func_name": "json_encode", "func_src_before": "      def json_encode(obj)\n        JSON.dump(obj)\n      end", "func_src_after": "      def json_encode(obj)\n        JSON.generate(obj)\n      end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 27, "char_end": 50, "line": "        JSON.dump(obj)\n"}], "added": [{"line_no": 2, "char_start": 27, "char_end": 54, "line": "        JSON.generate(obj)\n"}]}, "char_changes": {"deleted": [{"char_start": 40, "char_end": 44, "chars": "dump"}], "added": [{"char_start": 40, "char_end": 48, "chars": "generate"}]}, "commit_link": "github.com/Andreis13/sprockets/commit/e548f03540b4311adc47870815e8d5fd833825cf", "file_name": "manifest.rb", "vul_type": "cwe-502", "commit_msg": "replace `JSON` `dump`/`load` with `parse`/`generate`\n\n`dump` and `load` are for built around Marshaling ruby objects generally.\nThey correspond with those methods on Ruby's `Marshal` class. Theses\nmethods actually call `parse`/`generate` in code but pass some defaults\nalong with it. Sprockets only needs to parse JSON documents not Ruby\nobjects.\n\nFor `JSON.load`, we want to disable `create_additions`.\n`create_additions` could be considered a security hazard if set to true.\n`create_additions` allows the instantiation of any class that's\nmarshaled as json", "parent_commit": "b18af736eac52f11c8704a43be36f2d21cf44ce2", "description": "Write a Ruby method named `json_encode` that converts an object to a JSON string."}
{"func_name": "dateproto_setUTCMilliseconds", "func_src_before": "func (r *Runtime) dateproto_setUTCMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := int(call.Argument(0).ToInteger())\n\t\t\tt := d.time.In(time.UTC)\n\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local)\n\t\t\treturn intToValue(timeToMsec(d.time))\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setUTCMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "func_src_after": "func (r *Runtime) dateproto_setUTCMilliseconds(call FunctionCall) Value {\n\tobj := r.toObject(call.This)\n\tif d, ok := obj.self.(*dateObject); ok {\n\t\tif d.isSet {\n\t\t\tmsec := call.Argument(0).ToInteger()\n\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m)\n\t\t\treturn intToValue(m)\n\t\t} else {\n\t\t\treturn _NaN\n\t\t}\n\t}\n\tr.typeErrorResult(true, \"Method Date.prototype.setUTCMilliseconds is called on incompatible receiver\")\n\tpanic(\"Unreachable\")\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 161, "char_end": 206, "line": "\t\t\tmsec := int(call.Argument(0).ToInteger())\n"}, {"line_no": 6, "char_start": 206, "char_end": 234, "line": "\t\t\tt := d.time.In(time.UTC)\n"}, {"line_no": 7, "char_start": 234, "char_end": 355, "line": "\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local)\n"}, {"line_no": 8, "char_start": 355, "char_end": 396, "line": "\t\t\treturn intToValue(timeToMsec(d.time))\n"}], "added": [{"line_no": 5, "char_start": 161, "char_end": 201, "line": "\t\t\tmsec := call.Argument(0).ToInteger()\n"}, {"line_no": 6, "char_start": 201, "char_end": 268, "line": "\t\t\tm := timeToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n"}, {"line_no": 7, "char_start": 268, "char_end": 296, "line": "\t\t\td.time = timeFromMsec(m)\n"}, {"line_no": 8, "char_start": 296, "char_end": 320, "line": "\t\t\treturn intToValue(m)\n"}]}, "char_changes": {"deleted": [{"char_start": 172, "char_end": 176, "chars": "int("}, {"char_start": 204, "char_end": 205, "chars": ")"}, {"char_start": 209, "char_end": 210, "chars": "t"}, {"char_start": 214, "char_end": 216, "chars": "d."}, {"char_start": 220, "char_end": 353, "chars": ".In(time.UTC)\n\t\t\td.time = time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), t.Minute(), t.Second(), msec*1e6, time.UTC).In(time.Local"}, {"char_start": 376, "char_end": 394, "chars": "timeToMsec(d.time)"}], "added": [{"char_start": 204, "char_end": 205, "chars": "m"}, {"char_start": 213, "char_end": 294, "chars": "ToMsec(d.time) - int64(d.time.Nanosecond())/1e6 + msec\n\t\t\td.time = timeFromMsec(m"}, {"char_start": 317, "char_end": 318, "chars": "m"}]}, "commit_link": "github.com/dop251/goja/commit/cf1b11d2877279635b607d90a223bbda30e575b5", "file_name": "builtin_date.go", "vul_type": "cwe-681", "commit_msg": "Avoid integer overflow in Date.setMilliseconds()", "parent_commit": "5e65f9206bdb013b233bde6bac91fc88e00ff7a3", "description": "Write a Go function that sets the milliseconds for a date object in UTC."}
{"func_name": "multiSelect", "func_src_before": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "func_src_after": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  if( pParse->nErr ) goto multi_select_end;\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "commit_link": "github.com/sqlite/sqlite/commit/8428b3b437569338a9d1e10c4cd8154acbe33089", "file_name": "src/select.c", "vul_type": "cwe-476", "description": "Write a function in C for SQLite that handles the execution of compound SELECT queries with specific handling for UNION, INTERSECT, and EXCEPT operations."}
{"func_name": "create", "func_src_before": "def create(request):\n    return create_(User, request, encrypt_password)", "func_src_after": "def create(request):\n    return create_(User, request, encrypt_password_callback)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 72, "line": "    return create_(User, request, encrypt_password)\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 81, "line": "    return create_(User, request, encrypt_password_callback)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 71, "char_end": 80, "chars": "_callback"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/ddb9d55999151f37fd4c833a98d2f34648757293", "file_name": "users.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new encryption methods using passlib", "parent_commit": "8e92641fee542f6e7004e827136dea3ce5e99eb2", "description": "Write a Python function named `create` that takes a request and returns a user creation result using a specified encryption function."}
{"func_name": "_find_host_exhaustive", "func_src_before": "    def _find_host_exhaustive(self, connector, hosts):\n        for host in hosts:\n            ssh_cmd = 'svcinfo lshost -delim ! %s' % host\n            out, err = self._run_ssh(ssh_cmd)\n            self._assert_ssh_return(len(out.strip()),\n                                    '_find_host_exhaustive',\n                                    ssh_cmd, out, err)\n            for attr_line in out.split('\\n'):\n                # If '!' not found, return the string and two empty strings\n                attr_name, foo, attr_val = attr_line.partition('!')\n                if (attr_name == 'iscsi_name' and\n                        'initiator' in connector and\n                        attr_val == connector['initiator']):\n                    return host\n                elif (attr_name == 'WWPN' and\n                      'wwpns' in connector and\n                      attr_val.lower() in\n                      map(str.lower, map(str, connector['wwpns']))):\n                        return host\n        return None", "func_src_after": "    def _find_host_exhaustive(self, connector, hosts):\n        for host in hosts:\n            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]\n            out, err = self._run_ssh(ssh_cmd)\n            self._assert_ssh_return(len(out.strip()),\n                                    '_find_host_exhaustive',\n                                    ssh_cmd, out, err)\n            for attr_line in out.split('\\n'):\n                # If '!' not found, return the string and two empty strings\n                attr_name, foo, attr_val = attr_line.partition('!')\n                if (attr_name == 'iscsi_name' and\n                        'initiator' in connector and\n                        attr_val == connector['initiator']):\n                    return host\n                elif (attr_name == 'WWPN' and\n                      'wwpns' in connector and\n                      attr_val.lower() in\n                      map(str.lower, map(str, connector['wwpns']))):\n                        return host\n        return None", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function that searches for a host with matching iSCSI or WWPN attributes in a list of hosts using SSH commands."}
{"func_name": "handle_file", "func_src_before": "def handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = text\n    m.cachedText = compile_markdown(text)\n    m.category = category\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = headline\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))", "func_src_after": "def handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = escape(text)\n    m.cachedText = compile_markdown(escape(text))\n    m.category = escape(category)\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = escape(headline)\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))", "commit_link": "github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "file_name": "c3shop/frontpage/management/mediatools/media_actions.py", "vul_type": "cwe-079", "description": "Write a Python function to handle media file uploads, including image resizing and metadata processing."}
{"func_name": "refresh_select", "func_src_before": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerHTML = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "func_src_after": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerText = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "line_changes": {"deleted": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerHTML = label;\n"}], "added": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerText = label;\n"}]}, "char_changes": {"deleted": [{"char_start": 1316, "char_end": 1320, "chars": "HTML"}], "added": [{"char_start": 1316, "char_end": 1320, "chars": "Text"}]}, "commit_link": "github.com/Fabianlindfors/multi.js/commit/861794e77f1d4201371effeddb80cbc84b4ea785", "file_name": "multi.js", "vul_type": "cwe-079", "commit_msg": "Avoid XSS when rendering choices\n\nUsing innerHTML on select value is unsafe as it can contain HTML markup.", "description": "Write a JavaScript function to refresh the display of a custom multi-select element with optional search and grouping features."}
{"func_name": "html_content", "func_src_before": "    @property\n    async def html_content(self):\n        content = await self.content\n        if not content:\n            return ''\n        return markdown(content)", "func_src_after": "    @property\n    async def html_content(self):\n        content = markupsafe.escape(await self.content)\n        if not content:\n            return ''\n        return markdown(content)", "commit_link": "github.com/dongweiming/lyanna/commit/fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "file_name": "models/comment.py", "vul_type": "cwe-079", "description": "Generate a Python async property method that converts stored content to HTML, handling empty content and ensuring safe markup."}
{"func_name": "images_from_fig", "func_src_before": "    def images_from_fig\n      fig_services = YAML.load(fig_yml) || {}\n      fig_services.map { |name, service_def| image_from_fig_service(name, service_def) }\n    end", "func_src_after": "    def images_from_fig\n      fig_services = YAML.safe_load(fig_yml) || {}\n      fig_services.map { |name, service_def| image_from_fig_service(name, service_def) }\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 24, "char_end": 70, "line": "      fig_services = YAML.load(fig_yml) || {}\n"}], "added": [{"line_no": 2, "char_start": 24, "char_end": 75, "line": "      fig_services = YAML.safe_load(fig_yml) || {}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 50, "char_end": 55, "chars": "safe_"}]}, "commit_link": "github.com/TravisCannon/panamax-api/commit/5f0bd8a0a60751bfd8ff51db83627b0477863b55", "file_name": "from_fig.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load when parsing user templates", "parent_commit": "0f311d932ddb665f5ebdde98cca040ec858f1010", "description": "Write a Ruby method named `images_from_fig` that loads a YAML configuration and maps each service to an image processing method."}
{"func_name": "_modify_3par_iscsi_host", "func_src_before": "    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        self.common._cli_run('createhost -iscsi -add %s %s'\n                             % (hostname, iscsi_iqn), None)", "func_src_after": "    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]\n        self.common._cli_run(command)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_iscsi.py", "vul_type": "cwe-078", "description": "Write a Python function to add an iSCSI host to a 3PAR system without persona or domain options."}
{"func_name": "mrb_obj_clone", "func_src_before": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags = mrb_obj_ptr(self)->flags;\n\n  return clone;\n}", "func_src_after": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags |= mrb_obj_ptr(self)->flags & MRB_FLAG_IS_FROZEN;\n\n  return clone;\n}", "commit_link": "github.com/mruby/mruby/commit/55edae0226409de25e59922807cb09acb45731a2", "file_name": "src/kernel.c", "vul_type": "cwe-476", "description": "Write a function in C for the MRuby engine that clones an object, handling immediate values and singleton classes, and preserving the original object's frozen state."}
{"func_name": "fiber_switch", "func_src_before": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  if (resume && c->status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (c->status == MRB_FIBER_RUNNING || c->status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (c->status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  mrb->c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  if (c->status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    if (len >= c->stend - c->stack) {\n      mrb_raise(mrb, E_FIBER_ERROR, \"too many arguments to fiber\");\n    }\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n  fiber_switch_context(mrb, c);\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "func_src_after": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  enum mrb_fiber_state status;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  status = c->status;\n  if (resume && status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (status == MRB_FIBER_RUNNING || status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  old_c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  fiber_switch_context(mrb, c);\n  if (status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    mrb_stack_extend(mrb, len+2); /* for receiver and (optional) block */\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "commit_link": "github.com/mruby/mruby/commit/778500563a9f7ceba996937dc886bd8cde29b42b", "file_name": "mrbgems/mruby-fiber/src/fiber.c", "vul_type": "cwe-125", "description": "Write a C function `fiber_switch` for the MRuby language that handles fiber resumption, argument passing, and optional VM execution."}
{"func_name": "dex_parse_debug_item", "func_src_before": "static void dex_parse_debug_item(RBinFile *binfile, RBinDexObj *bin,\n\t\t\t\t  RBinDexClass *c, int MI, int MA, int paddr, int ins_size,\n\t\t\t\t  int insns_size, char *class_name, int regsz,\n\t\t\t\t  int debug_info_off) {\n\tstruct r_bin_t *rbin = binfile->rbin;\n\tconst ut8 *p4 = r_buf_get_at (binfile->buf, debug_info_off, NULL);\n\tconst ut8 *p4_end = p4 + binfile->buf->length - debug_info_off;\n\tut64 line_start;\n\tut64 parameters_size;\n\tut64 param_type_idx;\n\tut16 argReg = regsz - ins_size;\n\tut64 source_file_idx = c->source_file;\n\tRList *params, *debug_positions, *emitted_debug_locals = NULL; \n\tbool keep = true;\n\tif (argReg > regsz) {\n\t\treturn; // this return breaks tests\n\t}\n\tp4 = r_uleb128 (p4, p4_end - p4, &line_start);\n\tp4 = r_uleb128 (p4, p4_end - p4, &parameters_size);\n\t// TODO: check when we should use source_file\n\t// The state machine consists of five registers\n\tut32 address = 0;\n\tut32 line = line_start;\n\tif (!(debug_positions = r_list_newf ((RListFree)free))) {\n\t\treturn;\t\n\t}\n\tif (!(emitted_debug_locals = r_list_newf ((RListFree)free))) {\n\t\tr_list_free (debug_positions);\n\t\treturn;\n\t}\n\n\tstruct dex_debug_local_t debug_locals[regsz];\n\tmemset (debug_locals, 0, sizeof (struct dex_debug_local_t) * regsz);\n\tif (!(MA & 0x0008)) {\n\t\tdebug_locals[argReg].name = \"this\";\n\t\tdebug_locals[argReg].descriptor = r_str_newf(\"%s;\", class_name);\n\t\tdebug_locals[argReg].startAddress = 0;\n\t\tdebug_locals[argReg].signature = NULL;\n\t\tdebug_locals[argReg].live = true;\n\t\targReg++;\n\t}\n\tif (!(params = dex_method_signature2 (bin, MI))) {\n\t\tr_list_free (debug_positions);\n\t\tr_list_free (emitted_debug_locals);\n\t\treturn;\n\t}\n\n\tRListIter *iter = r_list_iterator (params);\n\tchar *name;\n\tchar *type;\n\tint reg;\n\n\tr_list_foreach (params, iter, type) {\n\t\tif ((argReg >= regsz) || !type || parameters_size <= 0) {\n\t\t\tr_list_free (debug_positions);\n\t\t\tr_list_free (params);\n\t\t\tr_list_free (emitted_debug_locals);\n\t\t\treturn;\n\t\t}\n\t\tp4 = r_uleb128 (p4, p4_end - p4, &param_type_idx); // read uleb128p1\n\t\tparam_type_idx -= 1;\n\t\tname = getstr (bin, param_type_idx);\n\t\treg = argReg;\n\t\tswitch (type[0]) {\n\t\tcase 'D':\n\t\tcase 'J':\n\t\t\targReg += 2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\targReg += 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (name) {\n\t\t\tdebug_locals[reg].name = name;\n\t\t\tdebug_locals[reg].descriptor = type;\n\t\t\tdebug_locals[reg].signature = NULL;\n\t\t\tdebug_locals[reg].startAddress = address;\n\t\t\tdebug_locals[reg].live = true;\n\t\t}\n\t\t--parameters_size;\n\t}\n\n\tut8 opcode = *(p4++) & 0xff;\n\twhile (keep) {\n\t\tswitch (opcode) {\n\t\tcase 0x0: // DBG_END_SEQUENCE\n\t\t\tkeep = false;\n\t\t\tbreak;\n\t\tcase 0x1: // DBG_ADVANCE_PC\n\t\t\t{\n\t\t\tut64 addr_diff;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &addr_diff);\n\t\t\taddress += addr_diff;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x2: // DBG_ADVANCE_LINE\n\t\t\t{\n\t\t\tst64 line_diff = r_sleb128 (&p4, p4_end);\n\t\t\tline += line_diff;\n\t\t\t}\n\t\t\tbreak;\t\n\t\tcase 0x3: // DBG_START_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tut64 name_idx;\n\t\t\tut64 type_idx;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &name_idx); \n\t\t\tname_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &type_idx); \n\t\t\ttype_idx -= 1;\n\t\t\tif (register_num >= regsz) {\n\t\t\t\tr_list_free (debug_positions);\n\t\t\t\tr_list_free (params);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\t// Emit what was previously there, if anything\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\t\t\tdebug_locals[register_num].name = getstr (bin, name_idx);\n\t\t\tdebug_locals[register_num].descriptor = dex_type_descriptor (bin, type_idx);\n\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\tdebug_locals[register_num].signature = NULL;\n\t\t\tdebug_locals[register_num].live = true;\n\t\t\t//eprintf(\"DBG_START_LOCAL %x %x %x\\n\", register_num, name_idx, type_idx);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x4: //DBG_START_LOCAL_EXTENDED\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tut64 name_idx;\n\t\t\tut64 type_idx;\n\t\t\tut64 sig_idx;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &name_idx);\n\t\t\tname_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &type_idx);\n\t\t\ttype_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &sig_idx);\n\t\t\tsig_idx -= 1;\n\t\t\tif (register_num >= regsz) {\n\t\t\t\tr_list_free (debug_positions);\n\t\t\t\tr_list_free (params);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// Emit what was previously there, if anything\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\n\t\t\tdebug_locals[register_num].name = getstr (bin, name_idx);\n\t\t\tdebug_locals[register_num].descriptor = dex_type_descriptor (bin, type_idx);\n\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\tdebug_locals[register_num].signature = getstr (bin, sig_idx);\n\t\t\tdebug_locals[register_num].live = true;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x5: // DBG_END_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\t\t\tdebug_locals[register_num].live = false;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x6: // DBG_RESTART_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tif (!debug_locals[register_num].live) {\n\t\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\t\tdebug_locals[register_num].live = true;\n\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x7: //DBG_SET_PROLOGUE_END\n\t\t\tbreak;\n\t\tcase 0x8: //DBG_SET_PROLOGUE_BEGIN\n\t\t\tbreak;\n\t\tcase 0x9:\n\t\t\t{\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &source_file_idx);\n\t\t\tsource_file_idx--;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t{\n\t\t\tint adjusted_opcode = opcode - 0x0a;\n\t\t\taddress += (adjusted_opcode / 15);\n\t\t\tline += -4 + (adjusted_opcode % 15);\n\t\t\tstruct dex_debug_position_t *position =\n\t\t\t\tmalloc (sizeof (struct dex_debug_position_t));\n\t\t\tif (!position) {\n\t\t\t\tkeep = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tposition->source_file_idx = source_file_idx;\n\t\t\tposition->address = address;\n\t\t\tposition->line = line;\n\t\t\tr_list_append (debug_positions, position);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\topcode = *(p4++) & 0xff;\n\t}\n\n\tif (!binfile->sdb_addrinfo) {\n\t\tbinfile->sdb_addrinfo = sdb_new0 ();\n\t}\n\n\tchar *fileline;\n\tchar offset[64];\n\tchar *offset_ptr;\n\n\tRListIter *iter1;\n\tstruct dex_debug_position_t *pos;\n\tr_list_foreach (debug_positions, iter1, pos) {\n\t\tfileline = r_str_newf (\"%s|%\"PFMT64d, getstr (bin, pos->source_file_idx), pos->line);\n\t\toffset_ptr = sdb_itoa (pos->address + paddr, offset, 16);\n\t\tsdb_set (binfile->sdb_addrinfo, offset_ptr, fileline, 0);\n\t\tsdb_set (binfile->sdb_addrinfo, fileline, offset_ptr, 0);\n\t}\n\n\tif (!dexdump) {\n\t\tr_list_free (debug_positions);\n\t\tr_list_free (emitted_debug_locals);\n\t\tr_list_free (params);\n\t\treturn;\n\t}\n\n\tRListIter *iter2;\n\tstruct dex_debug_position_t *position;\n\n\trbin->cb_printf (\"      positions     :\\n\");\n\tr_list_foreach (debug_positions, iter2, position) {\n\t\trbin->cb_printf (\"        0x%04llx line=%llu\\n\",\n\t\t\t\t position->address, position->line);\n\t}\n\n\trbin->cb_printf (\"      locals        :\\n\");\n\n\tRListIter *iter3;\n\tstruct dex_debug_local_t *local;\n\tr_list_foreach (emitted_debug_locals, iter3, local) {\n\t\tif (local->signature) {\n\t\t\trbin->cb_printf (\n\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s %s\\n\",\n\t\t\t\tlocal->startAddress, local->endAddress,\n\t\t\t\tlocal->reg, local->name, local->descriptor,\n\t\t\t\tlocal->signature);\n\t\t} else {\n\t\t\trbin->cb_printf (\n\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s\\n\",\n\t\t\t\tlocal->startAddress, local->endAddress,\n\t\t\t\tlocal->reg, local->name, local->descriptor);\n\t\t}\n\t}\n\n\tfor (reg = 0; reg < regsz; reg++) {\n\t\tif (debug_locals[reg].live) {\n\t\t\tif (debug_locals[reg].signature) {\n\t\t\t\trbin->cb_printf (\n\t\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s \"\n\t\t\t\t\t\"%s\\n\",\n\t\t\t\t\tdebug_locals[reg].startAddress,\n\t\t\t\t\tinsns_size, reg, debug_locals[reg].name,\n\t\t\t\t\tdebug_locals[reg].descriptor,\n\t\t\t\t\tdebug_locals[reg].signature);\n\t\t\t} else {\n\t\t\t\trbin->cb_printf (\n\t\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s\"\n\t\t\t\t\t\"\\n\",\n\t\t\t\t\tdebug_locals[reg].startAddress,\n\t\t\t\t\tinsns_size, reg, debug_locals[reg].name,\n\t\t\t\t\tdebug_locals[reg].descriptor);\n\t\t\t}\n\t\t}\n\t}\n\tr_list_free (debug_positions);\n\tr_list_free (emitted_debug_locals);\n\tr_list_free (params);\n}", "func_src_after": "static void dex_parse_debug_item(RBinFile *binfile, RBinDexObj *bin,\n\t\t\t\t  RBinDexClass *c, int MI, int MA, int paddr, int ins_size,\n\t\t\t\t  int insns_size, char *class_name, int regsz,\n\t\t\t\t  int debug_info_off) {\n\tstruct r_bin_t *rbin = binfile->rbin;\n\tconst ut8 *p4 = r_buf_get_at (binfile->buf, debug_info_off, NULL);\n\tconst ut8 *p4_end = p4 + binfile->buf->length - debug_info_off;\n\tut64 line_start;\n\tut64 parameters_size;\n\tut64 param_type_idx;\n\tut16 argReg = regsz - ins_size;\n\tut64 source_file_idx = c->source_file;\n\tRList *params, *debug_positions, *emitted_debug_locals = NULL; \n\tbool keep = true;\n\tif (argReg > regsz) {\n\t\treturn; // this return breaks tests\n\t}\n\tp4 = r_uleb128 (p4, p4_end - p4, &line_start);\n\tp4 = r_uleb128 (p4, p4_end - p4, &parameters_size);\n\t// TODO: check when we should use source_file\n\t// The state machine consists of five registers\n\tut32 address = 0;\n\tut32 line = line_start;\n\tif (!(debug_positions = r_list_newf ((RListFree)free))) {\n\t\treturn;\t\n\t}\n\tif (!(emitted_debug_locals = r_list_newf ((RListFree)free))) {\n\t\tr_list_free (debug_positions);\n\t\treturn;\n\t}\n\n\tstruct dex_debug_local_t debug_locals[regsz];\n\tmemset (debug_locals, 0, sizeof (struct dex_debug_local_t) * regsz);\n\tif (!(MA & 0x0008)) {\n\t\tdebug_locals[argReg].name = \"this\";\n\t\tdebug_locals[argReg].descriptor = r_str_newf(\"%s;\", class_name);\n\t\tdebug_locals[argReg].startAddress = 0;\n\t\tdebug_locals[argReg].signature = NULL;\n\t\tdebug_locals[argReg].live = true;\n\t\targReg++;\n\t}\n\tif (!(params = dex_method_signature2 (bin, MI))) {\n\t\tr_list_free (debug_positions);\n\t\tr_list_free (emitted_debug_locals);\n\t\treturn;\n\t}\n\n\tRListIter *iter = r_list_iterator (params);\n\tchar *name;\n\tchar *type;\n\tint reg;\n\n\tr_list_foreach (params, iter, type) {\n\t\tif ((argReg >= regsz) || !type || parameters_size <= 0) {\n\t\t\tr_list_free (debug_positions);\n\t\t\tr_list_free (params);\n\t\t\tr_list_free (emitted_debug_locals);\n\t\t\treturn;\n\t\t}\n\t\tp4 = r_uleb128 (p4, p4_end - p4, &param_type_idx); // read uleb128p1\n\t\tparam_type_idx -= 1;\n\t\tname = getstr (bin, param_type_idx);\n\t\treg = argReg;\n\t\tswitch (type[0]) {\n\t\tcase 'D':\n\t\tcase 'J':\n\t\t\targReg += 2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\targReg += 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (name) {\n\t\t\tdebug_locals[reg].name = name;\n\t\t\tdebug_locals[reg].descriptor = type;\n\t\t\tdebug_locals[reg].signature = NULL;\n\t\t\tdebug_locals[reg].startAddress = address;\n\t\t\tdebug_locals[reg].live = true;\n\t\t}\n\t\t--parameters_size;\n\t}\n\n\tif (p4 <= 0) {\n\t\treturn;\n\t}\n\tut8 opcode = *(p4++) & 0xff;\n\twhile (keep) {\n\t\tswitch (opcode) {\n\t\tcase 0x0: // DBG_END_SEQUENCE\n\t\t\tkeep = false;\n\t\t\tbreak;\n\t\tcase 0x1: // DBG_ADVANCE_PC\n\t\t\t{\n\t\t\tut64 addr_diff;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &addr_diff);\n\t\t\taddress += addr_diff;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x2: // DBG_ADVANCE_LINE\n\t\t\t{\n\t\t\tst64 line_diff = r_sleb128 (&p4, p4_end);\n\t\t\tline += line_diff;\n\t\t\t}\n\t\t\tbreak;\t\n\t\tcase 0x3: // DBG_START_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tut64 name_idx;\n\t\t\tut64 type_idx;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &name_idx); \n\t\t\tname_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &type_idx); \n\t\t\ttype_idx -= 1;\n\t\t\tif (register_num >= regsz) {\n\t\t\t\tr_list_free (debug_positions);\n\t\t\t\tr_list_free (params);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\t// Emit what was previously there, if anything\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\t\t\tdebug_locals[register_num].name = getstr (bin, name_idx);\n\t\t\tdebug_locals[register_num].descriptor = dex_type_descriptor (bin, type_idx);\n\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\tdebug_locals[register_num].signature = NULL;\n\t\t\tdebug_locals[register_num].live = true;\n\t\t\t//eprintf(\"DBG_START_LOCAL %x %x %x\\n\", register_num, name_idx, type_idx);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x4: //DBG_START_LOCAL_EXTENDED\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tut64 name_idx;\n\t\t\tut64 type_idx;\n\t\t\tut64 sig_idx;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &name_idx);\n\t\t\tname_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &type_idx);\n\t\t\ttype_idx -= 1;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &sig_idx);\n\t\t\tsig_idx -= 1;\n\t\t\tif (register_num >= regsz) {\n\t\t\t\tr_list_free (debug_positions);\n\t\t\t\tr_list_free (params);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// Emit what was previously there, if anything\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\n\t\t\tdebug_locals[register_num].name = getstr (bin, name_idx);\n\t\t\tdebug_locals[register_num].descriptor = dex_type_descriptor (bin, type_idx);\n\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\tdebug_locals[register_num].signature = getstr (bin, sig_idx);\n\t\t\tdebug_locals[register_num].live = true;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x5: // DBG_END_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\t// emitLocalCbIfLive\n\t\t\tif (debug_locals[register_num].live) {\n\t\t\t\tstruct dex_debug_local_t *local = malloc (\n\t\t\t\t\tsizeof (struct dex_debug_local_t));\n\t\t\t\tif (!local) {\n\t\t\t\t\tkeep = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlocal->name = debug_locals[register_num].name;\n\t\t\t\tlocal->descriptor = debug_locals[register_num].descriptor;\n\t\t\t\tlocal->startAddress = debug_locals[register_num].startAddress;\n\t\t\t\tlocal->signature = debug_locals[register_num].signature;\n\t\t\t\tlocal->live = true;\n\t\t\t\tlocal->reg = register_num;\n\t\t\t\tlocal->endAddress = address;\n\t\t\t\tr_list_append (emitted_debug_locals, local);\n\t\t\t}\n\t\t\tdebug_locals[register_num].live = false;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x6: // DBG_RESTART_LOCAL\n\t\t\t{\n\t\t\tut64 register_num;\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &register_num);\n\t\t\tif (!debug_locals[register_num].live) {\n\t\t\t\tdebug_locals[register_num].startAddress = address;\n\t\t\t\tdebug_locals[register_num].live = true;\n\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x7: //DBG_SET_PROLOGUE_END\n\t\t\tbreak;\n\t\tcase 0x8: //DBG_SET_PROLOGUE_BEGIN\n\t\t\tbreak;\n\t\tcase 0x9:\n\t\t\t{\n\t\t\tp4 = r_uleb128 (p4, p4_end - p4, &source_file_idx);\n\t\t\tsource_file_idx--;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t{\n\t\t\tint adjusted_opcode = opcode - 0x0a;\n\t\t\taddress += (adjusted_opcode / 15);\n\t\t\tline += -4 + (adjusted_opcode % 15);\n\t\t\tstruct dex_debug_position_t *position =\n\t\t\t\tmalloc (sizeof (struct dex_debug_position_t));\n\t\t\tif (!position) {\n\t\t\t\tkeep = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tposition->source_file_idx = source_file_idx;\n\t\t\tposition->address = address;\n\t\t\tposition->line = line;\n\t\t\tr_list_append (debug_positions, position);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\topcode = *(p4++) & 0xff;\n\t}\n\n\tif (!binfile->sdb_addrinfo) {\n\t\tbinfile->sdb_addrinfo = sdb_new0 ();\n\t}\n\n\tchar *fileline;\n\tchar offset[64];\n\tchar *offset_ptr;\n\n\tRListIter *iter1;\n\tstruct dex_debug_position_t *pos;\n\tr_list_foreach (debug_positions, iter1, pos) {\n\t\tfileline = r_str_newf (\"%s|%\"PFMT64d, getstr (bin, pos->source_file_idx), pos->line);\n\t\toffset_ptr = sdb_itoa (pos->address + paddr, offset, 16);\n\t\tsdb_set (binfile->sdb_addrinfo, offset_ptr, fileline, 0);\n\t\tsdb_set (binfile->sdb_addrinfo, fileline, offset_ptr, 0);\n\t}\n\n\tif (!dexdump) {\n\t\tr_list_free (debug_positions);\n\t\tr_list_free (emitted_debug_locals);\n\t\tr_list_free (params);\n\t\treturn;\n\t}\n\n\tRListIter *iter2;\n\tstruct dex_debug_position_t *position;\n\n\trbin->cb_printf (\"      positions     :\\n\");\n\tr_list_foreach (debug_positions, iter2, position) {\n\t\trbin->cb_printf (\"        0x%04llx line=%llu\\n\",\n\t\t\t\t position->address, position->line);\n\t}\n\n\trbin->cb_printf (\"      locals        :\\n\");\n\n\tRListIter *iter3;\n\tstruct dex_debug_local_t *local;\n\tr_list_foreach (emitted_debug_locals, iter3, local) {\n\t\tif (local->signature) {\n\t\t\trbin->cb_printf (\n\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s %s\\n\",\n\t\t\t\tlocal->startAddress, local->endAddress,\n\t\t\t\tlocal->reg, local->name, local->descriptor,\n\t\t\t\tlocal->signature);\n\t\t} else {\n\t\t\trbin->cb_printf (\n\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s\\n\",\n\t\t\t\tlocal->startAddress, local->endAddress,\n\t\t\t\tlocal->reg, local->name, local->descriptor);\n\t\t}\n\t}\n\n\tfor (reg = 0; reg < regsz; reg++) {\n\t\tif (debug_locals[reg].live) {\n\t\t\tif (debug_locals[reg].signature) {\n\t\t\t\trbin->cb_printf (\n\t\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s \"\n\t\t\t\t\t\"%s\\n\",\n\t\t\t\t\tdebug_locals[reg].startAddress,\n\t\t\t\t\tinsns_size, reg, debug_locals[reg].name,\n\t\t\t\t\tdebug_locals[reg].descriptor,\n\t\t\t\t\tdebug_locals[reg].signature);\n\t\t\t} else {\n\t\t\t\trbin->cb_printf (\n\t\t\t\t\t\"        0x%04x - 0x%04x reg=%d %s %s\"\n\t\t\t\t\t\"\\n\",\n\t\t\t\t\tdebug_locals[reg].startAddress,\n\t\t\t\t\tinsns_size, reg, debug_locals[reg].name,\n\t\t\t\t\tdebug_locals[reg].descriptor);\n\t\t\t}\n\t\t}\n\t}\n\tr_list_free (debug_positions);\n\tr_list_free (emitted_debug_locals);\n\tr_list_free (params);\n}", "commit_link": "github.com/radare/radare2/commit/252afb1cff9676f3ae1f341a28448bf2c8b6e308", "file_name": "libr/bin/p/bin_dex.c", "vul_type": "cwe-476", "description": "Write a C function to parse debug information from a DEX file in C."}
