{"func_name": "avpriv_ac3_parse_header", "func_src_before": "int avpriv_ac3_parse_header(AC3HeaderInfo **phdr, const uint8_t *buf,\n                            size_t size)\n{\n    GetBitContext gb;\n    AC3HeaderInfo *hdr;\n    int err;\n\n    if (!*phdr)\n        *phdr = av_mallocz(sizeof(AC3HeaderInfo));\n    if (!*phdr)\n        return AVERROR(ENOMEM);\n    hdr = *phdr;\n\n    init_get_bits8(&gb, buf, size);\n    err = ff_ac3_parse_header(&gb, hdr);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n\n    return get_bits_count(&gb);\n}", "func_src_after": "int avpriv_ac3_parse_header(AC3HeaderInfo **phdr, const uint8_t *buf,\n                            size_t size)\n{\n    GetBitContext gb;\n    AC3HeaderInfo *hdr;\n    int err;\n\n    if (!*phdr)\n        *phdr = av_mallocz(sizeof(AC3HeaderInfo));\n    if (!*phdr)\n        return AVERROR(ENOMEM);\n    hdr = *phdr;\n\n    err = init_get_bits8(&gb, buf, size);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n    err = ff_ac3_parse_header(&gb, hdr);\n    if (err < 0)\n        return AVERROR_INVALIDDATA;\n\n    return get_bits_count(&gb);\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/00e8181bd97c834fe60751b0c511d4bb97875f78", "file_name": "libavcodec/ac3_parser.c", "vul_type": "cwe-476", "description": "Write a C function named `avpriv_ac3_parse_header` that initializes a bit context and parses the AC3 header from a buffer, returning the number of bits read."}
{"func_name": "__get_data_block", "func_src_before": "static int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}", "func_src_after": "static int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = (u64)map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/b86e33075ed1909d8002745b56ecf73b833db143", "file_name": "fs/f2fs/data.c", "vul_type": "cwe-190", "description": "Write a C function named `__get_data_block` that maps blocks for a given inode in a filesystem, handling buffer head and block mapping with error checking."}
{"func_name": "testNonExistentFile", "func_src_before": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    non_existent = tempfile.mktemp(suffix=\".json\")\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "func_src_after": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 80, "char_end": 131, "line": "    non_existent = tempfile.mktemp(suffix=\".json\")\n"}], "added": [{"line_no": 3, "char_start": 80, "char_end": 156, "line": "    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 83, "char_end": 86, "chars": " _,"}, {"char_start": 113, "char_end": 114, "chars": "s"}, {"char_start": 134, "char_end": 155, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c63f6333a78ddf89ecd502d926fb877b8dce4f0f", "file_name": "upgrade_schema_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420335872\nChange-Id: I331ec2544a08d3cc3063a74af342cceae655b3dc", "description": "Write a Python unit test that checks if a custom converter raises an IOError when attempting to convert a non-existent JSON file."}
{"func_name": "self.register_user_defined_tag_type", "func_src_before": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.load(config_source)\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "func_src_after": "    def self.register_user_defined_tag_type(config_source)\n      config = YAML.safe_load(config_source, [Symbol])\n      check_validity_of_config(config)\n      TagType.register(registered_tag_name = config['tag_name'].to_sym,\n                       config['tag'],\n                       config['iteration_tag'],\n                       config['fallback_tag'],\n                       config['remove_indent'] || false)\n      registered_tag_name\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 59, "char_end": 99, "line": "      config = YAML.load(config_source)\n"}], "added": [{"line_no": 2, "char_start": 59, "char_end": 114, "line": "      config = YAML.safe_load(config_source, [Symbol])\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 79, "char_end": 84, "chars": "safe_"}, {"char_start": 102, "char_end": 112, "chars": ", [Symbol]"}]}, "commit_link": "github.com/nico-hn/AdHocTemplate/commit/4bc4ed79a2c45d64df03029bd05c3a426f5df020", "file_name": "parser.rb", "vul_type": "cwe-502", "commit_msg": "use YAML.safe_load() instead of .load()", "parent_commit": "f602247a29214ffce7e8d24caf690c2a420c3e99", "description": "Write a Ruby method to register a user-defined tag type from a YAML configuration source."}
{"func_name": "getCommentsLike", "func_src_before": "    def getCommentsLike(self,commentid):\n        sqlText=\"select userid from comment_like where commentid=%d\"%(commentid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;", "func_src_after": "    def getCommentsLike(self,commentid):\n        sqlText=\"select userid from comment_like where commentid=%s\"\n        params=[commentid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;", "commit_link": "github.com/ShaominLi/Twitter_project/commit/5329d91f9e569c95184053c8e7ef596949c33ce9", "file_name": "modules/comment.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch user IDs who liked a comment by its ID from a database."}
{"func_name": "update_title", "func_src_before": "  def update_title(self, title = None):\n    if (not self.title):\n      self.title = title\n\n    # This will fall to a sql injection \n    sql = \"UPDATE jdk_entries SET title = '\" + self.title + \"'\" + \\\n          \"WHERE jdk_entries.id = '\" + self.entry_id + \"';\" \n\n    db_execute(sql)\n    \n    self.update_date_modified()\n\n    return None", "func_src_after": "  def update_title(self, title = None):\n    if (not self.title):\n      self.title = title\n\n    quote_tuple = self.title, self.entry_id\n\n    # This will fall to a sql injection \n    sql = \"UPDATE jdk_entries SET title = ?\" + \\\n          \"WHERE jdk_entries.id = ?;\" \n\n    db_execute(sql, quote_tuple)\n    \n    self.update_date_modified()\n\n    return None", "commit_link": "github.com/peterlebrun/jdk/commit/000238566fbe55ba09676c3d57af04ae207235ae", "file_name": "entry.py", "vul_type": "cwe-089", "description": "Write a Python function to update the title of a database entry with basic SQL injection vulnerability and a version with parameterized query protection."}
{"func_name": "kmod_module_new_from_name", "func_src_before": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize((char *)name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "func_src_after": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize(name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 244, "char_end": 299, "line": "\tmodname_normalize((char *)name, name_norm, &namelen);\n"}], "added": [{"line_no": 12, "char_start": 244, "char_end": 291, "line": "\tmodname_normalize(name, name_norm, &namelen);\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 271, "chars": "(char *)"}], "added": []}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to create or reference a module by name in a given context."}
{"func_name": "render_page_name", "func_src_before": "@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )", "func_src_after": "@app.route('/<page_name>')\ndef render_page_name(page_name):\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    wiki_page = query.namedresult()\n    has_content = False\n    page_is_taken = False\n    if len(wiki_page) < 1:\n        content = \"\"\n    else:\n        page_is_taken = True\n        content = wiki_page[0].content\n    if len(content) > 0:\n        has_content = True\n    else:\n        pass\n    content = markdown.markdown(wiki_linkify(content))\n    return render_template(\n        'pageholder.html',\n        page_is_taken = page_is_taken,\n        page_name = page_name,\n        markdown = markdown,\n        wiki_linkify = wiki_linkify,\n        has_content = has_content,\n        content = content\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Create a Python Flask route to display a wiki page by its name, fetching the latest content from a database and rendering it with Markdown."}
{"func_name": "edit_coordinator", "func_src_before": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "func_src_after": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json_for_html(),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "commit_link": "github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "file_name": "apps/oozie/src/oozie/views/editor2.py", "vul_type": "cwe-079", "description": "In Python, write a view function `edit_coordinator` that checks access permissions, fetches coordinator and workflow details, and renders them to a template."}
{"func_name": "displaySearchHeading", "func_src_before": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.innerHTML = \"Search results for: \" + query;\n    }", "func_src_after": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.textContent = \"Search results for: \" + query;\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 107, "char_end": 167, "line": "        heading.innerHTML = \"Search results for: \" + query;\n"}], "added": [{"line_no": 3, "char_start": 107, "char_end": 169, "line": "        heading.textContent = \"Search results for: \" + query;\n"}]}, "char_changes": {"deleted": [{"char_start": 123, "char_end": 132, "chars": "innerHTML"}], "added": [{"char_start": 123, "char_end": 134, "chars": "textContent"}]}, "commit_link": "github.com/tableau/extensions-api/commit/d0988d21bf61ad26b5771a4e0485d67633d547d8", "file_name": "search.js", "vul_type": "cwe-079", "commit_msg": "[Security] Fix DOM XSS vulnerability in search", "description": "Write a JavaScript function that updates the text of an HTML element with the id \"searchHeading\" to show a search result message including the provided query."}
{"func_name": "_get_vdisk_fc_mappings", "func_src_before": "    def _get_vdisk_fc_mappings(self, vdisk_name):\n        \"\"\"Return FlashCopy mappings that this vdisk is associated with.\"\"\"\n\n        ssh_cmd = 'svcinfo lsvdiskfcmappings -nohdr %s' % vdisk_name\n        out, err = self._run_ssh(ssh_cmd)\n\n        mapping_ids = []\n        if (len(out.strip())):\n            lines = out.strip().split('\\n')\n            mapping_ids = [line.split()[0] for line in lines]\n        return mapping_ids", "func_src_after": "    def _get_vdisk_fc_mappings(self, vdisk_name):\n        \"\"\"Return FlashCopy mappings that this vdisk is associated with.\"\"\"\n\n        ssh_cmd = ['svcinfo', 'lsvdiskfcmappings', '-nohdr', vdisk_name]\n        out, err = self._run_ssh(ssh_cmd)\n\n        mapping_ids = []\n        if (len(out.strip())):\n            lines = out.strip().split('\\n')\n            mapping_ids = [line.split()[0] for line in lines]\n        return mapping_ids", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to fetch and return the IDs of FlashCopy mappings for a given virtual disk using SSH commands."}
{"func_name": "sign_up_page", "func_src_before": "def sign_up_page():\n  if request.method == 'POST':\n    username = request.form['username']\n    password = request.form['password']\n    hashed_password = pwd_context.encrypt(password)\n    email = request.form['email']\n    id = 1\n\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n\n      query = \"\"\"\n            INSERT INTO USERS (USERNAME, PASSWORD, EMAIL) \n            VALUES ('%s', '%s', '%s')\"\"\" % (username, hashed_password, email)\n\n      cursor.execute(query)\n\n      connection.commit()\n    return render_template('home.html')\n\n  else:\n    return render_template('sign_up.html')", "func_src_after": "def sign_up_page():\n  if request.method == 'POST':\n    username = request.form['username']\n    password = request.form['password']\n    hashed_password = pwd_context.encrypt(password)\n    email = request.form['email']\n    id = 1\n\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n\n      query = \"\"\"\n            INSERT INTO USERS (USERNAME, PASSWORD, EMAIL) \n            VALUES (%s, %s, %s)\"\"\"\n\n      cursor.execute(query, (username, hashed_password, email))\n\n      connection.commit()\n    return render_template('home.html')\n\n  else:\n    return render_template('sign_up.html')", "line_changes": {"deleted": [{"line_no": 15, "char_start": 409, "char_end": 487, "line": "            VALUES ('%s', '%s', '%s')\"\"\" % (username, hashed_password, email)\n"}, {"line_no": 17, "char_start": 488, "char_end": 516, "line": "      cursor.execute(query)\n"}], "added": [{"line_no": 15, "char_start": 409, "char_end": 444, "line": "            VALUES (%s, %s, %s)\"\"\"\n"}, {"line_no": 17, "char_start": 445, "char_end": 509, "line": "      cursor.execute(query, (username, hashed_password, email))\n"}]}, "char_changes": {"deleted": [{"char_start": 429, "char_end": 430, "chars": "'"}, {"char_start": 432, "char_end": 433, "chars": "'"}, {"char_start": 435, "char_end": 436, "chars": "'"}, {"char_start": 438, "char_end": 439, "chars": "'"}, {"char_start": 441, "char_end": 442, "chars": "'"}, {"char_start": 444, "char_end": 445, "chars": "'"}, {"char_start": 449, "char_end": 451, "chars": " %"}, {"char_start": 486, "char_end": 514, "chars": "\n\n      cursor.execute(query"}], "added": [{"char_start": 443, "char_end": 472, "chars": "\n\n      cursor.execute(query,"}]}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/252c65001a21f332da50e7d898d07285b3abd655", "file_name": "sign_up.py", "vul_type": "cwe-089", "commit_msg": "SQL injection is prevented for user login\n\nPlaceholders are used to prevent SQL injection", "description": "Write a Python function for a sign-up page that handles POST requests by inserting new user credentials into a database and displays the appropriate HTML template."}
{"func_name": "get_current_state", "func_src_before": "def get_current_state(chat_id):\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__))+\"\\\\bases\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"select * from users where chat_id = '\" + str(chat_id) + \"'\")\n    name = conn.fetchone()\n    if name != None:\n        return name[4]\n    else:\n        return False\n    settings.close()", "func_src_after": "def get_current_state(chat_id):\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__))+\"\\\\bases\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"select * from users where chat_id = ?\", (str(chat_id),))\n    name = conn.fetchone()\n    if name != None:\n        return name[4]\n    else:\n        return False\n    settings.close()", "commit_link": "github.com/lissrbay/codeforces_bot/commit/cc7f5143445a0030b1149ac60a65b1b1b9c92a90", "file_name": "bot.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch the fifth column value from a 'users' table for a given 'chat_id' in an SQLite database, returning False if not found."}
{"func_name": "render", "func_src_before": "    render()\n    {\n        const rows = [];\n        if (this.props.story)\n        {\n            const hasBackgrounds = this.props.story.backgrounds.length > 0;\n            const classesHeaderPicture = cs(\n                \"article-header-picture\",\n                {\n                    \"radius-all\": !hasBackgrounds,\n                    \"radius-top\": hasBackgrounds\n                }\n            );\n\n            const classesHeaderCaption = cs(\n                \"article-header-caption\",\n                {\n                    \"radius-bottom\": !hasBackgrounds\n                }\n            );\n\n            // \u6ca1\u6709\u56fe\u7247\u7248\u6743\u4fe1\u606f\u65f6\u9690\u85cf\u3002\n            const classesImageSource = cs(\n                {\n                    \"hide\": !this.props.story.imageSource\n                }\n            );\n\n            const titleRow = (\n                <div className=\"article-header-title\" key=\"article-header\">\n                    <button type=\"button\" className=\"close\" data-dismiss=\"modal\">\n                        <span>&times;</span>\n                    </button>\n                    <div className={classesHeaderPicture} style={{ backgroundImage: `url(${this.props.story.image})` }}>\n                        <div className={classesHeaderCaption}>\n                            <a href={this.props.story.shareURL} target=\"_blank\">\n                                <h3>{this.props.story.title}</h3>\n                            </a>\n                            <a\n                                className={classesImageSource}\n                                href={`https://www.google.com/search?q=${this.props.story.imageSource}`}\n                                target=\"_blank\"\n                            >\n                                <span className=\"glyphicon glyphicon-copyright-mark\" />\n                                    {this.props.story.imageSource}\n                            </a>\n                        </div>\n                    </div>\n                </div>\n            );\n            rows.push(titleRow);\n\n            if (hasBackgrounds)\n                                    {\n                const backgroundRows = map(this.props.story.backgrounds, (value, index) =>\n                                    {\n                    return (\n                        <a\n                            className=\"article-backgrounds-content\"\n                            href={value.href}\n                            target=\"_blank\"\n                            key={`background-${index}`}\n                        >\n                            <h4>{`${value.title}\uff1a${value.text}`}</h4>\n                        </a>\n                    );\n                });\n\n                rows.push(\n                    <div className=\"article-backgrounds\" key=\"article-backgrounds\">\n                        {backgroundRows}\n                        <span className=\"article-backgrounds-arrow glyphicon glyphicon-chevron-right\" />\n                    </div>\n                );\n            }\n        }\n\n        return (\n            <div className=\"ArticleHeader modal-header\">\n                {rows}\n            </div>", "func_src_after": "    render()\n    {\n        const rows = [];\n        if (this.props.story)\n        {\n            const hasBackgrounds = this.props.story.backgrounds.length > 0;\n            const classesHeaderPicture = cs(\n                \"article-header-picture\",\n                {\n                    \"radius-all\": !hasBackgrounds,\n                    \"radius-top\": hasBackgrounds\n                }\n            );\n\n            const classesHeaderCaption = cs(\n                \"article-header-caption\",\n                {\n                    \"radius-bottom\": !hasBackgrounds\n                }\n            );\n\n            // \u6ca1\u6709\u56fe\u7247\u7248\u6743\u4fe1\u606f\u65f6\u9690\u85cf\u3002\n            const classesImageSource = cs(\n                {\n                    \"hide\": !this.props.story.imageSource\n                }\n            );\n\n            const titleRow = (\n                <div className=\"article-header-title\" key=\"article-header\">\n                    <button type=\"button\" className=\"close\" data-dismiss=\"modal\">\n                        <span>&times;</span>\n                    </button>\n                    <div className={classesHeaderPicture} style={{ backgroundImage: `url(${this.props.story.image})` }}>\n                        <div className={classesHeaderCaption}>\n                            <a href={this.props.story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\n                                <h3>{this.props.story.title}</h3>\n                            </a>\n                            <a\n                                className={classesImageSource}\n                                href={`https://www.google.com/search?q=${this.props.story.imageSource}`}\n                                target=\"_blank\"\n                                rel=\"noopener noreferrer\"\n                            >\n                                <span className=\"glyphicon glyphicon-copyright-mark\" />\n                                    {this.props.story.imageSource}\n                            </a>\n                        </div>\n                    </div>\n                </div>\n            );\n            rows.push(titleRow);\n\n            if (hasBackgrounds)\n                                    {\n                const backgroundRows = map(this.props.story.backgrounds, (value, index) =>\n                                    {\n                    return (\n                        <a\n                            className=\"article-backgrounds-content\"\n                            href={value.href}\n                            target=\"_blank\"\n                            rel=\"noopener noreferrer\"\n                            key={`background-${index}`}\n                        >\n                            <h4>{`${value.title}\uff1a${value.text}`}</h4>\n                        </a>\n                    );\n                });\n\n                rows.push(\n                    <div className=\"article-backgrounds\" key=\"article-backgrounds\">\n                        {backgroundRows}\n                        <span className=\"article-backgrounds-arrow glyphicon glyphicon-chevron-right\" />\n                    </div>\n                );\n            }\n        }\n\n        return (\n            <div className=\"ArticleHeader modal-header\">\n                {rows}\n            </div>", "line_changes": {"deleted": [{"line_no": 36, "char_start": 1220, "char_end": 1301, "line": "                            <a href={this.props.story.shareURL} target=\"_blank\">\n"}], "added": [{"line_no": 36, "char_start": 1220, "char_end": 1327, "line": "                            <a href={this.props.story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\n"}, {"line_no": 43, "char_start": 1673, "char_end": 1731, "line": "                                rel=\"noopener noreferrer\"\n"}, {"line_no": 63, "char_start": 2492, "char_end": 2546, "line": "                            rel=\"noopener noreferrer\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1299, "char_end": 1325, "chars": " rel=\"noopener noreferrer\""}, {"char_start": 1673, "char_end": 1731, "chars": "                                rel=\"noopener noreferrer\"\n"}, {"char_start": 2492, "char_end": 2546, "chars": "                            rel=\"noopener noreferrer\"\n"}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5e310be94fd9adfaa058c7abe3ab2515b16f128", "file_name": "ArticleView.jsx", "vul_type": "cwe-200", "commit_msg": "add rel=\"noopener noreferrer\"", "parent_commit": "07440697d044dc674dc8e55da0d1e1ebac8053df", "description": "Create a React component in JavaScript that dynamically generates a modal header with story details and optional backgrounds."}
{"func_name": "getGameID", "func_src_before": "def getGameID(ID):\n\tdb.execute(\"SELECT * FROM games WHERE ID = %i\" % ID)\n\tID = db.fetchone()\n\treturn ID", "func_src_after": "def getGameID(ID):\n\tdb.execute(\"SELECT * FROM games WHERE ID = ?\", ID)\n\tID = db.fetchone()\n\treturn ID", "commit_link": "github.com/iScrE4m/XLeague/commit/59cab6e5fd8bd5e47f2418a7c71cb1d4e3cad0d2", "file_name": "plugins/database.py", "vul_type": "cwe-089", "description": "Write a Python function named `getGameID` that retrieves a game's record from a database by its ID using parameterized queries."}
{"func_name": "_execute_command_and_parse_attributes", "func_src_before": "    def _execute_command_and_parse_attributes(self, ssh_cmd):\n        \"\"\"Execute command on the Storwize/SVC and parse attributes.\n\n        Exception is raised if the information from the system\n        can not be obtained.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _execute_command_and_parse_attributes: '\n                    ' command %s') % ssh_cmd)\n\n        try:\n            out, err = self._run_ssh(ssh_cmd)\n        except exception.ProcessExecutionError as e:\n            # Didn't get details from the storage, return None\n            LOG.error(_('CLI Exception output:\\n command: %(cmd)s\\n '\n                        'stdout: %(out)s\\n stderr: %(err)s') %\n                      {'cmd': ssh_cmd,\n                       'out': e.stdout,\n                       'err': e.stderr})\n            return None\n\n        self._assert_ssh_return(len(out),\n                                '_execute_command_and_parse_attributes',\n                                ssh_cmd, out, err)\n        attributes = {}\n        for attrib_line in out.split('\\n'):\n            # If '!' not found, return the string and two empty strings\n            attrib_name, foo, attrib_value = attrib_line.partition('!')\n            if attrib_name is not None and len(attrib_name.strip()):\n                attributes[attrib_name] = attrib_value\n\n        LOG.debug(_('leave: _execute_command_and_parse_attributes:\\n'\n                    'command: %(cmd)s\\n'\n                    'attributes: %(attr)s')\n                  % {'cmd': ssh_cmd,\n                     'attr': str(attributes)})\n\n        return attributes", "func_src_after": "    def _execute_command_and_parse_attributes(self, ssh_cmd):\n        \"\"\"Execute command on the Storwize/SVC and parse attributes.\n\n        Exception is raised if the information from the system\n        can not be obtained.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _execute_command_and_parse_attributes: '\n                    ' command %s') % str(ssh_cmd))\n\n        try:\n            out, err = self._run_ssh(ssh_cmd)\n        except exception.ProcessExecutionError as e:\n            # Didn't get details from the storage, return None\n            LOG.error(_('CLI Exception output:\\n command: %(cmd)s\\n '\n                        'stdout: %(out)s\\n stderr: %(err)s') %\n                      {'cmd': ssh_cmd,\n                       'out': e.stdout,\n                       'err': e.stderr})\n            return None\n\n        self._assert_ssh_return(len(out),\n                                '_execute_command_and_parse_attributes',\n                                ssh_cmd, out, err)\n        attributes = {}\n        for attrib_line in out.split('\\n'):\n            # If '!' not found, return the string and two empty strings\n            attrib_name, foo, attrib_value = attrib_line.partition('!')\n            if attrib_name is not None and len(attrib_name.strip()):\n                attributes[attrib_name] = attrib_value\n\n        LOG.debug(_('leave: _execute_command_and_parse_attributes:\\n'\n                    'command: %(cmd)s\\n'\n                    'attributes: %(attr)s')\n                  % {'cmd': str(ssh_cmd),\n                     'attr': str(attributes)})\n\n        return attributes", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "In Python, write a function to execute an SSH command, parse the output into attributes, and handle any execution errors."}
{"func_name": "update_user", "func_src_before": "def update_user(username, chat_id, last_update):\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    cursor_settings = settings.cursor()\n    cursor_settings.execute(\"select last_problem from users where chat_id = '\" + str(chat_id) + \"'\")\n    update_eq = cursor_settings.fetchone()\n    cursor_settings.execute(\"select * from last_update_problemset\")\n    update_base = cursor_settings.fetchone()\n    last_problem = update_base[0]\n    if update_eq[0] != update_base[0]:\n        cursor2.execute(\"SELECT * FROM problems\")\n        x = cursor2.fetchone()\n        while x != None:\n            cursor.execute(\"select * from result where problem = '\" + str(x[0]) + \"' and diff = '\" + str(x[1]) + \"'\")\n            x2 = cursor.fetchone()\n            if x2 == None:\n                cursor.execute(\"insert into result values (?, ?, ? )\", (x[0], x[1], \"NULL\"))\n            last_problem = x\n            x = cursor2.fetchone()\n        conn2.close()\n        settings.close()\n    if len(last_problem) == 2:\n        last_problem = last_problem[0] + last_problem[1]\n\n    url = 'http://codeforces.com/submissions/' + username\n    r = requests.get(url)\n    max_page = 1\n    soup = BeautifulSoup(r.text, \"lxml\")\n\n    for link in soup.find_all(attrs={\"class\": \"page-index\"}):\n        s = link.find('a')\n        s2 = s.get(\"href\").split('/')\n        max_page = max(max_page, int(s2[4]))\n\n    v = False\n    r = requests.get('http://codeforces.com/submissions/' + username + '/page/0')\n    soup = BeautifulSoup(r.text, \"lxml\")\n    last_try_new = soup.find(attrs={\"class\": \"status-small\"})\n    last_try_new = str(last_try_new).split()\n    last_try_new = str(last_try_new[2]) + str(last_try_new[3])\n    for i in range(1, max_page + 1):\n        r = requests.get('http://codeforces.com/submissions/' + username + '/page/' + str(i))\n        soup = BeautifulSoup(r.text, \"lxml\")\n        count = 0\n        j = 0\n        ver = soup.find_all(attrs={\"class\": \"submissionVerdictWrapper\"})\n        last_try = soup.find_all(attrs={\"class\": \"status-small\"})\n        for link in soup.find_all('a'):\n            last_try_date = str(last_try[j]).split()\n            last_try_date = str(last_try_date[2]) + str(last_try_date[3])\n            if last_try_date == last_update:\n                v = True\n                break\n            s = link.get('href')\n            if s != None and s.find('/problemset') != -1:\n                s = s.split('/')\n                if len(s) == 5:\n                    s2 = str(ver[count]).split()\n                    s2 = s2[5].split('\\\"')\n                    count += 1\n                    j += 1\n                    cursor.execute(\"select * from result where problem = '\" + s[3] + \"'and diff = '\" + s[4] + \"'\")\n                    x = cursor.fetchone()\n                    if s2[1] == 'OK' and x != None:\n                        cursor.execute(\n                            \"update result set verdict = '\" + s2[1] + \"' where problem = '\" + s[3] + \"' and diff = '\" +\n                            s[4] + \"'\")\n                    if x[2] != 'OK':\n                        cursor.execute(\n                            \"update result set verdict = '\" + s2[1] + \"' where problem = '\" + s[3] + \"' and diff = '\" +\n                            s[4] + \"'\")\n        if v:\n            break\n\n    conn.commit()\n    conn.close()\n\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"update users set username = '\" + str(username) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n    conn.execute(\"update users set last_update = '\" + str(last_try_new) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n    conn.execute(\"update users set last_problem = '\" + str(last_problem) + \"' where chat_id = '\" + str(chat_id) + \"'\")\n\n    settings.commit()\n    settings.close()", "func_src_after": "def update_user(username, chat_id, last_update):\n    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\users\\\\\" + username + '.db')\n    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\\\cf.db')\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    cursor = conn.cursor()\n    cursor2 = conn2.cursor()\n    cursor_settings = settings.cursor()\n    cursor_settings.execute(\"select last_problem from users where chat_id = ?\", (str(chat_id), ))\n    update_eq = cursor_settings.fetchone()\n    cursor_settings.execute(\"select * from last_update_problemset\")\n    update_base = cursor_settings.fetchone()\n    last_problem = update_base[0]\n    if update_eq[0] != update_base[0]:\n        cursor2.execute(\"SELECT * FROM problems\")\n        x = cursor2.fetchone()\n        while x != None:\n            cursor.execute(\"select * from result where problem = ? and diff = ?\", (str(x[0]), str(x[1])))\n            x2 = cursor.fetchone()\n            if x2 == None:\n                cursor.execute(\"insert into result values (?, ?, ? )\", (x[0], x[1], \"NULL\"))\n            last_problem = x\n            x = cursor2.fetchone()\n        conn2.close()\n        settings.close()\n    if len(last_problem) == 2:\n        last_problem = last_problem[0] + last_problem[1]\n\n    url = 'http://codeforces.com/submissions/' + username\n    r = requests.get(url)\n    max_page = 1\n    soup = BeautifulSoup(r.text, \"lxml\")\n\n    for link in soup.find_all(attrs={\"class\": \"page-index\"}):\n        s = link.find('a')\n        s2 = s.get(\"href\").split('/')\n        max_page = max(max_page, int(s2[4]))\n\n    v = False\n    r = requests.get('http://codeforces.com/submissions/' + username + '/page/0')\n    soup = BeautifulSoup(r.text, \"lxml\")\n    last_try_new = soup.find(attrs={\"class\": \"status-small\"})\n    last_try_new = str(last_try_new).split()\n    last_try_new = str(last_try_new[2]) + str(last_try_new[3])\n    for i in range(1, max_page + 1):\n        r = requests.get('http://codeforces.com/submissions/' + username + '/page/' + str(i))\n        soup = BeautifulSoup(r.text, \"lxml\")\n        count = 0\n        j = 0\n        ver = soup.find_all(attrs={\"class\": \"submissionVerdictWrapper\"})\n        last_try = soup.find_all(attrs={\"class\": \"status-small\"})\n        for link in soup.find_all('a'):\n            last_try_date = str(last_try[j]).split()\n            last_try_date = str(last_try_date[2]) + str(last_try_date[3])\n            if last_try_date == last_update:\n                v = True\n                break\n            s = link.get('href')\n            if s != None and s.find('/problemset') != -1:\n                s = s.split('/')\n                if len(s) == 5:\n                    s2 = str(ver[count]).split()\n                    s2 = s2[5].split('\\\"')\n                    count += 1\n                    j += 1\n                    cursor.execute(\"select * from result where problem = ? and diff = ?\", (s[3], s[4]))\n                    x = cursor.fetchone()\n                    if s2[1] == 'OK' and x != None:\n                        cursor.execute(\"update result set verdict = ? where problem = ? and diff = ?\", (s2[1], s[3], s[4]))\n                    if x[2] != 'OK':\n                        cursor.execute(\"update result set verdict = ? where problem = ? and diff = ?\", (s2[1], s[3], s[4]))\n        if v:\n            break\n\n    conn.commit()\n    conn.close()\n\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"update users set username = ? where chat_id = ?\", (str(username), str(chat_id)))\n    conn.execute(\"update users set last_update = ? where chat_id = ?\", (str(last_try_new), str(chat_id)))\n    conn.execute(\"update users set last_problem = ? where chat_id = ?\", (str(last_problem), str(chat_id)))\n\n    settings.commit()\n    settings.close()", "commit_link": "github.com/lissrbay/codeforces_bot/commit/cc7f5143445a0030b1149ac60a65b1b1b9c92a90", "file_name": "bases/update.py", "vul_type": "cwe-089", "description": "Write a Python function to update a user's problem-solving status in a database using web scraping and SQLite."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "vault_decrypt", "func_src_before": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)", "func_src_after": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    aes = do_crypto_setup(mp)\n    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 70, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 70, "char_end": 122, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 4, "char_start": 122, "char_end": 190, "line": "    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 80, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 80, "char_end": 147, "line": "    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 120, "chars": "iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv"}, {"char_start": 146, "char_end": 147, "chars": "d"}, {"char_start": 149, "char_end": 150, "chars": "3"}], "added": [{"char_start": 54, "char_end": 55, "chars": "a"}, {"char_start": 60, "char_end": 78, "chars": "do_crypto_setup(mp"}, {"char_start": 104, "char_end": 105, "chars": "a"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_decrypt` that decrypts a list of ciphertexts using a provided master password and a fixed-size vault."}
{"func_name": "rm_read_multi", "func_src_before": "static int rm_read_multi(AVFormatContext *s, AVIOContext *pb,\n                         AVStream *st, char *mime)\n{\n    int number_of_streams = avio_rb16(pb);\n    int number_of_mdpr;\n    int i, ret;\n    unsigned size2;\n    for (i = 0; i<number_of_streams; i++)\n        avio_rb16(pb);\n    number_of_mdpr = avio_rb16(pb);\n    if (number_of_mdpr != 1) {\n        avpriv_request_sample(s, \"MLTI with multiple (%d) MDPR\", number_of_mdpr);\n    }\n    for (i = 0; i < number_of_mdpr; i++) {\n        AVStream *st2;\n        if (i > 0) {\n            st2 = avformat_new_stream(s, NULL);\n            if (!st2) {\n                ret = AVERROR(ENOMEM);\n                return ret;\n            }\n            st2->id = st->id + (i<<16);\n            st2->codecpar->bit_rate = st->codecpar->bit_rate;\n            st2->start_time = st->start_time;\n            st2->duration   = st->duration;\n            st2->codecpar->codec_type = AVMEDIA_TYPE_DATA;\n            st2->priv_data = ff_rm_alloc_rmstream();\n            if (!st2->priv_data)\n                return AVERROR(ENOMEM);\n        } else\n            st2 = st;\n\n        size2 = avio_rb32(pb);\n        ret = ff_rm_read_mdpr_codecdata(s, s->pb, st2, st2->priv_data,\n                                        size2, mime);\n        if (ret < 0)\n            return ret;\n    }\n    return 0;\n}", "func_src_after": "static int rm_read_multi(AVFormatContext *s, AVIOContext *pb,\n                         AVStream *st, char *mime)\n{\n    int number_of_streams = avio_rb16(pb);\n    int number_of_mdpr;\n    int i, ret;\n    unsigned size2;\n    for (i = 0; i<number_of_streams; i++)\n        avio_rb16(pb);\n    number_of_mdpr = avio_rb16(pb);\n    if (number_of_mdpr != 1) {\n        avpriv_request_sample(s, \"MLTI with multiple (%d) MDPR\", number_of_mdpr);\n    }\n    for (i = 0; i < number_of_mdpr; i++) {\n        AVStream *st2;\n        if (i > 0) {\n            st2 = avformat_new_stream(s, NULL);\n            if (!st2) {\n                ret = AVERROR(ENOMEM);\n                return ret;\n            }\n            st2->id = st->id + (i<<16);\n            st2->codecpar->bit_rate = st->codecpar->bit_rate;\n            st2->start_time = st->start_time;\n            st2->duration   = st->duration;\n            st2->codecpar->codec_type = AVMEDIA_TYPE_DATA;\n            st2->priv_data = ff_rm_alloc_rmstream();\n            if (!st2->priv_data)\n                return AVERROR(ENOMEM);\n        } else\n            st2 = st;\n\n        size2 = avio_rb32(pb);\n        ret = ff_rm_read_mdpr_codecdata(s, s->pb, st2, st2->priv_data,\n                                        size2, NULL);\n        if (ret < 0)\n            return ret;\n    }\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/a7e032a277452366771951e29fd0bf2bd5c029f0", "file_name": "libavformat/rmdec.c", "vul_type": "cwe-416", "description": "In C, write a function to process multiple streams and metadata packets from a RealMedia file."}
{"func_name": "mem_iconveha", "func_src_before": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "func_src_after": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      if (to_codeset_suffixed == NULL)\n        {\n          errno = ENOMEM;\n          return -1;\n        }\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "commit_link": "github.com/coreutils/gnulib/commit/fce9817d48c97339c3f66a92e72faba8e69d405c", "file_name": "lib/striconveha.c", "vul_type": "cwe-476", "description": "Write a C function `mem_iconveha` that converts a string from one character encoding to another, with an option for transliteration."}
{"func_name": "_drain_to_working_set", "func_src_before": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with open(self.working_set_filename, 'wb') as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        line = base64.b64encode(pickle.dumps({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }))\n                        work_file.write(line)\n                        work_file.write(b'\\n')\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )", "func_src_after": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        pickle.dump({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }, work_file)\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )\n\n                pickle.dump('eof', work_file)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 365, "char_end": 434, "line": "            with open(self.working_set_filename, 'wb') as work_file:\n"}, {"line_no": 27, "char_start": 839, "char_end": 902, "line": "                        line = base64.b64encode(pickle.dumps({\n"}, {"line_no": 34, "char_start": 1228, "char_end": 1256, "line": "                        }))\n"}, {"line_no": 35, "char_start": 1256, "char_end": 1302, "line": "                        work_file.write(line)\n"}, {"line_no": 36, "char_start": 1302, "char_end": 1349, "line": "                        work_file.write(b'\\n')\n"}], "added": [{"line_no": 12, "char_start": 365, "char_end": 456, "line": "            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n"}, {"line_no": 27, "char_start": 861, "char_end": 899, "line": "                        pickle.dump({\n"}, {"line_no": 34, "char_start": 1225, "char_end": 1263, "line": "                        }, work_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 863, "char_end": 887, "chars": "line = base64.b64encode("}, {"char_start": 898, "char_end": 899, "chars": "s"}, {"char_start": 1253, "char_end": 1347, "chars": "))\n                        work_file.write(line)\n                        work_file.write(b'\\n'"}], "added": [{"char_start": 382, "char_end": 387, "chars": "gzip."}, {"char_start": 423, "char_end": 440, "chars": ", compresslevel=1"}, {"char_start": 1250, "char_end": 1261, "chars": ", work_file"}, {"char_start": 2534, "char_end": 2581, "chars": "\n\n                pickle.dump('eof', work_file)"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to export a dataset to a file, with optional data filtering and deletion after export."}
{"func_name": "extend_volume", "func_src_before": "    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(\"growvv -f %s %sg\" % (volume_name, growth_size),\n                          None)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)", "func_src_after": "    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to resize a storage volume by increasing its size and log the operation."}
{"func_name": "main", "func_src_before": "def main(_):\n  sess = tf.Session()\n\n  # Construct the TensorFlow network.\n  ph_float = tf.placeholder(tf.float32, name=\"ph_float\")\n  x = tf.transpose(ph_float, name=\"x\")\n  v = tf.Variable(np.array([[-2.0], [-3.0], [6.0]], dtype=np.float32), name=\"v\")\n  m = tf.constant(\n      np.array([[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]]),\n      dtype=tf.float32,\n      name=\"m\")\n  y = tf.matmul(m, x, name=\"y\")\n  z = tf.matmul(m, v, name=\"z\")\n\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n\n  if FLAGS.error == \"shape_mismatch\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0], [1.0], [2.0]])}))\n  elif FLAGS.error == \"uninitialized_variable\":\n    print(sess.run(z))\n  elif FLAGS.error == \"no_error\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0, 1.0, 2.0]])}))\n  else:\n    raise ValueError(\"Unrecognized error type: \" + FLAGS.error)", "func_src_after": "def main(_):\n  sess = tf.Session()\n\n  # Construct the TensorFlow network.\n  ph_float = tf.placeholder(tf.float32, name=\"ph_float\")\n  x = tf.transpose(ph_float, name=\"x\")\n  v = tf.Variable(np.array([[-2.0], [-3.0], [6.0]], dtype=np.float32), name=\"v\")\n  m = tf.constant(\n      np.array([[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]]),\n      dtype=tf.float32,\n      name=\"m\")\n  y = tf.matmul(m, x, name=\"y\")\n  z = tf.matmul(m, v, name=\"z\")\n\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    sess = tf_debug.LocalCLIDebugWrapperSession(\n        sess, ui_type=FLAGS.ui_type, config_file_path=config_file_path)\n\n  if FLAGS.error == \"shape_mismatch\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0], [1.0], [2.0]])}))\n  elif FLAGS.error == \"uninitialized_variable\":\n    print(sess.run(z))\n  elif FLAGS.error == \"no_error\":\n    print(sess.run(y, feed_dict={ph_float: np.array([[0.0, 1.0, 2.0]])}))\n  else:\n    raise ValueError(\"Unrecognized error type: \" + FLAGS.error)", "line_changes": {"deleted": [{"line_no": 16, "char_start": 447, "char_end": 472, "line": "    config_file_path = (\n"}, {"line_no": 17, "char_start": 472, "char_end": 513, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 18, "char_start": 513, "char_end": 564, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 16, "char_start": 447, "char_end": 484, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 18, "char_start": 537, "char_end": 599, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 19, "char_start": 599, "char_end": 609, "line": "    else:\n"}, {"line_no": 20, "char_start": 609, "char_end": 639, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 469, "char_end": 479, "chars": " (\n       "}, {"char_start": 517, "char_end": 541, "chars": "    if FLAGS.use_random_"}, {"char_start": 553, "char_end": 557, "chars": "else"}, {"char_start": 562, "char_end": 563, "chars": ")"}], "added": [{"char_start": 451, "char_end": 546, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 576, "char_end": 577, "chars": "s"}, {"char_start": 603, "char_end": 615, "chars": "else:\n      "}, {"char_start": 622, "char_end": 627, "chars": "file_"}, {"char_start": 632, "char_end": 633, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c84353cb969723e97b9fd9fd346860bc15864bb7", "file_name": "debug_errors.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359156\nChange-Id: I992e93ed8423eef87bfcfc84b0c877131d6f916d", "description": "Write a Python script using TensorFlow to construct a neural network, run sessions with different error scenarios, and optionally enable debugging."}
{"func_name": "html_content", "func_src_before": "    @property\n    async def html_content(self):\n        content = await self.content\n        if not content:\n            return ''\n        return markdown(content)", "func_src_after": "    @property\n    async def html_content(self):\n        content = markupsafe.escape(await self.content)\n        if not content:\n            return ''\n        return markdown(content)", "commit_link": "github.com/dongweiming/lyanna/commit/fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "file_name": "models/comment.py", "vul_type": "cwe-079", "description": "Generate a Python async property method that converts stored content to HTML, handling empty content and ensuring safe markup."}
{"func_name": "_create_vdisk", "func_src_before": "    def _create_vdisk(self, name, size, units, opts):\n        \"\"\"Create a new vdisk.\"\"\"\n\n        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)\n\n        model_update = None\n        autoex = '-autoexpand' if opts['autoexpand'] else ''\n        easytier = '-easytier on' if opts['easytier'] else '-easytier off'\n\n        # Set space-efficient options\n        if opts['rsize'] == -1:\n            ssh_cmd_se_opt = ''\n        else:\n            ssh_cmd_se_opt = (\n                '-rsize %(rsize)d%% %(autoex)s -warning %(warn)d%%' %\n                {'rsize': opts['rsize'],\n                 'autoex': autoex,\n                 'warn': opts['warning']})\n            if opts['compression']:\n                ssh_cmd_se_opt = ssh_cmd_se_opt + ' -compressed'\n            else:\n                ssh_cmd_se_opt = ssh_cmd_se_opt + (\n                    ' -grainsize %d' % opts['grainsize'])\n\n        ssh_cmd = ('svctask mkvdisk -name %(name)s -mdiskgrp %(mdiskgrp)s '\n                   '-iogrp 0 -size %(size)s -unit '\n                   '%(unit)s %(easytier)s %(ssh_cmd_se_opt)s'\n                   % {'name': name,\n                   'mdiskgrp': self.configuration.storwize_svc_volpool_name,\n                   'size': size, 'unit': units, 'easytier': easytier,\n                   'ssh_cmd_se_opt': ssh_cmd_se_opt})\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return(len(out.strip()), '_create_vdisk',\n                                ssh_cmd, out, err)\n\n        # Ensure that the output is as expected\n        match_obj = re.search('Virtual Disk, id \\[([0-9]+)\\], '\n                              'successfully created', out)\n        # Make sure we got a \"successfully created\" message with vdisk id\n        self._driver_assert(\n            match_obj is not None,\n            _('_create_vdisk %(name)s - did not find '\n              'success message in CLI output.\\n '\n              'stdout: %(out)s\\n stderr: %(err)s')\n            % {'name': name, 'out': str(out), 'err': str(err)})\n\n        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)", "func_src_after": "    def _create_vdisk(self, name, size, units, opts):\n        \"\"\"Create a new vdisk.\"\"\"\n\n        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)\n\n        model_update = None\n        easytier = 'on' if opts['easytier'] else 'off'\n\n        # Set space-efficient options\n        if opts['rsize'] == -1:\n            ssh_cmd_se_opt = []\n        else:\n            ssh_cmd_se_opt = ['-rsize', '%s%%' % str(opts['rsize']),\n                              '-autoexpand', '-warning',\n                              '%s%%' % str(opts['warning'])]\n            if not opts['autoexpand']:\n                ssh_cmd_se_opt.remove('-autoexpand')\n\n            if opts['compression']:\n                ssh_cmd_se_opt.append('-compressed')\n            else:\n                ssh_cmd_se_opt.extend(['-grainsize', str(opts['grainsize'])])\n\n        ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp',\n                   self.configuration.storwize_svc_volpool_name,\n                   '-iogrp', '0', '-size', size, '-unit',\n                   units, '-easytier', easytier] + ssh_cmd_se_opt\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return(len(out.strip()), '_create_vdisk',\n                                ssh_cmd, out, err)\n\n        # Ensure that the output is as expected\n        match_obj = re.search('Virtual Disk, id \\[([0-9]+)\\], '\n                              'successfully created', out)\n        # Make sure we got a \"successfully created\" message with vdisk id\n        self._driver_assert(\n            match_obj is not None,\n            _('_create_vdisk %(name)s - did not find '\n              'success message in CLI output.\\n '\n              'stdout: %(out)s\\n stderr: %(err)s')\n            % {'name': name, 'out': str(out), 'err': str(err)})\n\n        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to create a virtual disk with options for size, auto-expansion, easy tier, and compression."}
{"func_name": "get_mapped_projects", "func_src_before": "    @staticmethod\n    def get_mapped_projects(user_id: int, preferred_locale: str) -> UserMappedProjectsDTO:\n        \"\"\" Get all projects a user has mapped on \"\"\"\n\n        # This query looks scary, but we're really just creating an outer join between the query that gets the\n        # counts of all mapped tasks and the query that gets counts of all validated tasks.  This is necessary to\n        # handle cases where users have only validated tasks on a project, or only mapped on a project.\n        sql = '''SELECT p.id,\n                        p.status,\n                        p.default_locale,\n                        c.mapped,\n                        c.validated,\n                        st_asgeojson(p.centroid)\n                   FROM projects p,\n                        (SELECT coalesce(v.project_id, m.project_id) project_id,\n                                coalesce(v.validated, 0) validated,\n                                coalesce(m.mapped, 0) mapped\n                          FROM (SELECT t.project_id,\n                                       count (t.validated_by) validated\n                                  FROM tasks t\n                                 WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = {0})\n                                   AND t.validated_by = {0}\n                                 GROUP BY t.project_id, t.validated_by) v\n                         FULL OUTER JOIN\n                        (SELECT t.project_id,\n                                count(t.mapped_by) mapped\n                           FROM tasks t\n                          WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = {0})\n                            AND t.mapped_by = {0}\n                          GROUP BY t.project_id, t.mapped_by) m\n                         ON v.project_id = m.project_id) c\n                   WHERE p.id = c.project_id ORDER BY p.id DESC'''.format(user_id)\n\n        results = db.engine.execute(sql)\n\n        if results.rowcount == 0:\n            raise NotFound()\n\n        mapped_projects_dto = UserMappedProjectsDTO()\n        for row in results:\n            mapped_project = MappedProject()\n            mapped_project.project_id = row[0]\n            mapped_project.status = ProjectStatus(row[1]).name\n            mapped_project.tasks_mapped = row[3]\n            mapped_project.tasks_validated = row[4]\n            mapped_project.centroid = geojson.loads(row[5])\n\n            project_info = ProjectInfo.get_dto_for_locale(row[0], preferred_locale, row[2])\n            mapped_project.name = project_info.name\n\n            mapped_projects_dto.mapped_projects.append(mapped_project)\n\n        return mapped_projects_dto", "func_src_after": "    @staticmethod\n    def get_mapped_projects(user_id: int, preferred_locale: str) -> UserMappedProjectsDTO:\n        \"\"\" Get all projects a user has mapped on \"\"\"\n\n        # This query looks scary, but we're really just creating an outer join between the query that gets the\n        # counts of all mapped tasks and the query that gets counts of all validated tasks.  This is necessary to\n        # handle cases where users have only validated tasks on a project, or only mapped on a project.\n        sql = '''SELECT p.id,\n                        p.status,\n                        p.default_locale,\n                        c.mapped,\n                        c.validated,\n                        st_asgeojson(p.centroid)\n                   FROM projects p,\n                        (SELECT coalesce(v.project_id, m.project_id) project_id,\n                                coalesce(v.validated, 0) validated,\n                                coalesce(m.mapped, 0) mapped\n                          FROM (SELECT t.project_id,\n                                       count (t.validated_by) validated\n                                  FROM tasks t\n                                 WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = :user_id)\n                                   AND t.validated_by = :user_id\n                                 GROUP BY t.project_id, t.validated_by) v\n                         FULL OUTER JOIN\n                        (SELECT t.project_id,\n                                count(t.mapped_by) mapped\n                           FROM tasks t\n                          WHERE t.project_id IN (SELECT unnest(projects_mapped) FROM users WHERE id = :user_id)\n                            AND t.mapped_by = :user_id\n                          GROUP BY t.project_id, t.mapped_by) m\n                         ON v.project_id = m.project_id) c\n                   WHERE p.id = c.project_id ORDER BY p.id DESC'''\n\n        results = db.engine.execute(text(sql), user_id=user_id)\n\n        if results.rowcount == 0:\n            raise NotFound()\n\n        mapped_projects_dto = UserMappedProjectsDTO()\n        for row in results:\n            mapped_project = MappedProject()\n            mapped_project.project_id = row[0]\n            mapped_project.status = ProjectStatus(row[1]).name\n            mapped_project.tasks_mapped = row[3]\n            mapped_project.tasks_validated = row[4]\n            mapped_project.centroid = geojson.loads(row[5])\n\n            project_info = ProjectInfo.get_dto_for_locale(row[0], preferred_locale, row[2])\n            mapped_project.name = project_info.name\n\n            mapped_projects_dto.mapped_projects.append(mapped_project)\n\n        return mapped_projects_dto", "commit_link": "github.com/hotosm/tasking-manager/commit/dee040a2d22b3c4d5e38e2dbf8c6b651ad4c241a", "file_name": "server/models/postgis/user.py", "vul_type": "cwe-089", "description": "In Python, write a method to retrieve a user's mapped projects with task counts and localization details."}
{"func_name": "_feed_input_sorters", "func_src_before": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with open(self.working_set_filename, 'rb') as work_file:\n            for line in work_file:\n                result = pickle.loads(base64.b64decode(line))\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "func_src_after": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with gzip.open(self.working_set_filename, 'rb') as work_file:\n            while True:\n                result = pickle.load(work_file)\n\n                if result == 'eof':\n                    break\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 60, "char_end": 125, "line": "        with open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 125, "char_end": 160, "line": "            for line in work_file:\n"}, {"line_no": 6, "char_start": 160, "char_end": 222, "line": "                result = pickle.loads(base64.b64decode(line))\n"}], "added": [{"line_no": 4, "char_start": 60, "char_end": 130, "line": "        with gzip.open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 130, "char_end": 154, "line": "            while True:\n"}, {"line_no": 6, "char_start": 154, "char_end": 202, "line": "                result = pickle.load(work_file)\n"}, {"line_no": 7, "char_start": 202, "char_end": 203, "line": "\n"}, {"line_no": 8, "char_start": 203, "char_end": 239, "line": "                if result == 'eof':\n"}, {"line_no": 9, "char_start": 239, "char_end": 265, "line": "                    break\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 157, "chars": "for line in work_fil"}, {"char_start": 196, "char_end": 221, "chars": "s(base64.b64decode(line))"}], "added": [{"char_start": 73, "char_end": 78, "chars": "gzip."}, {"char_start": 142, "char_end": 151, "chars": "while Tru"}, {"char_start": 190, "char_end": 264, "chars": "(work_file)\n\n                if result == 'eof':\n                    break"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to process and sort project results from a file, updating a sorter object for each project."}
{"func_name": "run", "func_src_before": "func run() error {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "func_src_after": "func run() error {\n\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tprivf, err := os.OpenFile(\"priv.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer privf.Close()\n\n\tprivblock := &pem.Block{\n\t\tType:  \"RSA PRIVATE KEY\",\n\t\tBytes: x509.MarshalPKCS1PrivateKey(priv),\n\t}\n\n\tif err := pem.Encode(privf, privblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpub, err := x509.MarshalPKIXPublicKey(priv.Public())\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\n\tpubf, err := os.OpenFile(\"pub.key\", os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)\n\tif err != nil {\n\t\tos.Remove(privf.Name())\n\t\treturn err\n\t}\n\tdefer pubf.Close()\n\n\tpubblock := &pem.Block{\n\t\tType:  \"PUBLIC KEY\",\n\t\tBytes: pub,\n\t}\n\n\tif err := pem.Encode(pubf, pubblock); err != nil {\n\t\tos.Remove(privf.Name())\n\t\tos.Remove(pubf.Name())\n\t\treturn err\n\t}\n\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 68, "line": "\tpriv, err := rsa.GenerateKey(rand.Reader, 1024)\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 81, "line": "\tpriv, err := rsa.GenerateMultiPrimeKey(rand.Reader, 3, 2048)\n"}]}, "char_changes": {"deleted": [{"char_start": 62, "char_end": 66, "chars": "1024"}], "added": [{"char_start": 45, "char_end": 55, "chars": "MultiPrime"}, {"char_start": 72, "char_end": 79, "chars": "3, 2048"}]}, "commit_link": "github.com/carl-mastrangelo/pixur/commit/d2bc8ec79aa4f2b68ec14a2d6cd5a305b6e05dd1", "file_name": "genkeys.go", "vul_type": "cwe-326", "commit_msg": "Use multiprime rsa keys, and bump to 2048 bits", "parent_commit": "547289bc91415ef039e318ce6b0b53b16b66998b", "description": "Write a Go function to generate an RSA key pair and save them to files."}
{"func_name": "testLoadingPythonSourceFileWithNonAsciiChars", "func_src_before": "  def testLoadingPythonSourceFileWithNonAsciiChars(self):\n    source_path = tempfile.mktemp()\n    with open(source_path, \"wb\") as source_file:\n      source_file.write(u\"print('\\U0001f642')\\n\".encode(\"utf-8\"))\n    source_lines, _ = source_utils.load_source(source_path)\n    self.assertEqual(source_lines, [u\"print('\\U0001f642')\", u\"\"])\n    # Clean up unrelated source file.\n    os.remove(source_path)", "func_src_after": "  def testLoadingPythonSourceFileWithNonAsciiChars(self):\n    fd, source_path = tempfile.mkstemp()\n    with open(fd, \"wb\") as source_file:\n      source_file.write(u\"print('\\U0001f642')\\n\".encode(\"utf-8\"))\n    source_lines, _ = source_utils.load_source(source_path)\n    self.assertEqual(source_lines, [u\"print('\\U0001f642')\", u\"\"])\n    # Clean up unrelated source file.\n    os.remove(source_path)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 58, "char_end": 94, "line": "    source_path = tempfile.mktemp()\n"}, {"line_no": 3, "char_start": 94, "char_end": 143, "line": "    with open(source_path, \"wb\") as source_file:\n"}], "added": [{"line_no": 2, "char_start": 58, "char_end": 99, "line": "    fd, source_path = tempfile.mkstemp()\n"}, {"line_no": 3, "char_start": 99, "char_end": 139, "line": "    with open(fd, \"wb\") as source_file:\n"}]}, "char_changes": {"deleted": [{"char_start": 108, "char_end": 119, "chars": "source_path"}], "added": [{"char_start": 61, "char_end": 65, "chars": " fd,"}, {"char_start": 91, "char_end": 92, "chars": "s"}, {"char_start": 113, "char_end": 115, "chars": "fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/3752cc4c6ba6b69f04f857c6047adde9e8487bd6", "file_name": "source_utils_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359237\nChange-Id: I7fa45e888deff612ca53a4f8610cfad8f28e9671", "description": "Write a Python function to test loading a temporary source file containing non-ASCII characters and then remove the file."}
{"func_name": "deleteKey", "func_src_before": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\n\tif re.search('[^a-zA-Z0-9]', token_data['key']):\n\t\traise FoxlockError(BAD_REQUEST, 'Invalid key requested')\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "func_src_after": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateKeyName(token_data['key'])\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "commit_link": "github.com/Mimickal/FoxLock/commit/7c665e556987f4e2c1a75e143a1e80ae066ad833", "file_name": "impl.py", "vul_type": "cwe-022", "description": "Write a Python function named `deleteKey` that removes a client's key file and handles the case where the key does not exist."}
{"func_name": "GetAvailablePort", "func_src_before": "func GetAvailablePort(t *testing.T) uint16 {\n\t// Retry has been added for windows as net.Listen can return a port that is not actually available. Details can be\n\t// found in https://github.com/docker/for-win/issues/3171 but to summarize Hyper-V will reserve ranges of ports\n\t// which do not show up under the \"netstat -ano\" but can only be found by\n\t// \"netsh interface ipv4 show excludedportrange protocol=tcp\".  We'll use []exclusions to hold those ranges and\n\t// retry if the port returned by GetAvailableLocalAddress falls in one of those them.\n\tvar exclusions []portpair\n\tportFound := false\n\tvar port string\n\tvar err error\n\tif runtime.GOOS == \"windows\" {\n\t\texclusions = getExclusionsList(t)\n\t}\n\n\tfor !portFound {\n\t\tendpoint := GetAvailableLocalAddress(t)\n\t\t_, port, err = net.SplitHostPort(endpoint)\n\t\trequire.NoError(t, err)\n\t\tportFound = true\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\tfor _, pair := range exclusions {\n\t\t\t\tif port >= pair.first && port <= pair.last {\n\t\t\t\t\tportFound = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tportInt, err := strconv.Atoi(port)\n\trequire.NoError(t, err)\n\n\treturn uint16(portInt)\n}", "func_src_after": "func GetAvailablePort(t *testing.T) uint16 {\n\t// Retry has been added for windows as net.Listen can return a port that is not actually available. Details can be\n\t// found in https://github.com/docker/for-win/issues/3171 but to summarize Hyper-V will reserve ranges of ports\n\t// which do not show up under the \"netstat -ano\" but can only be found by\n\t// \"netsh interface ipv4 show excludedportrange protocol=tcp\".  We'll use []exclusions to hold those ranges and\n\t// retry if the port returned by GetAvailableLocalAddress falls in one of those them.\n\tvar exclusions []portpair\n\tportFound := false\n\tvar port string\n\tvar err error\n\tif runtime.GOOS == \"windows\" {\n\t\texclusions = getExclusionsList(t)\n\t}\n\n\tfor !portFound {\n\t\tendpoint := GetAvailableLocalAddress(t)\n\t\t_, port, err = net.SplitHostPort(endpoint)\n\t\trequire.NoError(t, err)\n\t\tportFound = true\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\tfor _, pair := range exclusions {\n\t\t\t\tif port >= pair.first && port <= pair.last {\n\t\t\t\t\tportFound = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tportInt, err := strconv.ParseUint(port, 10, 16)\n\trequire.NoError(t, err)\n\n\treturn uint16(portInt)\n}", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1022, "char_end": 1058, "line": "\tportInt, err := strconv.Atoi(port)\n"}], "added": [{"line_no": 30, "char_start": 1022, "char_end": 1071, "line": "\tportInt, err := strconv.ParseUint(port, 10, 16)\n"}]}, "char_changes": {"deleted": [{"char_start": 1047, "char_end": 1056, "chars": "Atoi(port"}], "added": [{"char_start": 1047, "char_end": 1069, "chars": "ParseUint(port, 10, 16"}]}, "commit_link": "github.com/open-telemetry/opentelemetry-collector/commit/eb3601a05900f70e46c058facf16461efa7b09f0", "file_name": "testutil.go", "vul_type": "cwe-681", "commit_msg": "Avoid potential integer overflow (#4277)\n\nSigned-off-by: Bogdan Drutu <bogdandrutu@gmail.com>", "parent_commit": "db6d31e9acc546e043b7b5564377bd76998e74bd", "description": "Write a Go function that finds an available network port, with special handling for Windows due to reserved port ranges."}
{"func_name": "(anonymous)", "func_src_before": "  return buffer.join('');\n};\n\nvar FilesView = React.createClass({\n  onLoadMore: function(event) {\n    Model.LoadMore(this.props.repo);\n  },\n\n  render: function() {\n    var rev = this.props.rev,\n        repo = this.props.repo,\n        regexp = this.props.regexp,\n        matches = this.props.matches,\n        totalMatches = this.props.totalMatches;\n    var files = matches.map(function(match, index) {\n      var filename = match.Filename,\n          blocks = CoalesceMatches(match.Matches);\n      var matches = blocks.map(function(block) {\n        var lines = block.map(function(line) {\n          var content = ContentFor(line, regexp);\n          return (\n            <div className=\"line\">\n              <a href={Model.UrlToRepo(repo, filename, line.Number, rev)}\n                  className=\"lnum\"\n                  target=\"_blank\">{line.Number}</a>\n              <span className=\"lval\" dangerouslySetInnerHTML={{__html:content}} />\n            </div>\n          );\n        });\n\n        return (\n          <div className=\"match\">{lines}</div>", "func_src_after": "  return buffer.join('');\n};\n\nvar FilesView = React.createClass({\n  onLoadMore: function(event) {\n    Model.LoadMore(this.props.repo);\n  },\n\n  render: function() {\n    var rev = this.props.rev,\n        repo = this.props.repo,\n        regexp = this.props.regexp,\n        matches = this.props.matches,\n        totalMatches = this.props.totalMatches;\n    var files = matches.map(function(match, index) {\n      var filename = match.Filename,\n          blocks = CoalesceMatches(match.Matches);\n      var matches = blocks.map(function(block) {\n        var lines = block.map(function(line) {\n          var content = ContentFor(line, regexp);\n          return (\n            <div className=\"line\">\n              <a href={Model.UrlToRepo(repo, filename, line.Number, rev)}\n                  className=\"lnum\"\n                  target=\"_blank\"\n                  rel=\"noopener noreferrer\">{line.Number}</a>\n              <span className=\"lval\" dangerouslySetInnerHTML={{__html:content}} />\n            </div>\n          );\n        });\n\n        return (\n          <div className=\"match\">{lines}</div>\n        );\n      });", "line_changes": {"deleted": [{"line_no": 25, "char_start": 798, "char_end": 850, "line": "                  target=\"_blank\">{line.Number}</a>\n"}], "added": [{"line_no": 25, "char_start": 798, "char_end": 832, "line": "                  target=\"_blank\"\n"}, {"line_no": 26, "char_start": 832, "char_end": 894, "line": "                  rel=\"noopener noreferrer\">{line.Number}</a>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 831, "char_end": 875, "chars": "\n                  rel=\"noopener noreferrer\""}, {"char_start": 1085, "char_end": 1106, "chars": "\n        );\n      });"}]}, "commit_link": "github.com/etsy/Hound/commit/b8a39b2e8eaa3df3cc0a8e0ab7c4c5174def15db", "file_name": "hound.js", "vul_type": "cwe-200", "commit_msg": "Give repo links a target of blank (#404)\n\nAdd rel=\"noopener noreferrer\" to _blank links", "parent_commit": "ca5c7c8c1dc6753b0bbe2bdd0ad3c934969f7cf6", "description": "Create a React component in JavaScript that displays matched lines from a repository with links to the source."}
{"func_name": "get_last_active_users", "func_src_before": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "func_src_after": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT %s')\n\n        parameters = limit,\n\n        try:\n            cursor = db.execute_query(query, parameters)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch a specified number of the most recent active bot users from a database."}
{"func_name": "x86_decode_insn", "func_src_before": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}", "func_src_after": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative && likely(ctxt->memopp))\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}", "commit_link": "github.com/torvalds/linux/commit/d9092f52d7e61dd1557f2db2400ddb430e85937e", "file_name": "arch/x86/kvm/emulate.c", "vul_type": "cwe-476", "description": "Write a C function named `x86_decode_insn` that decodes an x86 instruction from a given context, instruction pointer, and length."}
{"func_name": "cut", "func_src_before": "    def cut(self, key):\n        try:\n            self.etcd.delete(os.path.join(self.namespace, key))\n        except etcd.EtcdKeyNotFound:\n            return False\n        except etcd.EtcdException as err:\n            log_error(\"Error removing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to cut key')\n        return True", "func_src_after": "    def cut(self, key):\n        try:\n            self.etcd.delete(self._absolute_key(key))\n        except etcd.EtcdKeyNotFound:\n            return False\n        except etcd.EtcdException as err:\n            log_error(\"Error removing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to cut key')\n        return True", "commit_link": "github.com/latchset/custodia/commit/785fc87f38b4811bc4ce43a0a9b2267ee7d500b4", "file_name": "custodia/store/etcdstore.py", "vul_type": "cwe-022", "description": "Write a Python function named `cut` that deletes a key from an etcd store and handles exceptions."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\t\tinput.keypress(function(event) {\n\t\t\t\t\t\tif(event.keyCode == 13) {\n\t\t\t\t\t\t\tevent.preventDefault();\n\t\t\t\t\t\t\tevent.stopPropagation();\n\t\t\t\t\t\t\tvar li=$(this).parent();\n\t\t\t\t\t\t\t$(this).remove();\n\t\t\t\t\t\t\tli.text('+ '+settings.createText);\n\t\t\t\t\t\t\tli.before(createItem(this));\n\t\t\t\t\t\t\tvar select=button.parent().next();\n\t\t\t\t\t\t\tselect.append($('<option selected=\"selected\" value=\"'+$(this).val()+'\">'+$(this).val()+'</option>'));\n\t\t\t\t\t\t\tli.prev().children('input').trigger('click');\n\t\t\t\t\t\t\tbutton.parent().data('preventHide',false);\n\t\t\t\t\t\t\tif(settings.createCallback){\n\t\t\t\t\t\t\t\tsettings.createCallback();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});", "func_src_after": "\t\t\t\t\tinput.keypress(function(event) {\n\t\t\t\t\t\tif(event.keyCode == 13) {\n\t\t\t\t\t\t\tevent.preventDefault();\n\t\t\t\t\t\t\tevent.stopPropagation();\n\t\t\t\t\t\t\tvar li=$(this).parent();\n\t\t\t\t\t\t\t$(this).remove();\n\t\t\t\t\t\t\tli.text('+ '+settings.createText);\n\t\t\t\t\t\t\tli.before(createItem(this));\n\t\t\t\t\t\t\tvar select=button.parent().next();\n\t\t\t\t\t\t\tvar option=$('<option selected=\"selected\"/>');\n\t\t\t\t\t\t\toption.attr('value',$(this).val());\n\t\t\t\t\t\t\toption.text($(this).val());\n\t\t\t\t\t\t\tselect.append(optione);\n\t\t\t\t\t\t\tli.prev().children('input').trigger('click');\n\t\t\t\t\t\t\tbutton.parent().data('preventHide',false);\n\t\t\t\t\t\t\tif(settings.createCallback){\n\t\t\t\t\t\t\t\tsettings.createCallback();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});", "line_changes": {"deleted": [{"line_no": 10, "char_start": 310, "char_end": 419, "line": "\t\t\t\t\t\t\tselect.append($('<option selected=\"selected\" value=\"'+$(this).val()+'\">'+$(this).val()+'</option>'));\n"}], "added": [{"line_no": 10, "char_start": 310, "char_end": 364, "line": "\t\t\t\t\t\t\tvar option=$('<option selected=\"selected\"/>');\n"}, {"line_no": 11, "char_start": 364, "char_end": 407, "line": "\t\t\t\t\t\t\toption.attr('value',$(this).val());\n"}, {"line_no": 12, "char_start": 407, "char_end": 442, "line": "\t\t\t\t\t\t\toption.text($(this).val());\n"}, {"line_no": 13, "char_start": 442, "char_end": 473, "line": "\t\t\t\t\t\t\tselect.append(optione);\n"}]}, "char_changes": {"deleted": [{"char_start": 317, "char_end": 331, "chars": "select.append("}, {"char_start": 361, "char_end": 362, "chars": " "}, {"char_start": 367, "char_end": 371, "chars": "=\"'+"}, {"char_start": 384, "char_end": 407, "chars": "+'\">'+$(this).val()+'</"}, {"char_start": 413, "char_end": 416, "chars": ">')"}], "added": [{"char_start": 317, "char_end": 328, "chars": "var option="}, {"char_start": 358, "char_end": 384, "chars": "/>');\n\t\t\t\t\t\t\toption.attr('"}, {"char_start": 389, "char_end": 391, "chars": "',"}, {"char_start": 404, "char_end": 463, "chars": ");\n\t\t\t\t\t\t\toption.text($(this).val());\n\t\t\t\t\t\t\tselect.append("}, {"char_start": 469, "char_end": 470, "chars": "e"}]}, "commit_link": "github.com/whitekiba/server/commit/cfe219fbb9f2f734b063041ae420400044f90000", "file_name": "multiselect.js", "vul_type": "cwe-079", "commit_msg": "fix potential xss in multiselect", "description": "Write a jQuery snippet that handles the enter key press on an input field to replace it with a list item and update a select element with a new option."}
{"func_name": "check", "func_src_before": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=\"%s\"' % current_num)\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "func_src_after": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=?', (current_num,))\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "commit_link": "github.com/lord63/a_bunch_of_code/commit/c0d67a1312306fd1257c354bfb5d6cac7643aa29", "file_name": "comics/check_comics.py", "vul_type": "cwe-089", "description": "Write a Python function named `check` that queries a SQLite database for a specific record and creates a table if it doesn't exist."}
{"func_name": "retrieve_playlist_by_id", "func_src_before": "def retrieve_playlist_by_id(id, db):\n    db.execute(\n        \"SELECT id, name, video_position from playlist WHERE id={id};\".format(id=id))\n    row = db.fetchone()\n    return row", "func_src_after": "def retrieve_playlist_by_id(id, db):\n    db.execute(\n        \"SELECT id, name, video_position from playlist WHERE id=%s;\", (id,))\n    row = db.fetchone()\n    return row", "commit_link": "github.com/Madmous/playlist/commit/666e52c5f0b8c1f4296e84471637033d9542a7a6", "file_name": "playlist/playlist_repository.py", "vul_type": "cwe-089", "description": "Write a Python function named `retrieve_playlist_by_id` that takes an ID and a database connection, executes a SQL query to fetch a playlist by its ID, and returns the result."}
{"func_name": "error", "func_src_before": "                error: function() {\n                    // Fail message\n                    $('#success').html(\"<div class='alert alert-danger'>\");\n                    $('#success > .alert-danger').html(\"<button type='button' class='close' data-dismiss='alert' aria-hidden='true'>&times;\")\n                        .append(\"</button>\");\n                    $('#success > .alert-danger').append(\"<strong>Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\");\n                    $('#success > .alert-danger').append('</div>');\n                    //clear all fields\n                    $('#contactForm').trigger(\"reset\");\n                },", "func_src_after": "                error: function() {\n                    // Fail message\n                    $('#success').html(\"<div class='alert alert-danger'>\");\n                    $('#success > .alert-danger').html(\"<button type='button' class='close' data-dismiss='alert' aria-hidden='true'>&times;\")\n                        .append(\"</button>\");\n                    $('#success > .alert-danger').append($(\"<strong>\").text(\"Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\"));\n                    $('#success > .alert-danger').append('</div>');\n                    //clear all fields\n                    $('#contactForm').trigger(\"reset\");\n                },", "line_changes": {"deleted": [{"line_no": 6, "char_start": 336, "char_end": 502, "line": "                    $('#success > .alert-danger').append(\"<strong>Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\");\n"}], "added": [{"line_no": 6, "char_start": 336, "char_end": 514, "line": "                    $('#success > .alert-danger').append($(\"<strong>\").text(\"Sorry \" + firstName + \", it seems that my mail server is not responding. Please try again later!\"));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 393, "char_end": 395, "chars": "$("}, {"char_start": 404, "char_end": 413, "chars": "\").text(\""}, {"char_start": 511, "char_end": 512, "chars": ")"}]}, "commit_link": "github.com/EmmavanKampen/What-sgood/commit/0d233641de67563a42ad58925dd6da7483062637", "file_name": "contact_me.js", "vul_type": "cwe-079", "commit_msg": "Fix xss issue", "description": "Write a JavaScript function to display an error message and reset a form when an AJAX request fails."}
{"func_name": "send_message", "func_src_before": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "func_src_after": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id = %s\"\"\", sender)\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "commit_link": "github.com/libracore/erpnext/commit/9acb885e60f77cd4e9ea8c98bdc39c18abcac731", "file_name": "erpnext/templates/utils.py", "vul_type": "cwe-089", "description": "Write a Python function in Frappe that handles incoming messages by creating leads, opportunities, and communications."}
{"func_name": "kvm_ioctl_create_device", "func_src_before": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tops->destroy(dev);\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\treturn ret;\n\t}\n\n\tkvm_get_kvm(kvm);\n\tcd->fd = ret;\n\treturn 0;\n}", "func_src_after": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tkvm_get_kvm(kvm);\n\tcd->fd = ret;\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/a0f1d21c1ccb1da66629627a74059dd7f5ac9c61", "file_name": "virt/kvm/kvm_main.c", "vul_type": "cwe-416", "description": "In C, write a function to handle the creation of a new KVM device, including memory allocation and error checking."}
{"func_name": "track_header", "func_src_before": "static int track_header(VividasDemuxContext *viv, AVFormatContext *s,  uint8_t *buf, int size)\n{\n    int i, j, ret;\n    int64_t off;\n    int val_1;\n    int num_video;\n    AVIOContext pb0, *pb = &pb0;\n\n    ffio_init_context(pb, buf, size, 0, NULL, NULL, NULL, NULL);\n\n    ffio_read_varlen(pb); // track_header_len\n    avio_r8(pb); // '1'\n\n    val_1 = ffio_read_varlen(pb);\n\n    for (i=0;i<val_1;i++) {\n        int c = avio_r8(pb);\n        if (avio_feof(pb))\n            return AVERROR_EOF;\n        for (j=0;j<c;j++) {\n            if (avio_feof(pb))\n                return AVERROR_EOF;\n            avio_r8(pb); // val_3\n            avio_r8(pb); // val_4\n        }\n    }\n\n    avio_r8(pb); // num_streams\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_5\n\n    avio_r8(pb); // '2'\n    num_video = avio_r8(pb);\n\n    avio_seek(pb, off, SEEK_SET);\n    if (num_video != 1) {\n        av_log(s, AV_LOG_ERROR, \"number of video tracks %d is not 1\\n\", num_video);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    for (i = 0; i < num_video; i++) {\n        AVStream *st = avformat_new_stream(s, NULL);\n        int num, den;\n\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;\n        st->codecpar->codec_id = AV_CODEC_ID_VP6;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb);\n        avio_r8(pb); // '3'\n        avio_r8(pb); // val_7\n        num = avio_rl32(pb); // frame_time\n        den = avio_rl32(pb); // time_base\n        avpriv_set_pts_info(st, 64, num, den);\n        st->nb_frames = avio_rl32(pb); // n frames\n        st->codecpar->width = avio_rl16(pb); // width\n        st->codecpar->height = avio_rl16(pb); // height\n        avio_r8(pb); // val_8\n        avio_rl32(pb); // val_9\n\n        avio_seek(pb, off, SEEK_SET);\n    }\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_10\n    avio_r8(pb); // '4'\n    viv->num_audio = avio_r8(pb);\n    avio_seek(pb, off, SEEK_SET);\n\n    if (viv->num_audio != 1)\n        av_log(s, AV_LOG_WARNING, \"number of audio tracks %d is not 1\\n\", viv->num_audio);\n\n    for(i=0;i<viv->num_audio;i++) {\n        int q;\n        AVStream *st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = num_video + i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_AUDIO;\n        st->codecpar->codec_id = AV_CODEC_ID_VORBIS;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb); // length\n        avio_r8(pb); // '5'\n        avio_r8(pb); //codec_id\n        avio_rl16(pb); //codec_subid\n        st->codecpar->channels = avio_rl16(pb); // channels\n        st->codecpar->sample_rate = avio_rl32(pb); // sample_rate\n        avio_seek(pb, 10, SEEK_CUR); // data_1\n        q = avio_r8(pb);\n        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 0;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n            ffio_read_varlen(pb); // len_3\n            num_data = avio_r8(pb);\n            for (j = 0; j < num_data; j++) {\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, 64 + xd_size + xd_size / 255);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                if (delta > data_len[j]) {\n                    return AVERROR_INVALIDDATA;\n                }\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;\n        }\n    }\n\n    return 0;\n}", "func_src_after": "static int track_header(VividasDemuxContext *viv, AVFormatContext *s,  uint8_t *buf, int size)\n{\n    int i, j, ret;\n    int64_t off;\n    int val_1;\n    int num_video;\n    AVIOContext pb0, *pb = &pb0;\n\n    ffio_init_context(pb, buf, size, 0, NULL, NULL, NULL, NULL);\n\n    ffio_read_varlen(pb); // track_header_len\n    avio_r8(pb); // '1'\n\n    val_1 = ffio_read_varlen(pb);\n\n    for (i=0;i<val_1;i++) {\n        int c = avio_r8(pb);\n        if (avio_feof(pb))\n            return AVERROR_EOF;\n        for (j=0;j<c;j++) {\n            if (avio_feof(pb))\n                return AVERROR_EOF;\n            avio_r8(pb); // val_3\n            avio_r8(pb); // val_4\n        }\n    }\n\n    avio_r8(pb); // num_streams\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_5\n\n    avio_r8(pb); // '2'\n    num_video = avio_r8(pb);\n\n    avio_seek(pb, off, SEEK_SET);\n    if (num_video != 1) {\n        av_log(s, AV_LOG_ERROR, \"number of video tracks %d is not 1\\n\", num_video);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    for (i = 0; i < num_video; i++) {\n        AVStream *st = avformat_new_stream(s, NULL);\n        int num, den;\n\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;\n        st->codecpar->codec_id = AV_CODEC_ID_VP6;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb);\n        avio_r8(pb); // '3'\n        avio_r8(pb); // val_7\n        num = avio_rl32(pb); // frame_time\n        den = avio_rl32(pb); // time_base\n        avpriv_set_pts_info(st, 64, num, den);\n        st->nb_frames = avio_rl32(pb); // n frames\n        st->codecpar->width = avio_rl16(pb); // width\n        st->codecpar->height = avio_rl16(pb); // height\n        avio_r8(pb); // val_8\n        avio_rl32(pb); // val_9\n\n        avio_seek(pb, off, SEEK_SET);\n    }\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_10\n    avio_r8(pb); // '4'\n    viv->num_audio = avio_r8(pb);\n    avio_seek(pb, off, SEEK_SET);\n\n    if (viv->num_audio != 1)\n        av_log(s, AV_LOG_WARNING, \"number of audio tracks %d is not 1\\n\", viv->num_audio);\n\n    for(i=0;i<viv->num_audio;i++) {\n        int q;\n        AVStream *st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = num_video + i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_AUDIO;\n        st->codecpar->codec_id = AV_CODEC_ID_VORBIS;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb); // length\n        avio_r8(pb); // '5'\n        avio_r8(pb); //codec_id\n        avio_rl16(pb); //codec_subid\n        st->codecpar->channels = avio_rl16(pb); // channels\n        st->codecpar->sample_rate = avio_rl32(pb); // sample_rate\n        avio_seek(pb, 10, SEEK_CUR); // data_1\n        q = avio_r8(pb);\n        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 1;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n            ffio_read_varlen(pb); // len_3\n            num_data = avio_r8(pb);\n            for (j = 0; j < num_data; j++) {\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len + 1 + len/255;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, xd_size);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                av_assert0(delta <= xd_size - offset);\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                av_assert0(data_len[j] <= xd_size - offset);\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;\n        }\n    }\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/27a99e2c7d450fef15594671eef4465c8a166bd7", "file_name": "libavformat/vividas.c", "vul_type": "cwe-787", "description": "Write a C function to parse video and audio track headers from a buffer in an AVFormatContext using FFmpeg libraries."}
{"func_name": "handle_file", "func_src_before": "def handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = text\n    m.cachedText = compile_markdown(text)\n    m.category = category\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = headline\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))", "func_src_after": "def handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = escape(text)\n    m.cachedText = compile_markdown(escape(text))\n    m.category = escape(category)\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = escape(headline)\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))", "commit_link": "github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "file_name": "c3shop/frontpage/management/mediatools/media_actions.py", "vul_type": "cwe-079", "description": "Write a Python function to handle media file uploads, including image resizing and metadata processing."}
{"func_name": "search", "func_src_before": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "func_src_after": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE ?\"\n\t\twhereSearchTerm += \"OR description LIKE ?\"\n\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + params[:query] + '%'\n\t\tend\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 168, "char_end": 224, "line": "  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 5, "char_start": 224, "char_end": 289, "line": "\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 6, "char_start": 289, "char_end": 351, "line": "\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 7, "char_start": 351, "char_end": 413, "line": "\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 8, "char_start": 413, "char_end": 477, "line": "\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 14, "char_start": 674, "char_end": 822, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 20, "char_start": 988, "char_end": 1096, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n"}, {"line_no": 26, "char_start": 1307, "char_end": 1457, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}], "added": [{"line_no": 4, "char_start": 168, "char_end": 204, "line": "  \twhereSearchTerm  = \"name LIKE ?\"\n"}, {"line_no": 5, "char_start": 204, "char_end": 249, "line": "\t\twhereSearchTerm += \"OR description LIKE ?\"\n"}, {"line_no": 6, "char_start": 249, "char_end": 291, "line": "\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n"}, {"line_no": 7, "char_start": 291, "char_end": 333, "line": "\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n"}, {"line_no": 8, "char_start": 333, "char_end": 377, "line": "\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n"}, {"line_no": 9, "char_start": 377, "char_end": 378, "line": "\n"}, {"line_no": 10, "char_start": 378, "char_end": 409, "line": "\t\tunless params[:query].empty?\n"}, {"line_no": 11, "char_start": 409, "char_end": 456, "line": "\t\t\tparams[:query] = '%' + params[:query] + '%'\n"}, {"line_no": 12, "char_start": 456, "char_end": 462, "line": "\t\tend\n"}, {"line_no": 18, "char_start": 659, "char_end": 887, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 24, "char_start": 1053, "char_end": 1241, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n"}, {"line_no": 30, "char_start": 1452, "char_end": 1682, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 222, "chars": "'%#{params[:query]}%'"}, {"char_start": 266, "char_end": 287, "chars": "'%#{params[:query]}%'"}, {"char_start": 328, "char_end": 349, "chars": "'%#{params[:query]}%'"}, {"char_start": 390, "char_end": 411, "chars": "'%#{params[:query]}%'"}, {"char_start": 454, "char_end": 458, "chars": "'%#{"}, {"char_start": 472, "char_end": 476, "chars": "}%'\""}], "added": [{"char_start": 201, "char_end": 202, "chars": "?"}, {"char_start": 246, "char_end": 247, "chars": "?"}, {"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 330, "char_end": 331, "chars": "?"}, {"char_start": 374, "char_end": 435, "chars": "?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + "}, {"char_start": 449, "char_end": 461, "chars": " + '%'\n\t\tend"}, {"char_start": 758, "char_end": 838, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1134, "char_end": 1214, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1553, "char_end": 1633, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}]}, "commit_link": "github.com/IOH/ioh-cover-maker/commit/ac636ca90521050a07b3f24ed4994594e369630e", "file_name": "posters_controller.rb", "vul_type": "cwe-089", "commit_msg": "prevent sql injection in search", "description": "Write a Ruby method to search and filter posters based on a query and state, returning the results as JSON."}
{"func_name": "AP4_AtomFactory::CreateAtomFromStream", "func_src_before": "AP4_AtomFactory::CreateAtomFromStream(AP4_ByteStream& stream, \n                                      AP4_UI32        type,\n                                      AP4_UI32        size_32,\n                                      AP4_UI64        size_64,\n                                      AP4_Atom*&      atom)\n{\n    bool atom_is_large = (size_32 == 1);\n    bool force_64 = (size_32==1 && ((size_64>>32) == 0));\n    \n    // create the atom\n    if (GetContext() == AP4_ATOM_TYPE_STSD) {\n        // sample entry\n        if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n        switch (type) {\n          case AP4_ATOM_TYPE_MP4A:\n            atom = new AP4_Mp4aSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MP4V:\n            atom = new AP4_Mp4vSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MP4S:\n            atom = new AP4_Mp4sSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ENCA:\n            atom = new AP4_EncaSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ENCV:\n            atom = new AP4_EncvSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_DRMS:\n            atom = new AP4_DrmsSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_DRMI:\n            atom = new AP4_DrmiSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_AVC1:\n          case AP4_ATOM_TYPE_AVC2:\n          case AP4_ATOM_TYPE_AVC3:\n          case AP4_ATOM_TYPE_AVC4:\n          case AP4_ATOM_TYPE_DVAV:\n          case AP4_ATOM_TYPE_DVA1:\n            atom = new AP4_AvcSampleEntry(type, size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_HEV1:\n          case AP4_ATOM_TYPE_HVC1:\n          case AP4_ATOM_TYPE_DVHE:\n          case AP4_ATOM_TYPE_DVH1:\n            atom = new AP4_HevcSampleEntry(type, size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ALAC:\n          case AP4_ATOM_TYPE_AC_3:\n          case AP4_ATOM_TYPE_EC_3:\n          case AP4_ATOM_TYPE_DTSC:\n          case AP4_ATOM_TYPE_DTSH:\n          case AP4_ATOM_TYPE_DTSL:\n          case AP4_ATOM_TYPE_DTSE:\n            atom = new AP4_AudioSampleEntry(type, size_32, stream, *this);\n            break;\n            \n          case AP4_ATOM_TYPE_RTP_:\n            atom = new AP4_RtpHintSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_STPP:\n            atom = new AP4_SubtitleSampleEntry(type, size_32, stream, *this);\n            break;\n\n          default: {\n            // try all the external type handlers\n            AP4_List<TypeHandler>::Item* handler_item = m_TypeHandlers.FirstItem();\n            while (handler_item) {\n                TypeHandler* handler = handler_item->GetData();\n                if (AP4_SUCCEEDED(handler->CreateAtom(type, size_32, stream, GetContext(), atom))) {\n                    break;\n                }\n                handler_item = handler_item->GetNext();\n            }\n\n            // no custom handler, create a generic entry\n            if (atom == NULL) {\n                atom = new AP4_UnknownSampleEntry(type, (AP4_UI32)size_64, stream);\n            }\n\n            break;\n          }\n        }\n    } else {\n        // regular atom\n        switch (type) {\n          case AP4_ATOM_TYPE_MOOV:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MoovAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MVHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MvhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MEHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MehdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MFHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MfhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TRAK:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrakAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_TREX:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrexAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HDLR:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HdlrAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TKHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TkhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TRUN:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrunAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFRA:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfraAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MFRO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MfroAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MDHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MdhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StsdAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_STSC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StscAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STCO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StcoAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_CO64:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_Co64Atom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSZ:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StszAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STZ2:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_Stz2Atom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STTS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SttsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_CTTS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_CttsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StssAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IODS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IodsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ESDS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_EsdsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_AVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AvccAtom::Create(size_32, stream);\n            break;\n            \n          case AP4_ATOM_TYPE_HVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HvccAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_DVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_DvccAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HVCE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HvccAtom::Create(size_32, stream);\n            atom->SetType(AP4_ATOM_TYPE_HVCE);\n            break;\n\n          case AP4_ATOM_TYPE_AVCE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AvccAtom::Create(size_32, stream);\n            atom->SetType(AP4_ATOM_TYPE_AVCE);\n            break;\n\n    #if !defined(AP4_CONFIG_MINI_BUILD)\n          case AP4_ATOM_TYPE_UUID: {\n              AP4_UI08 uuid[16];\n              AP4_Result result = stream.Read(uuid, 16);\n              if (AP4_FAILED(result)) return result;\n              \n              if (AP4_CompareMemory(uuid, AP4_UUID_PIFF_TRACK_ENCRYPTION_ATOM, 16) == 0) {\n                  atom = AP4_PiffTrackEncryptionAtom::Create((AP4_UI32)size_64, stream);\n              } else if (AP4_CompareMemory(uuid, AP4_UUID_PIFF_SAMPLE_ENCRYPTION_ATOM, 16) == 0) {\n                  atom = AP4_PiffSampleEncryptionAtom::Create((AP4_UI32)size_64, stream);\n              } else {\n                  atom = new AP4_UnknownUuidAtom(size_64, uuid, stream);\n              }\n              break;\n          }\n            \n          case AP4_ATOM_TYPE_8ID_:\n            atom = new AP4_NullTerminatedStringAtom(type, size_64, stream);\n            break;\n\n          case AP4_ATOM_TYPE_8BDL:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_8bdlAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_DREF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_DrefAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_URL:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_UrlAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ELST:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_ElstAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_VMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_VmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_NMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_NmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SthdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_FRMA:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_FrmaAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SCHM:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SchmAtom::Create(size_32, &m_ContextStack, stream);\n            break;\n\n          case AP4_ATOM_TYPE_FTYP:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_FtypAtom::Create(size_32, stream);\n            break;\n                        \n          case AP4_ATOM_TYPE_TIMS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TimsAtom::Create(size_32, stream);\n            break;\n     \n          case AP4_ATOM_TYPE_SDP_:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SdpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IKMS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IkmsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ISFM:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IsfmAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ISLT:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IsltAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ODHE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OdheAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_OHDR:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OhdrAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ODDA:\n            atom = AP4_OddaAtom::Create(size_64, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ODAF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OdafAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_GRPI:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_GrpiAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IPRO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IproAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_RTP_:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_RtpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFDT:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfdtAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TENC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TencAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SENC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SencAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SAIZ:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SaizAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SAIO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SaioAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_PDIN:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_PdinAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_BLOC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_BlocAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_AINF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AinfAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_PSSH:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_PsshAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SIDX:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SidxAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SBGP:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SbgpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SGPD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SgpdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MKID:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            if (GetContext() == AP4_ATOM_TYPE_MARL) {\n                atom = AP4_MkidAtom::Create(size_32, stream);\n            }\n            break;\n\n          case AP4_ATOM_TYPE_DEC3:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            if (GetContext() == AP4_ATOM_TYPE_EC_3 || GetContext() == AP4_ATOM_TYPE_ENCA) {\n                atom = AP4_Dec3Atom::Create(size_32, stream);\n            }\n            break;\n\n          // track ref types\n          case AP4_ATOM_TYPE_HINT:\n          case AP4_ATOM_TYPE_CDSC:\n          case AP4_ATOM_TYPE_SYNC:\n          case AP4_ATOM_TYPE_MPOD:\n          case AP4_ATOM_TYPE_DPND:\n          case AP4_ATOM_TYPE_IPIR:\n          case AP4_ATOM_TYPE_ALIS:\n          case AP4_ATOM_TYPE_CHAP:\n            if (GetContext() == AP4_ATOM_TYPE_TREF) {\n                if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n                atom = AP4_TrefTypeAtom::Create(type, size_32, stream);\n            }\n            break;\n\n    #endif // AP4_CONFIG_MINI_BUILD\n\n          // container atoms\n          case AP4_ATOM_TYPE_MOOF:\n          case AP4_ATOM_TYPE_MVEX:\n          case AP4_ATOM_TYPE_TRAF:\n          case AP4_ATOM_TYPE_TREF:\n          case AP4_ATOM_TYPE_MFRA:\n          case AP4_ATOM_TYPE_HNTI:\n          case AP4_ATOM_TYPE_STBL:\n          case AP4_ATOM_TYPE_MDIA:\n          case AP4_ATOM_TYPE_DINF:\n          case AP4_ATOM_TYPE_MINF:\n          case AP4_ATOM_TYPE_SCHI:\n          case AP4_ATOM_TYPE_SINF:\n          case AP4_ATOM_TYPE_UDTA:\n          case AP4_ATOM_TYPE_ILST:\n          case AP4_ATOM_TYPE_EDTS: \n          case AP4_ATOM_TYPE_MDRI:\n          case AP4_ATOM_TYPE_WAVE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_ContainerAtom::Create(type, size_64, false, force_64, stream, *this);\n            break;\n\n          // containers, only at the top\n          case AP4_ATOM_TYPE_MARL:\n            if (GetContext() == 0) {\n                atom = AP4_ContainerAtom::Create(type, size_64, false, force_64, stream, *this);\n            }\n            break;\n            \n          // full container atoms\n          case AP4_ATOM_TYPE_META:\n          case AP4_ATOM_TYPE_ODRM:\n          case AP4_ATOM_TYPE_ODKM:\n            atom = AP4_ContainerAtom::Create(type, size_64, true, force_64, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_FREE:\n          case AP4_ATOM_TYPE_WIDE:\n          case AP4_ATOM_TYPE_MDAT:\n            // generic atoms\n            break;\n            \n          default: {\n            // try all the external type handlers\n            AP4_List<TypeHandler>::Item* handler_item = m_TypeHandlers.FirstItem();\n            while (handler_item) {\n                TypeHandler* handler = handler_item->GetData();\n                if (AP4_SUCCEEDED(handler->CreateAtom(type, size_32, stream, GetContext(), atom))) {\n                    break;\n                }\n                handler_item = handler_item->GetNext();\n            }\n\n            break;\n          }\n        }\n    }\n    \n    return AP4_SUCCESS;\n}", "func_src_after": "AP4_AtomFactory::CreateAtomFromStream(AP4_ByteStream& stream, \n                                      AP4_UI32        type,\n                                      AP4_UI32        size_32,\n                                      AP4_UI64        size_64,\n                                      AP4_Atom*&      atom)\n{\n    bool atom_is_large = (size_32 == 1);\n    bool force_64 = (size_32==1 && ((size_64>>32) == 0));\n    \n    // create the atom\n    if (GetContext() == AP4_ATOM_TYPE_STSD) {\n        // sample entry\n        if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n        switch (type) {\n          case AP4_ATOM_TYPE_MP4A:\n            atom = new AP4_Mp4aSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MP4V:\n            atom = new AP4_Mp4vSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MP4S:\n            atom = new AP4_Mp4sSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ENCA:\n            atom = new AP4_EncaSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ENCV:\n            atom = new AP4_EncvSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_DRMS:\n            atom = new AP4_DrmsSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_DRMI:\n            atom = new AP4_DrmiSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_AVC1:\n          case AP4_ATOM_TYPE_AVC2:\n          case AP4_ATOM_TYPE_AVC3:\n          case AP4_ATOM_TYPE_AVC4:\n          case AP4_ATOM_TYPE_DVAV:\n          case AP4_ATOM_TYPE_DVA1:\n            atom = new AP4_AvcSampleEntry(type, size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_HEV1:\n          case AP4_ATOM_TYPE_HVC1:\n          case AP4_ATOM_TYPE_DVHE:\n          case AP4_ATOM_TYPE_DVH1:\n            atom = new AP4_HevcSampleEntry(type, size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ALAC:\n          case AP4_ATOM_TYPE_AC_3:\n          case AP4_ATOM_TYPE_EC_3:\n          case AP4_ATOM_TYPE_DTSC:\n          case AP4_ATOM_TYPE_DTSH:\n          case AP4_ATOM_TYPE_DTSL:\n          case AP4_ATOM_TYPE_DTSE:\n            atom = new AP4_AudioSampleEntry(type, size_32, stream, *this);\n            break;\n            \n          case AP4_ATOM_TYPE_RTP_:\n            atom = new AP4_RtpHintSampleEntry(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_STPP:\n            atom = new AP4_SubtitleSampleEntry(type, size_32, stream, *this);\n            break;\n\n          default: {\n            // try all the external type handlers\n            AP4_List<TypeHandler>::Item* handler_item = m_TypeHandlers.FirstItem();\n            while (handler_item) {\n                TypeHandler* handler = handler_item->GetData();\n                if (AP4_SUCCEEDED(handler->CreateAtom(type, size_32, stream, GetContext(), atom))) {\n                    break;\n                }\n                handler_item = handler_item->GetNext();\n            }\n\n            // no custom handler, create a generic entry\n            if (atom == NULL) {\n                atom = new AP4_UnknownSampleEntry(type, (AP4_UI32)size_64, stream);\n            }\n\n            break;\n          }\n        }\n    } else {\n        // regular atom\n        switch (type) {\n          case AP4_ATOM_TYPE_MOOV:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MoovAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_MVHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MvhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MEHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MehdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MFHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MfhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TRAK:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrakAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_TREX:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrexAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HDLR:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HdlrAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TKHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TkhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TRUN:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TrunAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFRA:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfraAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MFRO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MfroAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MDHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_MdhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StsdAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_STSC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StscAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STCO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StcoAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_CO64:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_Co64Atom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSZ:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StszAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STZ2:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_Stz2Atom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STTS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SttsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_CTTS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_CttsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STSS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_StssAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IODS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IodsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ESDS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_EsdsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_AVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AvccAtom::Create(size_32, stream);\n            break;\n            \n          case AP4_ATOM_TYPE_HVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HvccAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_DVCC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_DvccAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HVCE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HvccAtom::Create(size_32, stream);\n            if (atom) {\n                atom->SetType(AP4_ATOM_TYPE_HVCE);\n            }\n            break;\n\n          case AP4_ATOM_TYPE_AVCE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AvccAtom::Create(size_32, stream);\n            if (atom) {\n                atom->SetType(AP4_ATOM_TYPE_AVCE);\n            }\n            break;\n\n    #if !defined(AP4_CONFIG_MINI_BUILD)\n          case AP4_ATOM_TYPE_UUID: {\n              AP4_UI08 uuid[16];\n              AP4_Result result = stream.Read(uuid, 16);\n              if (AP4_FAILED(result)) return result;\n              \n              if (AP4_CompareMemory(uuid, AP4_UUID_PIFF_TRACK_ENCRYPTION_ATOM, 16) == 0) {\n                  atom = AP4_PiffTrackEncryptionAtom::Create((AP4_UI32)size_64, stream);\n              } else if (AP4_CompareMemory(uuid, AP4_UUID_PIFF_SAMPLE_ENCRYPTION_ATOM, 16) == 0) {\n                  atom = AP4_PiffSampleEncryptionAtom::Create((AP4_UI32)size_64, stream);\n              } else {\n                  atom = new AP4_UnknownUuidAtom(size_64, uuid, stream);\n              }\n              break;\n          }\n            \n          case AP4_ATOM_TYPE_8ID_:\n            atom = new AP4_NullTerminatedStringAtom(type, size_64, stream);\n            break;\n\n          case AP4_ATOM_TYPE_8BDL:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_8bdlAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_DREF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_DrefAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_URL:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_UrlAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ELST:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_ElstAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_VMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_VmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_NMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_NmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_STHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SthdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_HMHD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_HmhdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_FRMA:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_FrmaAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SCHM:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SchmAtom::Create(size_32, &m_ContextStack, stream);\n            break;\n\n          case AP4_ATOM_TYPE_FTYP:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_FtypAtom::Create(size_32, stream);\n            break;\n                        \n          case AP4_ATOM_TYPE_TIMS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TimsAtom::Create(size_32, stream);\n            break;\n     \n          case AP4_ATOM_TYPE_SDP_:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SdpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IKMS:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IkmsAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ISFM:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IsfmAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ISLT:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IsltAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ODHE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OdheAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_OHDR:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OhdrAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_ODDA:\n            atom = AP4_OddaAtom::Create(size_64, stream);\n            break;\n\n          case AP4_ATOM_TYPE_ODAF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_OdafAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_GRPI:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_GrpiAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_IPRO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_IproAtom::Create(size_32, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_RTP_:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_RtpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TFDT:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TfdtAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_TENC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_TencAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SENC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SencAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SAIZ:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SaizAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SAIO:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SaioAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_PDIN:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_PdinAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_BLOC:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_BlocAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_AINF:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_AinfAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_PSSH:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_PsshAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SIDX:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SidxAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SBGP:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SbgpAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_SGPD:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_SgpdAtom::Create(size_32, stream);\n            break;\n\n          case AP4_ATOM_TYPE_MKID:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            if (GetContext() == AP4_ATOM_TYPE_MARL) {\n                atom = AP4_MkidAtom::Create(size_32, stream);\n            }\n            break;\n\n          case AP4_ATOM_TYPE_DEC3:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            if (GetContext() == AP4_ATOM_TYPE_EC_3 || GetContext() == AP4_ATOM_TYPE_ENCA) {\n                atom = AP4_Dec3Atom::Create(size_32, stream);\n            }\n            break;\n\n          // track ref types\n          case AP4_ATOM_TYPE_HINT:\n          case AP4_ATOM_TYPE_CDSC:\n          case AP4_ATOM_TYPE_SYNC:\n          case AP4_ATOM_TYPE_MPOD:\n          case AP4_ATOM_TYPE_DPND:\n          case AP4_ATOM_TYPE_IPIR:\n          case AP4_ATOM_TYPE_ALIS:\n          case AP4_ATOM_TYPE_CHAP:\n            if (GetContext() == AP4_ATOM_TYPE_TREF) {\n                if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n                atom = AP4_TrefTypeAtom::Create(type, size_32, stream);\n            }\n            break;\n\n    #endif // AP4_CONFIG_MINI_BUILD\n\n          // container atoms\n          case AP4_ATOM_TYPE_MOOF:\n          case AP4_ATOM_TYPE_MVEX:\n          case AP4_ATOM_TYPE_TRAF:\n          case AP4_ATOM_TYPE_TREF:\n          case AP4_ATOM_TYPE_MFRA:\n          case AP4_ATOM_TYPE_HNTI:\n          case AP4_ATOM_TYPE_STBL:\n          case AP4_ATOM_TYPE_MDIA:\n          case AP4_ATOM_TYPE_DINF:\n          case AP4_ATOM_TYPE_MINF:\n          case AP4_ATOM_TYPE_SCHI:\n          case AP4_ATOM_TYPE_SINF:\n          case AP4_ATOM_TYPE_UDTA:\n          case AP4_ATOM_TYPE_ILST:\n          case AP4_ATOM_TYPE_EDTS: \n          case AP4_ATOM_TYPE_MDRI:\n          case AP4_ATOM_TYPE_WAVE:\n            if (atom_is_large) return AP4_ERROR_INVALID_FORMAT;\n            atom = AP4_ContainerAtom::Create(type, size_64, false, force_64, stream, *this);\n            break;\n\n          // containers, only at the top\n          case AP4_ATOM_TYPE_MARL:\n            if (GetContext() == 0) {\n                atom = AP4_ContainerAtom::Create(type, size_64, false, force_64, stream, *this);\n            }\n            break;\n            \n          // full container atoms\n          case AP4_ATOM_TYPE_META:\n          case AP4_ATOM_TYPE_ODRM:\n          case AP4_ATOM_TYPE_ODKM:\n            atom = AP4_ContainerAtom::Create(type, size_64, true, force_64, stream, *this);\n            break;\n\n          case AP4_ATOM_TYPE_FREE:\n          case AP4_ATOM_TYPE_WIDE:\n          case AP4_ATOM_TYPE_MDAT:\n            // generic atoms\n            break;\n            \n          default: {\n            // try all the external type handlers\n            AP4_List<TypeHandler>::Item* handler_item = m_TypeHandlers.FirstItem();\n            while (handler_item) {\n                TypeHandler* handler = handler_item->GetData();\n                if (AP4_SUCCEEDED(handler->CreateAtom(type, size_32, stream, GetContext(), atom))) {\n                    break;\n                }\n                handler_item = handler_item->GetNext();\n            }\n\n            break;\n          }\n        }\n    }\n    \n    return AP4_SUCCESS;\n}", "commit_link": "github.com/axiomatic-systems/Bento4/commit/be7185faf7f52674028977dcf501c6039ff03aa5", "file_name": "Source/C++/Core/Ap4AtomFactory.cpp", "vul_type": "cwe-476", "description": "Write a C++ function in AP4_AtomFactory that creates an atom from a stream based on type and size."}
{"func_name": "dd_load_text_ext", "func_src_before": "char* dd_load_text_ext(const struct dump_dir *dd, const char *name, unsigned flags)\n{\n//    if (!dd->locked)\n//        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    /* Compat with old abrt dumps. Remove in abrt-2.1 */\n    if (strcmp(name, \"release\") == 0)\n        name = FILENAME_OS_RELEASE;\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    char *ret = load_text_file(full_path, flags);\n    free(full_path);\n\n    return ret;\n}", "func_src_after": "char* dd_load_text_ext(const struct dump_dir *dd, const char *name, unsigned flags)\n{\n//    if (!dd->locked)\n//        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    if (!str_is_correct_filename(name))\n    {\n        error_msg(\"Cannot load text. '%s' is not a valid file name\", name);\n        if (!(flags & DD_LOAD_TEXT_RETURN_NULL_ON_FAILURE))\n            xfunc_die();\n    }\n\n    /* Compat with old abrt dumps. Remove in abrt-2.1 */\n    if (strcmp(name, \"release\") == 0)\n        name = FILENAME_OS_RELEASE;\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    char *ret = load_text_file(full_path, flags);\n    free(full_path);\n\n    return ret;\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "Write a C function named `dd_load_text_ext` that loads text from a file within a directory structure, handling special cases and errors."}
{"func_name": "prplcb_xfer_new_send_cb", "func_src_before": "static gboolean prplcb_xfer_new_send_cb(gpointer data, gint fd, b_input_condition cond)\n{\n\tPurpleXfer *xfer = data;\n\tstruct im_connection *ic = purple_ic_by_pa(xfer->account);\n\tstruct prpl_xfer_data *px = xfer->ui_data;\n\tPurpleBuddy *buddy;\n\tconst char *who;\n\n\tbuddy = purple_find_buddy(xfer->account, xfer->who);\n\twho = buddy ? purple_buddy_get_name(buddy) : xfer->who;\n\n\t/* TODO(wilmer): After spreading some more const goodness in BitlBee,\n\t   remove the evil cast below. */\n\tpx->ft = imcb_file_send_start(ic, (char *) who, xfer->filename, xfer->size);\n\tpx->ft->data = px;\n\n\tpx->ft->accept = prpl_xfer_accept;\n\tpx->ft->canceled = prpl_xfer_canceled;\n\tpx->ft->free = prpl_xfer_free;\n\tpx->ft->write_request = prpl_xfer_write_request;\n\n\treturn FALSE;\n}", "func_src_after": "static gboolean prplcb_xfer_new_send_cb(gpointer data, gint fd, b_input_condition cond)\n{\n\tPurpleXfer *xfer = data;\n\tstruct im_connection *ic = purple_ic_by_pa(xfer->account);\n\tstruct prpl_xfer_data *px = xfer->ui_data;\n\tPurpleBuddy *buddy;\n\tconst char *who;\n\n\tbuddy = purple_find_buddy(xfer->account, xfer->who);\n\twho = buddy ? purple_buddy_get_name(buddy) : xfer->who;\n\n\t/* TODO(wilmer): After spreading some more const goodness in BitlBee,\n\t   remove the evil cast below. */\n\tpx->ft = imcb_file_send_start(ic, (char *) who, xfer->filename, xfer->size);\n\n\tif (!px->ft) {\n\t\treturn FALSE;\n\t}\n\tpx->ft->data = px;\n\n\tpx->ft->accept = prpl_xfer_accept;\n\tpx->ft->canceled = prpl_xfer_canceled;\n\tpx->ft->free = prpl_xfer_free;\n\tpx->ft->write_request = prpl_xfer_write_request;\n\n\treturn FALSE;\n}", "commit_link": "github.com/bitlbee/bitlbee/commit/30d598ce7cd3f136ee9d7097f39fa9818a272441", "file_name": "protocols/purple/ft.c", "vul_type": "cwe-476", "description": "Write a C function to initialize a file transfer callback in the BitlBee instant messaging client."}
{"func_name": "tflite::GetOptionalInputTensor", "func_src_before": "const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                           const TfLiteNode* node, int index) {\n  const bool use_tensor = index < node->inputs->size &&\n                          node->inputs->data[index] != kTfLiteOptionalTensor;\n  if (use_tensor) {\n    return GetMutableInput(context, node, index);\n  }\n  return nullptr;\n}", "func_src_after": "const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                           const TfLiteNode* node, int index) {\n  return GetInput(context, node, index);\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/00302787b788c5ff04cb6f62aed5a74d936e86c0", "file_name": "tensorflow/lite/kernels/kernel_util.cc", "vul_type": "cwe-125", "description": "Write a C++ function named `GetOptionalInputTensor` that retrieves an optional input tensor from a TensorFlow Lite node if it exists, or returns null otherwise."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\tc => {\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v.toString(16);\n\t\t\t\t}", "func_src_after": "\t\t\t\tsymbol => {\n\t\t\t\t\tlet array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16).toString(16);\n\t\t\t\t}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 11, "line": "\t\t\t\tc => {\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 4, "char_end": 5, "chars": "c"}, {"char_start": 16, "char_end": 178, "chars": "// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v"}], "added": [{"char_start": 4, "char_end": 10, "chars": "symbol"}, {"char_start": 21, "char_end": 268, "chars": "let array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16)"}]}, "commit_link": "github.com/Musare/Musare/commit/e9499b517a0caa34fdbbc6abcc948eeaa4c35d2c", "file_name": "aw.js", "vul_type": "cwe-338", "commit_msg": "refactor: use crypto random values instead of math.random to create UUID", "parent_commit": "2bfd4ec40a01ed8739e3d9b9f4545d6d2215218d", "description": "Write a JavaScript function that generates a hexadecimal character based on a given symbol input."}
{"func_name": "ReadPSDChannelPixels", "func_src_before": "static MagickBooleanType ReadPSDChannelPixels(Image *image,\n  const size_t channels,const size_t row,const ssize_t type,\n  const unsigned char *pixels,ExceptionInfo *exception)\n{\n  Quantum\n    pixel;\n\n  register const unsigned char\n    *p;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    x;\n\n  size_t\n    packet_size;\n\n  unsigned short\n    nibble;\n\n  p=pixels;\n  q=GetAuthenticPixels(image,0,row,image->columns,1,exception);\n  if (q == (Quantum *) NULL)\n    return MagickFalse;\n  packet_size=GetPSDPacketSize(image);\n  for (x=0; x < (ssize_t) image->columns; x++)\n  {\n    if (packet_size == 1)\n      pixel=ScaleCharToQuantum(*p++);\n    else\n      {\n        p=PushShortPixel(MSBEndian,p,&nibble);\n        pixel=ScaleShortToQuantum(nibble);\n      }\n    switch (type)\n    {\n      case -1:\n      {\n        SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      case -2:\n      case 0:\n      {\n        SetPixelRed(image,pixel,q);\n        if (channels == 1 || type == -2)\n          SetPixelGray(image,pixel,q);\n        if (image->storage_class == PseudoClass)\n          {\n            if (packet_size == 1)\n              SetPixelIndex(image,ScaleQuantumToChar(pixel),q);\n            else\n              SetPixelIndex(image,ScaleQuantumToShort(pixel),q);\n            SetPixelViaPixelInfo(image,image->colormap+(ssize_t)\n              ConstrainColormapIndex(image,GetPixelIndex(image,q),exception),q);\n            if (image->depth == 1)\n              {\n                ssize_t\n                  bit,\n                  number_bits;\n  \n                number_bits=image->columns-x;\n                if (number_bits > 8)\n                  number_bits=8;\n                for (bit=0; bit < number_bits; bit++)\n                {\n                  SetPixelIndex(image,(((unsigned char) pixel) &\n                    (0x01 << (7-bit))) != 0 ? 0 : 255,q);\n                  SetPixelViaPixelInfo(image,image->colormap+(ssize_t)\n                    GetPixelIndex(image,q),q);\n                  q+=GetPixelChannels(image);\n                  x++;\n                }\n                x--;\n                continue;\n              }\n          }\n        break;\n      }\n      case 1:\n      {\n        if (image->storage_class == PseudoClass)\n          SetPixelAlpha(image,pixel,q);\n        else\n          SetPixelGreen(image,pixel,q);\n        break;\n      }\n      case 2:\n      {\n        if (image->storage_class == PseudoClass)\n          SetPixelAlpha(image,pixel,q);\n        else\n          SetPixelBlue(image,pixel,q);\n        break;\n      }\n      case 3:\n      {\n        if (image->colorspace == CMYKColorspace)\n          SetPixelBlack(image,pixel,q);\n        else\n          if (image->alpha_trait != UndefinedPixelTrait)\n            SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      case 4:\n      {\n        if ((IssRGBCompatibleColorspace(image->colorspace) != MagickFalse) &&\n            (channels > 3))\n          break;\n        if (image->alpha_trait != UndefinedPixelTrait)\n          SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      default:\n        break;\n    }\n    q+=GetPixelChannels(image);\n  }\n  return(SyncAuthenticPixels(image,exception));\n}", "func_src_after": "static MagickBooleanType ReadPSDChannelPixels(Image *image,\n  const size_t channels,const size_t row,const ssize_t type,\n  const unsigned char *pixels,ExceptionInfo *exception)\n{\n  Quantum\n    pixel;\n\n  register const unsigned char\n    *p;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    x;\n\n  size_t\n    packet_size;\n\n  unsigned short\n    nibble;\n\n  p=pixels;\n  q=GetAuthenticPixels(image,0,row,image->columns,1,exception);\n  if (q == (Quantum *) NULL)\n    return MagickFalse;\n  packet_size=GetPSDPacketSize(image);\n  for (x=0; x < (ssize_t) image->columns; x++)\n  {\n    if (packet_size == 1)\n      pixel=ScaleCharToQuantum(*p++);\n    else\n      {\n        p=PushShortPixel(MSBEndian,p,&nibble);\n        pixel=ScaleShortToQuantum(nibble);\n      }\n    switch (type)\n    {\n      case -1:\n      {\n        SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      case -2:\n      case 0:\n      {\n        SetPixelRed(image,pixel,q);\n        if (channels == 1 || type == -2)\n          SetPixelGray(image,pixel,q);\n        if (image->storage_class == PseudoClass)\n          {\n            if (packet_size == 1)\n              SetPixelIndex(image,ScaleQuantumToChar(pixel),q);\n            else\n              SetPixelIndex(image,ScaleQuantumToShort(pixel),q);\n            SetPixelViaPixelInfo(image,image->colormap+(ssize_t)\n              ConstrainColormapIndex(image,GetPixelIndex(image,q),exception),q);\n            if (image->depth == 1)\n              {\n                ssize_t\n                  bit,\n                  number_bits;\n  \n                number_bits=image->columns-x;\n                if (number_bits > 8)\n                  number_bits=8;\n                for (bit=0; bit < number_bits; bit++)\n                {\n                  SetPixelIndex(image,(((unsigned char) pixel) &\n                    (0x01 << (7-bit))) != 0 ? 0 : 255,q);\n                  SetPixelViaPixelInfo(image,image->colormap+(ssize_t)\n                    ConstrainColormapIndex(image,GetPixelIndex(image,q),\n                      exception),q);\n                  q+=GetPixelChannels(image);\n                  x++;\n                }\n                x--;\n                continue;\n              }\n          }\n        break;\n      }\n      case 1:\n      {\n        if (image->storage_class == PseudoClass)\n          SetPixelAlpha(image,pixel,q);\n        else\n          SetPixelGreen(image,pixel,q);\n        break;\n      }\n      case 2:\n      {\n        if (image->storage_class == PseudoClass)\n          SetPixelAlpha(image,pixel,q);\n        else\n          SetPixelBlue(image,pixel,q);\n        break;\n      }\n      case 3:\n      {\n        if (image->colorspace == CMYKColorspace)\n          SetPixelBlack(image,pixel,q);\n        else\n          if (image->alpha_trait != UndefinedPixelTrait)\n            SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      case 4:\n      {\n        if ((IssRGBCompatibleColorspace(image->colorspace) != MagickFalse) &&\n            (channels > 3))\n          break;\n        if (image->alpha_trait != UndefinedPixelTrait)\n          SetPixelAlpha(image,pixel,q);\n        break;\n      }\n      default:\n        break;\n    }\n    q+=GetPixelChannels(image);\n  }\n  return(SyncAuthenticPixels(image,exception));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/e14fd0a2801f73bdc123baf4fbab97dec55919eb", "file_name": "coders/psd.c", "vul_type": "cwe-125", "description": "In C, write a function to process and assign pixel values to an image channel based on the PSD format."}
{"func_name": "initialize", "func_src_before": "    def initialize(*configs)\n      @config = configs.each_with_object(default_config) do |path, obj|\n        new = YAML.load(File.read(path))\n        next unless new\n        obj.deep_merge! Cymbal.symbolize(new)\n      end\n      @paths = @config[:paths]\n    end", "func_src_after": "    def initialize(*configs)\n      @config = configs.each_with_object(default_config) do |path, obj|\n        new = YAML.safe_load(File.read(path))\n        next unless new\n        obj.deep_merge! Cymbal.symbolize(new)\n      end\n      @paths = @config[:paths]\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 101, "char_end": 142, "line": "        new = YAML.load(File.read(path))\n"}], "added": [{"line_no": 3, "char_start": 101, "char_end": 147, "line": "        new = YAML.safe_load(File.read(path))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 120, "char_end": 125, "chars": "safe_"}]}, "commit_link": "github.com/dock0/dock0/commit/c4800a6be1e62b124b401214b416b7adedca341a", "file_name": "dock0.rb", "vul_type": "cwe-502", "commit_msg": "use safe_load", "parent_commit": "243b9d713e05eebb5ef8e17bc568dfefcd2a32a2", "description": "Write a Ruby method named `initialize` that merges multiple YAML configuration files into a default configuration object and stores specific paths from the configuration."}
{"func_name": "uas_switch_interface", "func_src_before": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tint alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (alt < 0)\n\t\treturn alt;\n\n\treturn usb_set_interface(udev,\n\t\t\tintf->altsetting[0].desc.bInterfaceNumber, alt);\n}", "func_src_after": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tstruct usb_host_interface *alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (!alt)\n\t\treturn -ENODEV;\n\n\treturn usb_set_interface(udev, alt->desc.bInterfaceNumber,\n\t\t\talt->desc.bAlternateSetting);\n}", "commit_link": "github.com/torvalds/linux/commit/786de92b3cb26012d3d0f00ee37adf14527f35c4", "file_name": "drivers/usb/storage/uas.c", "vul_type": "cwe-125", "description": "Write a C function named `uas_switch_interface` that switches the USB interface to an alternate setting for a given USB device and interface."}
{"func_name": "get_bracket_graph_data", "func_src_before": "def get_bracket_graph_data(db, tag):\n    # First, we have to find out which scenes this player has brackets in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{}'\".format(tag)\n    scenes = db.exec(sql)\n    scenes = [s[0] for s in scenes]\n\n    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}\n\n    return bracket_placings_by_scene", "func_src_after": "def get_bracket_graph_data(db, tag):\n    # First, we have to find out which scenes this player has brackets in\n    sql = \"SELECT DISTINCT scene FROM ranks WHERE player='{tag}'\"\n    args = {'tag': tag}\n    scenes = db.exec(sql, args)\n    scenes = [s[0] for s in scenes]\n\n    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}\n\n    return bracket_placings_by_scene", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "bracket_utils.py", "vul_type": "cwe-089", "description": "Write a Python function named `get_bracket_graph_data` that retrieves distinct scenes for a player's brackets from a database and maps each scene to its bracket placings."}
{"func_name": "matchCurrentInput", "func_src_before": "matchCurrentInput(\n\t\tconst InString *input, int pos, const widechar *passInstructions, int passIC) {\n\tint k;\n\tint kk = pos;\n\tfor (k = passIC + 2; k < passIC + 2 + passInstructions[passIC + 1]; k++)\n\t\tif (input->chars[kk] == ENDSEGMENT || passInstructions[k] != input->chars[kk++])\n\t\t\treturn 0;\n\treturn 1;\n}", "func_src_after": "matchCurrentInput(\n\t\tconst InString *input, int pos, const widechar *passInstructions, int passIC) {\n\tint k;\n\tint kk = pos;\n\tfor (k = passIC + 2;\n\t\t\t((k < passIC + 2 + passInstructions[passIC + 1]) && (kk < input->length));\n\t\t\tk++)\n\t\tif (input->chars[kk] == ENDSEGMENT || passInstructions[k] != input->chars[kk++])\n\t\t\treturn 0;\n\treturn 1;\n}", "commit_link": "github.com/liblouis/liblouis/commit/5e4089659bb49b3095fa541fa6387b4c40d7396e", "file_name": "liblouis/lou_translateString.c", "vul_type": "cwe-125", "description": "Write a C function named `matchCurrentInput` that checks if a segment of an input string matches a sequence of pass instructions, starting at a given position."}
{"func_name": "refresh_select", "func_src_before": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerHTML = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "func_src_after": "  var refresh_select = function(select, settings) {\n    // Clear columns\n    select.wrapper.selected.innerHTML = \"\";\n    select.wrapper.non_selected.innerHTML = \"\";\n\n    // Add headers to columns\n    if (settings.non_selected_header && settings.selected_header) {\n      var non_selected_header = document.createElement(\"div\");\n      var selected_header = document.createElement(\"div\");\n\n      non_selected_header.className = \"header\";\n      selected_header.className = \"header\";\n\n      non_selected_header.innerText = settings.non_selected_header;\n      selected_header.innerText = settings.selected_header;\n\n      select.wrapper.non_selected.appendChild(non_selected_header);\n      select.wrapper.selected.appendChild(selected_header);\n    }\n\n    // Get search value\n    if (select.wrapper.search) {\n      var query = select.wrapper.search.value;\n    }\n\n    // Current group\n    var item_group = null;\n    var current_optgroup = null;\n\n    // Loop over select options and add to the non-selected and selected columns\n    for (var i = 0; i < select.options.length; i++) {\n      var option = select.options[i];\n\n      var value = option.value;\n      var label = option.textContent || option.innerText;\n\n      var row = document.createElement(\"a\");\n      row.tabIndex = 0;\n      row.className = \"item\";\n      row.innerText = label;\n      row.setAttribute(\"role\", \"button\");\n      row.setAttribute(\"data-value\", value);\n      row.setAttribute(\"multi-index\", i);\n\n      if (option.disabled) {\n        row.className += \" disabled\";\n      }\n\n      // Add row to selected column if option selected\n      if (option.selected) {\n        row.className += \" selected\";\n        var clone = row.cloneNode(true);\n        select.wrapper.selected.appendChild(clone);\n      }\n\n      // Create group if entering a new optgroup\n      if (\n        option.parentNode.nodeName == \"OPTGROUP\" &&\n        option.parentNode != current_optgroup\n      ) {\n        current_optgroup = option.parentNode;\n        item_group = document.createElement(\"div\");\n        item_group.className = \"item-group\";\n\n        if (option.parentNode.label) {\n          var groupLabel = document.createElement(\"span\");\n          groupLabel.innerHTML = option.parentNode.label;\n          groupLabel.className = \"group-label\";\n          item_group.appendChild(groupLabel);\n        }\n\n        select.wrapper.non_selected.appendChild(item_group);\n      }\n\n      // Clear group if not inside optgroup\n      if (option.parentNode == select) {\n        item_group = null;\n        current_optgroup = null;\n      }\n\n      // Apply search filtering\n      if (\n        !query ||\n        (query && label.toLowerCase().indexOf(query.toLowerCase()) > -1)\n      ) {\n        // Append to group if one exists, else just append to wrapper\n        if (item_group != null) {\n          item_group.appendChild(row);\n        } else {\n          select.wrapper.non_selected.appendChild(row);\n        }\n      }\n    }\n  };", "line_changes": {"deleted": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerHTML = label;\n"}], "added": [{"line_no": 40, "char_start": 1301, "char_end": 1330, "line": "      row.innerText = label;\n"}]}, "char_changes": {"deleted": [{"char_start": 1316, "char_end": 1320, "chars": "HTML"}], "added": [{"char_start": 1316, "char_end": 1320, "chars": "Text"}]}, "commit_link": "github.com/Fabianlindfors/multi.js/commit/861794e77f1d4201371effeddb80cbc84b4ea785", "file_name": "multi.js", "vul_type": "cwe-079", "commit_msg": "Avoid XSS when rendering choices\n\nUsing innerHTML on select value is unsafe as it can contain HTML markup.", "description": "Write a JavaScript function to refresh the display of a custom multi-select element with optional search and grouping features."}
{"func_name": "bin_symbols", "func_src_before": "static int bin_symbols(RCore *r, int mode, ut64 laddr, int va, ut64 at, const char *name, bool exponly, const char *args) {\n\tRBinInfo *info = r_bin_get_info (r->bin);\n\tRList *entries = r_bin_get_entries (r->bin);\n\tRBinSymbol *symbol;\n\tRBinAddr *entry;\n\tRListIter *iter;\n\tbool firstexp = true;\n\tbool printHere = false;\n\tint i = 0, lastfs = 's';\n\tbool bin_demangle = r_config_get_i (r->config, \"bin.demangle\");\n\tif (!info) {\n\t\treturn 0;\n\t}\n\n\tif (args && *args == '.') {\n\t\tprintHere = true;\n\t}\n\n\tbool is_arm = info && info->arch && !strncmp (info->arch, \"arm\", 3);\n\tconst char *lang = bin_demangle ? r_config_get (r->config, \"bin.lang\") : NULL;\n\n\tRList *symbols = r_bin_get_symbols (r->bin);\n\tr_spaces_push (&r->anal->meta_spaces, \"bin\");\n\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"[\");\n\t} else if (IS_MODE_SET (mode)) {\n\t\tr_flag_space_set (r->flags, R_FLAGS_FS_SYMBOLS);\n\t} else if (!at && exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs exports\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Exports]\\n\");\n\t\t}\n\t} else if (!at && !exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs symbols\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Symbols]\\n\");\n\t\t}\n\t}\n\tif (IS_MODE_NORMAL (mode)) {\n\t\tr_cons_printf (\"Num Paddr      Vaddr      Bind     Type Size Name\\n\");\n\t}\n\n\n\tsize_t count = 0;\n\tr_list_foreach (symbols, iter, symbol) {\n\t\tif (!symbol->name) {\n\t\t\tcontinue;\n\t\t}\n\t\tchar *r_symbol_name = r_str_escape_utf8 (symbol->name, false, true);\n\t\tut64 addr = compute_addr (r->bin, symbol->paddr, symbol->vaddr, va);\n\t\tint len = symbol->size ? symbol->size : 32;\n\t\tSymName sn = {0};\n\n\t\tif (exponly && !isAnExport (symbol)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (name && strcmp (r_symbol_name, name)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (at && (!symbol->size || !is_in_range (at, addr, symbol->size))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif ((printHere && !is_in_range (r->offset, symbol->paddr, len))\n\t\t\t\t&& (printHere && !is_in_range (r->offset, addr, len))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tcount ++;\n\t\tsnInit (r, &sn, symbol, lang);\n\n\t\tif (IS_MODE_SET (mode) && (is_section_symbol (symbol) || is_file_symbol (symbol))) {\n\t\t\t/*\n\t\t\t * Skip section symbols because they will have their own flag.\n\t\t\t * Skip also file symbols because not useful for now.\n\t\t\t */\n\t\t} else if (IS_MODE_SET (mode) && is_special_symbol (symbol)) {\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_special_symbol (r, symbol, va);\n\t\t\t}\n\t\t} else if (IS_MODE_SET (mode)) {\n\t\t\t// TODO: provide separate API in RBinPlugin to let plugins handle anal hints/metadata\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_symbol (r, symbol, info, va);\n\t\t\t}\n\t\t\tselect_flag_space (r, symbol);\n\t\t\t/* If that's a Classed symbol (method or so) */\n\t\t\tif (sn.classname) {\n\t\t\t\tRFlagItem *fi = r_flag_get (r->flags, sn.methflag);\n\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\tchar *prname = r_str_newf (\"%s.%s\", r->bin->prefix, sn.methflag);\n\t\t\t\t\tr_name_filter (sn.methflag, -1);\n\t\t\t\t\tfree (sn.methflag);\n\t\t\t\t\tsn.methflag = prname;\n\t\t\t\t}\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, sn.methname);\n\t\t\t\t\tif ((fi->offset - r->flags->base) == addr) {\n\t\t\t\t//\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\t\tr_flag_unset (r->flags, fi);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfi = r_flag_set (r->flags, sn.methflag, addr, symbol->size);\n\t\t\t\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\tif (comment) {\n\t\t\t\t\t\tr_flag_item_set_comment (fi, comment);\n\t\t\t\t\t\tR_FREE (comment);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *n = sn.demname ? sn.demname : sn.name;\n\t\t\t\tconst char *fn = sn.demflag ? sn.demflag : sn.nameflag;\n\t\t\t\tchar *fnp = (r->bin->prefix) ?\n\t\t\t\t\tr_str_newf (\"%s.%s\", r->bin->prefix, fn):\n\t\t\t\t\tstrdup (fn);\n\t\t\t\tRFlagItem *fi = r_flag_set (r->flags, fnp, addr, symbol->size);\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, n);\n\t\t\t\t\tfi->demangled = (bool)(size_t)sn.demname;\n\t\t\t\t} else {\n\t\t\t\t\tif (fn) {\n\t\t\t\t\t\teprintf (\"[Warning] Can't find flag (%s)\\n\", fn);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (fnp);\n\t\t\t}\n\t\t\tif (sn.demname) {\n\t\t\t\tr_meta_add (r->anal, R_META_TYPE_COMMENT,\n\t\t\t\t\taddr, symbol->size, sn.demname);\n\t\t\t}\n\t\t\tr_flag_space_pop (r->flags);\n\t\t} else if (IS_MODE_JSON (mode)) {\n\t\t\tchar *str = r_str_escape_utf8_for_json (r_symbol_name, -1);\n\t\t\t// str = r_str_replace (str, \"\\\"\", \"\\\\\\\"\", 1);\n\t\t\tr_cons_printf (\"%s{\\\"name\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"demname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"flagname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"ordinal\\\":%d,\"\n\t\t\t\t\"\\\"bind\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"size\\\":%d,\"\n\t\t\t\t\"\\\"type\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"vaddr\\\":%\"PFMT64d\",\"\n\t\t\t\t\"\\\"paddr\\\":%\"PFMT64d\"}\",\n\t\t\t\t((exponly && firstexp) || printHere) ? \"\" : (iter->p ? \",\" : \"\"),\n\t\t\t\tstr,\n\t\t\t\tsn.demname? sn.demname: \"\",\n\t\t\t\tsn.nameflag,\n\t\t\t\tsymbol->ordinal,\n\t\t\t\tsymbol->bind,\n\t\t\t\t(int)symbol->size,\n\t\t\t\tsymbol->type,\n\t\t\t\t(ut64)addr, (ut64)symbol->paddr);\n\t\t\tfree (str);\n\t\t} else if (IS_MODE_SIMPLE (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"0x%08\"PFMT64x\" %d %s\\n\",\n\t\t\t\taddr, (int)symbol->size, name);\n\t\t} else if (IS_MODE_SIMPLEST (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"%s\\n\", name);\n\t\t} else if (IS_MODE_RAD (mode)) {\n\t\t\t/* Skip special symbols because we do not flag them and\n\t\t\t * they shouldn't be printed in the rad format either */\n\t\t\tif (is_special_symbol (symbol)) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tRBinFile *binfile;\n\t\t\tRBinPlugin *plugin;\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tif (!name) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tif (!strncmp (name, \"imp.\", 4)) {\n\t\t\t\tif (lastfs != 'i') {\n\t\t\t\t\tr_cons_printf (\"fs imports\\n\");\n\t\t\t\t}\n\t\t\t\tlastfs = 'i';\n\t\t\t} else {\n\t\t\t\tif (lastfs != 's') {\n\t\t\t\t\tconst char *fs = exponly? \"exports\": \"symbols\";\n\t\t\t\t\tr_cons_printf (\"fs %s\\n\", fs);\n\t\t\t\t}\n\t\t\t\tlastfs = 's';\n\t\t\t}\n\t\t\tif (r->bin->prefix || *name) { // we don't want unnamed symbol flags\n\t\t\t\tchar *flagname = construct_symbol_flagname (\"sym\", name, MAXFLAG_LEN_DEFAULT);\n\t\t\t\tif (!flagname) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"f %s%s%s %u 0x%08\" PFMT64x \"\\\"\\n\",\n\t\t\t\t\tr->bin->prefix ? r->bin->prefix : \"\", r->bin->prefix ? \".\" : \"\",\n\t\t\t\t\tflagname, symbol->size, addr);\n\t\t\t\tfree (flagname);\n\t\t\t}\n\t\t\tbinfile = r_bin_cur (r->bin);\n\t\t\tplugin = r_bin_file_cur_plugin (binfile);\n\t\t\tif (plugin && plugin->name) {\n\t\t\t\tif (r_str_startswith (plugin->name, \"pe\")) {\n\t\t\t\t\tchar *module = strdup (r_symbol_name);\n\t\t\t\t\tchar *p = strstr (module, \".dll_\");\n\t\t\t\t\tif (p && strstr (module, \"imp.\")) {\n\t\t\t\t\t\tchar *symname = __filterShell (p + 5);\n\t\t\t\t\t\tchar *m = __filterShell (module);\n\t\t\t\t\t\t*p = 0;\n\t\t\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\t\t\tr_cons_printf (\"k bin/pe/%s/%d=%s.%s\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, r->bin->prefix, symname);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tr_cons_printf (\"k bin/pe/%s/%d=%s\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, symname);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfree (symname);\n\t\t\t\t\t\tfree (m);\n\t\t\t\t\t}\n\t\t\t\t\tfree (module);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tconst char *bind = symbol->bind? symbol->bind: \"NONE\";\n\t\t\tconst char *type = symbol->type? symbol->type: \"NONE\";\n\t\t\tconst char *name = r_str_get (sn.demname? sn.demname: r_symbol_name);\n\t\t\t// const char *fwd = r_str_get (symbol->forwarder);\n\t\t\tr_cons_printf (\"%03u\", symbol->ordinal);\n\t\t\tif (symbol->paddr == UT64_MAX) {\n\t\t\t\tr_cons_printf (\" ----------\");\n\t\t\t} else {\n\t\t\t\tr_cons_printf (\" 0x%08\"PFMT64x, symbol->paddr);\n\t\t\t}\n\t\t\tr_cons_printf (\" 0x%08\"PFMT64x\" %6s %6s %4d%s%s\\n\",\n\t\t\t               addr, bind, type, symbol->size, *name? \" \": \"\", name);\n\t\t}\nnext:\n\t\tsnFini (&sn);\n\t\ti++;\n\t\tfree (r_symbol_name);\n\t\tif (exponly && firstexp) {\n\t\t\tfirstexp = false;\n\t\t}\n\t\tif (printHere) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (count == 0 && IS_MODE_JSON (mode)) {\n\t\tr_cons_printf (\"{}\");\n\t}\n\n\n\t//handle thumb and arm for entry point since they are not present in symbols\n\tif (is_arm) {\n\t\tr_list_foreach (entries, iter, entry) {\n\t\t\tif (IS_MODE_SET (mode)) {\n\t\t\t\thandle_arm_entry (r, entry, info, va);\n\t\t\t}\n\t\t}\n\t}\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"]\");\n\t}\n\n\tr_spaces_pop (&r->anal->meta_spaces);\n\treturn true;\n}", "func_src_after": "static int bin_symbols(RCore *r, int mode, ut64 laddr, int va, ut64 at, const char *name, bool exponly, const char *args) {\n\tRBinInfo *info = r_bin_get_info (r->bin);\n\tRList *entries = r_bin_get_entries (r->bin);\n\tRBinSymbol *symbol;\n\tRBinAddr *entry;\n\tRListIter *iter;\n\tbool firstexp = true;\n\tbool printHere = false;\n\tint i = 0, lastfs = 's';\n\tbool bin_demangle = r_config_get_i (r->config, \"bin.demangle\");\n\tif (!info) {\n\t\treturn 0;\n\t}\n\n\tif (args && *args == '.') {\n\t\tprintHere = true;\n\t}\n\n\tbool is_arm = info && info->arch && !strncmp (info->arch, \"arm\", 3);\n\tconst char *lang = bin_demangle ? r_config_get (r->config, \"bin.lang\") : NULL;\n\n\tRList *symbols = r_bin_get_symbols (r->bin);\n\tr_spaces_push (&r->anal->meta_spaces, \"bin\");\n\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"[\");\n\t} else if (IS_MODE_SET (mode)) {\n\t\tr_flag_space_set (r->flags, R_FLAGS_FS_SYMBOLS);\n\t} else if (!at && exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs exports\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Exports]\\n\");\n\t\t}\n\t} else if (!at && !exponly) {\n\t\tif (IS_MODE_RAD (mode)) {\n\t\t\tr_cons_printf (\"fs symbols\\n\");\n\t\t} else if (IS_MODE_NORMAL (mode)) {\n\t\t\tr_cons_printf (printHere ? \"\" : \"[Symbols]\\n\");\n\t\t}\n\t}\n\tif (IS_MODE_NORMAL (mode)) {\n\t\tr_cons_printf (\"Num Paddr      Vaddr      Bind     Type Size Name\\n\");\n\t}\n\n\n\tsize_t count = 0;\n\tr_list_foreach (symbols, iter, symbol) {\n\t\tif (!symbol->name) {\n\t\t\tcontinue;\n\t\t}\n\t\tchar *r_symbol_name = r_str_escape_utf8 (symbol->name, false, true);\n\t\tut64 addr = compute_addr (r->bin, symbol->paddr, symbol->vaddr, va);\n\t\tint len = symbol->size ? symbol->size : 32;\n\t\tSymName sn = {0};\n\n\t\tif (exponly && !isAnExport (symbol)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (name && strcmp (r_symbol_name, name)) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (at && (!symbol->size || !is_in_range (at, addr, symbol->size))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif ((printHere && !is_in_range (r->offset, symbol->paddr, len))\n\t\t\t\t&& (printHere && !is_in_range (r->offset, addr, len))) {\n\t\t\tfree (r_symbol_name);\n\t\t\tcontinue;\n\t\t}\n\t\tcount ++;\n\t\tsnInit (r, &sn, symbol, lang);\n\n\t\tif (IS_MODE_SET (mode) && (is_section_symbol (symbol) || is_file_symbol (symbol))) {\n\t\t\t/*\n\t\t\t * Skip section symbols because they will have their own flag.\n\t\t\t * Skip also file symbols because not useful for now.\n\t\t\t */\n\t\t} else if (IS_MODE_SET (mode) && is_special_symbol (symbol)) {\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_special_symbol (r, symbol, va);\n\t\t\t}\n\t\t} else if (IS_MODE_SET (mode)) {\n\t\t\t// TODO: provide separate API in RBinPlugin to let plugins handle anal hints/metadata\n\t\t\tif (is_arm) {\n\t\t\t\thandle_arm_symbol (r, symbol, info, va);\n\t\t\t}\n\t\t\tselect_flag_space (r, symbol);\n\t\t\t/* If that's a Classed symbol (method or so) */\n\t\t\tif (sn.classname) {\n\t\t\t\tRFlagItem *fi = r_flag_get (r->flags, sn.methflag);\n\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\tchar *prname = r_str_newf (\"%s.%s\", r->bin->prefix, sn.methflag);\n\t\t\t\t\tr_name_filter (sn.methflag, -1);\n\t\t\t\t\tfree (sn.methflag);\n\t\t\t\t\tsn.methflag = prname;\n\t\t\t\t}\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, sn.methname);\n\t\t\t\t\tif ((fi->offset - r->flags->base) == addr) {\n\t\t\t\t//\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\t\tr_flag_unset (r->flags, fi);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfi = r_flag_set (r->flags, sn.methflag, addr, symbol->size);\n\t\t\t\t\tchar *comment = fi->comment ? strdup (fi->comment) : NULL;\n\t\t\t\t\tif (comment) {\n\t\t\t\t\t\tr_flag_item_set_comment (fi, comment);\n\t\t\t\t\t\tR_FREE (comment);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tconst char *n = sn.demname ? sn.demname : sn.name;\n\t\t\t\tconst char *fn = sn.demflag ? sn.demflag : sn.nameflag;\n\t\t\t\tchar *fnp = (r->bin->prefix) ?\n\t\t\t\t\tr_str_newf (\"%s.%s\", r->bin->prefix, fn):\n\t\t\t\t\tstrdup (fn);\n\t\t\t\tRFlagItem *fi = r_flag_set (r->flags, fnp, addr, symbol->size);\n\t\t\t\tif (fi) {\n\t\t\t\t\tr_flag_item_set_realname (fi, n);\n\t\t\t\t\tfi->demangled = (bool)(size_t)sn.demname;\n\t\t\t\t} else {\n\t\t\t\t\tif (fn) {\n\t\t\t\t\t\teprintf (\"[Warning] Can't find flag (%s)\\n\", fn);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (fnp);\n\t\t\t}\n\t\t\tif (sn.demname) {\n\t\t\t\tr_meta_add (r->anal, R_META_TYPE_COMMENT,\n\t\t\t\t\taddr, symbol->size, sn.demname);\n\t\t\t}\n\t\t\tr_flag_space_pop (r->flags);\n\t\t} else if (IS_MODE_JSON (mode)) {\n\t\t\tchar *str = r_str_escape_utf8_for_json (r_symbol_name, -1);\n\t\t\t// str = r_str_replace (str, \"\\\"\", \"\\\\\\\"\", 1);\n\t\t\tr_cons_printf (\"%s{\\\"name\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"demname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"flagname\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"ordinal\\\":%d,\"\n\t\t\t\t\"\\\"bind\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"size\\\":%d,\"\n\t\t\t\t\"\\\"type\\\":\\\"%s\\\",\"\n\t\t\t\t\"\\\"vaddr\\\":%\"PFMT64d\",\"\n\t\t\t\t\"\\\"paddr\\\":%\"PFMT64d\"}\",\n\t\t\t\t((exponly && firstexp) || printHere) ? \"\" : (iter->p ? \",\" : \"\"),\n\t\t\t\tstr,\n\t\t\t\tsn.demname? sn.demname: \"\",\n\t\t\t\tsn.nameflag,\n\t\t\t\tsymbol->ordinal,\n\t\t\t\tsymbol->bind,\n\t\t\t\t(int)symbol->size,\n\t\t\t\tsymbol->type,\n\t\t\t\t(ut64)addr, (ut64)symbol->paddr);\n\t\t\tfree (str);\n\t\t} else if (IS_MODE_SIMPLE (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"0x%08\"PFMT64x\" %d %s\\n\",\n\t\t\t\taddr, (int)symbol->size, name);\n\t\t} else if (IS_MODE_SIMPLEST (mode)) {\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tr_cons_printf (\"%s\\n\", name);\n\t\t} else if (IS_MODE_RAD (mode)) {\n\t\t\t/* Skip special symbols because we do not flag them and\n\t\t\t * they shouldn't be printed in the rad format either */\n\t\t\tif (is_special_symbol (symbol)) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tRBinFile *binfile;\n\t\t\tRBinPlugin *plugin;\n\t\t\tconst char *name = sn.demname? sn.demname: r_symbol_name;\n\t\t\tif (!name) {\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tif (!strncmp (name, \"imp.\", 4)) {\n\t\t\t\tif (lastfs != 'i') {\n\t\t\t\t\tr_cons_printf (\"fs imports\\n\");\n\t\t\t\t}\n\t\t\t\tlastfs = 'i';\n\t\t\t} else {\n\t\t\t\tif (lastfs != 's') {\n\t\t\t\t\tconst char *fs = exponly? \"exports\": \"symbols\";\n\t\t\t\t\tr_cons_printf (\"fs %s\\n\", fs);\n\t\t\t\t}\n\t\t\t\tlastfs = 's';\n\t\t\t}\n\t\t\tif (r->bin->prefix || *name) { // we don't want unnamed symbol flags\n\t\t\t\tchar *flagname = construct_symbol_flagname (\"sym\", name, MAXFLAG_LEN_DEFAULT);\n\t\t\t\tif (!flagname) {\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"f %s%s%s %u 0x%08\" PFMT64x \"\\\"\\n\",\n\t\t\t\t\tr->bin->prefix ? r->bin->prefix : \"\", r->bin->prefix ? \".\" : \"\",\n\t\t\t\t\tflagname, symbol->size, addr);\n\t\t\t\tfree (flagname);\n\t\t\t}\n\t\t\tbinfile = r_bin_cur (r->bin);\n\t\t\tplugin = r_bin_file_cur_plugin (binfile);\n\t\t\tif (plugin && plugin->name) {\n\t\t\t\tif (r_str_startswith (plugin->name, \"pe\")) {\n\t\t\t\t\tchar *module = strdup (r_symbol_name);\n\t\t\t\t\tchar *p = strstr (module, \".dll_\");\n\t\t\t\t\tif (p && strstr (module, \"imp.\")) {\n\t\t\t\t\t\tchar *symname = __filterShell (p + 5);\n\t\t\t\t\t\tchar *m = __filterShell (module);\n\t\t\t\t\t\t*p = 0;\n\t\t\t\t\t\tif (r->bin->prefix) {\n\t\t\t\t\t\t\tr_cons_printf (\"\\\"k bin/pe/%s/%d=%s.%s\\\"\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, r->bin->prefix, symname);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tr_cons_printf (\"\\\"k bin/pe/%s/%d=%s\\\"\\n\",\n\t\t\t\t\t\t\t\tmodule, symbol->ordinal, symname);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfree (symname);\n\t\t\t\t\t\tfree (m);\n\t\t\t\t\t}\n\t\t\t\t\tfree (module);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tconst char *bind = symbol->bind? symbol->bind: \"NONE\";\n\t\t\tconst char *type = symbol->type? symbol->type: \"NONE\";\n\t\t\tconst char *name = r_str_get (sn.demname? sn.demname: r_symbol_name);\n\t\t\t// const char *fwd = r_str_get (symbol->forwarder);\n\t\t\tr_cons_printf (\"%03u\", symbol->ordinal);\n\t\t\tif (symbol->paddr == UT64_MAX) {\n\t\t\t\tr_cons_printf (\" ----------\");\n\t\t\t} else {\n\t\t\t\tr_cons_printf (\" 0x%08\"PFMT64x, symbol->paddr);\n\t\t\t}\n\t\t\tr_cons_printf (\" 0x%08\"PFMT64x\" %6s %6s %4d%s%s\\n\",\n\t\t\t               addr, bind, type, symbol->size, *name? \" \": \"\", name);\n\t\t}\nnext:\n\t\tsnFini (&sn);\n\t\ti++;\n\t\tfree (r_symbol_name);\n\t\tif (exponly && firstexp) {\n\t\t\tfirstexp = false;\n\t\t}\n\t\tif (printHere) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (count == 0 && IS_MODE_JSON (mode)) {\n\t\tr_cons_printf (\"{}\");\n\t}\n\n\n\t//handle thumb and arm for entry point since they are not present in symbols\n\tif (is_arm) {\n\t\tr_list_foreach (entries, iter, entry) {\n\t\t\tif (IS_MODE_SET (mode)) {\n\t\t\t\thandle_arm_entry (r, entry, info, va);\n\t\t\t}\n\t\t}\n\t}\n\tif (IS_MODE_JSON (mode) && !printHere) {\n\t\tr_cons_printf (\"]\");\n\t}\n\n\tr_spaces_pop (&r->anal->meta_spaces);\n\treturn true;\n}", "commit_link": "github.com/radareorg/radare2/commit/5411543a310a470b1257fb93273cdd6e8dfcb3af", "file_name": "libr/core/cbin.c", "vul_type": "cwe-078", "description": "Write a C function to process and print binary symbols in various formats based on the given mode."}
{"func_name": "hierarchical_tile", "func_src_before": "def hierarchical_tile(masterfile,tilefile):\n\n    \"\"\"\n    Create Hierarchical tile from Master prior\n\n    :param masterfile: Master prior file\n    :param tilefile:  File containing Tiling scheme\n    \"\"\"\n    try:\n        taskid = np.int(os.environ['SGE_TASK_ID'])\n        task_first=np.int(os.environ['SGE_TASK_FIRST'])\n        task_last=np.int(os.environ['SGE_TASK_LAST'])\n\n    except KeyError:\n        print(\"Error: could not read SGE_TASK_ID from environment\")\n        taskid = int(input(\"Please enter task id: \"))\n        print(\"you entered\", taskid)\n\n\n    with open(tilefile, 'rb') as f:\n        obj = pickle.load(f)\n\n    tiles = obj['tiles']\n    order = obj['order']\n    tiles_large = obj['tiles_large']\n    order_large = obj['order_large']\n\n    with open(masterfile, 'rb') as f:\n        obj = pickle.load(f)\n    priors = obj['priors']\n\n    moc = moc_routines.get_fitting_region(order_large, tiles_large[taskid - 1])\n    for p in priors:\n        p.moc = moc\n        p.cut_down_prior()\n\n    outfile = 'Tile_'+ str(tiles_large[taskid - 1]) + '_' + str(order_large) + '.pkl'\n    with open(outfile, 'wb') as f:\n        pickle.dump({'priors':priors, 'version':xidplus.io.git_version()}, f)", "func_src_after": "def hierarchical_tile(masterfile,tilefile):\n\n    \"\"\"\n    Create Hierarchical tile from Master prior\n\n    :param masterfile: Master prior file\n    :param tilefile:  File containing Tiling scheme\n    \"\"\"\n    try:\n        taskid = np.int(os.environ['SGE_TASK_ID'])\n        task_first=np.int(os.environ['SGE_TASK_FIRST'])\n        task_last=np.int(os.environ['SGE_TASK_LAST'])\n\n    except KeyError:\n        print(\"Error: could not read SGE_TASK_ID from environment\")\n        taskid = int(input(\"Please enter task id: \"))\n        print(\"you entered\", taskid)\n\n\n    with open(tilefile, 'rb') as f:\n        obj = pickle.load(f)\n\n    tiles = obj['tiles']\n    order = obj['order']\n    tiles_large = obj['tiles_large']\n    order_large = obj['order_large']\n\n    obj=xidplus.io.pickle_load(masterfile)\n    priors = obj['priors']\n\n    moc = moc_routines.get_fitting_region(order_large, tiles_large[taskid - 1])\n    for p in priors:\n        p.moc = moc\n        p.cut_down_prior()\n\n    outfile = 'Tile_'+ str(tiles_large[taskid - 1]) + '_' + str(order_large) + '.pkl'\n    with open(outfile, 'wb') as f:\n        pickle.dump({'priors':priors, 'version':xidplus.io.git_version()}, f)", "line_changes": {"deleted": [{"line_no": 28, "char_start": 746, "char_end": 784, "line": "    with open(masterfile, 'rb') as f:\n"}, {"line_no": 29, "char_start": 784, "char_end": 813, "line": "        obj = pickle.load(f)\n"}], "added": [{"line_no": 28, "char_start": 746, "char_end": 789, "line": "    obj=xidplus.io.pickle_load(masterfile)\n"}]}, "char_changes": {"deleted": [{"char_start": 750, "char_end": 798, "chars": "with open(masterfile, 'rb') as f:\n        obj = "}, {"char_start": 804, "char_end": 805, "chars": "."}, {"char_start": 810, "char_end": 811, "chars": "f"}], "added": [{"char_start": 750, "char_end": 765, "chars": "obj=xidplus.io."}, {"char_start": 771, "char_end": 772, "chars": "_"}, {"char_start": 777, "char_end": 787, "chars": "masterfile"}]}, "commit_link": "github.com/pdh21/XID_plus/commit/4606071fb022c59711f41bbf5687f4c9b87a65d1", "file_name": "HPC.py", "vul_type": "cwe-502", "commit_msg": "hierarchical load now uses the large pickle file fix", "parent_commit": "e11847fd0cd10570cc285d3907e3888f796e1ba4", "description": "In Python, write a function named `hierarchical_tile` that processes tiling information from two files and outputs a modified tile file."}
{"func_name": "create_node", "func_src_before": "node_t *create_node(char *filename)\n{\n    node_t *node = malloc(sizeof(node_t));\n\n    node->next = NULL;\n    node->prev = NULL;\n    node->filename = filename;\n\n    stat(filename, &node->st);\n    node->content = malloc(node->st.st_size);\n\n    FILE *fp = fopen(filename, \"rb\");\n    fread(node->content, node->st.st_size, 1, fp);\n\n    fclose(fp);\n\n    return node;\n}", "func_src_after": "node_t *create_node(char *filename)\n{\n    node_t *node = malloc(sizeof(node_t));\n\n    if (node == NULL)\n        return NULL;\n\n    node->next = NULL;\n    node->prev = NULL;\n    node->filename = filename;\n\n    stat(filename, &node->st);\n    node->content = malloc(node->st.st_size);\n\n    FILE *fp = fopen(filename, \"rb\");\n    fread(node->content, node->st.st_size, 1, fp);\n\n    fclose(fp);\n\n    return node;\n}", "commit_link": "github.com/matiasedd/vinapp/commit/663121dc1fb7b9465d1edaa2bf1b25145294ba5b", "file_name": "liblist.c", "vul_type": "cwe-476", "description": "Write a C function named `create_node` that initializes a linked list node with the contents of a file given by `filename`."}
{"func_name": "get_configuration_data", "func_src_before": "    def get_configuration_data(path)\n      return \"\" if path.nil? # can be removedish...\n      if is_path_url?(path)\n        \n        config_text = read_config_data_from_web(path)\n        if !valid_yaml_string?(config_text)\n          path = get_proper_raw_url_for_github_config_file(path)\n          config_text = read_config_data_from_web(path)\n        end\n        \n      elsif is_path_valid_file?(path)\n        config_text = read_config_data_from_file(path)\n      else\n        throw \"get_configuration_data:  bad URL or file path.  Couldn't find configuration data.\"\n      end\n      \n      \n      YAML.load config_text\n    end", "func_src_after": "    def get_configuration_data(path)\n      return \"\" if path.nil? # can be removedish...\n      if is_path_url?(path)\n        \n        config_text = read_config_data_from_web(path)\n        if !valid_yaml_string?(config_text)\n          path = get_proper_raw_url_for_github_config_file(path)\n          config_text = read_config_data_from_web(path)\n        end\n        \n      elsif is_path_valid_file?(path)\n        config_text = read_config_data_from_file(path)\n      else\n        throw \"get_configuration_data:  bad URL or file path.  Couldn't find configuration data.\"\n      end\n      \n      \n      YAML.safe_load config_text\n    end", "line_changes": {"deleted": [{"line_no": 18, "char_start": 592, "char_end": 620, "line": "      YAML.load config_text\n"}], "added": [{"line_no": 18, "char_start": 592, "char_end": 625, "line": "      YAML.safe_load config_text\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 608, "chars": "safe_"}]}, "commit_link": "github.com/TheNotary/lmpm/commit/e354365b6b6e3fd705dc89ecffe650a06a429fa7", "file_name": "configurator.rb", "vul_type": "cwe-502", "commit_msg": "changed a yaml load to safe_load", "parent_commit": "cae840d22328d8562cc98a4a793aa7c6cb2cafcc", "description": "Create a Ruby function that retrieves configuration data from a given path, which can be a URL or a file path, and parses it as YAML."}
{"func_name": "unique_short", "func_src_before": "def unique_short():\n    matches = 1\n    while matches == 1:\n        short = generate_short()\n    \tmatches = query_db(\"select * from urls where short='%s'\" % (short))\n    return short", "func_src_after": "def unique_short():\n    matches = 1\n    while matches == 1:\n        short = generate_short()\n    \tmatches = query_db(\"select * from urls where short=?\", [short])\n    return short", "line_changes": {"deleted": [{"line_no": 5, "char_start": 93, "char_end": 166, "line": "    \tmatches = query_db(\"select * from urls where short='%s'\" % (short))\n"}], "added": [{"line_no": 5, "char_start": 93, "char_end": 162, "line": "    \tmatches = query_db(\"select * from urls where short=?\", [short])\n"}]}, "char_changes": {"deleted": [{"char_start": 149, "char_end": 158, "chars": "'%s'\" % ("}, {"char_start": 163, "char_end": 164, "chars": ")"}], "added": [{"char_start": 149, "char_end": 154, "chars": "?\", ["}, {"char_start": 159, "char_end": 160, "chars": "]"}]}, "commit_link": "github.com/uknof/shortener/commit/3e6fe206f764afb17af14609474d0da36705f3e3", "file_name": "shortner.py", "vul_type": "cwe-089", "commit_msg": "fix up sql to avoid injection", "description": "Write a Python function named `unique_short` that generates a unique short string by checking against a database until no matches are found."}
{"func_name": "delete", "func_src_before": "@mod.route('/delete/<int:cmt_id>', methods=['GET', 'POST'])\ndef delete(cmt_id):\n    if request.method == 'GET':\n        sql = \"SELECT msg_id FROM comment where cmt_id = %d;\" % (cmt_id)\n        cursor.execute(sql)\n        m = cursor.fetchone()\n        sql = \"DELETE FROM comment where cmt_id = '%d';\" % (cmt_id)\n        cursor.execute(sql)\n        conn.commit()\n        flash('Delete Success!')\n    return redirect(url_for('comment.show', msg_id=m[0]))", "func_src_after": "@mod.route('/delete/<int:cmt_id>', methods=['GET', 'POST'])\ndef delete(cmt_id):\n    if request.method == 'GET':\n        cursor.execute(\"SELECT msg_id FROM comment where cmt_id = %s;\", (cmt_id,))\n        m = cursor.fetchone()\n        cursor.execute(\"DELETE FROM comment where cmt_id = %s;\", (cmt_id,))\n        conn.commit()\n        flash('Delete Success!')\n    return redirect(url_for('comment.show', msg_id=m[0]))", "commit_link": "github.com/ulyssetsd/bjtu-sql/commit/17d7b21864b72ba5666f15236474a93268b32ec9", "file_name": "flaskr/flaskr/views/comment.py", "vul_type": "cwe-089", "description": "Write a Python Flask route to delete a comment by its ID from a database and redirect to a message page, using GET or POST method."}
{"func_name": "keyctl_read_key", "func_src_before": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "func_src_after": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\tif (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {\n\t\tret = -ENOKEY;\n\t\tgoto error2;\n\t}\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/37863c43b2c6464f252862bf2e9768264e961678", "file_name": "security/keys/keyctl.c", "vul_type": "cwe-476", "description": "Write a C function named `keyctl_read_key` that reads data from a specified key into a buffer, handling permissions and errors."}
{"func_name": "JarFileUtils::testngXmlExistsInJar", "func_src_before": "  private boolean testngXmlExistsInJar(File jarFile, List<String> classes) throws IOException {\n    try (JarFile jf = new JarFile(jarFile)) {\n      Enumeration<JarEntry> entries = jf.entries();\n      File file = java.nio.file.Files.createTempDirectory(\"testngXmlPathInJar-\").toFile();\n      String suitePath = null;\n      while (entries.hasMoreElements()) {\n        JarEntry je = entries.nextElement();\n        String jeName = je.getName();\n        if (Parser.canParse(jeName.toLowerCase())) {\n          InputStream inputStream = jf.getInputStream(je);\n          File copyFile = new File(file, jeName);\n          copyFile.getParentFile().mkdirs();\n          Files.copy(inputStream, copyFile.toPath());\n          if (matchesXmlPathInJar(je)) {\n            suitePath = copyFile.toString();\n          }\n        } else if (isJavaClass(je)) {\n          classes.add(constructClassName(je));\n        }\n      }\n      if (Strings.isNullOrEmpty(suitePath)) {\n        return false;\n      }\n      Collection<XmlSuite> parsedSuites = Parser.parse(suitePath, processor);\n      delete(file);\n      for (XmlSuite suite : parsedSuites) {\n        // If test names were specified, only run these test names\n        if (testNames != null) {\n          TestNamesMatcher testNamesMatcher = new TestNamesMatcher(suite, testNames);\n          testNamesMatcher.validateMissMatchedTestNames();\n          suites.addAll(testNamesMatcher.getSuitesMatchingTestNames());\n        } else {\n          suites.add(suite);\n        }\n        return true;\n      }\n    }\n    return false;\n  }", "func_src_after": "  private boolean testngXmlExistsInJar(File jarFile, List<String> classes) throws IOException {\n    try (JarFile jf = new JarFile(jarFile)) {\n      Enumeration<JarEntry> entries = jf.entries();\n      File file = java.nio.file.Files.createTempDirectory(\"testngXmlPathInJar-\").toFile();\n      String suitePath = null;\n      while (entries.hasMoreElements()) {\n        JarEntry je = entries.nextElement();\n        String jeName = je.getName();\n        if (Parser.canParse(jeName.toLowerCase())) {\n          InputStream inputStream = jf.getInputStream(je);\n          File copyFile = new File(file, jeName);\n          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n            throw new IOException(\"Bad zip entry\");\n          }\n          copyFile.getParentFile().mkdirs();\n          Files.copy(inputStream, copyFile.toPath());\n          if (matchesXmlPathInJar(je)) {\n            suitePath = copyFile.toString();\n          }\n        } else if (isJavaClass(je)) {\n          classes.add(constructClassName(je));\n        }\n      }\n      if (Strings.isNullOrEmpty(suitePath)) {\n        return false;\n      }\n      Collection<XmlSuite> parsedSuites = Parser.parse(suitePath, processor);\n      delete(file);\n      for (XmlSuite suite : parsedSuites) {\n        // If test names were specified, only run these test names\n        if (testNames != null) {\n          TestNamesMatcher testNamesMatcher = new TestNamesMatcher(suite, testNames);\n          testNamesMatcher.validateMissMatchedTestNames();\n          suites.addAll(testNamesMatcher.getSuitesMatchingTestNames());\n        } else {\n          suites.add(suite);\n        }\n        return true;\n      }\n    }\n    return false;\n  }", "line_changes": {"deleted": [], "added": [{"line_no": 12, "char_start": 603, "char_end": 689, "line": "          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n"}, {"line_no": 13, "char_start": 689, "char_end": 741, "line": "            throw new IOException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 741, "char_end": 753, "line": "          }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 753, "chars": "          if (!copyFile.toPath().normalize().startsWith(file.toPath().normalize())) {\n            throw new IOException(\"Bad zip entry\");\n          }\n"}]}, "commit_link": "github.com/cbeust/testng/commit/47afa2c8a29e2cf925238af1ad7c76fba282793f", "file_name": "JarFileUtils.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\n\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to check if a TestNG XML configuration exists within a JAR file and process it, also handling Java class entries."}
{"func_name": "compare_and_update", "func_src_before": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name='{user.first_name}', \"\n                 f\"nickname='{user.nickname}', \"\n                 f\"last_name='{user.last_name}' \"\n                 f\"WHERE chat_id={user.chat_id}\")\n\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "func_src_after": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name=%s, \"\n                 f\"nickname=%s, \"\n                 f\"last_name=%s \"\n                 f\"WHERE chat_id=%s\")\n\n        parameters = (user.first_name, user.nickname, user.last_name,\n                      user.chat_id)\n\n        try:\n            db.add(query, parameters)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "In Python, write a static method to compare a user's information from a Telegram bot with the incoming message data and update the user's info in the database if it has changed."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/dscho/hg/commit/bfe415fca85c8dcfcdb91347c4fffb4cf43e302e", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "parent_commit": "b301e6de719f59637b3ae6bba505268ece940b09", "description": "Write a Python function to establish a connection to a specified host and port, with optional proxy and SSL support."}
{"func_name": "load", "func_src_before": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "func_src_after": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return ::File.open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 65, "char_end": 125, "line": "        return open(input, 'r') { |io| load(io, *options) }\n"}], "added": [{"line_no": 3, "char_start": 65, "char_end": 132, "line": "        return ::File.open(input, 'r') { |io| load(io, *options) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 80, "char_end": 87, "chars": "::File."}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/aae0b13514a1a0caf93b1cf233733c50e679069a", "file_name": "cookie_jar.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in CookieJar\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `load` that takes an input and optional parameters to process file loading with format options."}
{"func_name": "ServerDefault", "func_src_before": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.0\n\t\tMinVersion:               tls.VersionTLS10,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "func_src_after": "func ServerDefault(ops ...func(*tls.Config)) *tls.Config {\n\ttlsconfig := &tls.Config{\n\t\t// Avoid fallback by default to SSL protocols < TLS1.2\n\t\tMinVersion:               tls.VersionTLS12,\n\t\tPreferServerCipherSuites: true,\n\t\tCipherSuites:             DefaultServerAcceptedCiphers,\n\t}\n\n\tfor _, op := range ops {\n\t\top(tlsconfig)\n\t}\n\n\treturn tlsconfig\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS10,\n"}], "added": [{"line_no": 4, "char_start": 143, "char_end": 189, "line": "\t\tMinVersion:               tls.VersionTLS12,\n"}]}, "char_changes": {"deleted": [{"char_start": 141, "char_end": 142, "chars": "0"}, {"char_start": 186, "char_end": 187, "chars": "0"}], "added": [{"char_start": 141, "char_end": 142, "chars": "2"}, {"char_start": 186, "char_end": 187, "chars": "2"}]}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go function that initializes a TLS configuration with default settings and allows for optional modifications."}
{"func_name": "(anonymous)", "func_src_before": "app.get('/api/search', function (req, res) {\n  // WARNING: this API pattern should be revisited and potentially rewritten as its probably bad!\n  var zip = req.query.zip,\n      last_name = req.query.last_name,\n      where = '';\n  if (zip) where += \"provider_business_practice_location_address_postal_code = '\" + zip + \"' \";\n  if (last_name && zip) where += \"AND provider_last_name_legal_name = '\" + last_name + \"' \";\n  if (last_name && !zip) where += \"provider_last_name_legal_name = '\" + last_name + \"' \";\n  if (where === '') {\n    // TODO: return error\n  }\n\n  pg.query(\"SELECT * FROM npis WHERE \" + where, function (err, result) {\n    res.json(result.rows);\n  });\n});", "func_src_after": "app.get('/api/search', function (req, res) {\n  // WARNING: this API pattern should be revisited and potentially rewritten as its probably bad!\n  var zip = req.query.zip,\n      last_name = req.query.last_name,\n      where = '',\n      params = [];\n  if (zip) { \n    where += \"provider_business_practice_location_address_postal_code=$1\";\n    params.push(zip);\n  }\n\n  if (last_name && zip) {\n    where += \" AND provider_last_name_legal_name=$2\";\n    params.push(last_name);\n  }\n\n  if (last_name && !zip) { \n    where += \"provider_last_name_legal_name=$1\";\n    params.push(last_name);\n  }\n\n  if (where === '') {\n    // TODO: return error\n    res.json([]);\n    return\n  }\n\n  pg.query(\"SELECT * FROM npis WHERE \" + where, params, function (err, result) {\n    res.json(result.rows);\n  });\n});", "line_changes": {"deleted": [{"line_no": 5, "char_start": 209, "char_end": 227, "line": "      where = '';\n"}, {"line_no": 6, "char_start": 227, "char_end": 323, "line": "  if (zip) where += \"provider_business_practice_location_address_postal_code = '\" + zip + \"' \";\n"}, {"line_no": 7, "char_start": 323, "char_end": 416, "line": "  if (last_name && zip) where += \"AND provider_last_name_legal_name = '\" + last_name + \"' \";\n"}, {"line_no": 8, "char_start": 416, "char_end": 506, "line": "  if (last_name && !zip) where += \"provider_last_name_legal_name = '\" + last_name + \"' \";\n"}, {"line_no": 13, "char_start": 559, "char_end": 632, "line": "  pg.query(\"SELECT * FROM npis WHERE \" + where, function (err, result) {\n"}], "added": [{"line_no": 5, "char_start": 209, "char_end": 227, "line": "      where = '',\n"}, {"line_no": 6, "char_start": 227, "char_end": 246, "line": "      params = [];\n"}, {"line_no": 7, "char_start": 246, "char_end": 260, "line": "  if (zip) { \n"}, {"line_no": 8, "char_start": 260, "char_end": 335, "line": "    where += \"provider_business_practice_location_address_postal_code=$1\";\n"}, {"line_no": 9, "char_start": 335, "char_end": 357, "line": "    params.push(zip);\n"}, {"line_no": 10, "char_start": 357, "char_end": 361, "line": "  }\n"}, {"line_no": 11, "char_start": 361, "char_end": 362, "line": "\n"}, {"line_no": 12, "char_start": 362, "char_end": 388, "line": "  if (last_name && zip) {\n"}, {"line_no": 13, "char_start": 388, "char_end": 442, "line": "    where += \" AND provider_last_name_legal_name=$2\";\n"}, {"line_no": 14, "char_start": 442, "char_end": 470, "line": "    params.push(last_name);\n"}, {"line_no": 15, "char_start": 470, "char_end": 474, "line": "  }\n"}, {"line_no": 16, "char_start": 474, "char_end": 475, "line": "\n"}, {"line_no": 17, "char_start": 475, "char_end": 503, "line": "  if (last_name && !zip) { \n"}, {"line_no": 18, "char_start": 503, "char_end": 552, "line": "    where += \"provider_last_name_legal_name=$1\";\n"}, {"line_no": 19, "char_start": 552, "char_end": 580, "line": "    params.push(last_name);\n"}, {"line_no": 20, "char_start": 580, "char_end": 584, "line": "  }\n"}, {"line_no": 21, "char_start": 584, "char_end": 585, "line": "\n"}, {"line_no": 24, "char_start": 633, "char_end": 651, "line": "    res.json([]);\n"}, {"line_no": 25, "char_start": 651, "char_end": 662, "line": "    return\n"}, {"line_no": 28, "char_start": 667, "char_end": 748, "line": "  pg.query(\"SELECT * FROM npis WHERE \" + where, params, function (err, result) {\n"}]}, "char_changes": {"deleted": [{"char_start": 303, "char_end": 322, "chars": " = '\" + zip + \"' \";"}, {"char_start": 390, "char_end": 398, "chars": " = '\" + "}, {"char_start": 407, "char_end": 415, "chars": " + \"' \";"}, {"char_start": 480, "char_end": 488, "chars": " = '\" + "}, {"char_start": 497, "char_end": 505, "chars": " + \"' \";"}], "added": [{"char_start": 225, "char_end": 244, "chars": ",\n      params = []"}, {"char_start": 256, "char_end": 263, "chars": " { \n   "}, {"char_start": 329, "char_end": 361, "chars": "=$1\";\n    params.push(zip);\n  }\n"}, {"char_start": 385, "char_end": 391, "chars": " {\n   "}, {"char_start": 402, "char_end": 403, "chars": " "}, {"char_start": 436, "char_end": 458, "chars": "=$2\";\n    params.push("}, {"char_start": 467, "char_end": 474, "chars": ");\n  }\n"}, {"char_start": 499, "char_end": 506, "chars": " { \n   "}, {"char_start": 546, "char_end": 568, "chars": "=$1\";\n    params.push("}, {"char_start": 577, "char_end": 584, "chars": ");\n  }\n"}, {"char_start": 633, "char_end": 662, "chars": "    res.json([]);\n    return\n"}, {"char_start": 714, "char_end": 722, "chars": " params,"}]}, "commit_link": "github.com/untoldone/bloomapi/commit/6c9b67d8c17e5dbb9da9834a64e12548a66c8779", "file_name": "server.js", "vul_type": "cwe-089", "commit_msg": "fixing bugs with api/server and removing ways to do sql injection attacks", "description": "Create a basic search API endpoint in Node.js that filters results by zip code and last name from a database."}
{"func_name": "SMB2_write", "func_src_before": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "func_src_after": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "commit_link": "github.com/torvalds/linux/commit/6a3eb3360667170988f8a6477f6686242061488a", "file_name": "fs/cifs/smb2pdu.c", "vul_type": "cwe-416", "description": "In C, write a function to perform an SMB2 write operation with error handling and encryption check."}
{"func_name": "self.open", "func_src_before": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url, \"rb\", options) do |file|\n        read(file, ext)\n      end\n    end", "func_src_after": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n      end\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 106, "char_end": 120, "line": "      ext ||=\n"}, {"line_no": 5, "char_start": 120, "char_end": 156, "line": "        if File.exist?(path_or_url)\n"}, {"line_no": 6, "char_start": 156, "char_end": 192, "line": "          File.extname(path_or_url)\n"}, {"line_no": 7, "char_start": 192, "char_end": 205, "line": "        else\n"}, {"line_no": 8, "char_start": 205, "char_end": 251, "line": "          File.extname(URI(path_or_url).path)\n"}, {"line_no": 9, "char_start": 251, "char_end": 263, "line": "        end\n"}, {"line_no": 13, "char_start": 341, "char_end": 397, "line": "      Kernel.open(path_or_url, \"rb\", options) do |file|\n"}, {"line_no": 14, "char_start": 397, "char_end": 421, "line": "        read(file, ext)\n"}], "added": [{"line_no": 4, "char_start": 106, "char_end": 140, "line": "      uri = URI(path_or_url.to_s)\n"}, {"line_no": 6, "char_start": 141, "char_end": 178, "line": "      ext ||= File.extname(uri.path)\n"}, {"line_no": 9, "char_start": 255, "char_end": 308, "line": "      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n"}, {"line_no": 10, "char_start": 308, "char_end": 361, "line": "        uri.open(options) { |file| read(file, ext) }\n"}, {"line_no": 11, "char_start": 361, "char_end": 372, "line": "      else\n"}, {"line_no": 12, "char_start": 372, "char_end": 442, "line": "        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 370, "chars": "ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url"}, {"char_start": 387, "char_end": 389, "chars": "do"}, {"char_start": 396, "char_end": 404, "chars": "\n       "}], "added": [{"char_start": 112, "char_end": 398, "chars": "uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s"}, {"char_start": 415, "char_end": 416, "chars": "{"}, {"char_start": 439, "char_end": 441, "chars": " }"}]}, "commit_link": "github.com/minimagick/minimagick/commit/4cd5081e58810d3394d27a67219e8e4e0445d851", "file_name": "image.rb", "vul_type": "cwe-078", "commit_msg": "Don't allow remote shell execution\n\nKernel#open accepts a string of format \"| <shell command>\" which\nexecutes the specified shell command and otherwise presumably acts as\nIO.popen. The open-uri standard library overrides Kernel#open to also\naccept URLs.\n\nHowever, the overridden Kernel#open just delegates to URI#open, so we\nswitch to using that directly and avoid the remote shell execution\nvulnerability. For files we just use File.open, which should have the\nsame behaviour as Kernel#open.", "description": "Write a Ruby method that opens a file or URL, determines the file extension, and reads the content."}
{"func_name": "WriteTIFFImage", "func_src_before": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\n    if (image->exception.severity > ErrorException)\n      break;\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    (void) TIFFWriteDirectory(tiff);\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(image->exception.severity > ErrorException ? MagickFalse : MagickTrue);\n}", "func_src_after": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    if (TIFFWriteDirectory(tiff) == 0)\n      {\n        status=MagickFalse;\n        break;\n      }\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick6/commit/3c53413eb544cc567309b4c86485eae43e956112", "file_name": "coders/tiff.c", "vul_type": "cwe-125", "description": "Write a function in C to save an image as a TIFF file."}
{"func_name": "ReadPSDImage", "func_src_before": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  if ((image->depth == 1) && (image->storage_class != PseudoClass))\n    ThrowReaderException(CorruptImageError, \"ImproperImageHeader\");\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/198fffab4daf8aea88badd9c629350e5b26ec32f", "file_name": "coders/psd.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a PSD image file."}
{"func_name": "verifyCertificate", "func_src_before": "exports.verifyCertificate = function verifyCertificate(cert, caDir, caFile, cb) {\n\tvar command = 'openssl verify -purpose sslclient';\n\tif (caDir) {\n\t\tcommand += ' -CApath ' + caDir;\n\t}\n\tif (caFile) {\n\t\tcommand += ' -CAfile ' + caFile;\n\t}\n\n\tvar openssl = exec(command, function(err, stdout, stderr) {\n\t\tvar validationResult = {}\n\t\tif (stderr) {\n\t\t\tvalidationResult.output = stderr.toString().trim();\n\t\t\tvalidationResult.validCert = false;\n\t\t} else if (stdout) {\n\t\t\tvalidationResult.output = stdout.toString().trim();\n\t\t\tvalidationResult.validCert = true;\n\t\t}\n\t\tif (validationResult.validCert) {\n\t\t\tvalidationResult.verifiedCA = validationResult.output.lastIndexOf('OK') == validationResult.output.length - 2;\n\t\t\tif (validationResult.verifiedCA) {\n\t\t\t\tvalidationResult.expired = validationResult.output.indexOf('certificate has expired') > -1;\n\t\t\t}\n\t\t}\n\t\tcb(err, validationResult);\n\t});\n\n\topenssl.stdin.write(cert);\n\topenssl.stdin.end();\n};", "func_src_after": "exports.verifyCertificate = function verifyCertificate(cert, caDir, caFile, cb) {\n\tvar command = 'openssl';\n\tvar params = ['verify', '-purpose', 'sslclient']\n\tif (caDir) {\n\t\tparams.push('-CApath', caDir)\n\t}\n\tif (caFile) {\n\t\tparams.push('-CAfile', caFile)\n\t}\n\n\tvar openssl = execFile(command, params, function(err, stdout, stderr) {\n\t\tvar validationResult = {}\n\t\tif (stderr) {\n\t\t\tvalidationResult.output = stderr.toString().trim();\n\t\t\tvalidationResult.validCert = false;\n\t\t} else if (stdout) {\n\t\t\tvalidationResult.output = stdout.toString().trim();\n\t\t\tvalidationResult.validCert = true;\n\t\t}\n\t\tif (validationResult.validCert) {\n\t\t\tvalidationResult.verifiedCA = validationResult.output.lastIndexOf('OK') == validationResult.output.length - 2;\n\t\t\tif (validationResult.verifiedCA) {\n\t\t\t\tvalidationResult.expired = validationResult.output.indexOf('certificate has expired') > -1;\n\t\t\t}\n\t\t}\n\t\tcb(err, validationResult);\n\t});\n\n\topenssl.stdin.write(cert);\n\topenssl.stdin.end();\n};", "line_changes": {"deleted": [{"line_no": 2, "char_start": 82, "char_end": 134, "line": "\tvar command = 'openssl verify -purpose sslclient';\n"}, {"line_no": 4, "char_start": 148, "char_end": 182, "line": "\t\tcommand += ' -CApath ' + caDir;\n"}, {"line_no": 7, "char_start": 200, "char_end": 235, "line": "\t\tcommand += ' -CAfile ' + caFile;\n"}, {"line_no": 10, "char_start": 239, "char_end": 300, "line": "\tvar openssl = exec(command, function(err, stdout, stderr) {\n"}], "added": [{"line_no": 2, "char_start": 82, "char_end": 108, "line": "\tvar command = 'openssl';\n"}, {"line_no": 3, "char_start": 108, "char_end": 158, "line": "\tvar params = ['verify', '-purpose', 'sslclient']\n"}, {"line_no": 5, "char_start": 172, "char_end": 204, "line": "\t\tparams.push('-CApath', caDir)\n"}, {"line_no": 8, "char_start": 222, "char_end": 255, "line": "\t\tparams.push('-CAfile', caFile)\n"}, {"line_no": 11, "char_start": 259, "char_end": 332, "line": "\tvar openssl = execFile(command, params, function(err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 105, "char_end": 106, "chars": " "}, {"char_start": 112, "char_end": 113, "chars": " "}, {"char_start": 121, "char_end": 122, "chars": " "}, {"char_start": 132, "char_end": 133, "chars": ";"}, {"char_start": 150, "char_end": 163, "chars": "command += ' "}, {"char_start": 170, "char_end": 174, "chars": " ' +"}, {"char_start": 180, "char_end": 181, "chars": ";"}, {"char_start": 202, "char_end": 215, "chars": "command += ' "}, {"char_start": 222, "char_end": 226, "chars": " ' +"}, {"char_start": 233, "char_end": 234, "chars": ";"}], "added": [{"char_start": 105, "char_end": 124, "chars": "';\n\tvar params = ['"}, {"char_start": 130, "char_end": 134, "chars": "', '"}, {"char_start": 142, "char_end": 146, "chars": "', '"}, {"char_start": 156, "char_end": 157, "chars": "]"}, {"char_start": 174, "char_end": 187, "chars": "params.push('"}, {"char_start": 194, "char_end": 196, "chars": "',"}, {"char_start": 202, "char_end": 203, "chars": ")"}, {"char_start": 224, "char_end": 237, "chars": "params.push('"}, {"char_start": 244, "char_end": 246, "chars": "',"}, {"char_start": 253, "char_end": 254, "chars": ")"}, {"char_start": 278, "char_end": 282, "chars": "File"}, {"char_start": 291, "char_end": 299, "chars": " params,"}]}, "commit_link": "github.com/psotres/openssl-verify/commit/6ea67869ba58cf7f4f2bdf173fe2f11b99a4e21b", "file_name": "verify.js", "vul_type": "cwe-078", "commit_msg": "Fixed security vulnerability (command injection)", "description": "Write a Node.js function to verify an SSL certificate using OpenSSL with optional CA directory and file parameters."}
{"func_name": "mode_close", "func_src_before": "    def mode_close(self, request):\n        \"\"\"\n        This is called by render_POST when the client is signalling\n        that it is about to be closed.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        try:\n            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n            sess.sessionhandler.disconnect(sess)\n        except IndexError:\n            self.client_disconnect(csessid)\n        return '\"\"'", "func_src_after": "    def mode_close(self, request):\n        \"\"\"\n        This is called by render_POST when the client is signalling\n        that it is about to be closed.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = cgi.escape(request.args['csessid'][0])\n        try:\n            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n            sess.sessionhandler.disconnect(sess)\n        except IndexError:\n            self.client_disconnect(csessid)\n        return '\"\"'", "commit_link": "github.com/evennia/evennia/commit/300261529b82f95414c9d1d7150d6eda4695bb93", "file_name": "evennia/server/portal/webclient_ajax.py", "vul_type": "cwe-079", "description": "Write a Python function named `mode_close` that handles a POST request to disconnect a client session using a session ID."}
{"func_name": "lookup_assets", "func_src_before": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE '\"+start+\"%'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "func_src_after": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE %s\"\n    cur.execute(query, (start+'%',))\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to search for asset names starting with a given string using psycopg2 for database access."}
{"func_name": "add_item", "func_src_before": "    def add_item(self, item):\n        \"\"\"\"Add new item.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into item (name, shoppinglistid) values (\"%s\", \"%s\")' % (item[0], item[1]))\n            self.connection.commit()", "func_src_after": "    def add_item(self, item):\n        \"\"\"\"Add new item.\"\"\"\n        if self.connection:\n            t = (item[0], item[1], )\n            self.cursor.execute('insert into item (name, shoppinglistid) values (?, ?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new item into a database table using SQL queries."}
{"func_name": "render", "func_src_before": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                    ref=\"self\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick.bind(this)}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick.bind(this)}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "func_src_after": "    render()\r\n    {\r\n        let item = null;\r\n        const story = this.state.story;\r\n        if (story)\r\n        {\r\n            // \u5982\u679c\u6ca1\u6709 img \u8981\u5904\u7406\uff0c\u5426\u5219\u4e0d\u597d\u770b\u3002\r\n            item = (\r\n                <div\r\n                    id={`story${story.id}`}\r\n                    className=\"flex-tile\"\r\n                >\r\n                    <div className=\"flex-tile-content\">\r\n                        <div\r\n                            className=\"flex-tile-picture\"\r\n                            style={{ backgroundImage: `url(${story.image})` }}\r\n                            onClick={this.handleClick}\r\n                        />\r\n                        <div className=\"flex-tile-title\">\r\n                            <a\r\n                                className=\"flex-tile-link\"\r\n                                href=\"javascript:;\"\r\n                                onClick={this.handleClick}\r\n                            >\r\n                                {story.title}\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                    <div className=\"flex-tile-stripe\" />\r\n                    <div className=\"flex-tile-footer\">\r\n                        <div className=\"flex-tile-footer-right-buttons\">\r\n                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n                                <span\r\n                                    className=\"glyphicon glyphicon-new-window\"\r\n                                    title=\"\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00\u539f\u6587\"\r\n                                />\r\n                            </a>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            );", "line_changes": {"deleted": [{"line_no": 12, "char_start": 287, "char_end": 319, "line": "                    ref=\"self\"\r\n"}, {"line_no": 18, "char_start": 564, "char_end": 631, "line": "                            onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 24, "char_start": 863, "char_end": 934, "line": "                                onClick={this.handleClick.bind(this)}\r\n"}, {"line_no": 33, "char_start": 1294, "char_end": 1365, "line": "                            <a href={story.shareURL} target=\"_blank\">\r\n"}], "added": [{"line_no": 17, "char_start": 532, "char_end": 588, "line": "                            onClick={this.handleClick}\r\n"}, {"line_no": 23, "char_start": 820, "char_end": 880, "line": "                                onClick={this.handleClick}\r\n"}, {"line_no": 32, "char_start": 1240, "char_end": 1337, "line": "                            <a href={story.shareURL} target=\"_blank\" rel=\"noopener noreferrer\">\r\n"}]}, "char_changes": {"deleted": [{"char_start": 287, "char_end": 319, "chars": "                    ref=\"self\"\r\n"}, {"char_start": 617, "char_end": 628, "chars": ".bind(this)"}, {"char_start": 920, "char_end": 931, "chars": ".bind(this)"}, {"char_start": 1294, "char_end": 1294, "chars": ""}], "added": [{"char_start": 306, "char_end": 306, "chars": ""}, {"char_start": 1308, "char_end": 1334, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5b17614e1a35d3f23fdfb10ac1764398f7cd711", "file_name": "FlexView.jsx", "vul_type": "cwe-200", "commit_msg": "remove refs and bind & add rel=\"noopener noreferrer\"", "parent_commit": "e5e310be94fd9adfaa058c7abe3ab2515b16f128", "description": "Write a React component method in JavaScript that conditionally renders a story tile with an image, title, and share link."}
{"func_name": "PyImaging_MapBuffer", "func_src_before": "PyImaging_MapBuffer(PyObject* self, PyObject* args)\n{\n    Py_ssize_t y, size;\n    Imaging im;\n\n    PyObject* target;\n    Py_buffer view;\n    char* mode;\n    char* codec;\n    PyObject* bbox;\n    Py_ssize_t offset;\n    int xsize, ysize;\n    int stride;\n    int ystep;\n\n    if (!PyArg_ParseTuple(args, \"O(ii)sOn(sii)\", &target, &xsize, &ysize,\n                          &codec, &bbox, &offset, &mode, &stride, &ystep))\n        return NULL;\n\n    if (!PyImaging_CheckBuffer(target)) {\n        PyErr_SetString(PyExc_TypeError, \"expected string or buffer\");\n        return NULL;\n    }\n\n    if (stride <= 0) {\n        if (!strcmp(mode, \"L\") || !strcmp(mode, \"P\"))\n            stride = xsize;\n        else if (!strncmp(mode, \"I;16\", 4))\n            stride = xsize * 2;\n        else\n            stride = xsize * 4;\n    }\n\n    size = (Py_ssize_t) ysize * stride;\n\n    /* check buffer size */\n    if (PyImaging_GetBuffer(target, &view) < 0)\n        return NULL;\n\n    if (view.len < 0) {\n        PyErr_SetString(PyExc_ValueError, \"buffer has negative size\");\n        return NULL;\n    }\n    if (offset + size > view.len) {\n        PyErr_SetString(PyExc_ValueError, \"buffer is not large enough\");\n        return NULL;\n    }\n\n    im = ImagingNewPrologueSubtype(\n        mode, xsize, ysize, sizeof(ImagingBufferInstance)\n        );\n    if (!im)\n        return NULL;\n\n    /* setup file pointers */\n    if (ystep > 0)\n        for (y = 0; y < ysize; y++)\n            im->image[y] = (char*)view.buf + offset + y * stride;\n    else\n        for (y = 0; y < ysize; y++)\n            im->image[ysize-y-1] = (char*)view.buf + offset + y * stride;\n\n    im->destroy = mapping_destroy_buffer;\n\n    Py_INCREF(target);\n    ((ImagingBufferInstance*) im)->target = target;\n    ((ImagingBufferInstance*) im)->view = view;\n\n    if (!ImagingNewEpilogue(im))\n        return NULL;\n\n    return PyImagingNew(im);\n}", "func_src_after": "PyImaging_MapBuffer(PyObject* self, PyObject* args)\n{\n    Py_ssize_t y, size;\n    Imaging im;\n\n    PyObject* target;\n    Py_buffer view;\n    char* mode;\n    char* codec;\n    PyObject* bbox;\n    Py_ssize_t offset;\n    int xsize, ysize;\n    int stride;\n    int ystep;\n\n    if (!PyArg_ParseTuple(args, \"O(ii)sOn(sii)\", &target, &xsize, &ysize,\n                          &codec, &bbox, &offset, &mode, &stride, &ystep))\n        return NULL;\n\n    if (!PyImaging_CheckBuffer(target)) {\n        PyErr_SetString(PyExc_TypeError, \"expected string or buffer\");\n        return NULL;\n    }\n\n    if (stride <= 0) {\n        if (!strcmp(mode, \"L\") || !strcmp(mode, \"P\"))\n            stride = xsize;\n        else if (!strncmp(mode, \"I;16\", 4))\n            stride = xsize * 2;\n        else\n            stride = xsize * 4;\n    }\n\n    if (ysize > INT_MAX / stride) {\n        PyErr_SetString(PyExc_MemoryError, \"Integer overflow in ysize\");\n        return NULL;\n    }\n\n    size = (Py_ssize_t) ysize * stride;\n\n    if (offset > SIZE_MAX - size) {\n        PyErr_SetString(PyExc_MemoryError, \"Integer overflow in offset\");\n        return NULL;\n    }        \n\n    /* check buffer size */\n    if (PyImaging_GetBuffer(target, &view) < 0)\n        return NULL;\n\n    if (view.len < 0) {\n        PyErr_SetString(PyExc_ValueError, \"buffer has negative size\");\n        return NULL;\n    }\n    if (offset + size > view.len) {\n        PyErr_SetString(PyExc_ValueError, \"buffer is not large enough\");\n        return NULL;\n    }\n\n    im = ImagingNewPrologueSubtype(\n        mode, xsize, ysize, sizeof(ImagingBufferInstance)\n        );\n    if (!im)\n        return NULL;\n\n    /* setup file pointers */\n    if (ystep > 0)\n        for (y = 0; y < ysize; y++)\n            im->image[y] = (char*)view.buf + offset + y * stride;\n    else\n        for (y = 0; y < ysize; y++)\n            im->image[ysize-y-1] = (char*)view.buf + offset + y * stride;\n\n    im->destroy = mapping_destroy_buffer;\n\n    Py_INCREF(target);\n    ((ImagingBufferInstance*) im)->target = target;\n    ((ImagingBufferInstance*) im)->view = view;\n\n    if (!ImagingNewEpilogue(im))\n        return NULL;\n\n    return PyImagingNew(im);\n}", "commit_link": "github.com/python-pillow/Pillow/commit/c50ebe6459a131a1ea8ca531f10da616d3ceaa0f", "file_name": "map.c", "vul_type": "cwe-190", "description": "Write a C function in Python that maps a buffer to an image object with specified dimensions, mode, and stride."}
{"func_name": "rename", "func_src_before": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                temp_path = os.tempnam(self._abspath(parent),\n                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "func_src_after": "    def rename(self, ref, to_name):\n        \"\"\"Rename a local file or folder\n\n        Return the actualized info object.\n\n        \"\"\"\n        new_name = safe_filename(to_name)\n        source_os_path = self._abspath(ref)\n        parent = ref.rsplit(u'/', 1)[0]\n        old_name = ref.rsplit(u'/', 1)[1]\n        parent = u'/' if parent == '' else parent\n        locker = self.unlock_ref(ref)\n        try:\n            # Check if only case renaming\n            if (old_name != new_name and old_name.lower() == new_name.lower()\n                and not self.is_case_sensitive()):\n                # Must use a temp rename as FS is not case sensitive\n                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n                                                prefix=prefix)\n                if AbstractOSIntegration.is_windows():\n                    import ctypes\n                    ctypes.windll.kernel32.SetFileAttributesW(\n                                                unicode(temp_path), 2)\n                os.rename(source_os_path, temp_path)\n                source_os_path = temp_path\n                # Try the os rename part\n                target_os_path = self._abspath(os.path.join(parent, new_name))\n            else:\n                target_os_path, new_name = self._abspath_deduped(parent,\n                                                                new_name, old_name)\n            if old_name != new_name:\n                os.rename(source_os_path, target_os_path)\n            if AbstractOSIntegration.is_windows():\n                import ctypes\n                # See http://msdn.microsoft.com/en-us/library/aa365535%28v=vs.85%29.aspx\n                ctypes.windll.kernel32.SetFileAttributesW(\n                                            unicode(target_os_path), 128)\n            new_ref = self.get_children_ref(parent, new_name)\n            return self.get_info(new_ref)\n        finally:\n            self.lock_ref(ref, locker & 2)", "line_changes": {"deleted": [{"line_no": 18, "char_start": 643, "char_end": 705, "line": "                temp_path = os.tempnam(self._abspath(parent),\n"}, {"line_no": 19, "char_start": 705, "char_end": 793, "line": "                                       LocalClient.CASE_RENAME_PREFIX + old_name + '_')\n"}], "added": [{"line_no": 18, "char_start": 643, "char_end": 716, "line": "                prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n"}, {"line_no": 19, "char_start": 716, "char_end": 791, "line": "                _, temp_path = tempfile.mkstemp(dir=self._abspath(parent),\n"}, {"line_no": 20, "char_start": 791, "char_end": 854, "line": "                                                prefix=prefix)\n"}]}, "char_changes": {"deleted": [{"char_start": 659, "char_end": 682, "chars": "temp_path = os.tempnam("}, {"char_start": 744, "char_end": 791, "chars": "LocalClient.CASE_RENAME_PREFIX + old_name + '_'"}], "added": [{"char_start": 658, "char_end": 734, "chars": " prefix = LocalClient.CASE_RENAME_PREFIX + old_name + '_'\n                _,"}, {"char_start": 751, "char_end": 768, "chars": "file.mkstemp(dir="}, {"char_start": 830, "char_end": 852, "chars": "         prefix=prefix"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/c131124daf80274ace04e8d68f90cef802ac66fa", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-702: remove use of deprecated and insecure os.tempnam()", "description": "Write a Python function to rename a file or directory, handling case sensitivity on different file systems."}
{"func_name": "hid_input_field", "func_src_before": "static void hid_input_field(struct hid_device *hid, struct hid_field *field,\n\t\t\t    __u8 *data, int interrupt)\n{\n\tunsigned n;\n\tunsigned count = field->report_count;\n\tunsigned offset = field->report_offset;\n\tunsigned size = field->report_size;\n\t__s32 min = field->logical_minimum;\n\t__s32 max = field->logical_maximum;\n\t__s32 *value;\n\n\tvalue = kmalloc(sizeof(__s32) * count, GFP_ATOMIC);\n\tif (!value)\n\t\treturn;\n\n\tfor (n = 0; n < count; n++) {\n\n\t\tvalue[n] = min < 0 ?\n\t\t\tsnto32(hid_field_extract(hid, data, offset + n * size,\n\t\t\t       size), size) :\n\t\t\thid_field_extract(hid, data, offset + n * size, size);\n\n\t\t/* Ignore report if ErrorRollOver */\n\t\tif (!(field->flags & HID_MAIN_ITEM_VARIABLE) &&\n\t\t    value[n] >= min && value[n] <= max &&\n\t\t    field->usage[value[n] - min].hid == HID_UP_KEYBOARD + 1)\n\t\t\tgoto exit;\n\t}\n\n\tfor (n = 0; n < count; n++) {\n\n\t\tif (HID_MAIN_ITEM_VARIABLE & field->flags) {\n\t\t\thid_process_event(hid, field, &field->usage[n], value[n], interrupt);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (field->value[n] >= min && field->value[n] <= max\n\t\t\t&& field->usage[field->value[n] - min].hid\n\t\t\t&& search(value, field->value[n], count))\n\t\t\t\thid_process_event(hid, field, &field->usage[field->value[n] - min], 0, interrupt);\n\n\t\tif (value[n] >= min && value[n] <= max\n\t\t\t&& field->usage[value[n] - min].hid\n\t\t\t&& search(field->value, value[n], count))\n\t\t\t\thid_process_event(hid, field, &field->usage[value[n] - min], 1, interrupt);\n\t}\n\n\tmemcpy(field->value, value, count * sizeof(__s32));\nexit:\n\tkfree(value);\n}", "func_src_after": "static void hid_input_field(struct hid_device *hid, struct hid_field *field,\n\t\t\t    __u8 *data, int interrupt)\n{\n\tunsigned n;\n\tunsigned count = field->report_count;\n\tunsigned offset = field->report_offset;\n\tunsigned size = field->report_size;\n\t__s32 min = field->logical_minimum;\n\t__s32 max = field->logical_maximum;\n\t__s32 *value;\n\n\tvalue = kmalloc(sizeof(__s32) * count, GFP_ATOMIC);\n\tif (!value)\n\t\treturn;\n\n\tfor (n = 0; n < count; n++) {\n\n\t\tvalue[n] = min < 0 ?\n\t\t\tsnto32(hid_field_extract(hid, data, offset + n * size,\n\t\t\t       size), size) :\n\t\t\thid_field_extract(hid, data, offset + n * size, size);\n\n\t\t/* Ignore report if ErrorRollOver */\n\t\tif (!(field->flags & HID_MAIN_ITEM_VARIABLE) &&\n\t\t    value[n] >= min && value[n] <= max &&\n\t\t    value[n] - min < field->maxusage &&\n\t\t    field->usage[value[n] - min].hid == HID_UP_KEYBOARD + 1)\n\t\t\tgoto exit;\n\t}\n\n\tfor (n = 0; n < count; n++) {\n\n\t\tif (HID_MAIN_ITEM_VARIABLE & field->flags) {\n\t\t\thid_process_event(hid, field, &field->usage[n], value[n], interrupt);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (field->value[n] >= min && field->value[n] <= max\n\t\t\t&& field->value[n] - min < field->maxusage\n\t\t\t&& field->usage[field->value[n] - min].hid\n\t\t\t&& search(value, field->value[n], count))\n\t\t\t\thid_process_event(hid, field, &field->usage[field->value[n] - min], 0, interrupt);\n\n\t\tif (value[n] >= min && value[n] <= max\n\t\t\t&& value[n] - min < field->maxusage\n\t\t\t&& field->usage[value[n] - min].hid\n\t\t\t&& search(field->value, value[n], count))\n\t\t\t\thid_process_event(hid, field, &field->usage[value[n] - min], 1, interrupt);\n\t}\n\n\tmemcpy(field->value, value, count * sizeof(__s32));\nexit:\n\tkfree(value);\n}", "commit_link": "github.com/torvalds/linux/commit/50220dead1650609206efe91f0cc116132d59b3f", "file_name": "drivers/hid/hid-core.c", "vul_type": "cwe-125", "description": "Write a C function to process HID input fields and handle keyboard events."}
{"func_name": "_kdc_as_rep", "func_src_before": "_kdc_as_rep(kdc_request_t r,\n\t    krb5_data *reply,\n\t    const char *from,\n\t    struct sockaddr *from_addr,\n\t    int datagram_reply)\n{\n    krb5_context context = r->context;\n    krb5_kdc_configuration *config = r->config;\n    KDC_REQ *req = &r->req;\n    KDC_REQ_BODY *b = NULL;\n    AS_REP rep;\n    KDCOptions f;\n    krb5_enctype setype;\n    krb5_error_code ret = 0;\n    Key *skey;\n    int found_pa = 0;\n    int i, flags = HDB_F_FOR_AS_REQ;\n    METHOD_DATA error_method;\n    const PA_DATA *pa;\n\n    memset(&rep, 0, sizeof(rep));\n    error_method.len = 0;\n    error_method.val = NULL;\n\n    /*\n     * Look for FAST armor and unwrap\n     */\n    ret = _kdc_fast_unwrap_request(r);\n    if (ret) {\n\t_kdc_r_log(r, 0, \"FAST unwrap request from %s failed: %d\", from, ret);\n\tgoto out;\n    }\n\n    b = &req->req_body;\n    f = b->kdc_options;\n\n    if (f.canonicalize)\n\tflags |= HDB_F_CANON;\n\n    if(b->sname == NULL){\n\tret = KRB5KRB_ERR_GENERIC;\n\t_kdc_set_e_text(r, \"No server in request\");\n    } else{\n\tret = _krb5_principalname2krb5_principal (context,\n\t\t\t\t\t\t  &r->server_princ,\n\t\t\t\t\t\t  *(b->sname),\n\t\t\t\t\t\t  b->realm);\n\tif (ret == 0)\n\t    ret = krb5_unparse_name(context, r->server_princ, &r->server_name);\n    }\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"AS-REQ malformed server name from %s\", from);\n\tgoto out;\n    }\n    if(b->cname == NULL){\n\tret = KRB5KRB_ERR_GENERIC;\n\t_kdc_set_e_text(r, \"No client in request\");\n    } else {\n\tret = _krb5_principalname2krb5_principal (context,\n\t\t\t\t\t\t  &r->client_princ,\n\t\t\t\t\t\t  *(b->cname),\n\t\t\t\t\t\t  b->realm);\n\tif (ret)\n\t    goto out;\n\n\tret = krb5_unparse_name(context, r->client_princ, &r->client_name);\n    }\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"AS-REQ malformed client name from %s\", from);\n\tgoto out;\n    }\n\n    kdc_log(context, config, 0, \"AS-REQ %s from %s for %s\",\n\t    r->client_name, from, r->server_name);\n\n    /*\n     *\n     */\n\n    if (_kdc_is_anonymous(context, r->client_princ)) {\n\tif (!_kdc_is_anon_request(b)) {\n\t    kdc_log(context, config, 0, \"Anonymous ticket w/o anonymous flag\");\n\t    ret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\t    goto out;\n\t}\n    } else if (_kdc_is_anon_request(b)) {\n\tkdc_log(context, config, 0,\n\t\t\"Request for a anonymous ticket with non \"\n\t\t\"anonymous client name: %s\", r->client_name);\n\tret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n\n    /*\n     *\n     */\n\n    ret = _kdc_db_fetch(context, config, r->client_princ,\n\t\t\tHDB_F_GET_CLIENT | flags, NULL,\n\t\t\t&r->clientdb, &r->client);\n    if(ret == HDB_ERR_NOT_FOUND_HERE) {\n\tkdc_log(context, config, 5, \"client %s does not have secrets at this KDC, need to proxy\",\n\t\tr->client_name);\n\tgoto out;\n    } else if (ret == HDB_ERR_WRONG_REALM) {\n\tchar *fixed_client_name = NULL;\n\n\tret = krb5_unparse_name(context, r->client->entry.principal,\n\t\t\t\t&fixed_client_name);\n\tif (ret) {\n\t    goto out;\n\t}\n\n\tkdc_log(context, config, 0, \"WRONG_REALM - %s -> %s\",\n\t\tr->client_name, fixed_client_name);\n\tfree(fixed_client_name);\n\n\tret = _kdc_fast_mk_error(context, r,\n\t\t\t\t &error_method,\n\t\t\t\t r->armor_crypto,\n\t\t\t\t &req->req_body,\n\t\t\t\t KRB5_KDC_ERR_WRONG_REALM,\n\t\t\t\t NULL,\n\t\t\t\t r->server_princ,\n\t\t\t\t NULL,\n\t\t\t\t &r->client->entry.principal->realm,\n\t\t\t\t NULL, NULL,\n\t\t\t\t reply);\n\tgoto out;\n    } else if(ret){\n\tconst char *msg = krb5_get_error_message(context, ret);\n\tkdc_log(context, config, 0, \"UNKNOWN -- %s: %s\", r->client_name, msg);\n\tkrb5_free_error_message(context, msg);\n\tret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n    ret = _kdc_db_fetch(context, config, r->server_princ,\n\t\t\tHDB_F_GET_SERVER|HDB_F_GET_KRBTGT | flags,\n\t\t\tNULL, NULL, &r->server);\n    if(ret == HDB_ERR_NOT_FOUND_HERE) {\n\tkdc_log(context, config, 5, \"target %s does not have secrets at this KDC, need to proxy\",\n\t\tr->server_name);\n\tgoto out;\n    } else if(ret){\n\tconst char *msg = krb5_get_error_message(context, ret);\n\tkdc_log(context, config, 0, \"UNKNOWN -- %s: %s\", r->server_name, msg);\n\tkrb5_free_error_message(context, msg);\n\tret = KRB5KDC_ERR_S_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n\n    /*\n     * Select a session enctype from the list of the crypto system\n     * supported enctypes that is supported by the client and is one of\n     * the enctype of the enctype of the service (likely krbtgt).\n     *\n     * The latter is used as a hint of what enctypes all KDC support,\n     * to make sure a newer version of KDC won't generate a session\n     * enctype that an older version of a KDC in the same realm can't\n     * decrypt.\n     */\n\n    ret = _kdc_find_etype(context,\n\t\t\t  krb5_principal_is_krbtgt(context, r->server_princ) ?\n\t\t\t  config->tgt_use_strongest_session_key :\n\t\t\t  config->svc_use_strongest_session_key, FALSE,\n\t\t\t  r->client, b->etype.val, b->etype.len, &r->sessionetype,\n\t\t\t  NULL);\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"Client (%s) from %s has no common enctypes with KDC \"\n\t\t\"to use for the session key\",\n\t\tr->client_name, from);\n\tgoto out;\n    }\n\n    /*\n     * Pre-auth processing\n     */\n\n    if(req->padata){\n\tunsigned int n;\n\n\tlog_patypes(context, config, req->padata);\n\n\t/* Check if preauth matching */\n\n\tfor (n = 0; !found_pa && n < sizeof(pat) / sizeof(pat[0]); n++) {\n\t    if (pat[n].validate == NULL)\n\t\tcontinue;\n\t    if (r->armor_crypto == NULL && (pat[n].flags & PA_REQ_FAST))\n\t\tcontinue;\n\n\t    kdc_log(context, config, 5,\n\t\t    \"Looking for %s pa-data -- %s\", pat[n].name, r->client_name);\n\t    i = 0;\n\t    pa = _kdc_find_padata(req, &i, pat[n].type);\n\t    if (pa) {\n\t\tret = pat[n].validate(r, pa);\n\t\tif (ret != 0) {\n\t\t    goto out;\n\t\t}\n\t\tkdc_log(context, config, 0,\n\t\t\t\"%s pre-authentication succeeded -- %s\",\n\t\t\tpat[n].name, r->client_name);\n\t\tfound_pa = 1;\n\t\tr->et.flags.pre_authent = 1;\n\t    }\n\t}\n    }\n\n    if (found_pa == 0) {\n\tKey *ckey = NULL;\n\tsize_t n;\n\n\tfor (n = 0; n < sizeof(pat) / sizeof(pat[0]); n++) {\n\t    if ((pat[n].flags & PA_ANNOUNCE) == 0)\n\t\tcontinue;\n\t    ret = krb5_padata_add(context, &error_method,\n\t\t\t\t  pat[n].type, NULL, 0);\n\t    if (ret)\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is a client key, send ETYPE_INFO{,2}\n\t */\n\tret = _kdc_find_etype(context,\n\t\t\t      config->preauth_use_strongest_session_key, TRUE,\n\t\t\t      r->client, b->etype.val, b->etype.len, NULL, &ckey);\n\tif (ret == 0) {\n\n\t    /*\n\t     * RFC4120 requires:\n\t     * - If the client only knows about old enctypes, then send\n\t     *   both info replies (we send 'info' first in the list).\n\t     * - If the client is 'modern', because it knows about 'new'\n\t     *   enctype types, then only send the 'info2' reply.\n\t     *\n\t     * Before we send the full list of etype-info data, we pick\n\t     * the client key we would have used anyway below, just pick\n\t     * that instead.\n\t     */\n\n\t    if (older_enctype(ckey->key.keytype)) {\n\t\tret = get_pa_etype_info(context, config,\n\t\t\t\t\t&error_method, ckey);\n\t\tif (ret)\n\t\t    goto out;\n\t    }\n\t    ret = get_pa_etype_info2(context, config,\n\t\t\t\t     &error_method, ckey);\n\t    if (ret)\n\t\tgoto out;\n\t}\n\n\t/* \n\t * send requre preauth is its required or anon is requested,\n\t * anon is today only allowed via preauth mechanisms.\n\t */\n\tif (require_preauth_p(r) || _kdc_is_anon_request(b)) {\n\t    ret = KRB5KDC_ERR_PREAUTH_REQUIRED;\n\t    _kdc_set_e_text(r, \"Need to use PA-ENC-TIMESTAMP/PA-PK-AS-REQ\");\n\t    goto out;\n\t}\n\n\tif (ckey == NULL) {\n\t    ret = KRB5KDC_ERR_CLIENT_NOTYET;\n\t    _kdc_set_e_text(r, \"Doesn't have a client key available\");\n\t    goto out;\n\t}\n\tkrb5_free_keyblock_contents(r->context,  &r->reply_key);\n\tret = krb5_copy_keyblock_contents(r->context, &ckey->key, &r->reply_key);\n\tif (ret)\n\t    goto out;\n    }\n\n    if (r->clientdb->hdb_auth_status) {\n\tr->clientdb->hdb_auth_status(context, r->clientdb, r->client, \n\t\t\t\t     HDB_AUTH_SUCCESS);\n    }\n\n    /*\n     * Verify flags after the user been required to prove its identity\n     * with in a preauth mech.\n     */\n\n    ret = _kdc_check_access(context, config, r->client, r->client_name,\n\t\t\t    r->server, r->server_name,\n\t\t\t    req, &error_method);\n    if(ret)\n\tgoto out;\n\n    /*\n     * Select the best encryption type for the KDC with out regard to\n     * the client since the client never needs to read that data.\n     */\n\n    ret = _kdc_get_preferred_key(context, config,\n\t\t\t\t r->server, r->server_name,\n\t\t\t\t &setype, &skey);\n    if(ret)\n\tgoto out;\n\n    if(f.renew || f.validate || f.proxy || f.forwarded || f.enc_tkt_in_skey\n       || (_kdc_is_anon_request(b) && !config->allow_anonymous)) {\n\tret = KRB5KDC_ERR_BADOPTION;\n\t_kdc_set_e_text(r, \"Bad KDC options\");\n\tgoto out;\n    }\n\n    /*\n     * Build reply\n     */\n\n    rep.pvno = 5;\n    rep.msg_type = krb_as_rep;\n\n    if (_kdc_is_anonymous(context, r->client_princ)) {\n\tRealm anon_realm=KRB5_ANON_REALM;\n\tret = copy_Realm(&anon_realm, &rep.crealm);\n    } else\n\tret = copy_Realm(&r->client->entry.principal->realm, &rep.crealm);\n    if (ret)\n\tgoto out;\n    ret = _krb5_principal2principalname(&rep.cname, r->client->entry.principal);\n    if (ret)\n\tgoto out;\n\n    rep.ticket.tkt_vno = 5;\n    ret = copy_Realm(&r->server->entry.principal->realm, &rep.ticket.realm);\n    if (ret)\n\tgoto out;\n    _krb5_principal2principalname(&rep.ticket.sname,\n\t\t\t\t  r->server->entry.principal);\n    /* java 1.6 expects the name to be the same type, lets allow that\n     * uncomplicated name-types. */\n#define CNT(sp,t) (((sp)->sname->name_type) == KRB5_NT_##t)\n    if (CNT(b, UNKNOWN) || CNT(b, PRINCIPAL) || CNT(b, SRV_INST) || CNT(b, SRV_HST) || CNT(b, SRV_XHST))\n\trep.ticket.sname.name_type = b->sname->name_type;\n#undef CNT\n\n    r->et.flags.initial = 1;\n    if(r->client->entry.flags.forwardable && r->server->entry.flags.forwardable)\n\tr->et.flags.forwardable = f.forwardable;\n    else if (f.forwardable) {\n\t_kdc_set_e_text(r, \"Ticket may not be forwardable\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n    if(r->client->entry.flags.proxiable && r->server->entry.flags.proxiable)\n\tr->et.flags.proxiable = f.proxiable;\n    else if (f.proxiable) {\n\t_kdc_set_e_text(r, \"Ticket may not be proxiable\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n    if(r->client->entry.flags.postdate && r->server->entry.flags.postdate)\n\tr->et.flags.may_postdate = f.allow_postdate;\n    else if (f.allow_postdate){\n\t_kdc_set_e_text(r, \"Ticket may not be postdate\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n\n    /* check for valid set of addresses */\n    if(!_kdc_check_addresses(context, config, b->addresses, from_addr)) {\n\t_kdc_set_e_text(r, \"Bad address list in requested\");\n\tret = KRB5KRB_AP_ERR_BADADDR;\n\tgoto out;\n    }\n\n    ret = copy_PrincipalName(&rep.cname, &r->et.cname);\n    if (ret)\n\tgoto out;\n    ret = copy_Realm(&rep.crealm, &r->et.crealm);\n    if (ret)\n\tgoto out;\n\n    {\n\ttime_t start;\n\ttime_t t;\n\t\n\tstart = r->et.authtime = kdc_time;\n\n\tif(f.postdated && req->req_body.from){\n\t    ALLOC(r->et.starttime);\n\t    start = *r->et.starttime = *req->req_body.from;\n\t    r->et.flags.invalid = 1;\n\t    r->et.flags.postdated = 1; /* XXX ??? */\n\t}\n\t_kdc_fix_time(&b->till);\n\tt = *b->till;\n\n\t/* be careful not overflowing */\n\n\tif(r->client->entry.max_life)\n\t    t = start + min(t - start, *r->client->entry.max_life);\n\tif(r->server->entry.max_life)\n\t    t = start + min(t - start, *r->server->entry.max_life);\n#if 0\n\tt = min(t, start + realm->max_life);\n#endif\n\tr->et.endtime = t;\n\tif(f.renewable_ok && r->et.endtime < *b->till){\n\t    f.renewable = 1;\n\t    if(b->rtime == NULL){\n\t\tALLOC(b->rtime);\n\t\t*b->rtime = 0;\n\t    }\n\t    if(*b->rtime < *b->till)\n\t\t*b->rtime = *b->till;\n\t}\n\tif(f.renewable && b->rtime){\n\t    t = *b->rtime;\n\t    if(t == 0)\n\t\tt = MAX_TIME;\n\t    if(r->client->entry.max_renew)\n\t\tt = start + min(t - start, *r->client->entry.max_renew);\n\t    if(r->server->entry.max_renew)\n\t\tt = start + min(t - start, *r->server->entry.max_renew);\n#if 0\n\t    t = min(t, start + realm->max_renew);\n#endif\n\t    ALLOC(r->et.renew_till);\n\t    *r->et.renew_till = t;\n\t    r->et.flags.renewable = 1;\n\t}\n    }\n\n    if (_kdc_is_anon_request(b))\n\tr->et.flags.anonymous = 1;\n\n    if(b->addresses){\n\tALLOC(r->et.caddr);\n\tcopy_HostAddresses(b->addresses, r->et.caddr);\n    }\n\n    r->et.transited.tr_type = DOMAIN_X500_COMPRESS;\n    krb5_data_zero(&r->et.transited.contents);\n\n    /* The MIT ASN.1 library (obviously) doesn't tell lengths encoded\n     * as 0 and as 0x80 (meaning indefinite length) apart, and is thus\n     * incapable of correctly decoding SEQUENCE OF's of zero length.\n     *\n     * To fix this, always send at least one no-op last_req\n     *\n     * If there's a pw_end or valid_end we will use that,\n     * otherwise just a dummy lr.\n     */\n    r->ek.last_req.val = malloc(2 * sizeof(*r->ek.last_req.val));\n    if (r->ek.last_req.val == NULL) {\n\tret = ENOMEM;\n\tgoto out;\n    }\n    r->ek.last_req.len = 0;\n    if (r->client->entry.pw_end\n\t&& (config->kdc_warn_pwexpire == 0\n\t    || kdc_time + config->kdc_warn_pwexpire >= *r->client->entry.pw_end)) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_PW_EXPTIME;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = *r->client->entry.pw_end;\n\t++r->ek.last_req.len;\n    }\n    if (r->client->entry.valid_end) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_ACCT_EXPTIME;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = *r->client->entry.valid_end;\n\t++r->ek.last_req.len;\n    }\n    if (r->ek.last_req.len == 0) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_NONE;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = 0;\n\t++r->ek.last_req.len;\n    }\n    r->ek.nonce = b->nonce;\n    if (r->client->entry.valid_end || r->client->entry.pw_end) {\n\tALLOC(r->ek.key_expiration);\n\tif (r->client->entry.valid_end) {\n\t    if (r->client->entry.pw_end)\n\t\t*r->ek.key_expiration = min(*r->client->entry.valid_end,\n\t\t\t\t\t *r->client->entry.pw_end);\n\t    else\n\t\t*r->ek.key_expiration = *r->client->entry.valid_end;\n\t} else\n\t    *r->ek.key_expiration = *r->client->entry.pw_end;\n    } else\n\tr->ek.key_expiration = NULL;\n    r->ek.flags = r->et.flags;\n    r->ek.authtime = r->et.authtime;\n    if (r->et.starttime) {\n\tALLOC(r->ek.starttime);\n\t*r->ek.starttime = *r->et.starttime;\n    }\n    r->ek.endtime = r->et.endtime;\n    if (r->et.renew_till) {\n\tALLOC(r->ek.renew_till);\n\t*r->ek.renew_till = *r->et.renew_till;\n    }\n    ret = copy_Realm(&rep.ticket.realm, &r->ek.srealm);\n    if (ret)\n\tgoto out;\n    ret = copy_PrincipalName(&rep.ticket.sname, &r->ek.sname);\n    if (ret)\n\tgoto out;\n    if(r->et.caddr){\n\tALLOC(r->ek.caddr);\n\tcopy_HostAddresses(r->et.caddr, r->ek.caddr);\n    }\n\n    /*\n     * Check and session and reply keys\n     */\n\n    if (r->session_key.keytype == ETYPE_NULL) {\n\tret = krb5_generate_random_keyblock(context, r->sessionetype, &r->session_key);\n\tif (ret)\n\t    goto out;\n    }\n\n    if (r->reply_key.keytype == ETYPE_NULL) {\n\t_kdc_set_e_text(r, \"Client have no reply key\");\n\tret = KRB5KDC_ERR_CLIENT_NOTYET;\n\tgoto out;\n    }\n\n    ret = copy_EncryptionKey(&r->session_key, &r->et.key);\n    if (ret)\n\tgoto out;\n\n    ret = copy_EncryptionKey(&r->session_key, &r->ek.key);\n    if (ret)\n\tgoto out;\n\n    if (r->outpadata.len) {\n\n\tALLOC(rep.padata);\n\tif (rep.padata == NULL) {\n\t    ret = ENOMEM;\n\t    goto out;\n\t}\n\tret = copy_METHOD_DATA(&r->outpadata, rep.padata);\n\tif (ret)\n\t    goto out;\n    }\n\n    /* Add the PAC */\n    if (send_pac_p(context, req)) {\n\tgenerate_pac(r, skey);\n    }\n\n    _kdc_log_timestamp(context, config, \"AS-REQ\", r->et.authtime, r->et.starttime,\n\t\t       r->et.endtime, r->et.renew_till);\n\n    /* do this as the last thing since this signs the EncTicketPart */\n    ret = _kdc_add_KRB5SignedPath(context,\n\t\t\t\t  config,\n\t\t\t\t  r->server,\n\t\t\t\t  setype,\n\t\t\t\t  r->client->entry.principal,\n\t\t\t\t  NULL,\n\t\t\t\t  NULL,\n\t\t\t\t  &r->et);\n    if (ret)\n\tgoto out;\n\n    log_as_req(context, config, r->reply_key.keytype, setype, b);\n\n    /*\n     * We always say we support FAST/enc-pa-rep\n     */\n\n    r->et.flags.enc_pa_rep = r->ek.flags.enc_pa_rep = 1;\n\n    /*\n     * Add REQ_ENC_PA_REP if client supports it\n     */\n\n    i = 0;\n    pa = _kdc_find_padata(req, &i, KRB5_PADATA_REQ_ENC_PA_REP);\n    if (pa) {\n\n\tret = add_enc_pa_rep(r);\n\tif (ret) {\n\t    const char *msg = krb5_get_error_message(r->context, ret);\n\t    _kdc_r_log(r, 0, \"add_enc_pa_rep failed: %s: %d\", msg, ret);\n\t    krb5_free_error_message(r->context, msg);\n\t    goto out;\n\t}\n    }\n\n    /*\n     *\n     */\n\n    ret = _kdc_encode_reply(context, config,\n\t\t\t    r->armor_crypto, req->req_body.nonce,\n\t\t\t    &rep, &r->et, &r->ek, setype, r->server->entry.kvno,\n\t\t\t    &skey->key, r->client->entry.kvno,\n\t\t\t    &r->reply_key, 0, &r->e_text, reply);\n    if (ret)\n\tgoto out;\n\n    /*\n     * Check if message too large\n     */\n    if (datagram_reply && reply->length > config->max_datagram_reply_length) {\n\tkrb5_data_free(reply);\n\tret = KRB5KRB_ERR_RESPONSE_TOO_BIG;\n\t_kdc_set_e_text(r, \"Reply packet too large\");\n    }\n\nout:\n    free_AS_REP(&rep);\n\n    /*\n     * In case of a non proxy error, build an error message.\n     */\n    if(ret != 0 && ret != HDB_ERR_NOT_FOUND_HERE && reply->length == 0) {\n\tret = _kdc_fast_mk_error(context, r,\n\t\t\t\t &error_method,\n\t\t\t\t r->armor_crypto,\n\t\t\t\t &req->req_body,\n\t\t\t\t ret, r->e_text,\n\t\t\t\t r->server_princ,\n\t\t\t\t &r->client_princ->name,\n\t\t\t\t &r->client_princ->realm,\n\t\t\t\t NULL, NULL,\n\t\t\t\t reply);\n\tif (ret)\n\t    goto out2;\n    }\nout2:\n    free_EncTicketPart(&r->et);\n    free_EncKDCRepPart(&r->ek);\n    free_KDCFastState(&r->fast);\n\n    if (error_method.len)\n\tfree_METHOD_DATA(&error_method);\n    if (r->outpadata.len)\n\tfree_METHOD_DATA(&r->outpadata);\n    if (r->client_princ) {\n\tkrb5_free_principal(context, r->client_princ);\n\tr->client_princ = NULL;\n    }\n    if (r->client_name) {\n\tfree(r->client_name);\n\tr->client_name = NULL;\n    }\n    if (r->server_princ){\n\tkrb5_free_principal(context, r->server_princ);\n\tr->server_princ = NULL;\n    }\n    if (r->server_name) {\n\tfree(r->server_name);\n\tr->server_name = NULL;\n    }\n    if (r->client)\n\t_kdc_free_ent(context, r->client);\n    if (r->server)\n\t_kdc_free_ent(context, r->server);\n    if (r->armor_crypto) {\n\tkrb5_crypto_destroy(r->context, r->armor_crypto);\n\tr->armor_crypto = NULL;\n    }\n    krb5_free_keyblock_contents(r->context, &r->reply_key);\n    krb5_free_keyblock_contents(r->context, &r->session_key);\n    return ret;\n}", "func_src_after": "_kdc_as_rep(kdc_request_t r,\n\t    krb5_data *reply,\n\t    const char *from,\n\t    struct sockaddr *from_addr,\n\t    int datagram_reply)\n{\n    krb5_context context = r->context;\n    krb5_kdc_configuration *config = r->config;\n    KDC_REQ *req = &r->req;\n    KDC_REQ_BODY *b = NULL;\n    AS_REP rep;\n    KDCOptions f;\n    krb5_enctype setype;\n    krb5_error_code ret = 0;\n    Key *skey;\n    int found_pa = 0;\n    int i, flags = HDB_F_FOR_AS_REQ;\n    METHOD_DATA error_method;\n    const PA_DATA *pa;\n\n    memset(&rep, 0, sizeof(rep));\n    error_method.len = 0;\n    error_method.val = NULL;\n\n    /*\n     * Look for FAST armor and unwrap\n     */\n    ret = _kdc_fast_unwrap_request(r);\n    if (ret) {\n\t_kdc_r_log(r, 0, \"FAST unwrap request from %s failed: %d\", from, ret);\n\tgoto out;\n    }\n\n    b = &req->req_body;\n    f = b->kdc_options;\n\n    if (f.canonicalize)\n\tflags |= HDB_F_CANON;\n\n    if(b->sname == NULL){\n\tret = KRB5KRB_ERR_GENERIC;\n\t_kdc_set_e_text(r, \"No server in request\");\n    } else{\n\tret = _krb5_principalname2krb5_principal (context,\n\t\t\t\t\t\t  &r->server_princ,\n\t\t\t\t\t\t  *(b->sname),\n\t\t\t\t\t\t  b->realm);\n\tif (ret == 0)\n\t    ret = krb5_unparse_name(context, r->server_princ, &r->server_name);\n    }\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"AS-REQ malformed server name from %s\", from);\n\tgoto out;\n    }\n    if(b->cname == NULL){\n\tret = KRB5KRB_ERR_GENERIC;\n\t_kdc_set_e_text(r, \"No client in request\");\n    } else {\n\tret = _krb5_principalname2krb5_principal (context,\n\t\t\t\t\t\t  &r->client_princ,\n\t\t\t\t\t\t  *(b->cname),\n\t\t\t\t\t\t  b->realm);\n\tif (ret)\n\t    goto out;\n\n\tret = krb5_unparse_name(context, r->client_princ, &r->client_name);\n    }\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"AS-REQ malformed client name from %s\", from);\n\tgoto out;\n    }\n\n    kdc_log(context, config, 0, \"AS-REQ %s from %s for %s\",\n\t    r->client_name, from, r->server_name);\n\n    /*\n     *\n     */\n\n    if (_kdc_is_anonymous(context, r->client_princ)) {\n\tif (!_kdc_is_anon_request(b)) {\n\t    kdc_log(context, config, 0, \"Anonymous ticket w/o anonymous flag\");\n\t    ret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\t    goto out;\n\t}\n    } else if (_kdc_is_anon_request(b)) {\n\tkdc_log(context, config, 0,\n\t\t\"Request for a anonymous ticket with non \"\n\t\t\"anonymous client name: %s\", r->client_name);\n\tret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n\n    /*\n     *\n     */\n\n    ret = _kdc_db_fetch(context, config, r->client_princ,\n\t\t\tHDB_F_GET_CLIENT | flags, NULL,\n\t\t\t&r->clientdb, &r->client);\n    if(ret == HDB_ERR_NOT_FOUND_HERE) {\n\tkdc_log(context, config, 5, \"client %s does not have secrets at this KDC, need to proxy\",\n\t\tr->client_name);\n\tgoto out;\n    } else if (ret == HDB_ERR_WRONG_REALM) {\n\tchar *fixed_client_name = NULL;\n\n\tret = krb5_unparse_name(context, r->client->entry.principal,\n\t\t\t\t&fixed_client_name);\n\tif (ret) {\n\t    goto out;\n\t}\n\n\tkdc_log(context, config, 0, \"WRONG_REALM - %s -> %s\",\n\t\tr->client_name, fixed_client_name);\n\tfree(fixed_client_name);\n\n\tret = _kdc_fast_mk_error(context, r,\n\t\t\t\t &error_method,\n\t\t\t\t r->armor_crypto,\n\t\t\t\t &req->req_body,\n\t\t\t\t KRB5_KDC_ERR_WRONG_REALM,\n\t\t\t\t NULL,\n\t\t\t\t r->server_princ,\n\t\t\t\t NULL,\n\t\t\t\t &r->client->entry.principal->realm,\n\t\t\t\t NULL, NULL,\n\t\t\t\t reply);\n\tgoto out;\n    } else if(ret){\n\tconst char *msg = krb5_get_error_message(context, ret);\n\tkdc_log(context, config, 0, \"UNKNOWN -- %s: %s\", r->client_name, msg);\n\tkrb5_free_error_message(context, msg);\n\tret = KRB5KDC_ERR_C_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n    ret = _kdc_db_fetch(context, config, r->server_princ,\n\t\t\tHDB_F_GET_SERVER|HDB_F_GET_KRBTGT | flags,\n\t\t\tNULL, NULL, &r->server);\n    if(ret == HDB_ERR_NOT_FOUND_HERE) {\n\tkdc_log(context, config, 5, \"target %s does not have secrets at this KDC, need to proxy\",\n\t\tr->server_name);\n\tgoto out;\n    } else if(ret){\n\tconst char *msg = krb5_get_error_message(context, ret);\n\tkdc_log(context, config, 0, \"UNKNOWN -- %s: %s\", r->server_name, msg);\n\tkrb5_free_error_message(context, msg);\n\tret = KRB5KDC_ERR_S_PRINCIPAL_UNKNOWN;\n\tgoto out;\n    }\n\n    /*\n     * Select a session enctype from the list of the crypto system\n     * supported enctypes that is supported by the client and is one of\n     * the enctype of the enctype of the service (likely krbtgt).\n     *\n     * The latter is used as a hint of what enctypes all KDC support,\n     * to make sure a newer version of KDC won't generate a session\n     * enctype that an older version of a KDC in the same realm can't\n     * decrypt.\n     */\n\n    ret = _kdc_find_etype(context,\n\t\t\t  krb5_principal_is_krbtgt(context, r->server_princ) ?\n\t\t\t  config->tgt_use_strongest_session_key :\n\t\t\t  config->svc_use_strongest_session_key, FALSE,\n\t\t\t  r->client, b->etype.val, b->etype.len, &r->sessionetype,\n\t\t\t  NULL);\n    if (ret) {\n\tkdc_log(context, config, 0,\n\t\t\"Client (%s) from %s has no common enctypes with KDC \"\n\t\t\"to use for the session key\",\n\t\tr->client_name, from);\n\tgoto out;\n    }\n\n    /*\n     * Pre-auth processing\n     */\n\n    if(req->padata){\n\tunsigned int n;\n\n\tlog_patypes(context, config, req->padata);\n\n\t/* Check if preauth matching */\n\n\tfor (n = 0; !found_pa && n < sizeof(pat) / sizeof(pat[0]); n++) {\n\t    if (pat[n].validate == NULL)\n\t\tcontinue;\n\t    if (r->armor_crypto == NULL && (pat[n].flags & PA_REQ_FAST))\n\t\tcontinue;\n\n\t    kdc_log(context, config, 5,\n\t\t    \"Looking for %s pa-data -- %s\", pat[n].name, r->client_name);\n\t    i = 0;\n\t    pa = _kdc_find_padata(req, &i, pat[n].type);\n\t    if (pa) {\n\t\tret = pat[n].validate(r, pa);\n\t\tif (ret != 0) {\n\t\t    goto out;\n\t\t}\n\t\tkdc_log(context, config, 0,\n\t\t\t\"%s pre-authentication succeeded -- %s\",\n\t\t\tpat[n].name, r->client_name);\n\t\tfound_pa = 1;\n\t\tr->et.flags.pre_authent = 1;\n\t    }\n\t}\n    }\n\n    if (found_pa == 0) {\n\tKey *ckey = NULL;\n\tsize_t n;\n\n\tfor (n = 0; n < sizeof(pat) / sizeof(pat[0]); n++) {\n\t    if ((pat[n].flags & PA_ANNOUNCE) == 0)\n\t\tcontinue;\n\t    ret = krb5_padata_add(context, &error_method,\n\t\t\t\t  pat[n].type, NULL, 0);\n\t    if (ret)\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is a client key, send ETYPE_INFO{,2}\n\t */\n\tret = _kdc_find_etype(context,\n\t\t\t      config->preauth_use_strongest_session_key, TRUE,\n\t\t\t      r->client, b->etype.val, b->etype.len, NULL, &ckey);\n\tif (ret == 0) {\n\n\t    /*\n\t     * RFC4120 requires:\n\t     * - If the client only knows about old enctypes, then send\n\t     *   both info replies (we send 'info' first in the list).\n\t     * - If the client is 'modern', because it knows about 'new'\n\t     *   enctype types, then only send the 'info2' reply.\n\t     *\n\t     * Before we send the full list of etype-info data, we pick\n\t     * the client key we would have used anyway below, just pick\n\t     * that instead.\n\t     */\n\n\t    if (older_enctype(ckey->key.keytype)) {\n\t\tret = get_pa_etype_info(context, config,\n\t\t\t\t\t&error_method, ckey);\n\t\tif (ret)\n\t\t    goto out;\n\t    }\n\t    ret = get_pa_etype_info2(context, config,\n\t\t\t\t     &error_method, ckey);\n\t    if (ret)\n\t\tgoto out;\n\t}\n\n\t/* \n\t * send requre preauth is its required or anon is requested,\n\t * anon is today only allowed via preauth mechanisms.\n\t */\n\tif (require_preauth_p(r) || _kdc_is_anon_request(b)) {\n\t    ret = KRB5KDC_ERR_PREAUTH_REQUIRED;\n\t    _kdc_set_e_text(r, \"Need to use PA-ENC-TIMESTAMP/PA-PK-AS-REQ\");\n\t    goto out;\n\t}\n\n\tif (ckey == NULL) {\n\t    ret = KRB5KDC_ERR_CLIENT_NOTYET;\n\t    _kdc_set_e_text(r, \"Doesn't have a client key available\");\n\t    goto out;\n\t}\n\tkrb5_free_keyblock_contents(r->context,  &r->reply_key);\n\tret = krb5_copy_keyblock_contents(r->context, &ckey->key, &r->reply_key);\n\tif (ret)\n\t    goto out;\n    }\n\n    if (r->clientdb->hdb_auth_status) {\n\tr->clientdb->hdb_auth_status(context, r->clientdb, r->client, \n\t\t\t\t     HDB_AUTH_SUCCESS);\n    }\n\n    /*\n     * Verify flags after the user been required to prove its identity\n     * with in a preauth mech.\n     */\n\n    ret = _kdc_check_access(context, config, r->client, r->client_name,\n\t\t\t    r->server, r->server_name,\n\t\t\t    req, &error_method);\n    if(ret)\n\tgoto out;\n\n    /*\n     * Select the best encryption type for the KDC with out regard to\n     * the client since the client never needs to read that data.\n     */\n\n    ret = _kdc_get_preferred_key(context, config,\n\t\t\t\t r->server, r->server_name,\n\t\t\t\t &setype, &skey);\n    if(ret)\n\tgoto out;\n\n    if(f.renew || f.validate || f.proxy || f.forwarded || f.enc_tkt_in_skey\n       || (_kdc_is_anon_request(b) && !config->allow_anonymous)) {\n\tret = KRB5KDC_ERR_BADOPTION;\n\t_kdc_set_e_text(r, \"Bad KDC options\");\n\tgoto out;\n    }\n\n    /*\n     * Build reply\n     */\n\n    rep.pvno = 5;\n    rep.msg_type = krb_as_rep;\n\n    if (_kdc_is_anonymous(context, r->client_princ)) {\n\tRealm anon_realm=KRB5_ANON_REALM;\n\tret = copy_Realm(&anon_realm, &rep.crealm);\n    } else\n\tret = copy_Realm(&r->client->entry.principal->realm, &rep.crealm);\n    if (ret)\n\tgoto out;\n    ret = _krb5_principal2principalname(&rep.cname, r->client->entry.principal);\n    if (ret)\n\tgoto out;\n\n    rep.ticket.tkt_vno = 5;\n    ret = copy_Realm(&r->server->entry.principal->realm, &rep.ticket.realm);\n    if (ret)\n\tgoto out;\n    _krb5_principal2principalname(&rep.ticket.sname,\n\t\t\t\t  r->server->entry.principal);\n    /* java 1.6 expects the name to be the same type, lets allow that\n     * uncomplicated name-types. */\n#define CNT(sp,t) (((sp)->sname->name_type) == KRB5_NT_##t)\n    if (CNT(b, UNKNOWN) || CNT(b, PRINCIPAL) || CNT(b, SRV_INST) || CNT(b, SRV_HST) || CNT(b, SRV_XHST))\n\trep.ticket.sname.name_type = b->sname->name_type;\n#undef CNT\n\n    r->et.flags.initial = 1;\n    if(r->client->entry.flags.forwardable && r->server->entry.flags.forwardable)\n\tr->et.flags.forwardable = f.forwardable;\n    else if (f.forwardable) {\n\t_kdc_set_e_text(r, \"Ticket may not be forwardable\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n    if(r->client->entry.flags.proxiable && r->server->entry.flags.proxiable)\n\tr->et.flags.proxiable = f.proxiable;\n    else if (f.proxiable) {\n\t_kdc_set_e_text(r, \"Ticket may not be proxiable\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n    if(r->client->entry.flags.postdate && r->server->entry.flags.postdate)\n\tr->et.flags.may_postdate = f.allow_postdate;\n    else if (f.allow_postdate){\n\t_kdc_set_e_text(r, \"Ticket may not be postdate\");\n\tret = KRB5KDC_ERR_POLICY;\n\tgoto out;\n    }\n\n    /* check for valid set of addresses */\n    if(!_kdc_check_addresses(context, config, b->addresses, from_addr)) {\n\t_kdc_set_e_text(r, \"Bad address list in requested\");\n\tret = KRB5KRB_AP_ERR_BADADDR;\n\tgoto out;\n    }\n\n    ret = copy_PrincipalName(&rep.cname, &r->et.cname);\n    if (ret)\n\tgoto out;\n    ret = copy_Realm(&rep.crealm, &r->et.crealm);\n    if (ret)\n\tgoto out;\n\n    {\n\ttime_t start;\n\ttime_t t;\n\t\n\tstart = r->et.authtime = kdc_time;\n\n\tif(f.postdated && req->req_body.from){\n\t    ALLOC(r->et.starttime);\n\t    start = *r->et.starttime = *req->req_body.from;\n\t    r->et.flags.invalid = 1;\n\t    r->et.flags.postdated = 1; /* XXX ??? */\n\t}\n\t_kdc_fix_time(&b->till);\n\tt = *b->till;\n\n\t/* be careful not overflowing */\n\n\tif(r->client->entry.max_life)\n\t    t = start + min(t - start, *r->client->entry.max_life);\n\tif(r->server->entry.max_life)\n\t    t = start + min(t - start, *r->server->entry.max_life);\n#if 0\n\tt = min(t, start + realm->max_life);\n#endif\n\tr->et.endtime = t;\n\tif(f.renewable_ok && r->et.endtime < *b->till){\n\t    f.renewable = 1;\n\t    if(b->rtime == NULL){\n\t\tALLOC(b->rtime);\n\t\t*b->rtime = 0;\n\t    }\n\t    if(*b->rtime < *b->till)\n\t\t*b->rtime = *b->till;\n\t}\n\tif(f.renewable && b->rtime){\n\t    t = *b->rtime;\n\t    if(t == 0)\n\t\tt = MAX_TIME;\n\t    if(r->client->entry.max_renew)\n\t\tt = start + min(t - start, *r->client->entry.max_renew);\n\t    if(r->server->entry.max_renew)\n\t\tt = start + min(t - start, *r->server->entry.max_renew);\n#if 0\n\t    t = min(t, start + realm->max_renew);\n#endif\n\t    ALLOC(r->et.renew_till);\n\t    *r->et.renew_till = t;\n\t    r->et.flags.renewable = 1;\n\t}\n    }\n\n    if (_kdc_is_anon_request(b))\n\tr->et.flags.anonymous = 1;\n\n    if(b->addresses){\n\tALLOC(r->et.caddr);\n\tcopy_HostAddresses(b->addresses, r->et.caddr);\n    }\n\n    r->et.transited.tr_type = DOMAIN_X500_COMPRESS;\n    krb5_data_zero(&r->et.transited.contents);\n\n    /* The MIT ASN.1 library (obviously) doesn't tell lengths encoded\n     * as 0 and as 0x80 (meaning indefinite length) apart, and is thus\n     * incapable of correctly decoding SEQUENCE OF's of zero length.\n     *\n     * To fix this, always send at least one no-op last_req\n     *\n     * If there's a pw_end or valid_end we will use that,\n     * otherwise just a dummy lr.\n     */\n    r->ek.last_req.val = malloc(2 * sizeof(*r->ek.last_req.val));\n    if (r->ek.last_req.val == NULL) {\n\tret = ENOMEM;\n\tgoto out;\n    }\n    r->ek.last_req.len = 0;\n    if (r->client->entry.pw_end\n\t&& (config->kdc_warn_pwexpire == 0\n\t    || kdc_time + config->kdc_warn_pwexpire >= *r->client->entry.pw_end)) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_PW_EXPTIME;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = *r->client->entry.pw_end;\n\t++r->ek.last_req.len;\n    }\n    if (r->client->entry.valid_end) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_ACCT_EXPTIME;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = *r->client->entry.valid_end;\n\t++r->ek.last_req.len;\n    }\n    if (r->ek.last_req.len == 0) {\n\tr->ek.last_req.val[r->ek.last_req.len].lr_type  = LR_NONE;\n\tr->ek.last_req.val[r->ek.last_req.len].lr_value = 0;\n\t++r->ek.last_req.len;\n    }\n    r->ek.nonce = b->nonce;\n    if (r->client->entry.valid_end || r->client->entry.pw_end) {\n\tALLOC(r->ek.key_expiration);\n\tif (r->client->entry.valid_end) {\n\t    if (r->client->entry.pw_end)\n\t\t*r->ek.key_expiration = min(*r->client->entry.valid_end,\n\t\t\t\t\t *r->client->entry.pw_end);\n\t    else\n\t\t*r->ek.key_expiration = *r->client->entry.valid_end;\n\t} else\n\t    *r->ek.key_expiration = *r->client->entry.pw_end;\n    } else\n\tr->ek.key_expiration = NULL;\n    r->ek.flags = r->et.flags;\n    r->ek.authtime = r->et.authtime;\n    if (r->et.starttime) {\n\tALLOC(r->ek.starttime);\n\t*r->ek.starttime = *r->et.starttime;\n    }\n    r->ek.endtime = r->et.endtime;\n    if (r->et.renew_till) {\n\tALLOC(r->ek.renew_till);\n\t*r->ek.renew_till = *r->et.renew_till;\n    }\n    ret = copy_Realm(&rep.ticket.realm, &r->ek.srealm);\n    if (ret)\n\tgoto out;\n    ret = copy_PrincipalName(&rep.ticket.sname, &r->ek.sname);\n    if (ret)\n\tgoto out;\n    if(r->et.caddr){\n\tALLOC(r->ek.caddr);\n\tcopy_HostAddresses(r->et.caddr, r->ek.caddr);\n    }\n\n    /*\n     * Check and session and reply keys\n     */\n\n    if (r->session_key.keytype == ETYPE_NULL) {\n\tret = krb5_generate_random_keyblock(context, r->sessionetype, &r->session_key);\n\tif (ret)\n\t    goto out;\n    }\n\n    if (r->reply_key.keytype == ETYPE_NULL) {\n\t_kdc_set_e_text(r, \"Client have no reply key\");\n\tret = KRB5KDC_ERR_CLIENT_NOTYET;\n\tgoto out;\n    }\n\n    ret = copy_EncryptionKey(&r->session_key, &r->et.key);\n    if (ret)\n\tgoto out;\n\n    ret = copy_EncryptionKey(&r->session_key, &r->ek.key);\n    if (ret)\n\tgoto out;\n\n    if (r->outpadata.len) {\n\n\tALLOC(rep.padata);\n\tif (rep.padata == NULL) {\n\t    ret = ENOMEM;\n\t    goto out;\n\t}\n\tret = copy_METHOD_DATA(&r->outpadata, rep.padata);\n\tif (ret)\n\t    goto out;\n    }\n\n    /* Add the PAC */\n    if (send_pac_p(context, req)) {\n\tgenerate_pac(r, skey);\n    }\n\n    _kdc_log_timestamp(context, config, \"AS-REQ\", r->et.authtime, r->et.starttime,\n\t\t       r->et.endtime, r->et.renew_till);\n\n    /* do this as the last thing since this signs the EncTicketPart */\n    ret = _kdc_add_KRB5SignedPath(context,\n\t\t\t\t  config,\n\t\t\t\t  r->server,\n\t\t\t\t  setype,\n\t\t\t\t  r->client->entry.principal,\n\t\t\t\t  NULL,\n\t\t\t\t  NULL,\n\t\t\t\t  &r->et);\n    if (ret)\n\tgoto out;\n\n    log_as_req(context, config, r->reply_key.keytype, setype, b);\n\n    /*\n     * We always say we support FAST/enc-pa-rep\n     */\n\n    r->et.flags.enc_pa_rep = r->ek.flags.enc_pa_rep = 1;\n\n    /*\n     * Add REQ_ENC_PA_REP if client supports it\n     */\n\n    i = 0;\n    pa = _kdc_find_padata(req, &i, KRB5_PADATA_REQ_ENC_PA_REP);\n    if (pa) {\n\n\tret = add_enc_pa_rep(r);\n\tif (ret) {\n\t    const char *msg = krb5_get_error_message(r->context, ret);\n\t    _kdc_r_log(r, 0, \"add_enc_pa_rep failed: %s: %d\", msg, ret);\n\t    krb5_free_error_message(r->context, msg);\n\t    goto out;\n\t}\n    }\n\n    /*\n     *\n     */\n\n    ret = _kdc_encode_reply(context, config,\n\t\t\t    r->armor_crypto, req->req_body.nonce,\n\t\t\t    &rep, &r->et, &r->ek, setype, r->server->entry.kvno,\n\t\t\t    &skey->key, r->client->entry.kvno,\n\t\t\t    &r->reply_key, 0, &r->e_text, reply);\n    if (ret)\n\tgoto out;\n\n    /*\n     * Check if message too large\n     */\n    if (datagram_reply && reply->length > config->max_datagram_reply_length) {\n\tkrb5_data_free(reply);\n\tret = KRB5KRB_ERR_RESPONSE_TOO_BIG;\n\t_kdc_set_e_text(r, \"Reply packet too large\");\n    }\n\nout:\n    free_AS_REP(&rep);\n\n    /*\n     * In case of a non proxy error, build an error message.\n     */\n    if (ret != 0 && ret != HDB_ERR_NOT_FOUND_HERE && reply->length == 0) {\n\tret = _kdc_fast_mk_error(context, r,\n\t\t\t\t &error_method,\n\t\t\t\t r->armor_crypto,\n\t\t\t\t &req->req_body,\n\t\t\t\t ret, r->e_text,\n\t\t\t\t r->server_princ,\n\t\t\t\t r->client_princ ?\n                                     &r->client_princ->name : NULL,\n\t\t\t\t r->client_princ ?\n                                     &r->client_princ->realm : NULL,\n\t\t\t\t NULL, NULL,\n\t\t\t\t reply);\n\tif (ret)\n\t    goto out2;\n    }\nout2:\n    free_EncTicketPart(&r->et);\n    free_EncKDCRepPart(&r->ek);\n    free_KDCFastState(&r->fast);\n\n    if (error_method.len)\n\tfree_METHOD_DATA(&error_method);\n    if (r->outpadata.len)\n\tfree_METHOD_DATA(&r->outpadata);\n    if (r->client_princ) {\n\tkrb5_free_principal(context, r->client_princ);\n\tr->client_princ = NULL;\n    }\n    if (r->client_name) {\n\tfree(r->client_name);\n\tr->client_name = NULL;\n    }\n    if (r->server_princ){\n\tkrb5_free_principal(context, r->server_princ);\n\tr->server_princ = NULL;\n    }\n    if (r->server_name) {\n\tfree(r->server_name);\n\tr->server_name = NULL;\n    }\n    if (r->client)\n\t_kdc_free_ent(context, r->client);\n    if (r->server)\n\t_kdc_free_ent(context, r->server);\n    if (r->armor_crypto) {\n\tkrb5_crypto_destroy(r->context, r->armor_crypto);\n\tr->armor_crypto = NULL;\n    }\n    krb5_free_keyblock_contents(r->context, &r->reply_key);\n    krb5_free_keyblock_contents(r->context, &r->session_key);\n    return ret;\n}", "commit_link": "github.com/heimdal/heimdal/commit/1a6a6e462dc2ac6111f9e02c6852ddec4849b887", "file_name": "kdc/kerberos5.c", "vul_type": "cwe-476", "description": "Write a function in C that processes an AS-REQ (Authentication Service Request) for Kerberos 5."}
{"func_name": "register", "func_src_before": "@mod.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        error = None\n        email = request.form['email'].strip()\n        nickname = request.form['nickname'].strip()\n        password = request.form['password'].strip()\n        password2 = request.form['password2'].strip()\n\n        email = email.lower()\n\n        if email == \"\" or nickname == \"\" or password == \"\" or password2 == \"\":\n            error = 'Please input all the information'\n        elif password2 != password:\n            error = 'The password is not repeated correctly'\n        elif len(password) < 6:\n            error = 'The password has at least 6 characters'\n        elif not re.match(r'^[0-9a-zA-Z_]{0,19}@' +\n                          '[0-9a-zA-Z]{1,15}\\.[com,cn,net]', email):\n            error = 'Please input the right email'\n\n        sql = \"SELECT * FROM users where email = '%s';\" % (email)\n        cursor.execute(sql)\n        u = cursor.fetchone()\n\n        if u is not None:\n            error = 'The email has already exsit'\n\n        if error is not None:\n            return render_template('register.html', error=error)\n        else:\n            password = bcrypt.generate_password_hash(password)\n            cursor.execute(\"INSERT INTO users(email,nickname,password) VALUES(%s,%s,%s);\", (email, nickname, password))\n            conn.commit()\n            flash('Register Success!')\n            return redirect(url_for('users.login'))\n\n    return render_template('register.html')", "func_src_after": "@mod.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        error = None\n        email = request.form['email'].strip()\n        nickname = request.form['nickname'].strip()\n        password = request.form['password'].strip()\n        password2 = request.form['password2'].strip()\n\n        email = email.lower()\n\n        if email == \"\" or nickname == \"\" or password == \"\" or password2 == \"\":\n            error = 'Please input all the information'\n        elif password2 != password:\n            error = 'The password is not repeated correctly'\n        elif len(password) < 6:\n            error = 'The password has at least 6 characters'\n        elif not re.match(r'^[0-9a-zA-Z_]{0,19}@' +\n                          '[0-9a-zA-Z]{1,15}\\.[com,cn,net]', email):\n            error = 'Please input the right email'\n\n        cursor.execute(\"SELECT * FROM users where email = %s;\", (email,))\n        u = cursor.fetchone()\n\n        if u is not None:\n            error = 'The email has already exsit'\n\n        if error is not None:\n            return render_template('register.html', error=error)\n        else:\n            password = bcrypt.generate_password_hash(password)\n            cursor.execute(\"INSERT INTO users(email,nickname,password) VALUES(%s,%s,%s);\", (email, nickname, password))\n            conn.commit()\n            flash('Register Success!')\n            return redirect(url_for('users.login'))\n\n    return render_template('register.html')", "commit_link": "github.com/ulyssetsd/bjtu-sql/commit/17d7b21864b72ba5666f15236474a93268b32ec9", "file_name": "flaskr/flaskr/views/users.py", "vul_type": "cwe-089", "description": "Write a Python Flask route for user registration with form validation and database interaction."}
{"func_name": "retrieve_last_video_position", "func_src_before": "def retrieve_last_video_position(playlist_id, db):\n    db.execute(\"SELECT max(position) as position from video WHERE playlist_id={playlist_id};\".format(\n        playlist_id=playlist_id))\n    row = db.fetchone()\n    return row['position']", "func_src_after": "def retrieve_last_video_position(playlist_id, db):\n    db.execute(\n        \"SELECT max(position) as position from video WHERE playlist_id=%s;\", (playlist_id,))\n    row = db.fetchone()\n    return row['position']", "commit_link": "github.com/Madmous/playlist/commit/666e52c5f0b8c1f4296e84471637033d9542a7a6", "file_name": "video/video_repository.py", "vul_type": "cwe-089", "description": "Write a Python function to get the highest position number of a video in a playlist from a database."}
{"func_name": "captureWritesToStream", "func_src_before": "  @contextlib.contextmanager\n  def captureWritesToStream(self, stream):\n    \"\"\"A context manager that captures the writes to a given stream.\n\n    This context manager captures all writes to a given stream inside of a\n    `CapturedWrites` object. When this context manager is created, it yields\n    the `CapturedWrites` object. The captured contents can be accessed  by\n    calling `.contents()` on the `CapturedWrites`.\n\n    For this function to work, the stream must have a file descriptor that\n    can be modified using `os.dup` and `os.dup2`, and the stream must support\n    a `.flush()` method. The default python sys.stdout and sys.stderr are\n    examples of this. Note that this does not work in Colab or Jupyter\n    notebooks, because those use alternate stdout streams.\n\n    Example:\n    ```python\n    class MyOperatorTest(test_util.TensorFlowTestCase):\n      def testMyOperator(self):\n        input = [1.0, 2.0, 3.0, 4.0, 5.0]\n        with self.captureWritesToStream(sys.stdout) as captured:\n          result = MyOperator(input).eval()\n        self.assertStartsWith(captured.contents(), \"This was printed.\")\n    ```\n\n    Args:\n      stream: The stream whose writes should be captured. This stream must have\n        a file descriptor, support writing via using that file descriptor, and\n        must have a `.flush()` method.\n\n    Yields:\n      A `CapturedWrites` object that contains all writes to the specified stream\n      made during this context.\n    \"\"\"\n    stream.flush()\n    fd = stream.fileno()\n    tmp_file_path = tempfile.mktemp(dir=self.get_temp_dir())\n    tmp_file = open(tmp_file_path, \"w\")\n    orig_fd = os.dup(fd)\n    os.dup2(tmp_file.fileno(), fd)\n    try:\n      yield CapturedWrites(tmp_file_path)\n    finally:\n      tmp_file.close()\n      os.dup2(orig_fd, fd)", "func_src_after": "  @contextlib.contextmanager\n  def captureWritesToStream(self, stream):\n    \"\"\"A context manager that captures the writes to a given stream.\n\n    This context manager captures all writes to a given stream inside of a\n    `CapturedWrites` object. When this context manager is created, it yields\n    the `CapturedWrites` object. The captured contents can be accessed  by\n    calling `.contents()` on the `CapturedWrites`.\n\n    For this function to work, the stream must have a file descriptor that\n    can be modified using `os.dup` and `os.dup2`, and the stream must support\n    a `.flush()` method. The default python sys.stdout and sys.stderr are\n    examples of this. Note that this does not work in Colab or Jupyter\n    notebooks, because those use alternate stdout streams.\n\n    Example:\n    ```python\n    class MyOperatorTest(test_util.TensorFlowTestCase):\n      def testMyOperator(self):\n        input = [1.0, 2.0, 3.0, 4.0, 5.0]\n        with self.captureWritesToStream(sys.stdout) as captured:\n          result = MyOperator(input).eval()\n        self.assertStartsWith(captured.contents(), \"This was printed.\")\n    ```\n\n    Args:\n      stream: The stream whose writes should be captured. This stream must have\n        a file descriptor, support writing via using that file descriptor, and\n        must have a `.flush()` method.\n\n    Yields:\n      A `CapturedWrites` object that contains all writes to the specified stream\n      made during this context.\n    \"\"\"\n    stream.flush()\n    fd = stream.fileno()\n    tmp_file, tmp_file_path = tempfile.mkstemp(dir=self.get_temp_dir())\n    orig_fd = os.dup(fd)\n    os.dup2(tmp_file, fd)\n    try:\n      yield CapturedWrites(tmp_file_path)\n    finally:\n      os.close(tmp_file)\n      os.dup2(orig_fd, fd)", "line_changes": {"deleted": [{"line_no": 37, "char_start": 1512, "char_end": 1573, "line": "    tmp_file_path = tempfile.mktemp(dir=self.get_temp_dir())\n"}, {"line_no": 38, "char_start": 1573, "char_end": 1613, "line": "    tmp_file = open(tmp_file_path, \"w\")\n"}, {"line_no": 40, "char_start": 1638, "char_end": 1673, "line": "    os.dup2(tmp_file.fileno(), fd)\n"}, {"line_no": 44, "char_start": 1737, "char_end": 1760, "line": "      tmp_file.close()\n"}], "added": [{"line_no": 37, "char_start": 1512, "char_end": 1584, "line": "    tmp_file, tmp_file_path = tempfile.mkstemp(dir=self.get_temp_dir())\n"}, {"line_no": 39, "char_start": 1609, "char_end": 1635, "line": "    os.dup2(tmp_file, fd)\n"}, {"line_no": 43, "char_start": 1699, "char_end": 1724, "line": "      os.close(tmp_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 1573, "char_end": 1613, "chars": "    tmp_file = open(tmp_file_path, \"w\")\n"}, {"char_start": 1658, "char_end": 1667, "chars": ".fileno()"}, {"char_start": 1751, "char_end": 1758, "chars": ".close("}], "added": [{"char_start": 1515, "char_end": 1525, "chars": " tmp_file,"}, {"char_start": 1553, "char_end": 1554, "chars": "s"}, {"char_start": 1705, "char_end": 1714, "chars": "os.close("}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/fb40e84a1d694ebd2203d0f9846b8b9004dccce0", "file_name": "test_util.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420355705\nChange-Id: If437973d0cd7686a221679d4123cb12f79697fe0", "description": "Create a Python function that captures all output written to a specified stream using a context manager."}
{"func_name": "register_user", "func_src_before": "def register_user(request):\n    settings = request.registry.settings\n    if not is_registration_enabled(settings):\n        raise exc.exception_response(503)\n    handle_history(request)\n    _ = request.translate\n    config = Config(load(get_path_to_form_config('auth.xml', 'ringo')))\n    form_config = config.get_form('register_user')\n    form = Form(form_config, csrf_token=request.session.get_csrf_token())\n    # Do extra validation which is not handled by formbar.\n    # Is the login unique?\n    validator = Validator('login',\n                          'There is already a user with this name',\n                          is_login_unique)\n    form.add_validator(validator)\n    if request.POST:\n        if form.validate(request.params.mixed()):\n            # 1. Create user. Do not activate him. Default role is user.\n            ufac = User.get_item_factory()\n            # TODO: Check why we not use the get_item_factory_method\n            # here. Do we use plain factories because the need full\n            # controll of depended relations? (ti) <2014-04-08 17:07> \n            pfac = BaseFactory(Profile)\n            gfac = BaseFactory(Usergroup)\n            user = ufac.create(None)\n            # Set login from formdata\n            user.login = form.data['login']\n            # Encrypt password and save\n            pw = hashlib.md5()\n            pw.update(form.data['pass'])\n            user.password = pw.hexdigest()\n            # Deactivate the user. To activate the user needs to confirm\n            # with the activation link\n            user.activated = False\n            atoken = str(uuid.uuid4())\n            user.activation_token = atoken\n            # Set profile data\n            user.profile[0].email = form.data['email']\n            # Set user group\n            group = gfac.load(USER_GROUP_ID)\n            user.groups.append(group)\n            # Set default user group.\n            user.gid = group.id\n            DBSession.add(user)\n\n            # 3. Send confirmation email. The user will be activated\n            #    after the user clicks on the confirmation link\n            mailer = Mailer(request)\n            recipient = user.profile[0].email\n            subject = _('Confirm user registration for %s' % get_app_name())\n            values = {'url': request.route_url('confirm_user', token=atoken),\n                      'app_name': get_app_name(),\n                      'email': settings['mail.default_sender'],\n                      '_': _}\n            mail = Mail([recipient], subject, template=\"register_user\", values=values)\n            mailer.send(mail)\n\n            target_url = request.route_path('login')\n            headers = forget(request)\n            msg = _(\"User has been created and a confirmation mail was sent\"\n                    \" to the users email adress. Please check your email :)\")\n            request.session.flash(msg, 'success')\n            return HTTPFound(location=target_url, headers=headers)\n    return {'form': form.render()}", "func_src_after": "def register_user(request):\n    settings = request.registry.settings\n    if not is_registration_enabled(settings):\n        raise exc.exception_response(503)\n    handle_history(request)\n    _ = request.translate\n    config = Config(load(get_path_to_form_config('auth.xml', 'ringo')))\n    form_config = config.get_form('register_user')\n    form = Form(form_config, csrf_token=request.session.get_csrf_token())\n    # Do extra validation which is not handled by formbar.\n    # Is the login unique?\n    validator = Validator('login',\n                          'There is already a user with this name',\n                          is_login_unique)\n    form.add_validator(validator)\n    if request.POST:\n        if form.validate(request.params.mixed()):\n            # 1. Create user. Do not activate him. Default role is user.\n            ufac = User.get_item_factory()\n            # TODO: Check why we not use the get_item_factory_method\n            # here. Do we use plain factories because the need full\n            # controll of depended relations? (ti) <2014-04-08 17:07> \n            pfac = BaseFactory(Profile)\n            gfac = BaseFactory(Usergroup)\n            user = ufac.create(None)\n            # Set login from formdata\n            user.login = form.data['login']\n            # Encrypt password and save\n            user.password = encrypt_password(form.data['pass'])\n            # Deactivate the user. To activate the user needs to confirm\n            # with the activation link\n            user.activated = False\n            atoken = str(uuid.uuid4())\n            user.activation_token = atoken\n            # Set profile data\n            user.profile[0].email = form.data['email']\n            # Set user group\n            group = gfac.load(USER_GROUP_ID)\n            user.groups.append(group)\n            # Set default user group.\n            user.gid = group.id\n            DBSession.add(user)\n\n            # 3. Send confirmation email. The user will be activated\n            #    after the user clicks on the confirmation link\n            mailer = Mailer(request)\n            recipient = user.profile[0].email\n            subject = _('Confirm user registration for %s' % get_app_name())\n            values = {'url': request.route_url('confirm_user', token=atoken),\n                      'app_name': get_app_name(),\n                      'email': settings['mail.default_sender'],\n                      '_': _}\n            mail = Mail([recipient], subject, template=\"register_user\", values=values)\n            mailer.send(mail)\n\n            target_url = request.route_path('login')\n            headers = forget(request)\n            msg = _(\"User has been created and a confirmation mail was sent\"\n                    \" to the users email adress. Please check your email :)\")\n            request.session.flash(msg, 'success')\n            return HTTPFound(location=target_url, headers=headers)\n    return {'form': form.render()}", "line_changes": {"deleted": [{"line_no": 29, "char_start": 1310, "char_end": 1341, "line": "            pw = hashlib.md5()\n"}, {"line_no": 30, "char_start": 1341, "char_end": 1382, "line": "            pw.update(form.data['pass'])\n"}, {"line_no": 31, "char_start": 1382, "char_end": 1425, "line": "            user.password = pw.hexdigest()\n"}], "added": [{"line_no": 29, "char_start": 1310, "char_end": 1374, "line": "            user.password = encrypt_password(form.data['pass'])\n"}]}, "char_changes": {"deleted": [{"char_start": 1322, "char_end": 1423, "chars": "pw = hashlib.md5()\n            pw.update(form.data['pass'])\n            user.password = pw.hexdigest("}], "added": [{"char_start": 1322, "char_end": 1372, "chars": "user.password = encrypt_password(form.data['pass']"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/ddb9d55999151f37fd4c833a98d2f34648757293", "file_name": "auth.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new encryption methods using passlib", "parent_commit": "8e92641fee542f6e7004e827136dea3ce5e99eb2", "description": "Write a Python function to handle user registration, including form validation, user creation, and sending a confirmation email."}
{"func_name": "glyph_cache_put", "func_src_before": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index > glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "func_src_after": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index >= glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/c0fd449ec0870b050d350d6d844b1ea6dad4bc7d", "file_name": "libfreerdp/cache/glyph.c", "vul_type": "cwe-125", "description": "Write a C function named `glyph_cache_put` that stores a glyph in a glyph cache, ensuring the cache ID and index are within valid bounds, and logs the operation."}
{"func_name": "_port_conf_generator", "func_src_before": "    def _port_conf_generator(self, cmd):\n        ssh_cmd = '%s -delim !' % cmd\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return\n        port_lines = out.strip().split('\\n')\n        if not len(port_lines):\n            return\n\n        header = port_lines.pop(0)\n        yield header\n        for portip_line in port_lines:\n            try:\n                port_data = self._get_hdr_dic(header, portip_line, '!')\n            except exception.VolumeBackendAPIException:\n                with excutils.save_and_reraise_exception():\n                    self._log_cli_output_error('_port_conf_generator',\n                                               ssh_cmd, out, err)\n            yield port_data", "func_src_after": "    def _port_conf_generator(self, cmd):\n        ssh_cmd = cmd + ['-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return\n        port_lines = out.strip().split('\\n')\n        if not len(port_lines):\n            return\n\n        header = port_lines.pop(0)\n        yield header\n        for portip_line in port_lines:\n            try:\n                port_data = self._get_hdr_dic(header, portip_line, '!')\n            except exception.VolumeBackendAPIException:\n                with excutils.save_and_reraise_exception():\n                    self._log_cli_output_error('_port_conf_generator',\n                                               ssh_cmd, out, err)\n            yield port_data", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function named `_port_conf_generator` that takes a command, executes it via SSH, and yields parsed port configuration data."}
{"func_name": "cancelFollow", "func_src_before": "    def cancelFollow(self,userid,friendid):\n        sqlText=\"delete from friends where userid=%d and friendid=%d;\"%(userid,friendid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;", "func_src_after": "    def cancelFollow(self,userid,friendid):\n        sqlText=\"delete from friends where userid=%d and friendid=%s;\"\n        params=[userid,friendid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;", "commit_link": "github.com/ShaominLi/Twitter_project/commit/5329d91f9e569c95184053c8e7ef596949c33ce9", "file_name": "modules/users.py", "vul_type": "cwe-089", "description": "Write a Python function named `cancelFollow` that removes a friend connection from a database using SQL."}
{"func_name": "check_clus_chain", "func_src_before": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tclus_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %u bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "func_src_after": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tuint64_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %\"\n\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 136, "char_end": 162, "line": "\tclus_t count, max_count;\n"}, {"line_no": 32, "char_start": 888, "char_end": 917, "line": "\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 48, "char_start": 1333, "char_end": 1379, "line": "\t\t\t\t\t\"the other file. truncated to %u bytes\",\n"}, {"line_no": 64, "char_start": 1783, "char_end": 1813, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 65, "char_start": 1813, "char_end": 1827, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 66, "char_start": 1827, "char_end": 1852, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 78, "char_start": 2116, "char_end": 2146, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 79, "char_start": 2146, "char_end": 2160, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 80, "char_start": 2160, "char_end": 2185, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 98, "char_start": 2496, "char_end": 2524, "line": "\t\t\t\"truncates to %u bytes\",\n"}], "added": [{"line_no": 5, "char_start": 136, "char_end": 164, "line": "\tuint64_t count, max_count;\n"}, {"line_no": 32, "char_start": 890, "char_end": 928, "line": "\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n"}, {"line_no": 48, "char_start": 1344, "char_end": 1382, "line": "\t\t\t\t\t\"the other file. truncated to %\"\n"}, {"line_no": 49, "char_start": 1382, "char_end": 1404, "line": "\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 65, "char_start": 1808, "char_end": 1830, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 66, "char_start": 1830, "char_end": 1853, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 67, "char_start": 1853, "char_end": 1886, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 79, "char_start": 2150, "char_end": 2172, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 80, "char_start": 2172, "char_end": 2195, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 81, "char_start": 2195, "char_end": 2228, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 99, "char_start": 2539, "char_end": 2576, "line": "\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 141, "chars": "clus"}, {"char_start": 907, "char_end": 908, "chars": "u"}, {"char_start": 1369, "char_end": 1370, "chars": "u"}, {"char_start": 1803, "char_end": 1804, "chars": "u"}, {"char_start": 1826, "char_end": 1833, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2136, "char_end": 2137, "chars": "u"}, {"char_start": 2159, "char_end": 2166, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2514, "char_end": 2515, "chars": "u"}], "added": [{"char_start": 137, "char_end": 143, "chars": "uint64"}, {"char_start": 909, "char_end": 919, "chars": "\" PRIu64 \""}, {"char_start": 1380, "char_end": 1395, "chars": "\"\n\t\t\t\t\tPRIu64 \""}, {"char_start": 1828, "char_end": 1844, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 1866, "char_end": 1867, "chars": " "}, {"char_start": 2170, "char_end": 2186, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 2208, "char_end": 2209, "chars": " "}, {"char_start": 2557, "char_end": 2567, "chars": "\" PRIu64 \""}]}, "commit_link": "github.com/exfatprogs/exfatprogs/commit/c1f48157c38df8d958ab81012abae2470750a785", "file_name": "fsck.c", "vul_type": "cwe-190", "commit_msg": "fsck: fix integer overflow in calculating size\n\nThe size must be 64-bit integer\n\nSigned-off-by: Hyunchul Lee <hyc.lee@gmail.com>", "parent_commit": "edf7a39b7252f4b63915292420aaa63a750f22d9", "description": "Write a C function to validate and repair the cluster chain of a file in an exFAT file system."}
{"func_name": "upnp_redirect", "func_src_before": "upnp_redirect(const char * rhost, unsigned short eport,\n              const char * iaddr, unsigned short iport,\n              const char * protocol, const char * desc,\n              unsigned int leaseduration)\n{\n\tint proto, r;\n\tchar iaddr_old[32];\n\tchar rhost_old[32];\n\tunsigned short iport_old;\n\tstruct in_addr address;\n\tunsigned int timestamp;\n\n\tproto = proto_atoi(protocol);\n\tif(inet_aton(iaddr, &address) <= 0) {\n\t\tsyslog(LOG_ERR, \"inet_aton(%s) FAILED\", iaddr);\n\t\treturn -1;\n\t}\n\n\tif(!check_upnp_rule_against_permissions(upnppermlist, num_upnpperm,\n\t                                        eport, address, iport)) {\n\t\tsyslog(LOG_INFO, \"redirection permission check failed for \"\n\t\t                 \"%hu->%s:%hu %s\", eport, iaddr, iport, protocol);\n\t\treturn -3;\n\t}\n\t/* IGDv1 (WANIPConnection:1 Service Template Version 1.01 / Nov 12, 2001)\n\t * - 2.2.20.PortMappingDescription :\n\t *  Overwriting Previous / Existing Port Mappings:\n\t * If the RemoteHost, ExternalPort, PortMappingProtocol and InternalClient\n\t * are exactly the same as an existing mapping, the existing mapping values\n\t * for InternalPort, PortMappingDescription, PortMappingEnabled and\n\t * PortMappingLeaseDuration are overwritten.\n\t *  Rejecting a New Port Mapping:\n\t * In cases where the RemoteHost, ExternalPort and PortMappingProtocol\n\t * are the same as an existing mapping, but the InternalClient is\n\t * different, the action is rejected with an appropriate error.\n\t *  Add or Reject New Port Mapping behavior based on vendor implementation:\n\t * In cases where the ExternalPort, PortMappingProtocol and InternalClient\n\t * are the same, but RemoteHost is different, the vendor can choose to\n\t * support both mappings simultaneously, or reject the second mapping\n\t * with an appropriate error.\n\t *\n\t * - 2.4.16.AddPortMapping\n\t * This action creates a new port mapping or overwrites an existing\n\t * mapping with the same internal client. If the ExternalPort and\n\t * PortMappingProtocol pair is already mapped to another internal client,\n\t * an error is returned.\n\t *\n\t * IGDv2 (WANIPConnection:2 Service Standardized DCP (SDCP) Sep 10, 2010)\n\t * Protocol ExternalPort RemoteHost InternalClient Result\n\t *     =         =           \u2260           \u2260         Failure\n\t *     =         =           \u2260           =         Failure or success\n\t *                                                 (vendor specific)\n\t *     =         =           =           \u2260         Failure\n\t *     =         =           =           =         Success (overwrite)\n\t */\n\trhost_old[0] = '\\0';\n\tr = get_redirect_rule(ext_if_name, eport, proto,\n\t                      iaddr_old, sizeof(iaddr_old), &iport_old, 0, 0,\n\t                      rhost_old, sizeof(rhost_old),\n\t                      &timestamp, 0, 0);\n\tif(r == 0) {\n\t\tif(strcmp(iaddr, iaddr_old)==0 &&\n\t\t   ((rhost == NULL && rhost_old[0]=='\\0') ||\n\t\t    (rhost && (strcmp(rhost, \"*\") == 0) && rhost_old[0]=='\\0') ||\n\t\t    (rhost && (strcmp(rhost, rhost_old) == 0)))) {\n\t\t\tsyslog(LOG_INFO, \"updating existing port mapping %hu %s (rhost '%s') => %s:%hu\",\n\t\t\t\teport, protocol, rhost_old, iaddr_old, iport_old);\n\t\t\ttimestamp = (leaseduration > 0) ? upnp_time() + leaseduration : 0;\n\t\t\tif(iport != iport_old) {\n\t\t\t\tr = update_portmapping(ext_if_name, eport, proto, iport, desc, timestamp);\n\t\t\t} else {\n\t\t\t\tr = update_portmapping_desc_timestamp(ext_if_name, eport, proto, desc, timestamp);\n\t\t\t}\n#ifdef ENABLE_LEASEFILE\n\t\t\tif(r == 0) {\n\t\t\t\tlease_file_remove(eport, proto);\n\t\t\t\tlease_file_add(eport, iaddr, iport, proto, desc, timestamp);\n\t\t\t}\n#endif /* ENABLE_LEASEFILE */\n\t\t\treturn r;\n\t\t} else {\n\t\t\tsyslog(LOG_INFO, \"port %hu %s (rhost '%s') already redirected to %s:%hu\",\n\t\t\t\teport, protocol, rhost_old, iaddr_old, iport_old);\n\t\t\treturn -2;\n\t\t}\n#ifdef CHECK_PORTINUSE\n\t} else if (port_in_use(ext_if_name, eport, proto, iaddr, iport) > 0) {\n\t\tsyslog(LOG_INFO, \"port %hu protocol %s already in use\",\n\t\t       eport, protocol);\n\t\treturn -4;\n#endif /* CHECK_PORTINUSE */\n\t} else {\n\t\ttimestamp = (leaseduration > 0) ? upnp_time() + leaseduration : 0;\n\t\tsyslog(LOG_INFO, \"redirecting port %hu to %s:%hu protocol %s for: %s\",\n\t\t\teport, iaddr, iport, protocol, desc);\n\t\treturn upnp_redirect_internal(rhost, eport, iaddr, iport, proto,\n\t\t                              desc, timestamp);\n\t}\n}", "func_src_after": "upnp_redirect(const char * rhost, unsigned short eport,\n              const char * iaddr, unsigned short iport,\n              const char * protocol, const char * desc,\n              unsigned int leaseduration)\n{\n\tint proto, r;\n\tchar iaddr_old[32];\n\tchar rhost_old[32];\n\tunsigned short iport_old;\n\tstruct in_addr address;\n\tunsigned int timestamp;\n\n\tproto = proto_atoi(protocol);\n\tif(inet_aton(iaddr, &address) <= 0) {\n\t\tsyslog(LOG_ERR, \"inet_aton(%s) FAILED\", iaddr);\n\t\treturn -1;\n\t}\n\n\tif(!check_upnp_rule_against_permissions(upnppermlist, num_upnpperm,\n\t                                        eport, address, iport)) {\n\t\tsyslog(LOG_INFO, \"redirection permission check failed for \"\n\t\t                 \"%hu->%s:%hu %s\", eport, iaddr, iport, protocol);\n\t\treturn -3;\n\t}\n\n\tif (desc == NULL)\n\t\tdesc = \"\";\t/* assume empty description */\n\n\t/* IGDv1 (WANIPConnection:1 Service Template Version 1.01 / Nov 12, 2001)\n\t * - 2.2.20.PortMappingDescription :\n\t *  Overwriting Previous / Existing Port Mappings:\n\t * If the RemoteHost, ExternalPort, PortMappingProtocol and InternalClient\n\t * are exactly the same as an existing mapping, the existing mapping values\n\t * for InternalPort, PortMappingDescription, PortMappingEnabled and\n\t * PortMappingLeaseDuration are overwritten.\n\t *  Rejecting a New Port Mapping:\n\t * In cases where the RemoteHost, ExternalPort and PortMappingProtocol\n\t * are the same as an existing mapping, but the InternalClient is\n\t * different, the action is rejected with an appropriate error.\n\t *  Add or Reject New Port Mapping behavior based on vendor implementation:\n\t * In cases where the ExternalPort, PortMappingProtocol and InternalClient\n\t * are the same, but RemoteHost is different, the vendor can choose to\n\t * support both mappings simultaneously, or reject the second mapping\n\t * with an appropriate error.\n\t *\n\t * - 2.4.16.AddPortMapping\n\t * This action creates a new port mapping or overwrites an existing\n\t * mapping with the same internal client. If the ExternalPort and\n\t * PortMappingProtocol pair is already mapped to another internal client,\n\t * an error is returned.\n\t *\n\t * IGDv2 (WANIPConnection:2 Service Standardized DCP (SDCP) Sep 10, 2010)\n\t * Protocol ExternalPort RemoteHost InternalClient Result\n\t *     =         =           \u2260           \u2260         Failure\n\t *     =         =           \u2260           =         Failure or success\n\t *                                                 (vendor specific)\n\t *     =         =           =           \u2260         Failure\n\t *     =         =           =           =         Success (overwrite)\n\t */\n\trhost_old[0] = '\\0';\n\tr = get_redirect_rule(ext_if_name, eport, proto,\n\t                      iaddr_old, sizeof(iaddr_old), &iport_old, 0, 0,\n\t                      rhost_old, sizeof(rhost_old),\n\t                      &timestamp, 0, 0);\n\tif(r == 0) {\n\t\tif(strcmp(iaddr, iaddr_old)==0 &&\n\t\t   ((rhost == NULL && rhost_old[0]=='\\0') ||\n\t\t    (rhost && (strcmp(rhost, \"*\") == 0) && rhost_old[0]=='\\0') ||\n\t\t    (rhost && (strcmp(rhost, rhost_old) == 0)))) {\n\t\t\tsyslog(LOG_INFO, \"updating existing port mapping %hu %s (rhost '%s') => %s:%hu\",\n\t\t\t\teport, protocol, rhost_old, iaddr_old, iport_old);\n\t\t\ttimestamp = (leaseduration > 0) ? upnp_time() + leaseduration : 0;\n\t\t\tif(iport != iport_old) {\n\t\t\t\tr = update_portmapping(ext_if_name, eport, proto, iport, desc, timestamp);\n\t\t\t} else {\n\t\t\t\tr = update_portmapping_desc_timestamp(ext_if_name, eport, proto, desc, timestamp);\n\t\t\t}\n#ifdef ENABLE_LEASEFILE\n\t\t\tif(r == 0) {\n\t\t\t\tlease_file_remove(eport, proto);\n\t\t\t\tlease_file_add(eport, iaddr, iport, proto, desc, timestamp);\n\t\t\t}\n#endif /* ENABLE_LEASEFILE */\n\t\t\treturn r;\n\t\t} else {\n\t\t\tsyslog(LOG_INFO, \"port %hu %s (rhost '%s') already redirected to %s:%hu\",\n\t\t\t\teport, protocol, rhost_old, iaddr_old, iport_old);\n\t\t\treturn -2;\n\t\t}\n#ifdef CHECK_PORTINUSE\n\t} else if (port_in_use(ext_if_name, eport, proto, iaddr, iport) > 0) {\n\t\tsyslog(LOG_INFO, \"port %hu protocol %s already in use\",\n\t\t       eport, protocol);\n\t\treturn -4;\n#endif /* CHECK_PORTINUSE */\n\t} else {\n\t\ttimestamp = (leaseduration > 0) ? upnp_time() + leaseduration : 0;\n\t\tsyslog(LOG_INFO, \"redirecting port %hu to %s:%hu protocol %s for: %s\",\n\t\t\teport, iaddr, iport, protocol, desc);\n\t\treturn upnp_redirect_internal(rhost, eport, iaddr, iport, proto,\n\t\t                              desc, timestamp);\n\t}\n}", "commit_link": "github.com/miniupnp/miniupnp/commit/f321c2066b96d18afa5158dfa2d2873a2957ef38", "file_name": "miniupnpd/upnpredirect.c", "vul_type": "cwe-476", "description": "In C, write a function `upnp_redirect` to manage port forwarding rules for UPnP, including permission checks and updating existing mappings."}
{"func_name": "get_markets", "func_src_before": "@app.route('/get_markets')\ndef get_markets():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "func_src_after": "@app.route('/get_markets')\ndef get_markets():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT * FROM markets WHERE aid=%s\"\n    cur.execute(query, (asset_id,))\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Create a Python Flask endpoint that retrieves market data for a given asset ID from a PostgreSQL database, handling the asset ID lookup and database query."}
{"func_name": "tensorflow::QuantizeAndDequantizeV2Op::Compute", "func_src_before": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    Tensor input_min_tensor;\n    Tensor input_max_tensor;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    if (range_given_) {\n      input_min_tensor = ctx->input(1);\n      input_max_tensor = ctx->input(2);\n      if (axis_ == -1) {\n        auto min_val = input_min_tensor.scalar<T>()();\n        auto max_val = input_max_tensor.scalar<T>()();\n        OP_REQUIRES(ctx, min_val <= max_val,\n                    errors::InvalidArgument(\"Invalid range: input_min \",\n                                            min_val, \" > input_max \", max_val));\n      } else {\n        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_min_tensor has incorrect size, was \",\n                        input_min_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_min_tensor.shape()));\n        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_max_tensor has incorrect size, was \",\n                        input_max_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_max_tensor.shape()));\n      }\n    } else {\n      auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_max_tensor));\n    }\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_, num_bits_,\n        range_given_, &input_min_tensor, &input_max_tensor, round_mode_,\n        narrow_range_, output->flat<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1), signed_input_,\n        num_bits_, range_given_, &input_min_tensor, &input_max_tensor,\n        round_mode_, narrow_range_,\n        output->template flat_inner_outer_dims<T, 3>(axis_ - 1));\n    }\n  }", "func_src_after": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    OP_REQUIRES(\n        ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n        errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n                                \" but is rank \", input.shape().dims()));\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    Tensor input_min_tensor;\n    Tensor input_max_tensor;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    if (range_given_) {\n      input_min_tensor = ctx->input(1);\n      input_max_tensor = ctx->input(2);\n      if (axis_ == -1) {\n        auto min_val = input_min_tensor.scalar<T>()();\n        auto max_val = input_max_tensor.scalar<T>()();\n        OP_REQUIRES(ctx, min_val <= max_val,\n                    errors::InvalidArgument(\"Invalid range: input_min \",\n                                            min_val, \" > input_max \", max_val));\n      } else {\n        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_min_tensor has incorrect size, was \",\n                        input_min_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_min_tensor.shape()));\n        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_max_tensor has incorrect size, was \",\n                        input_max_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_max_tensor.shape()));\n      }\n    } else {\n      auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_max_tensor));\n    }\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_, num_bits_,\n        range_given_, &input_min_tensor, &input_max_tensor, round_mode_,\n        narrow_range_, output->flat<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1), signed_input_,\n        num_bits_, range_given_, &input_min_tensor, &input_max_tensor,\n        round_mode_, narrow_range_,\n        output->template flat_inner_outer_dims<T, 3>(axis_ - 1));\n    }\n  }", "commit_link": "github.com/tensorflow/tensorflow/commit/eccb7ec454e6617738554a255d77f08e60ee0808", "file_name": "tensorflow/core/kernels/quantize_and_dequantize_op.cc", "vul_type": "cwe-125", "description": "Write a C++ function to quantize and dequantize tensors, handling both per-tensor and per-channel quantization."}
{"func_name": "setUp", "func_src_before": "  def setUp(self):\n    self._tmp_dir = tempfile.mktemp()\n\n    self.v = variables.VariableV1(10.0, name=\"v\")\n    self.w = variables.VariableV1(21.0, name=\"w\")\n    self.delta = constant_op.constant(1.0, name=\"delta\")\n    self.inc_v = state_ops.assign_add(self.v, self.delta, name=\"inc_v\")\n\n    self.w_int = control_flow_ops.with_dependencies(\n        [self.inc_v],\n        math_ops.cast(self.w, dtypes.int32, name=\"w_int_inner\"),\n        name=\"w_int_outer\")\n\n    self.ph = array_ops.placeholder(dtypes.float32, name=\"ph\")\n    self.xph = array_ops.transpose(self.ph, name=\"xph\")\n    self.m = constant_op.constant(\n        [[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]], dtype=dtypes.float32, name=\"m\")\n    self.y = math_ops.matmul(self.m, self.xph, name=\"y\")\n\n    self.sparse_ph = array_ops.sparse_placeholder(\n        dtypes.float32, shape=([5, 5]), name=\"sparse_placeholder\")\n    self.sparse_add = sparse_ops.sparse_add(self.sparse_ph, self.sparse_ph)\n\n    rewriter_config = rewriter_config_pb2.RewriterConfig(\n        disable_model_pruning=True,\n        arithmetic_optimization=rewriter_config_pb2.RewriterConfig.OFF,\n        dependency_optimization=rewriter_config_pb2.RewriterConfig.OFF)\n    graph_options = config_pb2.GraphOptions(rewrite_options=rewriter_config)\n    config_proto = config_pb2.ConfigProto(graph_options=graph_options)\n    self.sess = session.Session(config=config_proto)\n\n    # Initialize variable.\n    self.sess.run(variables.global_variables_initializer())", "func_src_after": "  def setUp(self):\n    self._tmp_dir = tempfile.mkdtemp()\n\n    self.v = variables.VariableV1(10.0, name=\"v\")\n    self.w = variables.VariableV1(21.0, name=\"w\")\n    self.delta = constant_op.constant(1.0, name=\"delta\")\n    self.inc_v = state_ops.assign_add(self.v, self.delta, name=\"inc_v\")\n\n    self.w_int = control_flow_ops.with_dependencies(\n        [self.inc_v],\n        math_ops.cast(self.w, dtypes.int32, name=\"w_int_inner\"),\n        name=\"w_int_outer\")\n\n    self.ph = array_ops.placeholder(dtypes.float32, name=\"ph\")\n    self.xph = array_ops.transpose(self.ph, name=\"xph\")\n    self.m = constant_op.constant(\n        [[0.0, 1.0, 2.0], [-4.0, -1.0, 0.0]], dtype=dtypes.float32, name=\"m\")\n    self.y = math_ops.matmul(self.m, self.xph, name=\"y\")\n\n    self.sparse_ph = array_ops.sparse_placeholder(\n        dtypes.float32, shape=([5, 5]), name=\"sparse_placeholder\")\n    self.sparse_add = sparse_ops.sparse_add(self.sparse_ph, self.sparse_ph)\n\n    rewriter_config = rewriter_config_pb2.RewriterConfig(\n        disable_model_pruning=True,\n        arithmetic_optimization=rewriter_config_pb2.RewriterConfig.OFF,\n        dependency_optimization=rewriter_config_pb2.RewriterConfig.OFF)\n    graph_options = config_pb2.GraphOptions(rewrite_options=rewriter_config)\n    config_proto = config_pb2.ConfigProto(graph_options=graph_options)\n    self.sess = session.Session(config=config_proto)\n\n    # Initialize variable.\n    self.sess.run(variables.global_variables_initializer())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 57, "line": "    self._tmp_dir = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 58, "line": "    self._tmp_dir = tempfile.mkdtemp()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 50, "char_end": 51, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/4f93d5f529a732dd533c063ae5b85e03e2006882", "file_name": "local_cli_wrapper_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420369603\nChange-Id: I2cf40b13f41cc01000c2c21a483a2d680194dba2", "description": "Write a Python function to set up a TensorFlow test environment with variables, placeholders, and a session configuration."}
{"func_name": "wwunpack", "func_src_before": "int wwunpack(uint8_t *exe, uint32_t exesz, uint8_t *wwsect, struct cli_exe_section *sects, uint16_t scount, uint32_t pe, int desc) {\n  uint8_t *structs = wwsect + 0x2a1, *compd, *ccur, *unpd, *ucur, bc;\n  uint32_t src, srcend, szd, bt, bits;\n  int error=0, i;\n\n  cli_dbgmsg(\"in wwunpack\\n\");\n  while (1) {\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, structs, 17)) {\n      cli_dbgmsg(\"WWPack: Array of structs out of section\\n\");\n      break;\n    }\n    src = sects[scount].rva - cli_readint32(structs); /* src delta / dst delta - not used / dwords / end of src */\n    structs+=8;\n    szd = cli_readint32(structs) * 4;\n    structs+=4;\n    srcend = cli_readint32(structs);\n    structs+=4;\n\n    unpd = ucur = exe+src+srcend+4-szd;\n    if (!szd || !CLI_ISCONTAINED(exe, exesz, unpd, szd)) {\n      cli_dbgmsg(\"WWPack: Compressed data out of file\\n\");\n      break;\n    }\n    cli_dbgmsg(\"WWP: src: %x, szd: %x, srcend: %x - %x\\n\", src, szd, srcend, srcend+4-szd);\n    if (!(compd = cli_malloc(szd))) {\n        cli_dbgmsg(\"WWPack: Unable to allocate memory for compd\\n\");\n        break;\n    }\n    memcpy(compd, unpd, szd);\n    memset(unpd, -1, szd); /*FIXME*/\n    ccur=compd;\n    \n    RESEED;\n    while(!error) {\n      uint32_t backbytes, backsize;\n      uint8_t saved;\n\n      BIT;\n      if (!bits) { /* BYTE copy */\n\tif(ccur-compd>=szd || !CLI_ISCONTAINED(exe, exesz, ucur, 1))\n\t  error=1;\n\telse\n\t  *ucur++=*ccur++;\n\tcontinue;\n      }\n\n      BITS(2);\n      if(bits==3) { /* WORD backcopy */\n\tuint8_t shifted, subbed = 31;\n\tBITS(2);\n\tshifted = bits + 5;\n\tif(bits>=2) {\n\t  shifted++;\n\t  subbed += 0x80;\n\t}\n\tbackbytes = (1<<shifted)-subbed; /* 1h, 21h, 61h, 161h */\n\tBITS(shifted); /* 5, 6, 8, 9 */\n\tif(error || bits == 0x1ff) break;\n\tbackbytes+=bits;\n\tif(!CLI_ISCONTAINED(exe, exesz, ucur, 2) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, 2)) {\n\t  error=1;\n\t} else {\n\t  ucur[0]=*(ucur-backbytes);\n\t  ucur[1]=*(ucur-backbytes+1);\n\t  ucur+=2;\n\t}\n\tcontinue;\n      }\n\n      /* BLOCK backcopy */\n      saved = bits; /* cmp al, 1 / pushf */\n\n      BITS(3);\n      if (bits<6) {\n\tbackbytes = bits;\n\tswitch(bits) {\n\tcase 4: /* 10,11 */\n\t  backbytes++;\n\tcase 3: /* 8,9 */\n\t  BIT;\n\t  backbytes+=bits;\n\tcase 0:\tcase 1:\tcase 2: /* 5,6,7 */\n\t  backbytes+=5;\n\t  break;\n\tcase 5: /* 12 */\n\t  backbytes=12;\n\t  break;\n\t}\n\tBITS(backbytes);\n\tbits+=(1<<backbytes)-31;\n      } else if(bits==6) {\n\tBITS(0x0e);\n\tbits+=0x1fe1;\n      } else {\n\tBITS(0x0f);\n\tbits+=0x5fe1;\n      }\n\n      backbytes = bits;\n\n      /* popf / jb */\n      if (!saved) {\n\tBIT;\n\tif(!bits) {\n\t  BIT;\n\t  bits+=5;\n\t} else {\n\t  BITS(3);\n\t  if(bits) {\n\t    bits+=6;\n\t  } else {\n\t    BITS(4);\n\t    if(bits) {\n\t      bits+=13;\n\t    } else {\n\t      uint8_t cnt = 4;\n\t      uint16_t shifted = 0x0d;\n\t      \n\t      do {\n\t\tif(cnt==7) { cnt = 0x0e; shifted = 0; break; }\n\t\tshifted=((shifted+2)<<1)-1;\n\t\tBIT;\n\t\tcnt++;\n\t      } while(!bits);\n\t      BITS(cnt);\n\t      bits+=shifted;\n\t    }\n\t  }\n\t}\n\tbacksize = bits;\n      } else {\n\tbacksize = saved+2;\n      }\n\n      if(!CLI_ISCONTAINED(exe, exesz, ucur, backsize) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, backsize)) error=1;\n      else while(backsize--) {\n\t*ucur=*(ucur-backbytes);\n\tucur++;\n      }\n    }\n    free(compd);\n    if(error) {\n      cli_dbgmsg(\"WWPack: decompression error\\n\");\n      break;\n    }\n    if (error || !*structs++) break;\n  }\n\n  if(!error) {\n    if (pe+6 > exesz || pe+7 > exesz || pe+0x28 > exesz ||\n\t\tpe+0x50 > exesz || pe+0x14 > exesz) \n\treturn CL_EFORMAT;\n    exe[pe+6]=(uint8_t)scount;\n    exe[pe+7]=(uint8_t)(scount>>8);\n    cli_writeint32(&exe[pe+0x28], cli_readint32(wwsect+0x295)+sects[scount].rva+0x299);\n    cli_writeint32(&exe[pe+0x50], cli_readint32(&exe[pe+0x50])-sects[scount].vsz);\n\n    structs = &exe[(0xffff&cli_readint32(&exe[pe+0x14]))+pe+0x18];\n    for(i=0 ; i<scount ; i++) {\n\t  if (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t    cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t    return CL_EFORMAT;\n\t  }\n\n      cli_writeint32(structs+8, sects[i].vsz);\n      cli_writeint32(structs+12, sects[i].rva);\n      cli_writeint32(structs+16, sects[i].vsz);\n      cli_writeint32(structs+20, sects[i].rva);\n      structs+=0x28;\n    }\n\tif (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t  cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t  return CL_EFORMAT;\n\t}\n\n    memset(structs, 0, 0x28);\n    error = (uint32_t)cli_writen(desc, exe, exesz)!=exesz;\n  }\n  return error;\n}", "func_src_after": "int wwunpack(uint8_t *exe, uint32_t exesz, uint8_t *wwsect, struct cli_exe_section *sects, uint16_t scount, uint32_t pe, int desc) {\n  uint8_t *structs = wwsect + 0x2a1, *compd, *ccur, *unpd, *ucur, bc;\n  uint32_t src, srcend, szd, bt, bits;\n  int error=0, i;\n\n  cli_dbgmsg(\"in wwunpack\\n\");\n  while (1) {\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, structs, 17)) {\n      cli_dbgmsg(\"WWPack: Array of structs out of section\\n\");\n      break;\n    }\n    src = sects[scount].rva - cli_readint32(structs); /* src delta / dst delta - not used / dwords / end of src */\n    structs+=8;\n    szd = cli_readint32(structs) * 4;\n    structs+=4;\n    srcend = cli_readint32(structs);\n    structs+=4;\n\n    unpd = ucur = exe+src+srcend+4-szd;\n    if (!szd || !CLI_ISCONTAINED(exe, exesz, unpd, szd)) {\n      cli_dbgmsg(\"WWPack: Compressed data out of file\\n\");\n      break;\n    }\n    cli_dbgmsg(\"WWP: src: %x, szd: %x, srcend: %x - %x\\n\", src, szd, srcend, srcend+4-szd);\n    if (!(compd = cli_malloc(szd))) {\n        cli_dbgmsg(\"WWPack: Unable to allocate memory for compd\\n\");\n        break;\n    }\n    memcpy(compd, unpd, szd);\n    memset(unpd, -1, szd); /*FIXME*/\n    ccur=compd;\n    \n    RESEED;\n    while(!error) {\n      uint32_t backbytes, backsize;\n      uint8_t saved;\n\n      BIT;\n      if (!bits) { /* BYTE copy */\n\tif(ccur-compd>=szd || !CLI_ISCONTAINED(exe, exesz, ucur, 1))\n\t  error=1;\n\telse\n\t  *ucur++=*ccur++;\n\tcontinue;\n      }\n\n      BITS(2);\n      if(bits==3) { /* WORD backcopy */\n\tuint8_t shifted, subbed = 31;\n\tBITS(2);\n\tshifted = bits + 5;\n\tif(bits>=2) {\n\t  shifted++;\n\t  subbed += 0x80;\n\t}\n\tbackbytes = (1<<shifted)-subbed; /* 1h, 21h, 61h, 161h */\n\tBITS(shifted); /* 5, 6, 8, 9 */\n\tif(error || bits == 0x1ff) break;\n\tbackbytes+=bits;\n\tif(!CLI_ISCONTAINED(exe, exesz, ucur, 2) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, 2)) {\n\t  error=1;\n\t} else {\n\t  ucur[0]=*(ucur-backbytes);\n\t  ucur[1]=*(ucur-backbytes+1);\n\t  ucur+=2;\n\t}\n\tcontinue;\n      }\n\n      /* BLOCK backcopy */\n      saved = bits; /* cmp al, 1 / pushf */\n\n      BITS(3);\n      if (bits<6) {\n\tbackbytes = bits;\n\tswitch(bits) {\n\tcase 4: /* 10,11 */\n\t  backbytes++;\n\tcase 3: /* 8,9 */\n\t  BIT;\n\t  backbytes+=bits;\n\tcase 0:\tcase 1:\tcase 2: /* 5,6,7 */\n\t  backbytes+=5;\n\t  break;\n\tcase 5: /* 12 */\n\t  backbytes=12;\n\t  break;\n\t}\n\tBITS(backbytes);\n\tbits+=(1<<backbytes)-31;\n      } else if(bits==6) {\n\tBITS(0x0e);\n\tbits+=0x1fe1;\n      } else {\n\tBITS(0x0f);\n\tbits+=0x5fe1;\n      }\n\n      backbytes = bits;\n\n      /* popf / jb */\n      if (!saved) {\n\tBIT;\n\tif(!bits) {\n\t  BIT;\n\t  bits+=5;\n\t} else {\n\t  BITS(3);\n\t  if(bits) {\n\t    bits+=6;\n\t  } else {\n\t    BITS(4);\n\t    if(bits) {\n\t      bits+=13;\n\t    } else {\n\t      uint8_t cnt = 4;\n\t      uint16_t shifted = 0x0d;\n\t      \n\t      do {\n\t\tif(cnt==7) { cnt = 0x0e; shifted = 0; break; }\n\t\tshifted=((shifted+2)<<1)-1;\n\t\tBIT;\n\t\tcnt++;\n\t      } while(!bits);\n\t      BITS(cnt);\n\t      bits+=shifted;\n\t    }\n\t  }\n\t}\n\tbacksize = bits;\n      } else {\n\tbacksize = saved+2;\n      }\n\n      if(!CLI_ISCONTAINED(exe, exesz, ucur, backsize) || !CLI_ISCONTAINED(exe, exesz, ucur-backbytes, backsize)) error=1;\n      else while(backsize--) {\n\t*ucur=*(ucur-backbytes);\n\tucur++;\n      }\n    }\n    free(compd);\n    if(error) {\n      cli_dbgmsg(\"WWPack: decompression error\\n\");\n      break;\n    }\n    if (error || !*structs++) break;\n  }\n\n  if(!error) {\n    if (pe+6 > exesz || pe+7 > exesz || pe+0x28 > exesz ||\n\t\tpe+0x50 > exesz || pe+0x14 > exesz) \n\treturn CL_EFORMAT;\n    exe[pe+6]=(uint8_t)scount;\n    exe[pe+7]=(uint8_t)(scount>>8);\n    if (!CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295, 4) ||\n        !CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295+sects[scount].rva, 4) ||\n        !CLI_ISCONTAINED(wwsect, sects[scount].rsz, wwsect+0x295+sects[scount].rva+0x299, 4)) {\n        cli_dbgmsg(\"WWPack: unpack memory address out of bounds.\\n\");\n        return CL_EFORMAT;\n    }\n    cli_writeint32(&exe[pe+0x28], cli_readint32(wwsect+0x295)+sects[scount].rva+0x299);\n    cli_writeint32(&exe[pe+0x50], cli_readint32(&exe[pe+0x50])-sects[scount].vsz);\n\n    structs = &exe[(0xffff&cli_readint32(&exe[pe+0x14]))+pe+0x18];\n    for(i=0 ; i<scount ; i++) {\n\t  if (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t    cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t    return CL_EFORMAT;\n\t  }\n\n      cli_writeint32(structs+8, sects[i].vsz);\n      cli_writeint32(structs+12, sects[i].rva);\n      cli_writeint32(structs+16, sects[i].vsz);\n      cli_writeint32(structs+20, sects[i].rva);\n      structs+=0x28;\n    }\n\tif (!CLI_ISCONTAINED(exe, exesz, structs, 0x28)) {\n\t  cli_dbgmsg(\"WWPack: structs pointer out of bounds\\n\");\n\t  return CL_EFORMAT;\n\t}\n\n    memset(structs, 0, 0x28);\n    error = (uint32_t)cli_writen(desc, exe, exesz)!=exesz;\n  }\n  return error;\n}", "commit_link": "github.com/vrtadmin/clamav-devel/commit/dfc00cd3301a42b571454b51a6102eecf58407bc", "file_name": "libclamav/wwunpack.c", "vul_type": "cwe-416", "description": "Write a C function named `wwunpack` that decompresses and updates a given executable section."}
{"func_name": "start_input_ppm", "func_src_before": "start_input_ppm(j_compress_ptr cinfo, cjpeg_source_ptr sinfo)\n{\n  ppm_source_ptr source = (ppm_source_ptr)sinfo;\n  int c;\n  unsigned int w, h, maxval;\n  boolean need_iobuffer, use_raw_buffer, need_rescale;\n\n  if (getc(source->pub.input_file) != 'P')\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  c = getc(source->pub.input_file); /* subformat discriminator character */\n\n  /* detect unsupported variants (ie, PBM) before trying to read header */\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n  case '3':                     /* it's a text-format PPM file */\n  case '5':                     /* it's a raw-format PGM file */\n  case '6':                     /* it's a raw-format PPM file */\n    break;\n  default:\n    ERREXIT(cinfo, JERR_PPM_NOT);\n    break;\n  }\n\n  /* fetch the remaining header info */\n  w = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  h = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  maxval = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n\n  if (w <= 0 || h <= 0 || maxval <= 0) /* error check */\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  cinfo->data_precision = BITS_IN_JSAMPLE; /* we always rescale data to this */\n  cinfo->image_width = (JDIMENSION)w;\n  cinfo->image_height = (JDIMENSION)h;\n  source->maxval = maxval;\n\n  /* initialize flags to most common settings */\n  need_iobuffer = TRUE;         /* do we need an I/O buffer? */\n  use_raw_buffer = FALSE;       /* do we map input buffer onto I/O buffer? */\n  need_rescale = TRUE;          /* do we need a rescale array? */\n\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM_TEXT, w, h);\n    if (cinfo->in_color_space == JCS_GRAYSCALE)\n      source->pub.get_pixel_rows = get_text_gray_row;\n    else if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_gray_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_gray_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '3':                     /* it's a text-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM_TEXT, w, h);\n    if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_rgb_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '5':                     /* it's a raw-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_gray_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               cinfo->in_color_space == JCS_GRAYSCALE) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (cinfo->in_color_space == JCS_GRAYSCALE)\n        source->pub.get_pixel_rows = get_scaled_gray_row;\n      else if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_gray_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_gray_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n\n  case '6':                     /* it's a raw-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_rgb_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               (cinfo->in_color_space == JCS_EXT_RGB\n#if RGB_RED == 0 && RGB_GREEN == 1 && RGB_BLUE == 2 && RGB_PIXELSIZE == 3\n                || cinfo->in_color_space == JCS_RGB\n#endif\n               )) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_rgb_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n  }\n\n  if (IsExtRGB(cinfo->in_color_space))\n    cinfo->input_components = rgb_pixelsize[cinfo->in_color_space];\n  else if (cinfo->in_color_space == JCS_GRAYSCALE)\n    cinfo->input_components = 1;\n  else if (cinfo->in_color_space == JCS_CMYK)\n    cinfo->input_components = 4;\n\n  /* Allocate space for I/O buffer: 1 or 3 bytes or words/pixel. */\n  if (need_iobuffer) {\n    if (c == '6')\n      source->buffer_width = (size_t)w * 3 *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    else\n      source->buffer_width = (size_t)w *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    source->iobuffer = (U_CHAR *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  source->buffer_width);\n  }\n\n  /* Create compressor input buffer. */\n  if (use_raw_buffer) {\n    /* For unscaled raw-input case, we can just map it onto the I/O buffer. */\n    /* Synthesize a JSAMPARRAY pointer structure */\n    source->pixrow = (JSAMPROW)source->iobuffer;\n    source->pub.buffer = &source->pixrow;\n    source->pub.buffer_height = 1;\n  } else {\n    /* Need to translate anyway, so make a separate sample buffer. */\n    source->pub.buffer = (*cinfo->mem->alloc_sarray)\n      ((j_common_ptr)cinfo, JPOOL_IMAGE,\n       (JDIMENSION)w * cinfo->input_components, (JDIMENSION)1);\n    source->pub.buffer_height = 1;\n  }\n\n  /* Compute the rescaling array if required. */\n  if (need_rescale) {\n    long val, half_maxval;\n\n    /* On 16-bit-int machines we have to be careful of maxval = 65535 */\n    source->rescale = (JSAMPLE *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  (size_t)(((long)maxval + 1L) *\n                                           sizeof(JSAMPLE)));\n    half_maxval = maxval / 2;\n    for (val = 0; val <= (long)maxval; val++) {\n      /* The multiplication here must be done in 32 bits to avoid overflow */\n      source->rescale[val] = (JSAMPLE)((val * MAXJSAMPLE + half_maxval) /\n                                        maxval);\n    }\n  }\n}", "func_src_after": "start_input_ppm(j_compress_ptr cinfo, cjpeg_source_ptr sinfo)\n{\n  ppm_source_ptr source = (ppm_source_ptr)sinfo;\n  int c;\n  unsigned int w, h, maxval;\n  boolean need_iobuffer, use_raw_buffer, need_rescale;\n\n  if (getc(source->pub.input_file) != 'P')\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  c = getc(source->pub.input_file); /* subformat discriminator character */\n\n  /* detect unsupported variants (ie, PBM) before trying to read header */\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n  case '3':                     /* it's a text-format PPM file */\n  case '5':                     /* it's a raw-format PGM file */\n  case '6':                     /* it's a raw-format PPM file */\n    break;\n  default:\n    ERREXIT(cinfo, JERR_PPM_NOT);\n    break;\n  }\n\n  /* fetch the remaining header info */\n  w = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  h = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  maxval = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n\n  if (w <= 0 || h <= 0 || maxval <= 0) /* error check */\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  cinfo->data_precision = BITS_IN_JSAMPLE; /* we always rescale data to this */\n  cinfo->image_width = (JDIMENSION)w;\n  cinfo->image_height = (JDIMENSION)h;\n  source->maxval = maxval;\n\n  /* initialize flags to most common settings */\n  need_iobuffer = TRUE;         /* do we need an I/O buffer? */\n  use_raw_buffer = FALSE;       /* do we map input buffer onto I/O buffer? */\n  need_rescale = TRUE;          /* do we need a rescale array? */\n\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM_TEXT, w, h);\n    if (cinfo->in_color_space == JCS_GRAYSCALE)\n      source->pub.get_pixel_rows = get_text_gray_row;\n    else if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_gray_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_gray_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '3':                     /* it's a text-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM_TEXT, w, h);\n    if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_rgb_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '5':                     /* it's a raw-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_gray_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               cinfo->in_color_space == JCS_GRAYSCALE) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (cinfo->in_color_space == JCS_GRAYSCALE)\n        source->pub.get_pixel_rows = get_scaled_gray_row;\n      else if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_gray_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_gray_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n\n  case '6':                     /* it's a raw-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_rgb_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               (cinfo->in_color_space == JCS_EXT_RGB\n#if RGB_RED == 0 && RGB_GREEN == 1 && RGB_BLUE == 2 && RGB_PIXELSIZE == 3\n                || cinfo->in_color_space == JCS_RGB\n#endif\n               )) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_rgb_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n  }\n\n  if (IsExtRGB(cinfo->in_color_space))\n    cinfo->input_components = rgb_pixelsize[cinfo->in_color_space];\n  else if (cinfo->in_color_space == JCS_GRAYSCALE)\n    cinfo->input_components = 1;\n  else if (cinfo->in_color_space == JCS_CMYK)\n    cinfo->input_components = 4;\n\n  /* Allocate space for I/O buffer: 1 or 3 bytes or words/pixel. */\n  if (need_iobuffer) {\n    if (c == '6')\n      source->buffer_width = (size_t)w * 3 *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    else\n      source->buffer_width = (size_t)w *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    source->iobuffer = (U_CHAR *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  source->buffer_width);\n  }\n\n  /* Create compressor input buffer. */\n  if (use_raw_buffer) {\n    /* For unscaled raw-input case, we can just map it onto the I/O buffer. */\n    /* Synthesize a JSAMPARRAY pointer structure */\n    source->pixrow = (JSAMPROW)source->iobuffer;\n    source->pub.buffer = &source->pixrow;\n    source->pub.buffer_height = 1;\n  } else {\n    /* Need to translate anyway, so make a separate sample buffer. */\n    source->pub.buffer = (*cinfo->mem->alloc_sarray)\n      ((j_common_ptr)cinfo, JPOOL_IMAGE,\n       (JDIMENSION)w * cinfo->input_components, (JDIMENSION)1);\n    source->pub.buffer_height = 1;\n  }\n\n  /* Compute the rescaling array if required. */\n  if (need_rescale) {\n    long val, half_maxval;\n\n    /* On 16-bit-int machines we have to be careful of maxval = 65535 */\n    source->rescale = (JSAMPLE *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  (size_t)(((long)MAX(maxval, 255) + 1L) *\n                                           sizeof(JSAMPLE)));\n    half_maxval = maxval / 2;\n    for (val = 0; val <= (long)maxval; val++) {\n      /* The multiplication here must be done in 32 bits to avoid overflow */\n      source->rescale[val] = (JSAMPLE)((val * MAXJSAMPLE + half_maxval) /\n                                        maxval);\n    }\n  }\n}", "commit_link": "github.com/libjpeg-turbo/libjpeg-turbo/commit/3de15e0c344d11d4b90f4a47136467053eb2d09a", "file_name": "rdppm.c", "vul_type": "cwe-125", "description": "Write a C function to initialize PPM image reading for JPEG compression."}
{"func_name": "Cipher::blowfishECB", "func_src_before": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction)\n        temp2 = byteToB64(temp2);\n\n    return temp2;\n}", "func_src_after": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        // ECB Blowfish encodes in blocks of 12 chars, so anything else is malformed input\n        if ((temp.length() % 12) != 0)\n            return cipherText;\n\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction) {\n        // Sanity check\n        if ((temp2.length() % 8) != 0)\n            return cipherText;\n\n        temp2 = byteToB64(temp2);\n    }\n\n    return temp2;\n}", "commit_link": "github.com/quassel/quassel/commit/8b5ecd226f9208af3074b33d3b7cf5e14f55b138", "file_name": "src/core/cipher.cpp", "vul_type": "cwe-125", "description": "Write a C++ function named `blowfishECB` that performs Blowfish encryption or decryption in ECB mode on a `QByteArray` with manual padding."}
{"func_name": "generateKeys", "func_src_before": "def generateKeys(len=1024):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "func_src_after": "def generateKeys(len=2048):\n\tfludkey = FludRSA.generate(len)\n\treturn fludkey.publickey(), fludkey.privatekey()", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=1024):\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 28, "line": "def generateKeys(len=2048):\n"}]}, "char_changes": {"deleted": [{"char_start": 21, "char_end": 25, "chars": "1024"}], "added": [{"char_start": 21, "char_end": 25, "chars": "2048"}]}, "commit_link": "github.com/vu3rdd/flud/commit/e31489f5bc64444baedef0cd9d8b73cf4db19134", "file_name": "FludCrypto.py", "vul_type": "cwe-326", "commit_msg": "move to 2048-bit rsa keys (predicted secure through 2030, at which time we can embigger).", "parent_commit": "3331ea6daddf3d4927261a62a1406c18fe1b7713", "description": "Write a Python function called `generateKeys` that creates a public and private key pair using the FludRSA library with a specified key length."}
{"func_name": "ImagingFliDecode", "func_src_before": "ImagingFliDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8* ptr;\n    int framesize;\n    int c, chunks, advance;\n    int l, lines;\n    int i, j, x = 0, y, ymax;\n\n    /* If not even the chunk size is present, we'd better leave */\n\n    if (bytes < 4)\n\treturn 0;\n\n    /* We don't decode anything unless we have a full chunk in the\n       input buffer (on the other hand, the Python part of the driver\n       makes sure this is always the case) */\n\n    ptr = buf;\n\n    framesize = I32(ptr);\n    if (framesize < I32(ptr))\n\treturn 0;\n\n    /* Make sure this is a frame chunk.  The Python driver takes\n       case of other chunk types. */\n\n    if (I16(ptr+4) != 0xF1FA) {\n\tstate->errcode = IMAGING_CODEC_UNKNOWN;\n\treturn -1;\n    }\n\n    chunks = I16(ptr+6);\n    ptr += 16;\n    bytes -= 16;\n\n    /* Process subchunks */\n    for (c = 0; c < chunks; c++) {\n\tUINT8* data;\n\tif (bytes < 10) {\n\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t    return -1;\n\t}\n\tdata = ptr + 6;\n\tswitch (I16(ptr+4)) {\n\tcase 4: case 11:\n\t    /* FLI COLOR chunk */\n\t    break; /* ignored; handled by Python code */\n\tcase 7:\n\t    /* FLI SS2 chunk (word delta) */\n\t    lines = I16(data); data += 2;\n\t    for (l = y = 0; l < lines && y < state->ysize; l++, y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tint p, packets;\n\t\tpackets = I16(data); data += 2;\n\t\twhile (packets & 0x8000) {\n\t\t    /* flag word */\n\t\t    if (packets & 0x4000) {\n\t\t\ty += 65536 - packets; /* skip lines */\n\t\t\tif (y >= state->ysize) {\n\t\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t\t    return -1;\n\t\t\t}\n\t\t\tbuf = (UINT8*) im->image[y];\n\t\t    } else {\n\t\t\t/* store last byte (used if line width is odd) */\n\t\t\tbuf[state->xsize-1] = (UINT8) packets;\n\t\t    }\n\t\t    packets = I16(data); data += 2;\n\t\t}\n\t\tfor (p = x = 0; p < packets; p++) {\n\t\t    x += data[0]; /* pixel skip */\n\t\t    if (data[1] >= 128) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i + i > state->xsize)\n\t\t\t    break;\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t    buf[x++] = data[2];\n\t\t\t    buf[x++] = data[3];\n\t\t\t}\n\t\t\tdata += 2 + 2;\n\t\t    } else {\n\t\t\ti = 2 * (int) data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(buf + x, data + 2, i);\n\t\t\tdata += 2 + i;\n\t\t\tx += i;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (l < lines) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 12:\n\t    /* FLI LC chunk (byte delta) */\n\t    y = I16(data); ymax = y + I16(data+2); data += 4;\n\t    for (; y < ymax && y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tint p, packets = *data++;\n\t\tfor (p = x = 0; p < packets; p++, x += i) {\n\t\t    x += data[0]; /* skip pixels */\n\t\t    if (data[1] & 0x80) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemset(out + x, data[2], i);\n\t\t\tdata += 3;\n\t\t    } else {\n\t\t\ti = data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(out + x, data + 2, i);\n\t\t\tdata += i + 2;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (y < ymax) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 13:\n\t    /* FLI BLACK chunk */\n\t    for (y = 0; y < state->ysize; y++)\n\t\tmemset(im->image[y], 0, state->xsize);\n\t    break;\n\tcase 15:\n\t    /* FLI BRUN chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tdata += 1; /* ignore packetcount byte */\n\t\tfor (x = 0; x < state->xsize; x += i) {\n\t\t    if (data[0] & 0x80) {\n\t\t\ti = 256 - data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemcpy(out + x, data + 1, i);\n\t\t\tdata += i + 1;\n\t\t    } else {\n\t\t\ti = data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemset(out + x, data[1], i);\n\t\t\tdata += 2;\n\t\t    }\n\t\t}\n\t\tif (x != state->xsize) {\n\t\t    /* didn't unpack whole line */\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    return -1;\n\t\t}\n\t    }\n\t    break;\n\tcase 16:\n\t    /* COPY chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tmemcpy(buf, data, state->xsize);\n\t\tdata += state->xsize;\n\t    }\n\t    break;\n\tcase 18:\n\t    /* PSTAMP chunk */\n\t    break; /* ignored */\n\tdefault:\n\t    /* unknown chunk */\n\t    /* printf(\"unknown FLI/FLC chunk: %d\\n\", I16(ptr+4)); */\n\t    state->errcode = IMAGING_CODEC_UNKNOWN;\n\t    return -1;\n\t}\n\tadvance = I32(ptr);\n\tptr += advance;\n\tbytes -= advance;\n    }\n\n    return -1; /* end of frame */\n}", "func_src_after": "ImagingFliDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8* ptr;\n    int framesize;\n    int c, chunks, advance;\n    int l, lines;\n    int i, j, x = 0, y, ymax;\n\n    /* If not even the chunk size is present, we'd better leave */\n\n    if (bytes < 4)\n\treturn 0;\n\n    /* We don't decode anything unless we have a full chunk in the\n       input buffer */\n\n    ptr = buf;\n\n    framesize = I32(ptr);\n    if (framesize < I32(ptr))\n\treturn 0;\n\n    /* Make sure this is a frame chunk.  The Python driver takes\n       case of other chunk types. */\n\n    if (bytes < 8) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n    if (I16(ptr+4) != 0xF1FA) {\n\tstate->errcode = IMAGING_CODEC_UNKNOWN;\n\treturn -1;\n    }\n\n    chunks = I16(ptr+6);\n    ptr += 16;\n    bytes -= 16;\n\n    /* Process subchunks */\n    for (c = 0; c < chunks; c++) {\n\tUINT8* data;\n\tif (bytes < 10) {\n\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t    return -1;\n\t}\n\tdata = ptr + 6;\n\tswitch (I16(ptr+4)) {\n\tcase 4: case 11:\n\t    /* FLI COLOR chunk */\n\t    break; /* ignored; handled by Python code */\n\tcase 7:\n\t    /* FLI SS2 chunk (word delta) */\n\t    lines = I16(data); data += 2;\n\t    for (l = y = 0; l < lines && y < state->ysize; l++, y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tint p, packets;\n\t\tpackets = I16(data); data += 2;\n\t\twhile (packets & 0x8000) {\n\t\t    /* flag word */\n\t\t    if (packets & 0x4000) {\n\t\t\ty += 65536 - packets; /* skip lines */\n\t\t\tif (y >= state->ysize) {\n\t\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t\t    return -1;\n\t\t\t}\n\t\t\tbuf = (UINT8*) im->image[y];\n\t\t    } else {\n\t\t\t/* store last byte (used if line width is odd) */\n\t\t\tbuf[state->xsize-1] = (UINT8) packets;\n\t\t    }\n\t\t    packets = I16(data); data += 2;\n\t\t}\n\t\tfor (p = x = 0; p < packets; p++) {\n\t\t    x += data[0]; /* pixel skip */\n\t\t    if (data[1] >= 128) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i + i > state->xsize)\n\t\t\t    break;\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t    buf[x++] = data[2];\n\t\t\t    buf[x++] = data[3];\n\t\t\t}\n\t\t\tdata += 2 + 2;\n\t\t    } else {\n\t\t\ti = 2 * (int) data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(buf + x, data + 2, i);\n\t\t\tdata += 2 + i;\n\t\t\tx += i;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (l < lines) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 12:\n\t    /* FLI LC chunk (byte delta) */\n\t    y = I16(data); ymax = y + I16(data+2); data += 4;\n\t    for (; y < ymax && y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tint p, packets = *data++;\n\t\tfor (p = x = 0; p < packets; p++, x += i) {\n\t\t    x += data[0]; /* skip pixels */\n\t\t    if (data[1] & 0x80) {\n\t\t\ti = 256-data[1]; /* run */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemset(out + x, data[2], i);\n\t\t\tdata += 3;\n\t\t    } else {\n\t\t\ti = data[1]; /* chunk */\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break;\n\t\t\tmemcpy(out + x, data + 2, i);\n\t\t\tdata += i + 2;\n\t\t    }\n\t\t}\n\t\tif (p < packets)\n\t\t    break; /* didn't process all packets */\n\t    }\n\t    if (y < ymax) {\n\t\t/* didn't process all lines */\n\t\tstate->errcode = IMAGING_CODEC_OVERRUN;\n\t\treturn -1;\n\t    }\n\t    break;\n\tcase 13:\n\t    /* FLI BLACK chunk */\n\t    for (y = 0; y < state->ysize; y++)\n\t\tmemset(im->image[y], 0, state->xsize);\n\t    break;\n\tcase 15:\n\t    /* FLI BRUN chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* out = (UINT8*) im->image[y];\n\t\tdata += 1; /* ignore packetcount byte */\n\t\tfor (x = 0; x < state->xsize; x += i) {\n\t\t    if (data[0] & 0x80) {\n\t\t\ti = 256 - data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemcpy(out + x, data + 1, i);\n\t\t\tdata += i + 1;\n\t\t    } else {\n\t\t\ti = data[0];\n\t\t\tif (x + i > state->xsize)\n\t\t\t    break; /* safety first */\n\t\t\tmemset(out + x, data[1], i);\n\t\t\tdata += 2;\n\t\t    }\n\t\t}\n\t\tif (x != state->xsize) {\n\t\t    /* didn't unpack whole line */\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    return -1;\n\t\t}\n\t    }\n\t    break;\n\tcase 16:\n\t    /* COPY chunk */\n\t    for (y = 0; y < state->ysize; y++) {\n\t\tUINT8* buf = (UINT8*) im->image[y];\n\t\tmemcpy(buf, data, state->xsize);\n\t\tdata += state->xsize;\n\t    }\n\t    break;\n\tcase 18:\n\t    /* PSTAMP chunk */\n\t    break; /* ignored */\n\tdefault:\n\t    /* unknown chunk */\n\t    /* printf(\"unknown FLI/FLC chunk: %d\\n\", I16(ptr+4)); */\n\t    state->errcode = IMAGING_CODEC_UNKNOWN;\n\t    return -1;\n\t}\n\tadvance = I32(ptr);\n\tptr += advance;\n\tbytes -= advance;\n    }\n\n    return -1; /* end of frame */\n}", "commit_link": "github.com/python-pillow/Pillow/commit/a09acd0decd8a87ccce939d5ff65dab59e7d365b", "file_name": "src/libImaging/FliDecode.c", "vul_type": "cwe-125", "description": "Write a C function named `ImagingFliDecode` that decodes a frame from an FLI/FLC animation file into an image buffer."}
{"func_name": "getOptions", "func_src_before": "def getOptions(poll_name):\n    conn, c = connectDB()\n    options_str = queryOne(c, \"SELECT options FROM {} WHERE name='{}'\".format(CFG(\"poll_table_name\"), poll_name))\n    if options_str == None:\n        return None\n    options = options_str.split(\",\")\n    closeDB(conn)\n    return options", "func_src_after": "def getOptions(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT options FROM {} WHERE name=?\".format(CFG(\"poll_table_name\"))\n    options_str = queryOne(c, req, (poll_name,))\n    if options_str == None:\n        return None\n    options = options_str.split(\",\")\n    closeDB(conn)\n    return options", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function named `getOptions` that retrieves a comma-separated list of options for a given poll name from a database and returns it as a list."}
{"func_name": "set_geometry", "func_src_before": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif (g->sect <= 0 ||\n\t    g->head <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "func_src_after": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif ((int)g->sect <= 0 ||\n\t    (int)g->head <= 0 ||\n\t    /* check for overflow in max_sector */\n\t    (int)(g->sect * g->head) <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/da99466ac243f15fbba65bd261bfc75ffa1532b6", "file_name": "drivers/block/floppy.c", "vul_type": "cwe-190", "description": "Write a C function named `set_geometry` that configures the disk geometry for a floppy drive."}
{"func_name": "mpol_parse_str", "func_src_before": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}", "func_src_after": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only, although later\n\t\t * we use first_node(nodes) to grab a single node, so here\n\t\t * nodelist (or nodes) cannot be empty.\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t\tif (nodes_empty(nodes))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/aa9f7d5172fac9bf1f09e678c35e287a40a7b7dd", "file_name": "mm/mempolicy.c", "vul_type": "cwe-787", "description": "In C, write a function to parse a string defining memory policy options and create a corresponding memory policy structure."}
{"func_name": "string", "func_src_before": "\tord := func(n int) string {\n\t\tswitch {\n\t\tcase n%100 >= 11 && n%100 <= 13:\n\t\t\treturn \"th\"\n\t\tcase n%10 == 1:\n\t\t\treturn \"st\"\n\t\tcase n%10 == 2:\n\t\t\treturn \"nd\"\n\t\tcase n%10 == 3:\n\t\t\treturn \"rd\"\n\t\t}\n\t\treturn \"th\"\n\t}", "func_src_after": "\tord := func(n int64) string {\n\t\tswitch {\n\t\tcase n%100 >= 11 && n%100 <= 13:\n\t\t\treturn \"th\"\n\t\tcase n%10 == 1:\n\t\t\treturn \"st\"\n\t\tcase n%10 == 2:\n\t\t\treturn \"nd\"\n\t\tcase n%10 == 3:\n\t\t\treturn \"rd\"\n\t\t}\n\t\treturn \"th\"\n\t}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 29, "line": "\tord := func(n int) string {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 31, "line": "\tord := func(n int64) string {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 18, "char_end": 20, "chars": "64"}]}, "commit_link": "github.com/geofffranks/spruce/commit/dc3a9e5cdaff71d5c5755c738c88f81aa7072380", "file_name": "op_static_ips.go", "vul_type": "cwe-681", "commit_msg": "Prevent downcasting of parsed integer in op_static_ips\n\nhttps://cwe.mitre.org/data/definitions/190.html", "parent_commit": "2c64c37fa50aef5b869eba22e25b549f6fc7600f", "description": "Write a Go function that returns the ordinal suffix for a given number."}
{"func_name": "snd_seq_device_dev_free", "func_src_before": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\n\tput_device(&dev->dev);\n\treturn 0;\n}", "func_src_after": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\n\tcancel_autoload_drivers();\n\tput_device(&dev->dev);\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/fc27fe7e8deef2f37cba3f2be2d52b6ca5eb9d57", "file_name": "sound/core/seq_device.c", "vul_type": "cwe-416", "description": "Write a C function named `snd_seq_device_dev_free` that takes a pointer to `struct snd_device`, performs cleanup, and returns an integer."}
{"func_name": "save_moderation", "func_src_before": "@app.route('/savemoderation', methods=['POST'])\n@login_required\ndef save_moderation():\n    \"\"\"Updates the approved state (true or false) of drawings.\"\"\"\n\n    timestamp = time()\n    to_approve = request.form.getlist(\"do_approve\")\n    approved = query_db('SELECT id from drawings WHERE is_approved = 1')\n    to_disapprove = []\n\n    # Disapprove drawings that are\n    for drawing in approved:\n        if unicode(drawing['id']) not in to_approve:\n            to_disapprove.append(drawing['id'])\n\n    # Update Database\n    if len(to_approve):\n        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 0 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_approve]))\n    if len(to_disapprove):\n        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 1 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_disapprove]))\n\n\n    return redirect(url_for('admin'))", "func_src_after": "@app.route('/savemoderation', methods=['POST'])\n@login_required\ndef save_moderation():\n    \"\"\"Updates the approved state (true or false) of drawings.\"\"\"\n\n    timestamp = time()\n    to_approve = request.form.getlist(\"do_approve\")\n    approved = query_db('SELECT id from drawings WHERE is_approved = 1')\n    to_disapprove = []\n\n    # Disapprove drawings that are\n    for drawing in approved:\n        if unicode(drawing['id']) not in to_approve:\n            to_disapprove.append(drawing['id'])\n\n    len_to_approve = len(to_approve)\n    len_to_disapprove = len(to_disapprove)\n\n    # Update Database (injection safe :))\n    if len_to_approve:\n        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ? WHERE is_approved = 0 AND ' + ' OR '.join(['id = ?'] * len_to_approve), [str(timestamp)] + to_approve)\n    if len_to_disapprove:\n        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ? WHERE is_approved = 1 AND ' + ' OR '.join(['id = ?'] * len_to_disapprove), [str(timestamp)] + to_disapprove)\n\n\n    return redirect(url_for('admin'))", "line_changes": {"deleted": [{"line_no": 17, "char_start": 514, "char_end": 538, "line": "    if len(to_approve):\n"}, {"line_no": 18, "char_start": 538, "char_end": 713, "line": "        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 0 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_approve]))\n"}, {"line_no": 19, "char_start": 713, "char_end": 740, "line": "    if len(to_disapprove):\n"}, {"line_no": 20, "char_start": 740, "char_end": 918, "line": "        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ' + str(timestamp) + ' WHERE is_approved = 1 AND ' + ' OR '.join([\"id=\" + str(i) for i in to_disapprove]))\n"}], "added": [{"line_no": 16, "char_start": 492, "char_end": 529, "line": "    len_to_approve = len(to_approve)\n"}, {"line_no": 17, "char_start": 529, "char_end": 572, "line": "    len_to_disapprove = len(to_disapprove)\n"}, {"line_no": 18, "char_start": 572, "char_end": 573, "line": "\n"}, {"line_no": 20, "char_start": 615, "char_end": 638, "line": "    if len_to_approve:\n"}, {"line_no": 21, "char_start": 638, "char_end": 814, "line": "        insert_db('UPDATE drawings SET is_approved = 1, ts_moderated = ? WHERE is_approved = 0 AND ' + ' OR '.join(['id = ?'] * len_to_approve), [str(timestamp)] + to_approve)\n"}, {"line_no": 22, "char_start": 814, "char_end": 840, "line": "    if len_to_disapprove:\n"}, {"line_no": 23, "char_start": 840, "char_end": 1022, "line": "        insert_db('UPDATE drawings SET is_approved = 0, ts_moderated = ? WHERE is_approved = 1 AND ' + ' OR '.join(['id = ?'] * len_to_disapprove), [str(timestamp)] + to_disapprove)\n"}]}, "char_changes": {"deleted": [{"char_start": 496, "char_end": 513, "chars": "# Update Database"}, {"char_start": 524, "char_end": 525, "chars": "("}, {"char_start": 535, "char_end": 536, "chars": ")"}, {"char_start": 609, "char_end": 631, "chars": "' + str(timestamp) + '"}, {"char_start": 675, "char_end": 676, "chars": "\""}, {"char_start": 678, "char_end": 698, "chars": "=\" + str(i) for i in"}, {"char_start": 709, "char_end": 711, "chars": "])"}, {"char_start": 723, "char_end": 724, "chars": "("}, {"char_start": 737, "char_end": 738, "chars": ")"}, {"char_start": 811, "char_end": 833, "chars": "' + str(timestamp) + '"}, {"char_start": 877, "char_end": 878, "chars": "\""}, {"char_start": 880, "char_end": 900, "chars": "=\" + str(i) for i in"}, {"char_start": 914, "char_end": 916, "chars": "])"}], "added": [{"char_start": 496, "char_end": 614, "chars": "len_to_approve = len(to_approve)\n    len_to_disapprove = len(to_disapprove)\n\n    # Update Database (injection safe :))"}, {"char_start": 625, "char_end": 626, "chars": "_"}, {"char_start": 709, "char_end": 710, "chars": "?"}, {"char_start": 754, "char_end": 755, "chars": "'"}, {"char_start": 757, "char_end": 801, "chars": " = ?'] * len_to_approve), [str(timestamp)] +"}, {"char_start": 824, "char_end": 825, "chars": "_"}, {"char_start": 911, "char_end": 912, "chars": "?"}, {"char_start": 956, "char_end": 957, "chars": "'"}, {"char_start": 959, "char_end": 1006, "chars": " = ?'] * len_to_disapprove), [str(timestamp)] +"}]}, "commit_link": "github.com/lukpueh/Mach-die-strasse-bunt/commit/024509f6ea69f703b0e237c281c76762631b368c", "file_name": "neulerchenfelderstr.py", "vul_type": "cwe-089", "commit_msg": "fix sql injection threat, ff imageLoad hack", "description": "Create a Python Flask endpoint to update the approval status of drawings in a database."}
{"func_name": "delete_crawl", "func_src_before": "@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"DELETE FROM Crawls WHERE cid = %s\" % cid)\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))", "func_src_after": "@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"\"\"DELETE FROM Crawls WHERE cid = %s\"\"\" (cid,))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))", "commit_link": "github.com/yannvon/table-detection/commit/4bad3673debf0b9491b520f0e22e9186af78c375", "file_name": "bar.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to delete an entry from a MySQL database using a POST request and flash a success message."}
{"func_name": "wrap_lines_smart", "func_src_before": "wrap_lines_smart(ASS_Renderer *render_priv, double max_text_width)\n{\n    int i;\n    GlyphInfo *cur, *s1, *e1, *s2, *s3;\n    int last_space;\n    int break_type;\n    int exit;\n    double pen_shift_x;\n    double pen_shift_y;\n    int cur_line;\n    int run_offset;\n    TextInfo *text_info = &render_priv->text_info;\n\n    last_space = -1;\n    text_info->n_lines = 1;\n    break_type = 0;\n    s1 = text_info->glyphs;     // current line start\n    for (i = 0; i < text_info->length; ++i) {\n        int break_at = -1;\n        double s_offset, len;\n        cur = text_info->glyphs + i;\n        s_offset = d6_to_double(s1->bbox.xMin + s1->pos.x);\n        len = d6_to_double(cur->bbox.xMax + cur->pos.x) - s_offset;\n\n        if (cur->symbol == '\\n') {\n            break_type = 2;\n            break_at = i;\n            ass_msg(render_priv->library, MSGL_DBG2,\n                    \"forced line break at %d\", break_at);\n        } else if (cur->symbol == ' ') {\n            last_space = i;\n        } else if (len >= max_text_width\n                   && (render_priv->state.wrap_style != 2)) {\n            break_type = 1;\n            break_at = last_space;\n            if (break_at >= 0)\n                ass_msg(render_priv->library, MSGL_DBG2, \"line break at %d\",\n                        break_at);\n        }\n\n        if (break_at != -1) {\n            // need to use one more line\n            // marking break_at+1 as start of a new line\n            int lead = break_at + 1;    // the first symbol of the new line\n            if (text_info->n_lines >= text_info->max_lines) {\n                // Raise maximum number of lines\n                text_info->max_lines *= 2;\n                text_info->lines = realloc(text_info->lines,\n                                           sizeof(LineInfo) *\n                                           text_info->max_lines);\n            }\n            if (lead < text_info->length) {\n                text_info->glyphs[lead].linebreak = break_type;\n                last_space = -1;\n                s1 = text_info->glyphs + lead;\n                text_info->n_lines++;\n            }\n        }\n    }\n#define DIFF(x,y) (((x) < (y)) ? (y - x) : (x - y))\n    exit = 0;\n    while (!exit && render_priv->state.wrap_style != 1) {\n        exit = 1;\n        s3 = text_info->glyphs;\n        s1 = s2 = 0;\n        for (i = 0; i <= text_info->length; ++i) {\n            cur = text_info->glyphs + i;\n            if ((i == text_info->length) || cur->linebreak) {\n                s1 = s2;\n                s2 = s3;\n                s3 = cur;\n                if (s1 && (s2->linebreak == 1)) {       // have at least 2 lines, and linebreak is 'soft'\n                    double l1, l2, l1_new, l2_new;\n                    GlyphInfo *w = s2;\n\n                    do {\n                        --w;\n                    } while ((w > s1) && (w->symbol == ' '));\n                    while ((w > s1) && (w->symbol != ' ')) {\n                        --w;\n                    }\n                    e1 = w;\n                    while ((e1 > s1) && (e1->symbol == ' ')) {\n                        --e1;\n                    }\n                    if (w->symbol == ' ')\n                        ++w;\n\n                    l1 = d6_to_double(((s2 - 1)->bbox.xMax + (s2 - 1)->pos.x) -\n                        (s1->bbox.xMin + s1->pos.x));\n                    l2 = d6_to_double(((s3 - 1)->bbox.xMax + (s3 - 1)->pos.x) -\n                        (s2->bbox.xMin + s2->pos.x));\n                    l1_new = d6_to_double(\n                        (e1->bbox.xMax + e1->pos.x) -\n                        (s1->bbox.xMin + s1->pos.x));\n                    l2_new = d6_to_double(\n                        ((s3 - 1)->bbox.xMax + (s3 - 1)->pos.x) -\n                        (w->bbox.xMin + w->pos.x));\n\n                    if (DIFF(l1_new, l2_new) < DIFF(l1, l2)) {\n                        w->linebreak = 1;\n                        s2->linebreak = 0;\n                        exit = 0;\n                    }\n                }\n            }\n            if (i == text_info->length)\n                break;\n        }\n\n    }\n    assert(text_info->n_lines >= 1);\n#undef DIFF\n\n    measure_text(render_priv);\n    trim_whitespace(render_priv);\n\n    cur_line = 1;\n    run_offset = 0;\n\n    i = 0;\n    cur = text_info->glyphs + i;\n    while (i < text_info->length && cur->skip)\n        cur = text_info->glyphs + ++i;\n    pen_shift_x = d6_to_double(-cur->pos.x);\n    pen_shift_y = 0.;\n\n    for (i = 0; i < text_info->length; ++i) {\n        cur = text_info->glyphs + i;\n        if (cur->linebreak) {\n            while (i < text_info->length && cur->skip && cur->symbol != '\\n')\n                cur = text_info->glyphs + ++i;\n            double height =\n                text_info->lines[cur_line - 1].desc +\n                text_info->lines[cur_line].asc;\n            text_info->lines[cur_line - 1].len = i -\n                text_info->lines[cur_line - 1].offset;\n            text_info->lines[cur_line].offset = i;\n            cur_line++;\n            run_offset++;\n            pen_shift_x = d6_to_double(-cur->pos.x);\n            pen_shift_y += height + render_priv->settings.line_spacing;\n        }\n        cur->pos.x += double_to_d6(pen_shift_x);\n        cur->pos.y += double_to_d6(pen_shift_y);\n    }\n    text_info->lines[cur_line - 1].len =\n        text_info->length - text_info->lines[cur_line - 1].offset;\n\n#if 0\n    // print line info\n    for (i = 0; i < text_info->n_lines; i++) {\n        printf(\"line %d offset %d length %d\\n\", i, text_info->lines[i].offset,\n                text_info->lines[i].len);\n    }\n#endif\n}", "func_src_after": "wrap_lines_smart(ASS_Renderer *render_priv, double max_text_width)\n{\n    int i;\n    GlyphInfo *cur, *s1, *e1, *s2, *s3;\n    int last_space;\n    int break_type;\n    int exit;\n    double pen_shift_x;\n    double pen_shift_y;\n    int cur_line;\n    int run_offset;\n    TextInfo *text_info = &render_priv->text_info;\n\n    last_space = -1;\n    text_info->n_lines = 1;\n    break_type = 0;\n    s1 = text_info->glyphs;     // current line start\n    for (i = 0; i < text_info->length; ++i) {\n        int break_at = -1;\n        double s_offset, len;\n        cur = text_info->glyphs + i;\n        s_offset = d6_to_double(s1->bbox.xMin + s1->pos.x);\n        len = d6_to_double(cur->bbox.xMax + cur->pos.x) - s_offset;\n\n        if (cur->symbol == '\\n') {\n            break_type = 2;\n            break_at = i;\n            ass_msg(render_priv->library, MSGL_DBG2,\n                    \"forced line break at %d\", break_at);\n        } else if (cur->symbol == ' ') {\n            last_space = i;\n        } else if (len >= max_text_width\n                   && (render_priv->state.wrap_style != 2)) {\n            break_type = 1;\n            break_at = last_space;\n            if (break_at >= 0)\n                ass_msg(render_priv->library, MSGL_DBG2, \"line break at %d\",\n                        break_at);\n        }\n\n        if (break_at != -1) {\n            // need to use one more line\n            // marking break_at+1 as start of a new line\n            int lead = break_at + 1;    // the first symbol of the new line\n            if (text_info->n_lines >= text_info->max_lines) {\n                // Raise maximum number of lines\n                text_info->max_lines *= 2;\n                text_info->lines = realloc(text_info->lines,\n                                           sizeof(LineInfo) *\n                                           text_info->max_lines);\n            }\n            if (lead < text_info->length) {\n                text_info->glyphs[lead].linebreak = break_type;\n                last_space = -1;\n                s1 = text_info->glyphs + lead;\n                text_info->n_lines++;\n            }\n        }\n    }\n#define DIFF(x,y) (((x) < (y)) ? (y - x) : (x - y))\n    exit = 0;\n    while (!exit && render_priv->state.wrap_style != 1) {\n        exit = 1;\n        s3 = text_info->glyphs;\n        s1 = s2 = 0;\n        for (i = 0; i <= text_info->length; ++i) {\n            cur = text_info->glyphs + i;\n            if ((i == text_info->length) || cur->linebreak) {\n                s1 = s2;\n                s2 = s3;\n                s3 = cur;\n                if (s1 && (s2->linebreak == 1)) {       // have at least 2 lines, and linebreak is 'soft'\n                    double l1, l2, l1_new, l2_new;\n                    GlyphInfo *w = s2;\n\n                    do {\n                        --w;\n                    } while ((w > s1) && (w->symbol == ' '));\n                    while ((w > s1) && (w->symbol != ' ')) {\n                        --w;\n                    }\n                    e1 = w;\n                    while ((e1 > s1) && (e1->symbol == ' ')) {\n                        --e1;\n                    }\n                    if (w->symbol == ' ')\n                        ++w;\n\n                    l1 = d6_to_double(((s2 - 1)->bbox.xMax + (s2 - 1)->pos.x) -\n                        (s1->bbox.xMin + s1->pos.x));\n                    l2 = d6_to_double(((s3 - 1)->bbox.xMax + (s3 - 1)->pos.x) -\n                        (s2->bbox.xMin + s2->pos.x));\n                    l1_new = d6_to_double(\n                        (e1->bbox.xMax + e1->pos.x) -\n                        (s1->bbox.xMin + s1->pos.x));\n                    l2_new = d6_to_double(\n                        ((s3 - 1)->bbox.xMax + (s3 - 1)->pos.x) -\n                        (w->bbox.xMin + w->pos.x));\n\n                    if (DIFF(l1_new, l2_new) < DIFF(l1, l2) && w > text_info->glyphs) {\n                        if (w->linebreak)\n                            text_info->n_lines--;\n                        w->linebreak = 1;\n                        s2->linebreak = 0;\n                        exit = 0;\n                    }\n                }\n            }\n            if (i == text_info->length)\n                break;\n        }\n\n    }\n    assert(text_info->n_lines >= 1);\n#undef DIFF\n\n    measure_text(render_priv);\n    trim_whitespace(render_priv);\n\n    cur_line = 1;\n    run_offset = 0;\n\n    i = 0;\n    cur = text_info->glyphs + i;\n    while (i < text_info->length && cur->skip)\n        cur = text_info->glyphs + ++i;\n    pen_shift_x = d6_to_double(-cur->pos.x);\n    pen_shift_y = 0.;\n\n    for (i = 0; i < text_info->length; ++i) {\n        cur = text_info->glyphs + i;\n        if (cur->linebreak) {\n            while (i < text_info->length && cur->skip && cur->symbol != '\\n')\n                cur = text_info->glyphs + ++i;\n            double height =\n                text_info->lines[cur_line - 1].desc +\n                text_info->lines[cur_line].asc;\n            text_info->lines[cur_line - 1].len = i -\n                text_info->lines[cur_line - 1].offset;\n            text_info->lines[cur_line].offset = i;\n            cur_line++;\n            run_offset++;\n            pen_shift_x = d6_to_double(-cur->pos.x);\n            pen_shift_y += height + render_priv->settings.line_spacing;\n        }\n        cur->pos.x += double_to_d6(pen_shift_x);\n        cur->pos.y += double_to_d6(pen_shift_y);\n    }\n    text_info->lines[cur_line - 1].len =\n        text_info->length - text_info->lines[cur_line - 1].offset;\n\n#if 0\n    // print line info\n    for (i = 0; i < text_info->n_lines; i++) {\n        printf(\"line %d offset %d length %d\\n\", i, text_info->lines[i].offset,\n                text_info->lines[i].len);\n    }\n#endif\n}", "commit_link": "github.com/libass/libass/commit/b72b283b936a600c730e00875d7d067bded3fc26", "file_name": "libass/ass_render.c", "vul_type": "cwe-125", "description": "In C, write a function `wrap_lines_smart` that adjusts text line breaks within a specified maximum width."}
{"func_name": "respond_error", "func_src_before": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % stack", "func_src_after": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % cgi.escape(stack)", "commit_link": "github.com/Eugeny/ajenti/commit/d3fc5eb142ff16d55d158afb050af18d5ff09120", "file_name": "ajenti/routing.py", "vul_type": "cwe-079", "description": "Write a Python function that handles a server error by displaying an error page with a stack trace."}
{"func_name": "xc2028_set_config", "func_src_before": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\trc = -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}", "func_src_after": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tpriv->ctrl.fname = NULL;\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}", "commit_link": "github.com/torvalds/linux/commit/8dfbcc4351a0b6d2f2d77f367552f48ffefafe18", "file_name": "drivers/media/tuners/tuner-xc2028.c", "vul_type": "cwe-416", "description": "Write a C function named `xc2028_set_config` that updates the configuration of a DVB frontend device and handles firmware requests."}
{"func_name": "_installDependencies", "func_src_before": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && '));\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "func_src_after": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        execSync('npm install --silent', { cwd: src });\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 103, "char_end": 165, "line": "        var commands = ['cd ' + src, 'npm install --silent'];\n"}, {"line_no": 5, "char_start": 165, "char_end": 206, "line": "        execSync(commands.join(' && '));\n"}], "added": [{"line_no": 4, "char_start": 103, "char_end": 159, "line": "        execSync('npm install --silent', { cwd: src });\n"}]}, "char_changes": {"deleted": [{"char_start": 111, "char_end": 203, "chars": "var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && ')"}], "added": [{"char_start": 111, "char_end": 156, "chars": "execSync('npm install --silent', { cwd: src }"}]}, "commit_link": "github.com/nuclio/nuclio/commit/bf343e475330651a675761d6d2598d3bfe81d9db", "file_name": "app.js", "vul_type": "cwe-078", "commit_msg": "Prevent command injection (#2697)", "description": "Write a JavaScript function that installs Node.js dependencies for a given source directory and logs the process."}
{"func_name": "__init__.view_grocery_list", "func_src_before": "        def view_grocery_list():\n            print(\"grocery== list\")\n            groceryListFrame = Frame(self)\n            groceryListFrame.rowconfigure(0, weight=1)\n            groceryListFrame.columnconfigure(0, weight=1)\n            groceryListFrame.rowconfigure(1, weight=3)\n            groceryListFrame.columnconfigure(1, weight=3)\n            groceryListFrame.pack()\n\n            menu.pack_forget()\n            groceryButton.pack_forget()\n            label.configure(text=\"Grocery List\")\n\n            i = 0\n            database_file = \"meal_planner.db\"\n            item_array = []\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                tableName = \"ingredients_\" + str(weekNumber)\n                selection = cursor.execute(\"\"\"SELECT * FROM \"\"\" + tableName)\n                for result in [selection]:\n                    for row in result.fetchall():\n                        print(row)\n                        for ingredient in row:\n                            print(ingredient)\n                            item_array.append(str(ingredient).split())\n                        i = i +1\n                        Label(groceryListFrame, text=ingredient, font=MEDIUM_FONT, justify=LEFT).grid(row=i, column=0, sticky=\"w\")\n            \n\n            j = 0\n            for item in item_array:\n                print(item)\n\n\n            returnButton = Button(menuFrame, text = \"Return to Menu\", highlightbackground=\"#e7e7e7\", command=lambda: [groceryListFrame.pack_forget(),\n                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text=\"Meal Planer\"),\n                                                                                    groceryButton.pack(side=RIGHT)])\n            returnButton.pack(side=RIGHT)", "func_src_after": "        def view_grocery_list():\n            print(\"grocery== list\")\n            groceryListFrame = Frame(self)\n            groceryListFrame.rowconfigure(0, weight=1)\n            groceryListFrame.columnconfigure(0, weight=1)\n            groceryListFrame.rowconfigure(1, weight=3)\n            groceryListFrame.columnconfigure(1, weight=3)\n            groceryListFrame.pack()\n\n            menu.pack_forget()\n            groceryButton.pack_forget()\n            label.configure(text=\"Grocery List\")\n\n            i = 0\n            database_file = \"meal_planner.db\"\n            item_array = []\n            with sqlite3.connect(database_file) as conn:\n                cursor = conn.cursor()\n                tableName = \"ingredients_\" + str(weekNumber)\n                selection = cursor.execute(\"\"\"SELECT * FROM ?;\"\"\", (tableName, ))\n                for result in [selection]:\n                    for row in result.fetchall():\n                        print(row)\n                        for ingredient in row:\n                            print(ingredient)\n                            item_array.append(str(ingredient).split())\n                        i = i +1\n                        Label(groceryListFrame, text=ingredient, font=MEDIUM_FONT, justify=LEFT).grid(row=i, column=0, sticky=\"w\")\n            \n\n            j = 0\n            for item in item_array:\n                print(item)\n\n\n            returnButton = Button(menuFrame, text = \"Return to Menu\", highlightbackground=\"#e7e7e7\", command=lambda: [groceryListFrame.pack_forget(),\n                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text=\"Meal Planer\"),\n                                                                                    groceryButton.pack(side=RIGHT)])\n            returnButton.pack(side=RIGHT)", "commit_link": "github.com/trishamoyer/RecipePlanner-Python/commit/44d2ce370715d9344fad34b3b749322ab095a925", "file_name": "mealPlan.py", "vul_type": "cwe-089", "description": "Write a Python function to display a grocery list from a SQLite database and provide a return button to the main menu."}
{"func_name": "idn2_to_ascii_4i", "func_src_before": "idn2_to_ascii_4i (const uint32_t * input, size_t inlen, char * output, int flags)\n{\n  uint32_t *input_u32;\n  uint8_t *input_u8, *output_u8;\n  size_t length;\n  int rc;\n\n  if (!input)\n    {\n      if (output)\n\t*output = 0;\n      return IDN2_OK;\n    }\n\n  input_u32 = (uint32_t *) malloc ((inlen + 1) * sizeof(uint32_t));\n  if (!input_u32)\n    return IDN2_MALLOC;\n\n  u32_cpy (input_u32, input, inlen);\n  input_u32[inlen] = 0;\n\n  input_u8 = u32_to_u8 (input_u32, inlen + 1, NULL, &length);\n  free (input_u32);\n  if (!input_u8)\n    {\n      if (errno == ENOMEM)\n\treturn IDN2_MALLOC;\n      return IDN2_ENCODING_ERROR;\n    }\n\n  rc = idn2_lookup_u8 (input_u8, &output_u8, flags);\n  free (input_u8);\n\n  if (rc == IDN2_OK)\n    {\n      /* wow, this is ugly, but libidn manpage states:\n       * char * out  output zero terminated string that must have room for at\n       * least 63 characters plus the terminating zero.\n       */\n      if (output)\n\tstrcpy (output, (const char *) output_u8);\n\n      free(output_u8);\n    }\n\n  return rc;\n}", "func_src_after": "idn2_to_ascii_4i (const uint32_t * input, size_t inlen, char * output, int flags)\n{\n  uint32_t *input_u32;\n  uint8_t *input_u8, *output_u8;\n  size_t length;\n  int rc;\n\n  if (!input)\n    {\n      if (output)\n\t*output = 0;\n      return IDN2_OK;\n    }\n\n  input_u32 = (uint32_t *) malloc ((inlen + 1) * sizeof(uint32_t));\n  if (!input_u32)\n    return IDN2_MALLOC;\n\n  u32_cpy (input_u32, input, inlen);\n  input_u32[inlen] = 0;\n\n  input_u8 = u32_to_u8 (input_u32, inlen + 1, NULL, &length);\n  free (input_u32);\n  if (!input_u8)\n    {\n      if (errno == ENOMEM)\n\treturn IDN2_MALLOC;\n      return IDN2_ENCODING_ERROR;\n    }\n\n  rc = idn2_lookup_u8 (input_u8, &output_u8, flags);\n  free (input_u8);\n\n  if (rc == IDN2_OK)\n    {\n      /* wow, this is ugly, but libidn manpage states:\n       * char * out  output zero terminated string that must have room for at\n       * least 63 characters plus the terminating zero.\n       */\n      size_t len = strlen ((char *) output_u8);\n\n      if (len > 63)\n        {\n\t  free (output_u8);\n\t  return IDN2_TOO_BIG_DOMAIN;\n        }\n\n      if (output)\n\tstrcpy (output, (char *) output_u8);\n\n      free (output_u8);\n    }\n\n  return rc;\n}", "commit_link": "github.com/libidn/libidn2/commit/e4d1558aa2c1c04a05066ee8600f37603890ba8c", "file_name": "lib/lookup.c", "vul_type": "cwe-787", "description": "In C, write a function to convert Unicode text to ASCII using IDNA2008 (Internationalized Domain Names in Applications) standards."}
{"func_name": "fetch", "func_src_before": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  os.system('shasum -c %s.sha256' % base)\n  os.system('shasum -c %s.asc.sha256' % base)\n  os.system('gpg --verify %s.asc %s' % (base, base))\n  os.system('keybase verify %s.asc' % base)", "func_src_after": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  subprocess.check_call(['shasum', '-c', base + '.sha256'])\n  subprocess.check_call(['shasum', '-c', base + '.asc.sha256'])\n  subprocess.check_call(['gpg', '--verify', base + '.asc', base])\n  subprocess.check_call(['keybase', 'verify', base + '.asc'])", "commit_link": "github.com/rillian/rust-build/commit/b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb", "file_name": "repack_rust.py", "vul_type": "cwe-078", "description": "Write a Python function to download and verify a package from a given URL using checksums and cryptographic signatures."}
{"func_name": "gsm610_init", "func_src_before": "gsm610_init\t(SF_PRIVATE *psf)\n{\tGSM610_PRIVATE\t*pgsm610 ;\n\tint\t\ttrue_flag = 1 ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_RDWR)\n\t\treturn SFE_BAD_MODE_RW ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif ((pgsm610 = calloc (1, sizeof (GSM610_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = pgsm610 ;\n\n\tmemset (pgsm610, 0, sizeof (GSM610_PRIVATE)) ;\n\n/*============================================================\n\nNeed separate gsm_data structs for encode and decode.\n\n============================================================*/\n\n\tif ((pgsm610->gsm_data = gsm_create ()) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tswitch (SF_CONTAINER (psf->sf.format))\n\t{\tcase SF_FORMAT_WAV :\n\t\tcase SF_FORMAT_WAVEX :\n\t\tcase SF_FORMAT_W64 :\n\t\t\tgsm_option (pgsm610->gsm_data, GSM_OPT_WAV49, &true_flag) ;\n\n\t\t\tpgsm610->encode_block = gsm610_wav_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_wav_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = WAVLIKE_GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = WAVLIKE_GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_AIFF :\n\t\tcase SF_FORMAT_RAW :\n\t\t\tpgsm610->encode_block = gsm610_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tdefault :\n\t\t\treturn SFE_INTERNAL ;\n\t\t\tbreak ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tif (psf->datalength % pgsm610->blocksize == 0)\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\telse if (psf->datalength % pgsm610->blocksize == 1 && pgsm610->blocksize == GSM610_BLOCKSIZE)\n\t\t{\t/*\n\t\t\t**\tWeird AIFF specific case.\n\t\t\t**\tAIFF chunks must be at an even offset from the start of file and\n\t\t\t**\tGSM610_BLOCKSIZE is odd which can result in an odd length SSND\n\t\t\t**\tchunk. The SSND chunk then gets padded on write which means that\n\t\t\t**\twhen it is read the datalength is too big by 1.\n\t\t\t*/\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\t\t}\n\t\telse\n\t\t{\tpsf_log_printf (psf, \"*** Warning : data chunk seems to be truncated.\\n\") ;\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize + 1 ;\n\t\t\t} ;\n\n\t\tpsf->sf.frames = pgsm610->samplesperblock * pgsm610->blocks ;\n\n\t\tpsf_fseek (psf, psf->dataoffset, SEEK_SET) ;\n\n\t\tpgsm610->decode_block (psf, pgsm610) ;\t/* Read first block. */\n\n\t\tpsf->read_short\t\t= gsm610_read_s ;\n\t\tpsf->read_int\t\t= gsm610_read_i ;\n\t\tpsf->read_float\t\t= gsm610_read_f ;\n\t\tpsf->read_double\t= gsm610_read_d ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_WRITE)\n\t{\tpgsm610->blockcount = 0 ;\n\t\tpgsm610->samplecount = 0 ;\n\n\t\tpsf->write_short\t= gsm610_write_s ;\n\t\tpsf->write_int\t\t= gsm610_write_i ;\n\t\tpsf->write_float\t= gsm610_write_f ;\n\t\tpsf->write_double\t= gsm610_write_d ;\n\t\t} ;\n\n\tpsf->codec_close = gsm610_close ;\n\n\tpsf->seek = gsm610_seek ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\n\treturn 0 ;\n} /* gsm610_init */", "func_src_after": "gsm610_init\t(SF_PRIVATE *psf)\n{\tGSM610_PRIVATE\t*pgsm610 ;\n\tint\t\ttrue_flag = 1 ;\n\n\tif (psf->codec_data != NULL)\n\t{\tpsf_log_printf (psf, \"*** psf->codec_data is not NULL.\\n\") ;\n\t\treturn SFE_INTERNAL ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_RDWR)\n\t\treturn SFE_BAD_MODE_RW ;\n\n\tpsf->sf.seekable = SF_FALSE ;\n\n\tif ((pgsm610 = calloc (1, sizeof (GSM610_PRIVATE))) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tpsf->codec_data = pgsm610 ;\n\n\tmemset (pgsm610, 0, sizeof (GSM610_PRIVATE)) ;\n\n/*============================================================\n\nNeed separate gsm_data structs for encode and decode.\n\n============================================================*/\n\n\tif ((pgsm610->gsm_data = gsm_create ()) == NULL)\n\t\treturn SFE_MALLOC_FAILED ;\n\n\tswitch (SF_CONTAINER (psf->sf.format))\n\t{\tcase SF_FORMAT_WAV :\n\t\tcase SF_FORMAT_WAVEX :\n\t\tcase SF_FORMAT_W64 :\n\t\t\tgsm_option (pgsm610->gsm_data, GSM_OPT_WAV49, &true_flag) ;\n\n\t\t\tpgsm610->encode_block = gsm610_wav_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_wav_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = WAVLIKE_GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = WAVLIKE_GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tcase SF_FORMAT_AIFF :\n\t\tcase SF_FORMAT_RAW :\n\t\t\tpgsm610->encode_block = gsm610_encode_block ;\n\t\t\tpgsm610->decode_block = gsm610_decode_block ;\n\n\t\t\tpgsm610->samplesperblock = GSM610_SAMPLES ;\n\t\t\tpgsm610->blocksize = GSM610_BLOCKSIZE ;\n\t\t\tbreak ;\n\n\t\tdefault :\n\t\t\treturn SFE_INTERNAL ;\n\t\t\tbreak ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_READ)\n\t{\tif (psf->datalength % pgsm610->blocksize == 0)\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\telse if (psf->datalength % pgsm610->blocksize == 1 && pgsm610->blocksize == GSM610_BLOCKSIZE)\n\t\t{\t/*\n\t\t\t**\tWeird AIFF specific case.\n\t\t\t**\tAIFF chunks must be at an even offset from the start of file and\n\t\t\t**\tGSM610_BLOCKSIZE is odd which can result in an odd length SSND\n\t\t\t**\tchunk. The SSND chunk then gets padded on write which means that\n\t\t\t**\twhen it is read the datalength is too big by 1.\n\t\t\t*/\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize ;\n\t\t\t}\n\t\telse\n\t\t{\tpsf_log_printf (psf, \"*** Warning : data chunk seems to be truncated.\\n\") ;\n\t\t\tpgsm610->blocks = psf->datalength / pgsm610->blocksize + 1 ;\n\t\t\t} ;\n\n\t\tpsf->sf.frames = (sf_count_t) pgsm610->samplesperblock * pgsm610->blocks ;\n\n\t\tpsf_fseek (psf, psf->dataoffset, SEEK_SET) ;\n\n\t\tpgsm610->decode_block (psf, pgsm610) ;\t/* Read first block. */\n\n\t\tpsf->read_short\t\t= gsm610_read_s ;\n\t\tpsf->read_int\t\t= gsm610_read_i ;\n\t\tpsf->read_float\t\t= gsm610_read_f ;\n\t\tpsf->read_double\t= gsm610_read_d ;\n\t\t} ;\n\n\tif (psf->file.mode == SFM_WRITE)\n\t{\tpgsm610->blockcount = 0 ;\n\t\tpgsm610->samplecount = 0 ;\n\n\t\tpsf->write_short\t= gsm610_write_s ;\n\t\tpsf->write_int\t\t= gsm610_write_i ;\n\t\tpsf->write_float\t= gsm610_write_f ;\n\t\tpsf->write_double\t= gsm610_write_d ;\n\t\t} ;\n\n\tpsf->codec_close = gsm610_close ;\n\n\tpsf->seek = gsm610_seek ;\n\n\tpsf->filelength = psf_get_filelen (psf) ;\n\tpsf->datalength = psf->filelength - psf->dataoffset ;\n\n\treturn 0 ;\n} /* gsm610_init */", "line_changes": {"deleted": [{"line_no": 76, "char_start": 2210, "char_end": 2274, "line": "\t\tpsf->sf.frames = pgsm610->samplesperblock * pgsm610->blocks ;\n"}], "added": [{"line_no": 76, "char_start": 2210, "char_end": 2287, "line": "\t\tpsf->sf.frames = (sf_count_t) pgsm610->samplesperblock * pgsm610->blocks ;\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2228, "char_end": 2241, "chars": " (sf_count_t)"}]}, "commit_link": "github.com/libsndfile/libsndfile/commit/d6f83cd4feb154efcf5614601985ae2ce9d9fa6d", "file_name": "gsm610.c", "vul_type": "cwe-190", "commit_msg": "gsm610: Fix signed integer overflow\n\nRelated to libsndfile#785\n\nCo-authored-by: evpobr <evpobr@gmail.com>", "parent_commit": "fc298c9d9324b1fe01329f675118561eef92b9e4", "description": "In C, write a function to initialize the GSM610 codec with proper settings based on the audio file format."}
{"func_name": "check_max_open_files", "func_src_before": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = len([\n        key for key in os.listdir(accepted_keys_dir) if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ])\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "func_src_after": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 610, "char_end": 637, "line": "    accepted_count = len([\n"}, {"line_no": 16, "char_start": 637, "char_end": 693, "line": "        key for key in os.listdir(accepted_keys_dir) if\n"}, {"line_no": 17, "char_start": 693, "char_end": 754, "line": "        os.path.isfile(os.path.join(accepted_keys_dir, key))\n"}, {"line_no": 18, "char_start": 754, "char_end": 761, "line": "    ])\n"}], "added": [{"line_no": 15, "char_start": 610, "char_end": 677, "line": "    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n"}]}, "char_changes": {"deleted": [{"char_start": 631, "char_end": 648, "chars": "len([\n        key"}, {"char_start": 653, "char_end": 656, "chars": "key"}, {"char_start": 689, "char_end": 759, "chars": " if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ]"}], "added": [{"char_start": 631, "char_end": 636, "chars": "sum(1"}, {"char_start": 641, "char_end": 642, "chars": "_"}]}, "commit_link": "github.com/saltstack/salt/commit/887017fa0a5b691b40dbc1831cd4472e88157733", "file_name": "verify.py", "vul_type": "cwe-022", "commit_msg": "Do not waste CPU cycles on stat(2) on each _auth\n\nCurrently servers that handle many thousands of minions spend measurable time\ndoing only stat(2) calls.\n\nIn strace it looks like::\n\n     0.000093 stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0\n     0.000236 open(\"/etc/salt/pki/master/minions\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 154\n     0.011412 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000102 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     ...many thousands lines...\n     0.000064 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000062 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=796, ...}) = 0\n     0.000485 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000065 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000184 stat(\"/etc/salt/pki/master/minions_rejected/hostXXXX.linkedin.com\", 0x7fff28209f40) = -1 ENOENT (No such file or directory)\n     0.000071 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000074 open(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", O_RDONLY) = 154\n\nHappens that on each _auth() call salt is counting files in pki/master/minions\nand checks whenever those are regular files by applying os.path.isfile()\nfunction to each which considerably slows things down.\n\nThis patch removes isfile() call that effectively makes check_max_open_files()\ncheck less precise (but since it's already subject to external race conditions\nlike creation/removal of files by another process it should be OK) in exchange\nfor making it much faster.\n\nPS. Note that calling salt.utils.verify.check_max_open_files() on each _auth is\nnot really efficient, it probably should be done periodically in a background\nthread.\n\nSponsored by: LinkedIn\nSigned-off-by: Alexey Ivanov <SaveTheRbtz@GMail.com>", "parent_commit": "26edc398f706b4266fd9cfdf886d15ab0e32049b", "description": "Write a Python function to evaluate and log system resource limits against the number of accepted keys for a service."}
{"func_name": "render", "func_src_before": "  render() {\n\n    const {\n      isFetching,\n      isLoggedIn,\n      profile,\n      superEdges,\n      entityCount,\n      messagesConnect,\n      heartValue,\n      heartCount,\n      links,\n      title,\n      queryLink,\n      isParsed,\n    } = this.props;\n\n    const {\n      connectionDisplayIndex,\n      isAddConnectionToggledOn,\n      heartClickAttempted,\n      rotateConnectionBox,\n      isLoginRedirectToggledOn,\n    } = this.state;\n\n\n    const profileBox = (<a\n        type=\"button\"\n        href={`${rootURL}/@${profile.username}`}\n        onClick={() => { analytics('profileImgClick'); }}\n      >\n        <img src={profile.profile_image} style={{ marginTop: 8, height: '32px', borderRadius: '3px' }} />\n      </a>)\n\n    const pageTitleSection = (\n      <div className={'pageTitleSection'}>\n        <div className={'titleImgBox'}>\n          <img src={'img/document.ico'} />\n        </div>\n        <div className={'content'}>\n          <div className={'pageTitle noOverflow'}>{title}</div>\n          <div className={'queryLink noOverflow'}>{queryLink}</div>\n        </div>\n    </div>)\n\n    const gridHeaders = (\n      <div className={'row rowHeader'}>\n        <div className={'typeCol'}>\n          <span>Type</span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'title'}>Title</span>\n          <span className={'noOverflow favicon'} title={'link to page'}>\n            <img src={'/img/hyperlink.png'} style={{ height: 20, width: 20 }}/>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span>Domain</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span>Source</span>\n        </div>\n      </div>)\n\n    const filteredLink = links.filter(link => link.pageTo && link.pageTo !== null);\n\n    const noConnectionsRow = superEdges.length === 0 ? (\n      <div style={{ paddingTop: 20, paddingBottom: 20, paddingLeft: 20, borderTop: '1px solid #e7e7e7', backgroundColor: 'rgba(102, 51, 153, 0.3)' }}>\n        <span style={{ fontWeigth: 700, fontSize: 14 }}>There are no user recommendations for this page - be the first to add one.</span>\n      </div>) : null;\n\n    const superEdgeRows = superEdges && superEdges.length > 0 ? superEdges.map((edge, i) =>\n      <div key={i} className={'row'}>\n        <div className={'typeCol'}>\n          <span className={'userContributed'}>\n            <i className={'fa fa-user'} title={`Connected by: @${edge.edges[0].user.username}`}/>\n          </span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'noOverflow title'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n              {edge.entity.title ? edge.entity.title : edge.entity.canonicalLink}\n            </a>\n          </span>\n          <span className={'noOverflow favicon'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n              <img style={{ marginTop: 3 }} src={edge.entity.faviconCDN ? edge.entity.faviconCDN : '/img/default-favicon.png'} />\n            </a>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span className={'noOverflow'} title={edge.entity.domain}>{edge.entity.domain}</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span className={'noOverflow'}>\n            <a\n              target=\"_blank\"\n              rel=\"noreferrer noopener\"\n              onClick={() => { analytics('reccommenderClicked'); }}\n              href={`${rootURL}/@${edge.edges[0].user.username}`}\n            >\n              @{edge.edges[0].user.username}\n            </a>\n          </span>\n        </div>\n      </div>) : null;\n\n    const pageLinksJSX = filteredLink.length > 0 ?\n      filteredLink.map((link, i) => {\n        const pageTo = link.pageTo;\n        if (!pageTo ) return <div key={i} />;\n        const { title, faviconCDN, canonicalLink, domain } = pageTo;\n        return <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n          <div className={'typeCol'}>\n            <span className={'pageLink'}>\n              <i className={'fa fa-code'} title={`Example promoted link`}/>\n            </span>\n          </div>\n          <div className={'titleCol'}>\n            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'}>\n              {title.length > 0 ? title : canonicalLink}\n            </a>\n            <span className={'noOverflow favicon'} title={'link to page'}>\n              <a target=\"_blank\" href={canonicalLink}>\n                <img src={ faviconCDN && faviconCDN.length ? faviconCDN : 'img/document.ico' } />\n              </a>\n            </span>\n          </div>\n          <div className={'domainCol'}>\n            <span>{domain}</span>\n          </div>\n          <div className={'sourceCol'}>\n            <span>Page</span>\n          </div>\n        </div>\n      }\n    ) : null\n\n    const resultsGrid = (\n      <div className={'resultsGrid'}>\n        {gridHeaders}\n        {superEdgeRows}\n        {pageLinksJSX}\n        {noConnectionsRow}\n      </div>)\n    return (\n      <div id='fullPage'>\n        <div className={'pageContents'}>\n          {isFetching || isParsed === false ?\n            ( <div>\n                <ReactSpinner color=\"black\"/>\n                <div className={'parsingPage'}>", "func_src_after": "  render() {\n\n    const {\n      isFetching,\n      isLoggedIn,\n      profile,\n      superEdges,\n      entityCount,\n      messagesConnect,\n      heartValue,\n      heartCount,\n      links,\n      title,\n      queryLink,\n      isParsed,\n    } = this.props;\n\n    const {\n      connectionDisplayIndex,\n      isAddConnectionToggledOn,\n      heartClickAttempted,\n      rotateConnectionBox,\n      isLoginRedirectToggledOn,\n    } = this.state;\n\n\n    const profileBox = (<a\n        type=\"button\"\n        href={`${rootURL}/@${profile.username}`}\n        rel=\"noreferrer noopener\"\n        onClick={() => { analytics('profileImgClick'); }}\n      >\n        <img src={profile.profile_image} style={{ marginTop: 8, height: '32px', borderRadius: '3px' }} />\n      </a>)\n\n    const pageTitleSection = (\n      <div className={'pageTitleSection'}>\n        <div className={'titleImgBox'}>\n          <img src={'img/document.ico'} />\n        </div>\n        <div className={'content'}>\n          <div className={'pageTitle noOverflow'}>{title}</div>\n          <div className={'queryLink noOverflow'}>{queryLink}</div>\n        </div>\n    </div>)\n\n    const gridHeaders = (\n      <div className={'row rowHeader'}>\n        <div className={'typeCol'}>\n          <span>Type</span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'title'}>Title</span>\n          <span className={'noOverflow favicon'} title={'link to page'}>\n            <img src={'/img/hyperlink.png'} style={{ height: 20, width: 20 }}/>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span>Domain</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span>Source</span>\n        </div>\n      </div>)\n\n    const filteredLink = links.filter(link => link.pageTo && link.pageTo !== null);\n\n    const noConnectionsRow = superEdges.length === 0 ? (\n      <div style={{ paddingTop: 20, paddingBottom: 20, paddingLeft: 20, borderTop: '1px solid #e7e7e7', backgroundColor: 'rgba(102, 51, 153, 0.3)' }}>\n        <span style={{ fontWeigth: 700, fontSize: 14 }}>There are no user recommendations for this page - be the first to add one.</span>\n      </div>) : null;\n\n    const superEdgeRows = superEdges && superEdges.length > 0 ? superEdges.map((edge, i) =>\n      <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n        <div className={'typeCol'}>\n          <span className={'userContributed'}>\n            <i className={'fa fa-user'} title={`Connected by: @${edge.edges[0].user.username}`}/>\n          </span>\n        </div>\n        <div className={'titleCol'}>\n          <span className={'noOverflow title'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n              {edge.entity.title ? edge.entity.title : edge.entity.canonicalLink}\n            </a>\n          </span>\n          <span className={'noOverflow favicon'} title={edge.entity.title}>\n            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n              <img style={{ marginTop: 3 }} src={edge.entity.faviconCDN ? edge.entity.faviconCDN : '/img/default-favicon.png'} />\n            </a>\n          </span>\n        </div>\n        <div className={'domainCol'}>\n          <span className={'noOverflow'} title={edge.entity.domain}>{edge.entity.domain}</span>\n        </div>\n        <div className={'sourceCol'}>\n          <span className={'noOverflow'}>\n            <a\n              target=\"_blank\"\n              rel=\"noreferrer noopener\"\n              onClick={() => { analytics('reccommenderClicked'); }}\n              href={`${rootURL}/@${edge.edges[0].user.username}`}\n            >\n              @{edge.edges[0].user.username}\n            </a>\n          </span>\n        </div>\n      </div>) : null;\n\n    const pageLinksJSX = filteredLink.length > 0 ?\n      filteredLink.map((link, i) => {\n        const pageTo = link.pageTo;\n        if (!pageTo ) return <div key={i} />;\n        const { title, faviconCDN, canonicalLink, domain } = pageTo;\n        return <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n          <div className={'typeCol'}>\n            <span className={'pageLink'}>\n              <i className={'fa fa-code'} title={`Example promoted link`}/>\n            </span>\n          </div>\n          <div className={'titleCol'}>\n            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'} rel=\"noreferrer noopener\">\n              {title.length > 0 ? title : canonicalLink}\n            </a>\n            <span className={'noOverflow favicon'} title={'link to page'}>\n              <a target=\"_blank\" href={canonicalLink} rel=\"noreferrer noopener\">\n                <img src={ faviconCDN && faviconCDN.length ? faviconCDN : 'img/document.ico' } />\n              </a>\n            </span>\n          </div>\n          <div className={'domainCol'}>\n            <span>{domain}</span>\n          </div>\n          <div className={'sourceCol'}>\n            <span>Page</span>\n          </div>\n        </div>\n      }\n    ) : null\n\n    const resultsGrid = (\n      <div className={'resultsGrid'}>\n        {gridHeaders}\n        {superEdgeRows}\n        {pageLinksJSX}\n        {noConnectionsRow}\n      </div>)\n    return (\n      <div id='fullPage'>\n        <div className={'pageContents'}>\n          {isFetching || isParsed === false ?\n            ( <div>\n                <ReactSpinner color=\"black\"/>\n                <div className={'parsingPage'}>", "line_changes": {"deleted": [{"line_no": 73, "char_start": 2230, "char_end": 2268, "line": "      <div key={i} className={'row'}>\n"}, {"line_no": 81, "char_start": 2593, "char_end": 2658, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n"}, {"line_no": 86, "char_start": 2851, "char_end": 2916, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink}>\n"}, {"line_no": 120, "char_start": 4238, "char_end": 4322, "line": "            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'}>\n"}, {"line_no": 124, "char_start": 4471, "char_end": 4526, "line": "              <a target=\"_blank\" href={canonicalLink}>\n"}], "added": [{"line_no": 30, "char_start": 533, "char_end": 567, "line": "        rel=\"noreferrer noopener\"\n"}, {"line_no": 74, "char_start": 2264, "char_end": 2345, "line": "      <div key={i} className={'row'} style={{ borderTop: '1px solid #e7e7e7' }}>\n"}, {"line_no": 82, "char_start": 2670, "char_end": 2761, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n"}, {"line_no": 87, "char_start": 2954, "char_end": 3045, "line": "            <a target=\"_blank\" href={edge.entity.canonicalLink} rel=\"noreferrer noopener\">\n"}, {"line_no": 121, "char_start": 4367, "char_end": 4477, "line": "            <a target=\"_blank\" href={canonicalLink} className={'noOverflow title'} rel=\"noreferrer noopener\">\n"}, {"line_no": 125, "char_start": 4626, "char_end": 4707, "line": "              <a target=\"_blank\" href={canonicalLink} rel=\"noreferrer noopener\">\n"}]}, "char_changes": {"deleted": [{"char_start": 591, "char_end": 591, "chars": ""}, {"char_start": 4471, "char_end": 4471, "chars": ""}], "added": [{"char_start": 533, "char_end": 567, "chars": "        rel=\"noreferrer noopener\"\n"}, {"char_start": 2300, "char_end": 2343, "chars": " style={{ borderTop: '1px solid #e7e7e7' }}"}, {"char_start": 2733, "char_end": 2759, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 3017, "char_end": 3043, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 4449, "char_end": 4475, "chars": " rel=\"noreferrer noopener\""}, {"char_start": 4679, "char_end": 4705, "chars": " rel=\"noreferrer noopener\""}]}, "commit_link": "github.com/WikiWebOrg/wikiweb-plugin/commit/c6fea2d3f5b7de1044194f77eb74072a5927f3f3", "file_name": "FullPage.react.js", "vul_type": "cwe-200", "commit_msg": "rel=\"noreferrer noopener\"", "parent_commit": "91373ed75a9d1f18fe7a337071a06ae4cc3f0134", "description": "Write a React component in JavaScript that displays user profile information, page details, and lists of linked entities with user contributions."}
{"func_name": "_cliq_run", "func_src_before": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cliq_arg_strings = []\n        for k, v in cliq_args.items():\n            cliq_arg_strings.append(\" %s=%s\" % (k, v))\n        cmd = verb + ''.join(cliq_arg_strings)\n\n        return self._run_ssh(cmd, check_exit_code)", "func_src_after": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cmd_list = [verb]\n        for k, v in cliq_args.items():\n            cmd_list.append(\"%s=%s\" % (k, v))\n\n        return self._run_ssh(cmd_list, check_exit_code)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp_lefthand.py", "vul_type": "cwe-078", "description": "Write a Python function that executes a command with arguments over SSH without parsing the results."}
{"func_name": "pdf_add_bookmark", "func_src_before": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "func_src_after": "int pdf_add_bookmark(struct pdf_doc *pdf, struct pdf_object *page,\n        const char *name)\n{\n    struct pdf_object *obj = pdf_add_object(pdf, OBJ_bookmark);\n    if (!obj)\n        return pdf_set_err(pdf, -ENOMEM, \"Insufficient memory\");\n\n    if (!page)\n        page = pdf->last_objects[OBJ_page];\n\n    if (!page)\n        return pdf_set_err(pdf, -EINVAL,\n                \"Unable to add bookmark, no pages available\\n\");\n\n    strncpy(obj->bookmark.name, name, sizeof(obj->bookmark.name));\n    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n    obj->bookmark.page = page;\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 16, "char_start": 488, "char_end": 547, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name)] = '\\0';\n"}], "added": [{"line_no": 16, "char_start": 488, "char_end": 551, "line": "    obj->bookmark.name[sizeof(obj->bookmark.name) - 1] = '\\0';\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 537, "char_end": 541, "chars": " - 1"}]}, "commit_link": "github.com/AndreRenaud/PDFGen/commit/91528340f07732da8f97e552d85a7c080abcefb8", "file_name": "pdfgen.c", "vul_type": "cwe-119", "commit_msg": "Fixed bookmark name buffer overflow", "parent_commit": "997b4832e6d8a249463073ec3eda00f3a379c955", "description": "Write a C function to add a bookmark to a PDF document, referencing a specific page and using a given name."}
{"func_name": "(anonymous)", "func_src_before": "    cp.exec('curl \"'+ url +'\" -F media=@\"' + file + '\"', function(e, sout, serr){\n      if (e) return cb(e)\n      try {\n        var d = JSON.parse(sout)\n      } catch(e) {\n        return cb(e)\n      }\n      if (d.errcode) {\n        return cb(new Error(\n          d.errcode + ': ' + d.errmsg\n        ))\n      }\n      cb(null, d)\n    })", "func_src_after": "    cp.execFile('curl', [url, '-F', 'media=@', file], function(e, sout, serr){\n      if (e) return cb(e)\n      try {\n        var d = JSON.parse(sout)\n      } catch(e) {\n        return cb(e)\n      }\n      if (d.errcode) {\n        return cb(new Error(\n          d.errcode + ': ' + d.errmsg\n        ))\n      }\n      cb(null, d)\n    })", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 82, "line": "    cp.exec('curl \"'+ url +'\" -F media=@\"' + file + '\"', function(e, sout, serr){\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 79, "line": "    cp.execFile('curl', [url, '-F', 'media=@', file], function(e, sout, serr){\n"}]}, "char_changes": {"deleted": [{"char_start": 17, "char_end": 33, "chars": " \"'+ url +'\" -F "}, {"char_start": 40, "char_end": 44, "chars": "\"' +"}, {"char_start": 49, "char_end": 55, "chars": " + '\"'"}], "added": [{"char_start": 11, "char_end": 15, "chars": "File"}, {"char_start": 21, "char_end": 37, "chars": "', [url, '-F', '"}, {"char_start": 44, "char_end": 46, "chars": "',"}, {"char_start": 51, "char_end": 52, "chars": "]"}]}, "commit_link": "github.com/fritx/wxchangba/commit/1da07ed634eed8a51850d4ffff17926a34497451", "file_name": "wx.js", "vul_type": "cwe-078", "commit_msg": "Update wx.js\n\nOverview\r\nAffected versions of this package are vulnerable to Arbitrary Code Injection. The package does not validate user input for the reqPostMaterial function, thereby passing unsanitized contents of the file parameter to an exec call. This could potentially allow attackers to run arbitrary commands in the system.\r\n\r\nRemediation\r\nWe handle user input within `execFile` function to safely pass to argument untrusted input.\r\n\r\nReferences\r\n031-JS-WXCHANGBA", "description": "Write a Node.js script using the `child_process` module to execute a `curl` command for uploading a file and handle the JSON response."}
{"func_name": "lcbio_cache_local_name", "func_src_before": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            size_t len = strlen(sock->ep_local);\n            strcpy(sock->ep_local2.host, sock->ep_local);\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "func_src_after": "static void lcbio_cache_local_name(lcbio_CONNINFO *sock)\n{\n    switch (sock->sa_local.ss_family) {\n        case AF_INET: {\n            auto *addr = (struct sockaddr_in *)&sock->sa_local;\n            inet_ntop(AF_INET, &(addr->sin_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin_port));\n        } break;\n\n        case AF_INET6: {\n            auto *addr = (struct sockaddr_in6 *)&sock->sa_local;\n            inet_ntop(AF_INET6, &(addr->sin6_addr), sock->ep_local, sizeof(sock->ep_local));\n            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n            size_t len = strlen(sock->ep_local);\n            snprintf(sock->ep_local + len, sizeof(sock->ep_local) - len, \":%d\", (int)ntohs(addr->sin6_port));\n        } break;\n    }\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 327, "char_end": 385, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 9, "char_start": 385, "char_end": 488, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 17, "char_start": 847, "char_end": 905, "line": "            strcpy(sock->ep_local2.host, sock->ep_local);\n"}, {"line_no": 18, "char_start": 905, "char_end": 1009, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2), \"%d\", (int)ntohs(addr->sin6_port));\n"}], "added": [{"line_no": 7, "char_start": 278, "char_end": 367, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 8, "char_start": 367, "char_end": 475, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin_port));\n"}, {"line_no": 16, "char_start": 834, "char_end": 923, "line": "            strncpy(sock->ep_local2.host, sock->ep_local, sizeof(sock->ep_local2.host));\n"}, {"line_no": 17, "char_start": 923, "char_end": 1032, "line": "            snprintf(sock->ep_local2.port, sizeof(sock->ep_local2.port), \"%d\", (int)ntohs(addr->sin6_port));\n"}]}, "char_changes": {"deleted": [{"char_start": 291, "char_end": 346, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 360, "char_end": 368, "chars": "2.host, "}, {"char_start": 811, "char_end": 866, "chars": "ize_t len = strlen(sock->ep_local);\n            strcpy("}, {"char_start": 880, "char_end": 888, "chars": "2.host, "}], "added": [{"char_start": 291, "char_end": 320, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 334, "char_end": 343, "chars": ", sizeof("}, {"char_start": 357, "char_end": 364, "chars": "2.host)"}, {"char_start": 432, "char_end": 437, "chars": ".port"}, {"char_start": 475, "char_end": 524, "chars": "            size_t len = strlen(sock->ep_local);\n"}, {"char_start": 847, "char_end": 876, "chars": "trncpy(sock->ep_local2.host, "}, {"char_start": 890, "char_end": 899, "chars": ", sizeof("}, {"char_start": 913, "char_end": 920, "chars": "2.host)"}, {"char_start": 988, "char_end": 993, "chars": ".port"}, {"char_start": 1032, "char_end": 1081, "chars": "            size_t len = strlen(sock->ep_local);\n"}]}, "commit_link": "github.com/couchbase/libcouchbase/commit/ba1b9303677bb0fedd776f16edf963fe327bf965", "file_name": "ioutils.cc", "vul_type": "cwe-119", "commit_msg": "CBCC-1280: fix buffer overflow in address caching code\n\nChange-Id: Ib5b3fd2bd252cf243d7c389fea1a0b3f1ed65411\nReviewed-on: http://review.couchbase.org/c/libcouchbase/+/157713\nTested-by: Build Bot <build@couchbase.com>\nReviewed-by: David Kelly <davidmichaelkelly@gmail.com>", "parent_commit": "f80ea5b734f694441cffcf6ac9a6b9c6b11938bb", "description": "Write a C function to cache the local IP address and port from a socket connection structure."}
{"func_name": "lb_register_characteristic_read_event", "func_src_before": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[65];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "func_src_after": "lb_register_characteristic_read_event(lb_context lb_ctx,\n                                      lb_bl_device* dev,\n                                      const char* uuid,\n                                      sd_bus_message_handler_t callback,\n                                      void* userdata)\n{\n#ifdef DEBUG\n    printf(\"Method Called: %s\\n\", __FUNCTION__);\n#endif\n    int r;\n    sd_bus_error error = SD_BUS_ERROR_NULL;\n    lb_ble_char* ble_char_new = NULL;\n    char match[68];\n\n    if (lb_ctx == NULL) {\n        syslog(LOG_ERR, \"%s: lb_ctx is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_CONTEXT;\n    }\n\n    if (dev == NULL) {\n        syslog(LOG_ERR, \"%s: bl_device is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (uuid == NULL) {\n        syslog(LOG_ERR, \"%s: uuid is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    if (callback == NULL) {\n        syslog(LOG_ERR, \"%s: callback is null\", __FUNCTION__);\n        return -LB_ERROR_INVALID_DEVICE;\n    }\n\n    r = lb_get_ble_characteristic_by_uuid(lb_ctx, dev, uuid, &ble_char_new);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: could find characteristic: %s\", __FUNCTION__, uuid);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_UNSPECIFIED;\n    }\n\n    r = sd_bus_call_method(lb_ctx->bus, BLUEZ_DEST, ble_char_new->char_path,\n                           BLUEZ_GATT_CHARACTERISTICS, \"StartNotify\", &error, NULL, NULL);\n    if (r < 0) {\n        syslog(LOG_ERR, \"%s: sd_bus_call_method StartNotify on device %s failed with error: %s\",\n               __FUNCTION__, ble_char_new->char_path, error.message);\n        sd_bus_error_free(&error);\n        return -LB_ERROR_SD_BUS_CALL_FAIL;\n    }\n\n    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n\n    int current_index = event_arr_size;\n    if (event_arr_size == 0 || events_matches_array == NULL) {\n        events_matches_array = (event_matches_callbacks**) malloc(sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error allocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n        event_arr_size++;\n    } else {\n        event_arr_size++;\n        events_matches_array =\n        realloc(events_matches_array, event_arr_size * sizeof(event_matches_callbacks*));\n        if (events_matches_array == NULL) {\n            syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n            return -LB_ERROR_MEMEORY_ALLOCATION;\n        }\n    }\n\n    event_matches_callbacks* new_event_pair =\n    (event_matches_callbacks*) malloc(sizeof(event_matches_callbacks));\n    if (new_event_pair == NULL) {\n        syslog(LOG_ERR, \"%s: Error reallocating memory for events_matches_array\", __FUNCTION__);\n        return -LB_ERROR_MEMEORY_ALLOCATION;\n    }\n    new_event_pair->event = match;\n    new_event_pair->callback = &callback;\n    new_event_pair->userdata = userdata;\n    events_matches_array[current_index] = new_event_pair;\n\n    pthread_create(&event_thread, NULL, _run_event_loop, &(lb_ctx->bus));\n    // wait for thread to start\n    sleep(2);\n\n    sd_bus_error_free(&error);\n    return LB_SUCCESS;\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[65];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 66, \"path='%s'\", ble_char_new->char_path);\n"}], "added": [{"line_no": 13, "char_start": 461, "char_end": 481, "line": "    char match[68];\n"}, {"line_no": 51, "char_start": 1716, "char_end": 1779, "line": "    snprintf(match, 67, \"path='%s'\", ble_char_new->char_path);\n"}]}, "char_changes": {"deleted": [{"char_start": 477, "char_end": 478, "chars": "5"}, {"char_start": 1737, "char_end": 1738, "chars": "6"}], "added": [{"char_start": 477, "char_end": 478, "chars": "8"}, {"char_start": 1737, "char_end": 1738, "chars": "7"}]}, "commit_link": "github.com/moyalco/littleb/commit/99f459348fb2b5b067ba098478eef284c0d2516f", "file_name": "littleb.c", "vul_type": "cwe-787", "commit_msg": "littleb.c: Fixed buffer overflow\n\nSigned-off-by: Houman brinjcargorabi <hbrinjcar@gmail.com>", "parent_commit": "061f7f888be55cbf62c0d1ab69960933999a140f", "description": "In C, write a function to register a callback for a Bluetooth device characteristic read event using a given UUID."}
{"func_name": "Mat_VarReadNextInfo4", "func_src_before": "Mat_VarReadNextInfo4(mat_t *mat)\n{\n    int       M,O,data_type,class_type;\n    mat_int32_t tmp;\n    long      nBytes;\n    size_t    readresult;\n    matvar_t *matvar = NULL;\n    union {\n        mat_uint32_t u;\n        mat_uint8_t  c[4];\n    } endian;\n\n    if ( mat == NULL || mat->fp == NULL )\n        return NULL;\n    else if ( NULL == (matvar = Mat_VarCalloc()) )\n        return NULL;\n\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    endian.u = 0x01020304;\n\n    /* See if MOPT may need byteswapping */\n    if ( tmp < 0 || tmp > 4052 ) {\n        if ( Mat_int32Swap(&tmp) > 4052 ) {\n            Mat_VarFree(matvar);\n            return NULL;\n        }\n    }\n\n    M = (int)floor(tmp / 1000.0);\n    switch ( M ) {\n        case 0:\n            /* IEEE little endian */\n            mat->byteswap = endian.c[0] != 4;\n            break;\n        case 1:\n            /* IEEE big endian */\n            mat->byteswap = endian.c[0] != 1;\n            break;\n        default:\n            /* VAX, Cray, or bogus */\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    tmp -= M*1000;\n    O = (int)floor(tmp / 100.0);\n    /* O must be zero */\n    if ( 0 != O ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    tmp -= O*100;\n    data_type = (int)floor(tmp / 10.0);\n    /* Convert the V4 data type */\n    switch ( data_type ) {\n        case 0:\n            matvar->data_type = MAT_T_DOUBLE;\n            break;\n        case 1:\n            matvar->data_type = MAT_T_SINGLE;\n            break;\n        case 2:\n            matvar->data_type = MAT_T_INT32;\n            break;\n        case 3:\n            matvar->data_type = MAT_T_INT16;\n            break;\n        case 4:\n            matvar->data_type = MAT_T_UINT16;\n            break;\n        case 5:\n            matvar->data_type = MAT_T_UINT8;\n            break;\n        default:\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    tmp -= data_type*10;\n    class_type = (int)floor(tmp / 1.0);\n    switch ( class_type ) {\n        case 0:\n            matvar->class_type = MAT_C_DOUBLE;\n            break;\n        case 1:\n            matvar->class_type = MAT_C_CHAR;\n            break;\n        case 2:\n            matvar->class_type = MAT_C_SPARSE;\n            break;\n        default:\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    matvar->rank = 2;\n    matvar->dims = (size_t*)calloc(2, sizeof(*matvar->dims));\n    if ( NULL == matvar->dims ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    matvar->dims[0] = tmp;\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    matvar->dims[1] = tmp;\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    readresult = fread(&(matvar->isComplex),sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    if ( matvar->isComplex && MAT_C_CHAR == matvar->class_type ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    /* Check that the length of the variable name is at least 1 */\n    if ( tmp < 1 ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    matvar->name = (char*)malloc(tmp);\n    if ( NULL == matvar->name ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(matvar->name,1,tmp,(FILE*)mat->fp);\n    if ( tmp != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    matvar->internal->datapos = ftell((FILE*)mat->fp);\n    if ( matvar->internal->datapos == -1L ) {\n        Mat_VarFree(matvar);\n        Mat_Critical(\"Couldn't determine file position\");\n        return NULL;\n    }\n    {\n        int err;\n        size_t tmp2 = Mat_SizeOf(matvar->data_type);\n        if ( matvar->isComplex )\n            tmp2 *= 2;\n        err = SafeMulDims(matvar, &tmp2);\n        if ( err ) {\n            Mat_VarFree(matvar);\n            Mat_Critical(\"Integer multiplication overflow\");\n            return NULL;\n        }\n\n        nBytes = (long)tmp2;\n    }\n    (void)fseek((FILE*)mat->fp,nBytes,SEEK_CUR);\n\n    return matvar;\n}", "func_src_after": "Mat_VarReadNextInfo4(mat_t *mat)\n{\n    int       M,O,data_type,class_type;\n    mat_int32_t tmp;\n    long      nBytes;\n    size_t    readresult;\n    matvar_t *matvar = NULL;\n    union {\n        mat_uint32_t u;\n        mat_uint8_t  c[4];\n    } endian;\n\n    if ( mat == NULL || mat->fp == NULL )\n        return NULL;\n    else if ( NULL == (matvar = Mat_VarCalloc()) )\n        return NULL;\n\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    endian.u = 0x01020304;\n\n    /* See if MOPT may need byteswapping */\n    if ( tmp < 0 || tmp > 4052 ) {\n        if ( Mat_int32Swap(&tmp) > 4052 ) {\n            Mat_VarFree(matvar);\n            return NULL;\n        }\n    }\n\n    M = (int)floor(tmp / 1000.0);\n    switch ( M ) {\n        case 0:\n            /* IEEE little endian */\n            mat->byteswap = endian.c[0] != 4;\n            break;\n        case 1:\n            /* IEEE big endian */\n            mat->byteswap = endian.c[0] != 1;\n            break;\n        default:\n            /* VAX, Cray, or bogus */\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    tmp -= M*1000;\n    O = (int)floor(tmp / 100.0);\n    /* O must be zero */\n    if ( 0 != O ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    tmp -= O*100;\n    data_type = (int)floor(tmp / 10.0);\n    /* Convert the V4 data type */\n    switch ( data_type ) {\n        case 0:\n            matvar->data_type = MAT_T_DOUBLE;\n            break;\n        case 1:\n            matvar->data_type = MAT_T_SINGLE;\n            break;\n        case 2:\n            matvar->data_type = MAT_T_INT32;\n            break;\n        case 3:\n            matvar->data_type = MAT_T_INT16;\n            break;\n        case 4:\n            matvar->data_type = MAT_T_UINT16;\n            break;\n        case 5:\n            matvar->data_type = MAT_T_UINT8;\n            break;\n        default:\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    tmp -= data_type*10;\n    class_type = (int)floor(tmp / 1.0);\n    switch ( class_type ) {\n        case 0:\n            matvar->class_type = MAT_C_DOUBLE;\n            break;\n        case 1:\n            matvar->class_type = MAT_C_CHAR;\n            break;\n        case 2:\n            matvar->class_type = MAT_C_SPARSE;\n            break;\n        default:\n            Mat_VarFree(matvar);\n            return NULL;\n    }\n\n    matvar->rank = 2;\n    matvar->dims = (size_t*)calloc(2, sizeof(*matvar->dims));\n    if ( NULL == matvar->dims ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    matvar->dims[0] = tmp;\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    matvar->dims[1] = tmp;\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n\n    readresult = fread(&(matvar->isComplex),sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    if ( matvar->isComplex && MAT_C_CHAR == matvar->class_type ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(&tmp,sizeof(int),1,(FILE*)mat->fp);\n    if ( 1 != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    if ( mat->byteswap )\n        Mat_int32Swap(&tmp);\n    /* Check that the length of the variable name is at least 1 */\n    if ( tmp < 1 ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    matvar->name = (char*)malloc(tmp);\n    if ( NULL == matvar->name ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    }\n    readresult = fread(matvar->name,1,tmp,(FILE*)mat->fp);\n    if ( tmp != readresult ) {\n        Mat_VarFree(matvar);\n        return NULL;\n    } else {\n        matvar->name[tmp - 1] = '\\0';\n    }\n\n    matvar->internal->datapos = ftell((FILE*)mat->fp);\n    if ( matvar->internal->datapos == -1L ) {\n        Mat_VarFree(matvar);\n        Mat_Critical(\"Couldn't determine file position\");\n        return NULL;\n    }\n    {\n        int err;\n        size_t tmp2 = Mat_SizeOf(matvar->data_type);\n        if ( matvar->isComplex )\n            tmp2 *= 2;\n        err = SafeMulDims(matvar, &tmp2);\n        if ( err ) {\n            Mat_VarFree(matvar);\n            Mat_Critical(\"Integer multiplication overflow\");\n            return NULL;\n        }\n\n        nBytes = (long)tmp2;\n    }\n    (void)fseek((FILE*)mat->fp,nBytes,SEEK_CUR);\n\n    return matvar;\n}", "commit_link": "github.com/tbeu/matio/commit/651a8e28099edb5fbb9e4e1d4d3238848f446c9a", "file_name": "src/mat4.c", "vul_type": "cwe-125", "description": "Write a C function named `Mat_VarReadNextInfo4` that reads the next variable information from a MAT file."}
{"func_name": "add_translationname", "func_src_before": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (item[0], trname[1], trname[2]))\n            self.connection.commit()", "func_src_after": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                t = (item[0], trname[1], trname[2], )\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (?, ?, ?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new translation into a database given an item name and translation details."}
{"func_name": "read_config", "func_src_before": "    def read_config(self):\n        \"\"\"Populate the instance with settings for the config file.\n\n        If we can't find any section for the given site, error gracefully.\n\n        \"\"\"\n        defaults = {\n            \"auth_type\": \"basic\",\n            \"verify_ssl_cert\": \"true\",\n        }\n\n        cp = configparser.RawConfigParser(defaults)\n        cp.read(CONFIG_LOCATIONS)\n\n        if not cp.has_option(self.site, \"base_url\"):\n            raise exceptions.ConfigError(\"unable to find a [{}] section with \"\n                                         \"a base_url.\".format(self.site))\n\n        self.base_url = cp.get(self.site, \"base_url\").rstrip(\"/\")\n        self.username = cp.get(self.site, \"username\")\n        self.password = cp.get(self.site, \"password\")\n        self.verify_ssl_cert = cp.getboolean(self.site, \"verify_ssl_cert\")\n\n        # load auth\n        auth_type = cp.get(self.site, \"auth_type\")\n        auth_type = auth_type.lower()\n        if auth_type not in AUTH_TYPES:\n            supported_auths = \", \".join(sorted(AUTH_TYPES.keys()))\n            msg = (\"invalid auth setting '{}', supported: {}\"\n                  .format(auth_type, supported_auths))\n            raise exceptions.ConfigError(msg)\n        self.auth_type = auth_type\n\n        self.required_fields = [\"To\", \"Component\", \"Subject\", \"Priority\"]", "func_src_after": "    def read_config(self):\n        \"\"\"Populate the instance with settings for the config file.\n\n        If we can't find any section for the given site, error gracefully.\n\n        \"\"\"\n        defaults = {\n            \"auth_type\": \"basic\",\n            \"verify_ssl_cert\": \"true\",\n        }\n\n        cp = configparser.RawConfigParser(defaults)\n        cp.read(CONFIG_LOCATIONS)\n\n        if not cp.has_option(self.site, \"base_url\"):\n            raise exceptions.ConfigError(\"unable to find a [{}] section with \"\n                                         \"a base_url.\".format(self.site))\n\n        self.base_url = cp.get(self.site, \"base_url\").rstrip(\"/\")\n        self.username = cp.get(self.site, \"username\")\n        self.password = cp.get(self.site, \"password\")\n        self.verify_ssl_cert = cp.getboolean(self.site, \"verify_ssl_cert\")\n\n        # load auth\n        auth_type = cp.get(self.site, \"auth_type\")\n        auth_type = auth_type.lower()\n        if auth_type not in AUTH_TYPES:\n            supported_auths = \", \".join(sorted(AUTH_TYPES.keys()))\n            msg = (\"invalid auth setting '{}', supported: {}\"\n                  .format(auth_type, supported_auths))\n            raise exceptions.ConfigError(msg)\n        self.auth_type = auth_type\n\n        if cp.has_option(self.site, \"editor\"):\n            self.config_editor = cp.get(self.site, \"editor\")\n        else:\n            self.config_editor = os.getenv(\"EDITOR\")\n\n        self.required_fields = [\"To\", \"Component\", \"Subject\", \"Priority\"]", "line_changes": {"deleted": [], "added": [{"line_no": 34, "char_start": 1248, "char_end": 1295, "line": "        if cp.has_option(self.site, \"editor\"):\n"}, {"line_no": 35, "char_start": 1295, "char_end": 1356, "line": "            self.config_editor = cp.get(self.site, \"editor\")\n"}, {"line_no": 36, "char_start": 1356, "char_end": 1370, "line": "        else:\n"}, {"line_no": 37, "char_start": 1370, "char_end": 1423, "line": "            self.config_editor = os.getenv(\"EDITOR\")\n"}, {"line_no": 38, "char_start": 1423, "char_end": 1424, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1248, "char_end": 1424, "chars": "        if cp.has_option(self.site, \"editor\"):\n            self.config_editor = cp.get(self.site, \"editor\")\n        else:\n            self.config_editor = os.getenv(\"EDITOR\")\n\n"}]}, "commit_link": "github.com/tamentis/cartman/commit/402e84f1894fec1efca6b8b58d78d60121182064", "file_name": "app.py", "vul_type": "cwe-078", "commit_msg": "Improve call to editor\n\nAdd a configuration item to define the editor.\n\nUse subprocess.call() to avoid shell usage and escaping problems.\n\nCheck editor return value.", "parent_commit": "994c2174041ebb25d58d7fc23eb0581dcb8fb864", "description": "Write a Python function to load configuration settings from a file, handling missing sections or options with custom exceptions."}
{"func_name": "mkdir", "func_src_before": "    def mkdir(self, data, path):\n        credentials = self._formatCredentials(data, name='current')\n\n        command = (\n            '{credentials} '\n            'rclone touch current:{path}/.keep'\n        ).format(\n            credentials=credentials,\n            path=path,\n        )\n\n        try:\n            result = self._execute(command)\n            return {\n                'message': 'Success',\n            }\n        except subprocess.CalledProcessError as e:\n            raise RcloneException(sanitize(str(e)))", "func_src_after": "    def mkdir(self, data, path):\n        credentials = self._formatCredentials(data, name='current')\n        command = [\n            'rclone',\n            'touch',\n            'current:{}/.keep'.format(path),\n        ]\n\n        try:\n            result = self._execute(command, credentials)\n            return {\n                'message': 'Success',\n            }\n        except subprocess.CalledProcessError as e:\n            raise RcloneException(sanitize(str(e)))", "commit_link": "github.com/FredHutch/motuz/commit/045468cb9bff47bb3bb72268b6d5a3fe44e383db", "file_name": "src/backend/api/utils/rclone_connection.py", "vul_type": "cwe-078", "description": "Write a Python function to create a directory using rclone with error handling."}
{"func_name": "ExtractPostscript", "func_src_before": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while(PS_Size-- > 0)\n    {\n      (void) fputc(ReadBlobByte(image),ps_file);\n    }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) CopyMagickMemory(clone_info->magick,magic_info->name,MaxTextExtent);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickMemory(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}", "func_src_after": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while(PS_Size-- > 0)\n    {\n      (void) fputc(ReadBlobByte(image),ps_file);\n    }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) strncpy(clone_info->magick,magic_info->name,MaxTextExtent);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickMemory(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickMemory(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/a251039393f423c7858e63cab6aa98d17b8b7a41", "file_name": "coders/wpg.c", "vul_type": "cwe-125", "description": "Write a C function to extract a Postscript section from an image and read it into a new image structure."}
{"func_name": "_update_volume_stats", "func_src_before": "    def _update_volume_stats(self):\n        \"\"\"Retrieve stats info from volume group.\"\"\"\n\n        LOG.debug(_(\"Updating volume stats\"))\n        data = {}\n\n        data['vendor_name'] = 'IBM'\n        data['driver_version'] = '1.1'\n        data['storage_protocol'] = list(self._enabled_protocols)\n\n        data['total_capacity_gb'] = 0  # To be overwritten\n        data['free_capacity_gb'] = 0   # To be overwritten\n        data['reserved_percentage'] = 0\n        data['QoS_support'] = False\n\n        pool = self.configuration.storwize_svc_volpool_name\n        #Get storage system name\n        ssh_cmd = 'svcinfo lssystem -delim !'\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes or not attributes['name']:\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get system name'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        if not backend_name:\n            backend_name = '%s_%s' % (attributes['name'], pool)\n        data['volume_backend_name'] = backend_name\n\n        ssh_cmd = 'svcinfo lsmdiskgrp -bytes -delim ! %s' % pool\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes:\n            LOG.error(_('Could not get pool data from the storage'))\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get storage pool data'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        data['total_capacity_gb'] = (float(attributes['capacity']) /\n                                    (1024 ** 3))\n        data['free_capacity_gb'] = (float(attributes['free_capacity']) /\n                                    (1024 ** 3))\n        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']\n        data['compression_support'] = self._compression_enabled\n\n        self._stats = data", "func_src_after": "    def _update_volume_stats(self):\n        \"\"\"Retrieve stats info from volume group.\"\"\"\n\n        LOG.debug(_(\"Updating volume stats\"))\n        data = {}\n\n        data['vendor_name'] = 'IBM'\n        data['driver_version'] = '1.1'\n        data['storage_protocol'] = list(self._enabled_protocols)\n\n        data['total_capacity_gb'] = 0  # To be overwritten\n        data['free_capacity_gb'] = 0   # To be overwritten\n        data['reserved_percentage'] = 0\n        data['QoS_support'] = False\n\n        pool = self.configuration.storwize_svc_volpool_name\n        #Get storage system name\n        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes or not attributes['name']:\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get system name'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        if not backend_name:\n            backend_name = '%s_%s' % (attributes['name'], pool)\n        data['volume_backend_name'] = backend_name\n\n        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes:\n            LOG.error(_('Could not get pool data from the storage'))\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get storage pool data'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        data['total_capacity_gb'] = (float(attributes['capacity']) /\n                                    (1024 ** 3))\n        data['free_capacity_gb'] = (float(attributes['free_capacity']) /\n                                    (1024 ** 3))\n        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']\n        data['compression_support'] = self._compression_enabled\n\n        self._stats = data", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to update storage volume statistics with IBM vendor details and capacity information."}
{"func_name": "ndpi_reset_packet_line_info", "func_src_before": "static void ndpi_reset_packet_line_info(struct ndpi_packet_struct *packet) {\n  packet->parsed_lines = 0, packet->empty_line_position_set = 0, packet->host_line.ptr = NULL,\n    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "func_src_after": "static void ndpi_reset_packet_line_info(struct ndpi_packet_struct *packet) {\n  packet->parsed_lines = 0, packet->empty_line_position_set = 0, packet->host_line.ptr = NULL,\n    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->content_disposition_line.ptr = NULL,\n    packet->content_disposition_line.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "commit_link": "github.com/ntop/nDPI/commit/6a9f5e4f7c3fd5ddab3e6727b071904d76773952", "file_name": "src/lib/ndpi_main.c", "vul_type": "cwe-416", "description": "In C, write a function to reset all fields of a given `ndpi_packet_struct` to their initial state."}
{"func_name": "_delete_3par_host", "func_src_before": "    def _delete_3par_host(self, hostname):\n        self._cli_run('removehost %s' % hostname, None)", "func_src_after": "    def _delete_3par_host(self, hostname):\n        self._cli_run(['removehost', hostname])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function named `_delete_3par_host` that takes a hostname as an argument and calls a method `_cli_run` to execute a command to remove that host."}
{"func_name": "fiber_switch", "func_src_before": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  if (resume && c->status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (c->status == MRB_FIBER_RUNNING || c->status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (c->status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  mrb->c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  if (c->status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    if (len >= c->stend - c->stack) {\n      mrb_raise(mrb, E_FIBER_ERROR, \"too many arguments to fiber\");\n    }\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n  fiber_switch_context(mrb, c);\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "func_src_after": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  enum mrb_fiber_state status;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  status = c->status;\n  if (resume && status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (status == MRB_FIBER_RUNNING || status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  old_c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  fiber_switch_context(mrb, c);\n  if (status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    mrb_stack_extend(mrb, len+2); /* for receiver and (optional) block */\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "commit_link": "github.com/mruby/mruby/commit/778500563a9f7ceba996937dc886bd8cde29b42b", "file_name": "mrbgems/mruby-fiber/src/fiber.c", "vul_type": "cwe-125", "description": "Write a C function `fiber_switch` for the MRuby language that handles fiber resumption, argument passing, and optional VM execution."}
{"func_name": "stralgoLCS", "func_src_before": "void stralgoLCS(client *c) {\n    uint32_t i, j;\n    long long minmatchlen = 0;\n    sds a = NULL, b = NULL;\n    int getlen = 0, getidx = 0, withmatchlen = 0;\n    robj *obja = NULL, *objb = NULL;\n\n    for (j = 2; j < (uint32_t)c->argc; j++) {\n        char *opt = c->argv[j]->ptr;\n        int moreargs = (c->argc-1) - j;\n\n        if (!strcasecmp(opt,\"IDX\")) {\n            getidx = 1;\n        } else if (!strcasecmp(opt,\"LEN\")) {\n            getlen = 1;\n        } else if (!strcasecmp(opt,\"WITHMATCHLEN\")) {\n            withmatchlen = 1;\n        } else if (!strcasecmp(opt,\"MINMATCHLEN\") && moreargs) {\n            if (getLongLongFromObjectOrReply(c,c->argv[j+1],&minmatchlen,NULL)\n                != C_OK) goto cleanup;\n            if (minmatchlen < 0) minmatchlen = 0;\n            j++;\n        } else if (!strcasecmp(opt,\"STRINGS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            a = c->argv[j+1]->ptr;\n            b = c->argv[j+2]->ptr;\n            j += 2;\n        } else if (!strcasecmp(opt,\"KEYS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            obja = lookupKeyRead(c->db,c->argv[j+1]);\n            objb = lookupKeyRead(c->db,c->argv[j+2]);\n            if ((obja && obja->type != OBJ_STRING) ||\n                (objb && objb->type != OBJ_STRING))\n            {\n                addReplyError(c,\n                    \"The specified keys must contain string values\");\n                /* Don't cleanup the objects, we need to do that\n                 * only after calling getDecodedObject(). */\n                obja = NULL;\n                objb = NULL;\n                goto cleanup;\n            }\n            obja = obja ? getDecodedObject(obja) : createStringObject(\"\",0);\n            objb = objb ? getDecodedObject(objb) : createStringObject(\"\",0);\n            a = obja->ptr;\n            b = objb->ptr;\n            j += 2;\n        } else {\n            addReplyErrorObject(c,shared.syntaxerr);\n            goto cleanup;\n        }\n    }\n\n    /* Complain if the user passed ambiguous parameters. */\n    if (a == NULL) {\n        addReplyError(c,\"Please specify two strings: \"\n                        \"STRINGS or KEYS options are mandatory\");\n        goto cleanup;\n    } else if (getlen && getidx) {\n        addReplyError(c,\n            \"If you want both the length and indexes, please \"\n            \"just use IDX.\");\n        goto cleanup;\n    }\n\n    /* Compute the LCS using the vanilla dynamic programming technique of\n     * building a table of LCS(x,y) substrings. */\n    uint32_t alen = sdslen(a);\n    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {\n            if (i == 0 || j == 0) {\n                /* If one substring has length of zero, the\n                 * LCS length is zero. */\n                LCS(i,j) = 0;\n            } else if (a[i-1] == b[j-1]) {\n                /* The len LCS (and the LCS itself) of two\n                 * sequences with the same final character, is the\n                 * LCS of the two sequences without the last char\n                 * plus that last char. */\n                LCS(i,j) = LCS(i-1,j-1)+1;\n            } else {\n                /* If the last character is different, take the longest\n                 * between the LCS of the first string and the second\n                 * minus the last char, and the reverse. */\n                uint32_t lcs1 = LCS(i-1,j);\n                uint32_t lcs2 = LCS(i,j-1);\n                LCS(i,j) = lcs1 > lcs2 ? lcs1 : lcs2;\n            }\n        }\n    }\n\n    /* Store the actual LCS string in \"result\" if needed. We create\n     * it backward, but the length is already known, we store it into idx. */\n    uint32_t idx = LCS(alen,blen);\n    sds result = NULL;        /* Resulting LCS string. */\n    void *arraylenptr = NULL; /* Deferred length of the array for IDX. */\n    uint32_t arange_start = alen, /* alen signals that values are not set. */\n             arange_end = 0,\n             brange_start = 0,\n             brange_end = 0;\n\n    /* Do we need to compute the actual LCS string? Allocate it in that case. */\n    int computelcs = getidx || !getlen;\n    if (computelcs) result = sdsnewlen(SDS_NOINIT,idx);\n\n    /* Start with a deferred array if we have to emit the ranges. */\n    uint32_t arraylen = 0;  /* Number of ranges emitted in the array. */\n    if (getidx) {\n        addReplyMapLen(c,2);\n        addReplyBulkCString(c,\"matches\");\n        arraylenptr = addReplyDeferredLen(c);\n    }\n\n    i = alen, j = blen;\n    while (computelcs && i > 0 && j > 0) {\n        int emit_range = 0;\n        if (a[i-1] == b[j-1]) {\n            /* If there is a match, store the character and reduce\n             * the indexes to look for a new match. */\n            result[idx-1] = a[i-1];\n\n            /* Track the current range. */\n            if (arange_start == alen) {\n                arange_start = i-1;\n                arange_end = i-1;\n                brange_start = j-1;\n                brange_end = j-1;\n            } else {\n                /* Let's see if we can extend the range backward since\n                 * it is contiguous. */\n                if (arange_start == i && brange_start == j) {\n                    arange_start--;\n                    brange_start--;\n                } else {\n                    emit_range = 1;\n                }\n            }\n            /* Emit the range if we matched with the first byte of\n             * one of the two strings. We'll exit the loop ASAP. */\n            if (arange_start == 0 || brange_start == 0) emit_range = 1;\n            idx--; i--; j--;\n        } else {\n            /* Otherwise reduce i and j depending on the largest\n             * LCS between, to understand what direction we need to go. */\n            uint32_t lcs1 = LCS(i-1,j);\n            uint32_t lcs2 = LCS(i,j-1);\n            if (lcs1 > lcs2)\n                i--;\n            else\n                j--;\n            if (arange_start != alen) emit_range = 1;\n        }\n\n        /* Emit the current range if needed. */\n        uint32_t match_len = arange_end - arange_start + 1;\n        if (emit_range) {\n            if (minmatchlen == 0 || match_len >= minmatchlen) {\n                if (arraylenptr) {\n                    addReplyArrayLen(c,2+withmatchlen);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,arange_start);\n                    addReplyLongLong(c,arange_end);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,brange_start);\n                    addReplyLongLong(c,brange_end);\n                    if (withmatchlen) addReplyLongLong(c,match_len);\n                    arraylen++;\n                }\n            }\n            arange_start = alen; /* Restart at the next match. */\n        }\n    }\n\n    /* Signal modified key, increment dirty, ... */\n\n    /* Reply depending on the given options. */\n    if (arraylenptr) {\n        addReplyBulkCString(c,\"len\");\n        addReplyLongLong(c,LCS(alen,blen));\n        setDeferredArrayLen(c,arraylenptr,arraylen);\n    } else if (getlen) {\n        addReplyLongLong(c,LCS(alen,blen));\n    } else {\n        addReplyBulkSds(c,result);\n        result = NULL;\n    }\n\n    /* Cleanup. */\n    sdsfree(result);\n    zfree(lcs);\n\ncleanup:\n    if (obja) decrRefCount(obja);\n    if (objb) decrRefCount(objb);\n    return;\n}", "func_src_after": "void stralgoLCS(client *c) {\n    uint32_t i, j;\n    long long minmatchlen = 0;\n    sds a = NULL, b = NULL;\n    int getlen = 0, getidx = 0, withmatchlen = 0;\n    robj *obja = NULL, *objb = NULL;\n\n    for (j = 2; j < (uint32_t)c->argc; j++) {\n        char *opt = c->argv[j]->ptr;\n        int moreargs = (c->argc-1) - j;\n\n        if (!strcasecmp(opt,\"IDX\")) {\n            getidx = 1;\n        } else if (!strcasecmp(opt,\"LEN\")) {\n            getlen = 1;\n        } else if (!strcasecmp(opt,\"WITHMATCHLEN\")) {\n            withmatchlen = 1;\n        } else if (!strcasecmp(opt,\"MINMATCHLEN\") && moreargs) {\n            if (getLongLongFromObjectOrReply(c,c->argv[j+1],&minmatchlen,NULL)\n                != C_OK) goto cleanup;\n            if (minmatchlen < 0) minmatchlen = 0;\n            j++;\n        } else if (!strcasecmp(opt,\"STRINGS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            a = c->argv[j+1]->ptr;\n            b = c->argv[j+2]->ptr;\n            j += 2;\n        } else if (!strcasecmp(opt,\"KEYS\") && moreargs > 1) {\n            if (a != NULL) {\n                addReplyError(c,\"Either use STRINGS or KEYS\");\n                goto cleanup;\n            }\n            obja = lookupKeyRead(c->db,c->argv[j+1]);\n            objb = lookupKeyRead(c->db,c->argv[j+2]);\n            if ((obja && obja->type != OBJ_STRING) ||\n                (objb && objb->type != OBJ_STRING))\n            {\n                addReplyError(c,\n                    \"The specified keys must contain string values\");\n                /* Don't cleanup the objects, we need to do that\n                 * only after calling getDecodedObject(). */\n                obja = NULL;\n                objb = NULL;\n                goto cleanup;\n            }\n            obja = obja ? getDecodedObject(obja) : createStringObject(\"\",0);\n            objb = objb ? getDecodedObject(objb) : createStringObject(\"\",0);\n            a = obja->ptr;\n            b = objb->ptr;\n            j += 2;\n        } else {\n            addReplyErrorObject(c,shared.syntaxerr);\n            goto cleanup;\n        }\n    }\n\n    /* Complain if the user passed ambiguous parameters. */\n    if (a == NULL) {\n        addReplyError(c,\"Please specify two strings: \"\n                        \"STRINGS or KEYS options are mandatory\");\n        goto cleanup;\n    } else if (getlen && getidx) {\n        addReplyError(c,\n            \"If you want both the length and indexes, please \"\n            \"just use IDX.\");\n        goto cleanup;\n    }\n\n    /* Compute the LCS using the vanilla dynamic programming technique of\n     * building a table of LCS(x,y) substrings. */\n    uint32_t alen = sdslen(a);\n    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {\n            if (i == 0 || j == 0) {\n                /* If one substring has length of zero, the\n                 * LCS length is zero. */\n                LCS(i,j) = 0;\n            } else if (a[i-1] == b[j-1]) {\n                /* The len LCS (and the LCS itself) of two\n                 * sequences with the same final character, is the\n                 * LCS of the two sequences without the last char\n                 * plus that last char. */\n                LCS(i,j) = LCS(i-1,j-1)+1;\n            } else {\n                /* If the last character is different, take the longest\n                 * between the LCS of the first string and the second\n                 * minus the last char, and the reverse. */\n                uint32_t lcs1 = LCS(i-1,j);\n                uint32_t lcs2 = LCS(i,j-1);\n                LCS(i,j) = lcs1 > lcs2 ? lcs1 : lcs2;\n            }\n        }\n    }\n\n    /* Store the actual LCS string in \"result\" if needed. We create\n     * it backward, but the length is already known, we store it into idx. */\n    uint32_t idx = LCS(alen,blen);\n    sds result = NULL;        /* Resulting LCS string. */\n    void *arraylenptr = NULL; /* Deferred length of the array for IDX. */\n    uint32_t arange_start = alen, /* alen signals that values are not set. */\n             arange_end = 0,\n             brange_start = 0,\n             brange_end = 0;\n\n    /* Do we need to compute the actual LCS string? Allocate it in that case. */\n    int computelcs = getidx || !getlen;\n    if (computelcs) result = sdsnewlen(SDS_NOINIT,idx);\n\n    /* Start with a deferred array if we have to emit the ranges. */\n    uint32_t arraylen = 0;  /* Number of ranges emitted in the array. */\n    if (getidx) {\n        addReplyMapLen(c,2);\n        addReplyBulkCString(c,\"matches\");\n        arraylenptr = addReplyDeferredLen(c);\n    }\n\n    i = alen, j = blen;\n    while (computelcs && i > 0 && j > 0) {\n        int emit_range = 0;\n        if (a[i-1] == b[j-1]) {\n            /* If there is a match, store the character and reduce\n             * the indexes to look for a new match. */\n            result[idx-1] = a[i-1];\n\n            /* Track the current range. */\n            if (arange_start == alen) {\n                arange_start = i-1;\n                arange_end = i-1;\n                brange_start = j-1;\n                brange_end = j-1;\n            } else {\n                /* Let's see if we can extend the range backward since\n                 * it is contiguous. */\n                if (arange_start == i && brange_start == j) {\n                    arange_start--;\n                    brange_start--;\n                } else {\n                    emit_range = 1;\n                }\n            }\n            /* Emit the range if we matched with the first byte of\n             * one of the two strings. We'll exit the loop ASAP. */\n            if (arange_start == 0 || brange_start == 0) emit_range = 1;\n            idx--; i--; j--;\n        } else {\n            /* Otherwise reduce i and j depending on the largest\n             * LCS between, to understand what direction we need to go. */\n            uint32_t lcs1 = LCS(i-1,j);\n            uint32_t lcs2 = LCS(i,j-1);\n            if (lcs1 > lcs2)\n                i--;\n            else\n                j--;\n            if (arange_start != alen) emit_range = 1;\n        }\n\n        /* Emit the current range if needed. */\n        uint32_t match_len = arange_end - arange_start + 1;\n        if (emit_range) {\n            if (minmatchlen == 0 || match_len >= minmatchlen) {\n                if (arraylenptr) {\n                    addReplyArrayLen(c,2+withmatchlen);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,arange_start);\n                    addReplyLongLong(c,arange_end);\n                    addReplyArrayLen(c,2);\n                    addReplyLongLong(c,brange_start);\n                    addReplyLongLong(c,brange_end);\n                    if (withmatchlen) addReplyLongLong(c,match_len);\n                    arraylen++;\n                }\n            }\n            arange_start = alen; /* Restart at the next match. */\n        }\n    }\n\n    /* Signal modified key, increment dirty, ... */\n\n    /* Reply depending on the given options. */\n    if (arraylenptr) {\n        addReplyBulkCString(c,\"len\");\n        addReplyLongLong(c,LCS(alen,blen));\n        setDeferredArrayLen(c,arraylenptr,arraylen);\n    } else if (getlen) {\n        addReplyLongLong(c,LCS(alen,blen));\n    } else {\n        addReplyBulkSds(c,result);\n        result = NULL;\n    }\n\n    /* Cleanup. */\n    sdsfree(result);\n    zfree(lcs);\n\ncleanup:\n    if (obja) decrRefCount(obja);\n    if (objb) decrRefCount(objb);\n    return;\n}", "line_changes": {"deleted": [{"line_no": 80, "char_start": 2951, "char_end": 3016, "line": "    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n"}], "added": [{"line_no": 80, "char_start": 2951, "char_end": 3024, "line": "    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 2980, "char_end": 2988, "chars": "size_t)("}]}, "commit_link": "github.com/oranagra/redis/commit/f0c5f920d0f88bd8aa376a2c05af4902789d1ef9", "file_name": "t_string.c", "vul_type": "cwe-190", "commit_msg": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.", "parent_commit": "29900d4e6bccdf3691bedf0ea9a5d84863fa3592", "description": "Write a C function named `stralgoLCS` that computes the longest common subsequence (LCS) of two strings."}
{"func_name": "referrer_count", "func_src_before": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchone()\n\n    return jsonify(results)", "func_src_after": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer=%s\"\n    cur.execute(query, (account_id,))\n    results = cur.fetchone()\n\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to count referrer entries in a PostgreSQL database using an account ID parameter."}
{"func_name": "updateLabel", "func_src_before": "\tfunction updateLabel () {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisLabel = jQuery('#fb-new-label').val();\n\t\t\t// Update preview\n\t\t\tif (thisLabel.length === 0) {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(\"New field\");\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(thisLabel);\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].label = thisLabel;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateLabel(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tfunction updateLabel () {\n\t\ttry {\n\t\t\tvar thisId = jQuery('#fb-field-id').val();\n\t\t\tvar thisLabel = jQuery('#fb-new-label').val();\n\t\t\t// Update preview\n\t\t\tif (thisLabel.length === 0) {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( GrunionFB_i18n.newLabel );\n\t\t\t} else {\n\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( thisLabel );\n\t\t\t}\n\t\t\t// Update fbForm object\n\t\t\tfbForm.fields[thisId].label = thisLabel;\n\t\t} catch(e) {\n\t\t\tif (debug) {\n\t\t\t\tconsole.log(\"updateLabel(): \" + e);\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 7, "char_start": 185, "char_end": 264, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(\"New field\");\n"}, {"line_no": 9, "char_start": 276, "char_end": 353, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').html(thisLabel);\n"}], "added": [{"line_no": 7, "char_start": 185, "char_end": 278, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( GrunionFB_i18n.newLabel );\n"}, {"line_no": 9, "char_start": 290, "char_end": 369, "line": "\t\t\t\tjQuery('#fb-new-field' + thisId + ' label .label-text').text( thisLabel );\n"}]}, "char_changes": {"deleted": [{"char_start": 245, "char_end": 261, "chars": "html(\"New field\""}, {"char_start": 336, "char_end": 341, "chars": "html("}], "added": [{"char_start": 245, "char_end": 275, "chars": "text( GrunionFB_i18n.newLabel "}, {"char_start": 350, "char_end": 356, "chars": "text( "}, {"char_start": 365, "char_end": 366, "chars": " "}]}, "commit_link": "github.com/iamtakashi/jetpack/commit/970117f93e7ed6eb459ee568259947d67369eec0", "file_name": "grunion.js", "vul_type": "cwe-079", "commit_msg": "Grunion: Fix 2 XSS vulnerabilities.\nPreview of field labels.\nPreview of radio option labels.\nAlso:\nPrevent future potential XSS in feedback message.\nFix i18n\nFix encoding of field labels bug (every preview would add another level of HTML encoding to the label)\nprops @mdawaffe", "description": "Write a JavaScript function using jQuery to update a form label with user input or a default value."}
{"func_name": "HPHP::extractFileTo", "func_src_before": "static bool extractFileTo(zip* zip, const std::string &file, std::string& to,\n                          char* buf, size_t len) {\n  auto sep = file.rfind('/');\n  if (sep != std::string::npos) {\n    auto path = to + file.substr(0, sep);\n    if (!HHVM_FN(is_dir)(path) && !HHVM_FN(mkdir)(path, 0777, true)) {\n      return false;\n    }\n\n    if (sep == file.length() - 1) {\n      return true;\n    }\n  }\n\n  to.append(file);\n  struct zip_stat zipStat;\n  if (zip_stat(zip, file.c_str(), 0, &zipStat) != 0) {\n    return false;\n  }\n\n  auto zipFile = zip_fopen_index(zip, zipStat.index, 0);\n  FAIL_IF_INVALID_PTR(zipFile);\n\n  auto outFile = fopen(to.c_str(), \"wb\");\n  if (outFile == nullptr) {\n    zip_fclose(zipFile);\n    return false;\n  }\n\n  for (auto n = zip_fread(zipFile, buf, len); n != 0;\n       n = zip_fread(zipFile, buf, len)) {\n    if (n < 0 || fwrite(buf, sizeof(char), n, outFile) != n) {\n      zip_fclose(zipFile);\n      fclose(outFile);\n      remove(to.c_str());\n      return false;\n    }\n  }\n\n  zip_fclose(zipFile);\n  if (fclose(outFile) != 0) {\n    return false;\n  }\n\n  return true;\n}", "func_src_after": "static bool extractFileTo(zip* zip, const std::string &file, std::string& to,\n                          char* buf, size_t len) {\n\n  struct zip_stat zipStat;\n  // Verify the file to be extracted is actually in the zip file\n  if (zip_stat(zip, file.c_str(), 0, &zipStat) != 0) {\n    return false;\n  }\n\n  auto clean_file = file;\n  auto sep = std::string::npos;\n  // Normally would just use std::string::rfind here, but if we want to be\n  // consistent between Windows and Linux, even if techincally Linux won't use\n  // backslash for a separator, we are checking for both types.\n  int idx = file.length() - 1;\n  while (idx >= 0) {\n    if (FileUtil::isDirSeparator(file[idx])) {\n      sep = idx;\n      break;\n    }\n    idx--;\n  }\n  if (sep != std::string::npos) {\n    // make_relative_path so we do not try to put files or dirs in bad\n    // places. This securely \"cleans\" the file.\n    clean_file = make_relative_path(file);\n    std::string path = to + clean_file;\n    bool is_dir_only = true;\n    if (sep < file.length() - 1) { // not just a directory\n      auto clean_file_dir = HHVM_FN(dirname)(clean_file);\n      path = to + clean_file_dir.toCppString();\n      is_dir_only = false;\n    }\n\n    // Make sure the directory path to extract to exists or can be created\n    if (!HHVM_FN(is_dir)(path) && !HHVM_FN(mkdir)(path, 0777, true)) {\n      return false;\n    }\n\n    // If we have a good directory to extract to above, we now check whether\n    // the \"file\" parameter passed in is a directory or actually a file.\n    if (is_dir_only) { // directory, like /usr/bin/\n      return true;\n    }\n    // otherwise file is actually a file, so we actually extract.\n  }\n\n  // We have ensured that clean_file will be added to a relative path by the\n  // time we get here.\n  to.append(clean_file);\n\n  auto zipFile = zip_fopen_index(zip, zipStat.index, 0);\n  FAIL_IF_INVALID_PTR(zipFile);\n\n  auto outFile = fopen(to.c_str(), \"wb\");\n  if (outFile == nullptr) {\n    zip_fclose(zipFile);\n    return false;\n  }\n\n  for (auto n = zip_fread(zipFile, buf, len); n != 0;\n       n = zip_fread(zipFile, buf, len)) {\n    if (n < 0 || fwrite(buf, sizeof(char), n, outFile) != n) {\n      zip_fclose(zipFile);\n      fclose(outFile);\n      remove(to.c_str());\n      return false;\n    }\n  }\n\n  zip_fclose(zipFile);\n  if (fclose(outFile) != 0) {\n    return false;\n  }\n\n  return true;\n}", "commit_link": "github.com/facebook/hhvm/commit/65c95a01541dd2fbc9c978ac53bed235b5376686", "file_name": "hphp/runtime/ext/zip/ext_zip.cpp", "vul_type": "cwe-022", "description": "Write a C++ function to extract a file from a zip archive to a specified directory, handling directory creation and path sanitization."}
{"func_name": "update_inverter", "func_src_before": "    def update_inverter(self, inverter_serial, ts, status, etoday, etotal):\n        query = '''\n            UPDATE Inverters\n            SET     \n                TimeStamp='%s', \n                Status='%s', \n                eToday='%s',\n                eTotal='%s'\n            WHERE Serial='%s';\n        ''' % (ts, status, etoday, etotal, inverter_serial)\n        self.c.execute(query)", "func_src_after": "    def update_inverter(self, inverter_serial, ts, status, etoday, etotal):\n        query = '''\n            UPDATE Inverters\n            SET     \n                TimeStamp=?, \n                Status=?, \n                eToday=?,\n                eTotal=?\n            WHERE Serial=?;\n        '''\n        self.c.execute(query, (ts, status, etoday, etotal, inverter_serial))", "commit_link": "github.com/philipptrenz/s0-bridge/commit/269b48caa05377b7c58c3e6d1622a4429cb5ba65", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function to update an inverter's status and energy readings in a database using SQL."}
{"func_name": "init_settings", "func_src_before": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = jinja_env_options if jinja_env_options else {}\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "func_src_after": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = {\"autoescape\": True}\n        jenv_opt.update(jinja_env_options if jinja_env_options else {})\n\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "commit_link": "github.com/ipython/ipython/commit/3ab41641cf6fce3860c73d5cf4645aa12e1e5892", "file_name": "IPython/html/notebookapp.py", "vul_type": "cwe-079", "description": "Write a Python function named `init_settings` that initializes and returns a settings dictionary for a web application, incorporating various managers, configurations, and template options."}
{"func_name": "get_roster", "func_src_before": "    def get_roster(self, server_id):\n        sql = \"\"\"SELECT username, role\n                 FROM roles\n                 WHERE roles.server_id = {0};\n                 \"\"\".format(server_id)\n        self.cur.execute(sql)\n        return self.cur.fetchall()", "func_src_after": "    def get_roster(self, server_id):\n        sql = \"\"\"\n              SELECT username, role\n              FROM roles\n              WHERE roles.server_id = %s;\n              \"\"\"\n        self.cur.execute(sql, (server_id,))\n        return self.cur.fetchall()", "commit_link": "github.com/jgayfer/spirit/commit/01c846c534c8d3cf6763f8b7444a0efe2caa3799", "file_name": "db/dbase.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch a list of usernames and roles from a database table 'roles' filtered by 'server_id'."}
{"func_name": "auto_complete_for_user_name", "func_src_before": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n  end", "func_src_after": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 73, "char_end": 189, "line": "    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n"}], "added": [{"line_no": 3, "char_start": 73, "char_end": 155, "line": "    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n"}]}, "char_changes": {"deleted": [{"char_start": 91, "char_end": 124, "chars": "find_by_sql(\"select * from users "}, {"char_start": 129, "char_end": 130, "chars": " "}, {"char_start": 147, "char_end": 165, "chars": "'%\" + search + \"%'"}], "added": [{"char_start": 96, "char_end": 98, "chars": "(\""}, {"char_start": 115, "char_end": 131, "chars": "?\", \"%#{search}%"}]}, "commit_link": "github.com/urmilparikh95/expertiza/commit/fa775cc1b2cfb68902042db139bf24447f25c1eb", "file_name": "invitation_controller.rb", "vul_type": "cwe-089", "commit_msg": "Handle possible SQL injections.", "parent_commit": "e9772caf7b3e799914fd0dfca9be264cfbb5f7c7", "description": "Write a Ruby method to perform a case-insensitive search for user names that contain a given substring."}
{"func_name": "parse_hid_report_descriptor", "func_src_before": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i];\n\n\t\t/* Skip over prefix */\n\t\ti++;\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = PREF_SIZE(prefix);\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tsize = 4;\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "func_src_after": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i++];\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = (1U << PREF_SIZE(prefix)) >> 1;\n\t\tif (i + size > length) {\n\t\t\tdev_err(ddev,\n\t\t\t\t\"Not enough data (need %d, have %d)\\n\",\n\t\t\t\ti + size, length);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/a50829479f58416a013a4ccca791336af3c584c7", "file_name": "drivers/input/tablet/gtco.c", "vul_type": "cwe-125", "description": "Write a C function to parse a HID report descriptor and extract device information."}
{"func_name": "get_first_ranked_month", "func_src_before": "def get_first_ranked_month(db, scene, player):\n    sql = \"select date from ranks where scene='{}' and player='{}' order by date limit 1;\".format(scene, player)\n    res = db.exec(sql)\n    date = res[0][0]\n    return date", "func_src_after": "def get_first_ranked_month(db, scene, player):\n    sql = \"select date from ranks where scene='{scene}' and player='{player}' order by date limit 1;\"\n    args = {'scene': scene, 'player': player}\n    res = db.exec(sql, args)\n    date = res[0][0]\n    return date", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "bracket_utils.py", "vul_type": "cwe-089", "description": "Write a Python function to retrieve the earliest ranking date for a specific player and scene from a database."}
{"func_name": "get_list_context", "func_src_before": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "func_src_after": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "commit_link": "github.com/omirajkar/bench_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "file_name": "frappe/website/doctype/blog_post/blog_post.py", "vul_type": "cwe-079", "description": "Write a Python function in Frappe to customize the context of a blog list page based on filters like category, blogger, or search text."}
{"func_name": "_exec_cmd", "func_src_before": "    def _exec_cmd(self, cmd):\n        \"\"\"Executes adb commands in a new shell.\n\n        This is specific to executing adb binary because stderr is not a good\n        indicator of cmd execution status.\n\n        Args:\n            cmds: A string that is the adb command to execute.\n\n        Returns:\n            The output of the adb command run if exit code is 0.\n\n        Raises:\n            AdbError is raised if the adb command exit code is not 0.\n        \"\"\"\n        proc = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        (out, err) = proc.communicate()\n        ret = proc.returncode\n        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s', cmd, out,\n                      err, ret)\n        if ret == 0:\n            return out\n        else:\n            raise AdbError(cmd=cmd, stdout=out, stderr=err, ret_code=ret)", "func_src_after": "    def _exec_cmd(self, args, shell):\n        \"\"\"Executes adb commands.\n\n        Args:\n            args: string or list of strings, program arguments.\n                See subprocess.Popen() documentation.\n            shell: bool, True to run this command through the system shell,\n                False to invoke it directly. See subprocess.Popen() docs.\n\n        Returns:\n            The output of the adb command run if exit code is 0.\n\n        Raises:\n            AdbError is raised if the adb command exit code is not 0.\n        \"\"\"\n        proc = subprocess.Popen(\n            args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=shell)\n        (out, err) = proc.communicate()\n        ret = proc.returncode\n        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s', args, out,\n                      err, ret)\n        if ret == 0:\n            return out\n        else:\n            raise AdbError(cmd=args, stdout=out, stderr=err, ret_code=ret)", "commit_link": "github.com/google/mobly/commit/3862e8ba359040fbdd6e1a6d36e51d07cda8e1ee", "file_name": "mobly/controllers/android_device_lib/adb.py", "vul_type": "cwe-078", "description": "In Python, write a function to execute adb commands and handle the output based on the command's success or failure."}
{"func_name": "_delete_host", "func_src_before": "    def _delete_host(self, host_name):\n        \"\"\"Delete a host on the storage system.\"\"\"\n\n        LOG.debug(_('enter: _delete_host: host %s ') % host_name)\n\n        ssh_cmd = 'svctask rmhost %s ' % host_name\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_delete_host', ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _delete_host: host %s ') % host_name)", "func_src_after": "    def _delete_host(self, host_name):\n        \"\"\"Delete a host on the storage system.\"\"\"\n\n        LOG.debug(_('enter: _delete_host: host %s ') % host_name)\n\n        ssh_cmd = ['svctask', 'rmhost', host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_delete_host', ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _delete_host: host %s ') % host_name)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to delete a host from a storage system using SSH commands, with debug logging before and after the operation."}
{"func_name": "set_pre_prov_vars", "func_src_before": "  def set_pre_prov_vars\n    @layout = \"miq_request_vm\"\n    @edit = {}\n    @edit[:explorer] = @explorer\n    @edit[:vm_sortdir] ||= \"ASC\"\n    @edit[:vm_sortcol] ||= \"name\"\n    @edit[:prov_type] = \"VM Provision\"\n    @edit[:hide_deprecated_templates] = true if request.parameters[:controller] == \"vm_cloud\"\n\n    unless %w(image_miq_request_new miq_template_miq_request_new).include?(params[:pressed])\n      report_name = \"ProvisionTemplates.yaml\"\n      path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", report_name).to_s\n      @view = MiqReport.new(YAML.load(File.open(path_to_report)))\n      @view.db = get_template_kls.to_s\n      report_scopes = %i(eligible_for_provisioning non_deprecated)\n      options = {\n        :model         => @view.db,\n        :gtl_type      => \"table\",\n        :named_scope   => report_scopes,\n        :report_name   => report_name,\n        :custom_action => {\n          :url  => \"/miq_request/pre_prov/?sel_id=\",\n          :type => 'provisioning'\n        }\n      }\n\n      @report_data_additional_options = ApplicationController::ReportDataAdditionalOptions.from_options(options)\n      @report_data_additional_options.with_no_checkboxes(true)\n\n      @edit[:template_kls] = get_template_kls\n    end\n    session[:changed] = false # Turn off the submit button\n    @edit[:explorer] = true if @explorer\n    @in_a_form = true\n  end", "func_src_after": "  def set_pre_prov_vars\n    @layout = \"miq_request_vm\"\n    @edit = {}\n    @edit[:explorer] = @explorer\n    @edit[:vm_sortdir] ||= \"ASC\"\n    @edit[:vm_sortcol] ||= \"name\"\n    @edit[:prov_type] = \"VM Provision\"\n    @edit[:hide_deprecated_templates] = true if request.parameters[:controller] == \"vm_cloud\"\n\n    unless %w(image_miq_request_new miq_template_miq_request_new).include?(params[:pressed])\n      report_name = \"ProvisionTemplates.yaml\"\n      path_to_report = ManageIQ::UI::Classic::Engine.root.join(\"product\", \"views\", report_name).to_s\n      @view = MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n      @view.db = get_template_kls.to_s\n      report_scopes = %i(eligible_for_provisioning non_deprecated)\n      options = {\n        :model         => @view.db,\n        :gtl_type      => \"table\",\n        :named_scope   => report_scopes,\n        :report_name   => report_name,\n        :custom_action => {\n          :url  => \"/miq_request/pre_prov/?sel_id=\",\n          :type => 'provisioning'\n        }\n      }\n\n      @report_data_additional_options = ApplicationController::ReportDataAdditionalOptions.from_options(options)\n      @report_data_additional_options.with_no_checkboxes(true)\n\n      @edit[:template_kls] = get_template_kls\n    end\n    session[:changed] = false # Turn off the submit button\n    @edit[:explorer] = true if @explorer\n    @in_a_form = true\n  end", "line_changes": {"deleted": [{"line_no": 13, "char_start": 544, "char_end": 610, "line": "      @view = MiqReport.new(YAML.load(File.open(path_to_report)))\n"}], "added": [{"line_no": 13, "char_start": 544, "char_end": 625, "line": "      @view = MiqReport.new(YAML.safe_load(File.open(path_to_report), [Symbol]))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 577, "char_end": 582, "chars": "safe_"}, {"char_start": 612, "char_end": 622, "chars": ", [Symbol]"}]}, "commit_link": "github.com/ManageIQ/manageiq-ui-classic/commit/b199ca1d7b5049ee4c0bd8323ea3977bf3de3341", "file_name": "miq_request_methods.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML.safe_load instead of YAML.load", "parent_commit": "cf54d8a126322759948bcf196e99a1a4399c62d0", "description": "Write a Ruby method named `set_pre_prov_vars` that initializes instance variables for VM provisioning settings and loads a YAML report configuration."}
{"func_name": "patch", "func_src_before": "    @jwt_required\n    def patch(self, user_id):\n        \"\"\" Replaces information of corresponding user_id with request body \"\"\"\n        query = f\"\"\"update users set user_id = %s \"\"\"\n        query += f\"\"\"where user_id = '{user_id}'\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['user_id'], )\n        database_utilities.execute_query(query, parameters)", "func_src_after": "    @jwt_required\n    def patch(self, user_id):\n        \"\"\" Replaces information of corresponding user_id with request body \"\"\"\n        query = f\"\"\"update users set user_id = %s \"\"\"\n        query += f\"\"\"where user_id = %s\"\"\"\n        json_data = request.get_json()\n        parameters = (json_data['user_id'], user_id)\n        database_utilities.execute_query(query, parameters)", "commit_link": "github.com/sgosal2/tiger-boards-backend/commit/4670109dd613df2f2fe7e8403ebd149df2b55485", "file_name": "apis/users.py", "vul_type": "cwe-089", "description": "Write a Python function to update a user's ID in the database using JWT authentication and the user's ID from the request body."}
{"func_name": "get", "func_src_before": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError as ex:\n      self.response.write(ex.message)\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(gae_users.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "func_src_after": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError:\n      self.response.write('invalid build id')\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(self.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "commit_link": "github.com/asdfghjjklllllaaa/infra/commit/2f39f3df54fb79b56744f00bcf97583b3807851f", "file_name": "appengine/cr-buildbucket/handlers.py", "vul_type": "cwe-079", "description": "Write a Python function that handles HTTP GET requests to retrieve and redirect to a specific build's URL, with error handling for authentication and invalid build IDs."}
{"func_name": "encryptPassword", "func_src_before": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n    }", "func_src_after": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return bcrypt.hashSync(password, 10);\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 76, "char_end": 160, "line": "        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n"}], "added": [{"line_no": 3, "char_start": 76, "char_end": 122, "line": "        return bcrypt.hashSync(password, 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 96, "char_end": 157, "chars": "o.createHmac('sha1', this.salt).update(password).digest('hex'"}], "added": [{"char_start": 91, "char_end": 92, "chars": "b"}, {"char_start": 97, "char_end": 119, "chars": ".hashSync(password, 10"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `encryptPassword` that takes a password string and returns its hashed version."}
{"func_name": "tflite::Subgraph::Invoke", "func_src_before": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n\n  // This is only needed for UseNNAPI(true);\n  if (should_apply_nnapi_delegate_ && !applied_nnapi_delegate_) {\n    TF_LITE_ENSURE_OK(&context_, ModifyGraphWithDelegate(NnApiDelegate()));\n    // only need to modify the graph once upon the first invocation.\n    applied_nnapi_delegate_ = true;\n  }\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    // TODO(ycling): This is an extra loop through inputs to check if the data\n    // need to be copied from Delegate buffer to raw memory, which is often not\n    // needed. We may want to cache this in prepare to know if this needs to be\n    // done for a node or not.\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}", "func_src_after": "TfLiteStatus Subgraph::Invoke() {\n  if (!consistent_) {\n    ReportError(\"Invoke called on model that is not consistent.\");\n    return kTfLiteError;\n  }\n\n  TfLiteStatus status = kTfLiteOk;\n  if (state_ == kStateUninvokable) {\n    ReportError(\"Invoke called on model that is not ready.\");\n    return kTfLiteError;\n  } else if (memory_planner_ && !memory_planner_->HasNonPersistentMemory()) {\n    ReportError(\"Non-persistent memory is not available.\");\n    return kTfLiteError;\n  }\n\n  // This is only needed for UseNNAPI(true);\n  if (should_apply_nnapi_delegate_ && !applied_nnapi_delegate_) {\n    TF_LITE_ENSURE_OK(&context_, ModifyGraphWithDelegate(NnApiDelegate()));\n    // only need to modify the graph once upon the first invocation.\n    applied_nnapi_delegate_ = true;\n  }\n\n  // Invocations are always done in node order.\n  // Note that calling Invoke repeatedly will cause the original memory plan to\n  // be reused, unless either ResizeInputTensor() or AllocateTensors() has been\n  // called.\n  for (int execution_plan_index = 0;\n       execution_plan_index < execution_plan_.size(); execution_plan_index++) {\n    if (execution_plan_index == next_execution_plan_index_to_prepare_) {\n      TF_LITE_ENSURE_STATUS(PrepareOpsAndTensors());\n      TF_LITE_ENSURE(&context_, next_execution_plan_index_to_prepare_ >=\n                                    execution_plan_index);\n    }\n    int node_index = execution_plan_[execution_plan_index];\n    TfLiteNode& node = nodes_and_registration_[node_index].first;\n    const TfLiteRegistration& registration =\n        nodes_and_registration_[node_index].second;\n\n    const char* op_name = nullptr;\n    if (profiler_) op_name = GetTFLiteOpName(registration);\n    TFLITE_SCOPED_TAGGED_OPERATOR_PROFILE(profiler_.get(), op_name, node_index);\n\n    // TODO(ycling): This is an extra loop through inputs to check if the data\n    // need to be copied from Delegate buffer to raw memory, which is often not\n    // needed. We may want to cache this in prepare to know if this needs to be\n    // done for a node or not.\n    for (int i = 0; i < node.inputs->size; ++i) {\n      int tensor_index = node.inputs->data[i];\n      if (tensor_index == kTfLiteOptionalTensor) {\n        continue;\n      }\n      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n      if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n          // In general, having a tensor here with no buffer will be an error.\n          // However, for the reshape operator, the second input tensor is only\n          // used for the shape, not for the data. Thus, null buffer is ok.\n          continue;\n        } else {\n          // In all other cases, we need to return an error as otherwise we will\n          // trigger a null pointer dereference (likely).\n          ReportError(\"Input tensor %d lacks data\", tensor_index);\n          return kTfLiteError;\n        }\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) != kTfLiteOk) {\n      return ReportOpError(&context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n\n      // This happens when an intermediate dynamic tensor is resized.\n      // We don't have to prepare all the ops, but we need to recompute\n      // the allocation plan.\n      if (next_execution_plan_index_to_plan_allocation_ >\n          next_execution_plan_index_to_prepare_) {\n        next_execution_plan_index_to_plan_allocation_ =\n            next_execution_plan_index_to_prepare_;\n        if (memory_planner_) {\n          TF_LITE_ENSURE_STATUS(memory_planner_->ResetAllocationsAfter(\n              next_execution_plan_index_to_plan_allocation_ - 1));\n        }\n      }\n    }\n  }\n\n  return status;\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/0b5662bc2be13a8c8f044d925d87fb6e56247cd8", "file_name": "tensorflow/lite/core/subgraph.cc", "vul_type": "cwe-476", "description": "Write a C++ function named `Invoke` in the `Subgraph` class of TensorFlow Lite that checks model consistency, prepares tensors, and invokes operations in order."}
{"func_name": "login_page", "func_src_before": "def login_page():\n  if request.method == 'POST':\n    login_email = request.form['login_email']\n    # print( \"%s\" % login_email)\n    login_password = request.form['login_password']\n    hashed_login_password = pwd_context.encrypt(login_password)\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n      statement = \"\"\"SELECT USERNAME FROM USERS WHERE USERNAME = %s\"\"\"\n      cursor.execute(statement, [login_email])\n      db_username = cursor.fetchone()\n\n      if db_username is not None:  # check whether the user exists\n        print('%s' % db_username)\n        user = load_user(db_username);\n        login_user(user);\n        print(\"%s\" % user.username)\n        # print('%s %s' % db_username[0][0], db_username[0][1] ) if the fetchall method is used debug using this line\n\n    return render_template('home.html')\n  else:\n    return render_template('login.html')", "func_src_after": "def login_page():\n  if request.method == 'POST':\n    login_email = request.form['login_email']\n    # print( \"%s\" % login_email)\n    login_password = request.form['login_password']\n    hashed_login_password = pwd_context.encrypt(login_password)\n\n    with dbapi2.connect(current_app.config['dsn']) as connection:\n      cursor = connection.cursor()\n      statement = \"\"\"SELECT USERNAME FROM USERS WHERE USERNAME = %s\"\"\"\n      cursor.execute(statement, [login_email])\n      db_username = cursor.fetchone()\n\n      if db_username is not None:  # check whether the user exists\n        print('%s' % db_username)\n        user = load_user(db_username)\n        login_user(user)\n        print(\"%s\" % user.username)\n        # print('%s %s' % db_username[0][0], db_username[0][1] ) if the fetchall method is used debug using this line\n\n    return render_template('home.html')\n  else:\n    return render_template('login.html')", "line_changes": {"deleted": [{"line_no": 16, "char_start": 604, "char_end": 643, "line": "        user = load_user(db_username);\n"}, {"line_no": 17, "char_start": 643, "char_end": 669, "line": "        login_user(user);\n"}], "added": [{"line_no": 16, "char_start": 604, "char_end": 642, "line": "        user = load_user(db_username)\n"}, {"line_no": 17, "char_start": 642, "char_end": 667, "line": "        login_user(user)\n"}]}, "char_changes": {"deleted": [{"char_start": 641, "char_end": 642, "chars": ";"}, {"char_start": 667, "char_end": 668, "chars": ";"}], "added": []}, "commit_link": "github.com/itucsdb1705/itucsdb1705/commit/252c65001a21f332da50e7d898d07285b3abd655", "file_name": "login.py", "vul_type": "cwe-089", "commit_msg": "SQL injection is prevented for user login\n\nPlaceholders are used to prevent SQL injection", "description": "Create a Python function for handling user login that checks credentials and renders different templates based on the HTTP method."}
{"func_name": "login", "func_src_before": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = '{}' AND password = '{}'\n            LIMIT 1\n        \"\"\".format(username, password)\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query)\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "func_src_after": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = ? AND password = ?\n            LIMIT 1\n        \"\"\"\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query, (username, password))\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "commit_link": "github.com/AnetaStoycheva/Programming101_HackBulgaria/commit/c0d6f4b8fe83a375832845a45952b5153e4c34f3", "file_name": "Week_9/sql_manager.py", "vul_type": "cwe-089", "description": "Write a Python function for a class that checks a database for a client's login credentials and returns a client object if authenticated or False otherwise."}
{"func_name": "updateDevice", "func_src_before": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "func_src_after": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\t*pp = p->next;\t/* remove \"p\" from the list */\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "commit_link": "github.com/miniupnp/miniupnp/commit/cd506a67e174a45c6a202eff182a712955ed6d6f", "file_name": "minissdpd/minissdpd.c", "vul_type": "cwe-416", "description": "In C, write a function `updateDevice` to manage a device list by updating an existing device or adding a new one based on provided headers and a timestamp."}
{"func_name": "AP4_HdlrAtom::AP4_HdlrAtom", "func_src_before": "AP4_HdlrAtom::AP4_HdlrAtom(AP4_UI32        size, \n                           AP4_UI08        version,\n                           AP4_UI32        flags,\n                           AP4_ByteStream& stream) :\n    AP4_Atom(AP4_ATOM_TYPE_HDLR, size, version, flags)\n{\n    AP4_UI32 predefined;\n    stream.ReadUI32(predefined);\n    stream.ReadUI32(m_HandlerType);\n    stream.ReadUI32(m_Reserved[0]);\n    stream.ReadUI32(m_Reserved[1]);\n    stream.ReadUI32(m_Reserved[2]);\n    \n    // read the name unless it is empty\n    int name_size = size-(AP4_FULL_ATOM_HEADER_SIZE+20);\n    if (name_size == 0) return;\n    char* name = new char[name_size+1];\n    stream.Read(name, name_size);\n    name[name_size] = '\\0'; // force a null termination\n    // handle a special case: the Quicktime files have a pascal\n    // string here, but ISO MP4 files have a C string.\n    // we try to detect a pascal encoding and correct it.\n    if (name[0] == name_size-1) {\n        m_HandlerName = name+1;\n    } else {\n        m_HandlerName = name;\n    }\n    delete[] name;\n}", "func_src_after": "AP4_HdlrAtom::AP4_HdlrAtom(AP4_UI32        size, \n                           AP4_UI08        version,\n                           AP4_UI32        flags,\n                           AP4_ByteStream& stream) :\n    AP4_Atom(AP4_ATOM_TYPE_HDLR, size, version, flags)\n{\n    AP4_UI32 predefined;\n    stream.ReadUI32(predefined);\n    stream.ReadUI32(m_HandlerType);\n    stream.ReadUI32(m_Reserved[0]);\n    stream.ReadUI32(m_Reserved[1]);\n    stream.ReadUI32(m_Reserved[2]);\n    \n    // read the name unless it is empty\n    if (size < AP4_FULL_ATOM_HEADER_SIZE+20) return;\n    AP4_UI32 name_size = size-(AP4_FULL_ATOM_HEADER_SIZE+20);\n    char* name = new char[name_size+1];\n    if (name == NULL) return;\n    stream.Read(name, name_size);\n    name[name_size] = '\\0'; // force a null termination\n    // handle a special case: the Quicktime files have a pascal\n    // string here, but ISO MP4 files have a C string.\n    // we try to detect a pascal encoding and correct it.\n    if (name[0] == name_size-1) {\n        m_HandlerName = name+1;\n    } else {\n        m_HandlerName = name;\n    }\n    delete[] name;\n}", "commit_link": "github.com/axiomatic-systems/Bento4/commit/22192de5367fa0cee985917f092be4060b7c00b0", "file_name": "Source/C++/Core/Ap4HdlrAtom.cpp", "vul_type": "cwe-476", "description": "Write a C++ constructor for the `AP4_HdlrAtom` class that initializes an atom and reads its handler type and name from a byte stream."}
{"func_name": "lys_restr_dup", "func_src_before": "lys_restr_dup(struct lys_module *mod, struct lys_restr *old, int size, int shallow, struct unres_schema *unres)\n{\n    struct lys_restr *result;\n    int i;\n\n    if (!size) {\n        return NULL;\n    }\n\n    result = calloc(size, sizeof *result);\n    LY_CHECK_ERR_RETURN(!result, LOGMEM(mod->ctx), NULL);\n\n    for (i = 0; i < size; i++) {\n        result[i].ext_size = old[i].ext_size;\n        lys_ext_dup(mod->ctx, mod, old[i].ext, old[i].ext_size, &result[i], LYEXT_PAR_RESTR, &result[i].ext, shallow, unres);\n        result[i].expr = lydict_insert(mod->ctx, old[i].expr, 0);\n        result[i].dsc = lydict_insert(mod->ctx, old[i].dsc, 0);\n        result[i].ref = lydict_insert(mod->ctx, old[i].ref, 0);\n        result[i].eapptag = lydict_insert(mod->ctx, old[i].eapptag, 0);\n        result[i].emsg = lydict_insert(mod->ctx, old[i].emsg, 0);\n    }\n\n    return result;\n}", "func_src_after": "lys_restr_dup(struct lys_module *mod, struct lys_restr *old, int size, int shallow, struct unres_schema *unres)\n{\n    struct lys_restr *result;\n    int i;\n\n    if (!size) {\n        return NULL;\n    }\n\n    result = calloc(size, sizeof *result);\n    LY_CHECK_ERR_RETURN(!result, LOGMEM(mod->ctx), NULL);\n\n    for (i = 0; i < size; i++) {\n        /* copying unresolved extensions is not supported */\n        if (unres_schema_find(unres, -1, (void *)&old[i].ext, UNRES_EXT) == -1) {\n            result[i].ext_size = old[i].ext_size;\n            lys_ext_dup(mod->ctx, mod, old[i].ext, old[i].ext_size, &result[i], LYEXT_PAR_RESTR, &result[i].ext, shallow, unres);\n        }\n        result[i].expr = lydict_insert(mod->ctx, old[i].expr, 0);\n        result[i].dsc = lydict_insert(mod->ctx, old[i].dsc, 0);\n        result[i].ref = lydict_insert(mod->ctx, old[i].ref, 0);\n        result[i].eapptag = lydict_insert(mod->ctx, old[i].eapptag, 0);\n        result[i].emsg = lydict_insert(mod->ctx, old[i].emsg, 0);\n    }\n\n    return result;\n}", "commit_link": "github.com/CESNET/libyang/commit/7852b272ef77f8098c35deea6c6f09cb78176f08", "file_name": "src/tree_schema.c", "vul_type": "cwe-476", "description": "Write a C function `lys_restr_dup` to duplicate an array of `lys_restr` structures, handling memory allocation and deep copying of contained strings and extensions."}
{"func_name": "setUp", "func_src_before": "  def setUp(self):\n    self._dump_root = tempfile.mktemp()\n    os.mkdir(self._dump_root)", "func_src_after": "  def setUp(self):\n    self._dump_root = tempfile.mkdtemp()", "line_changes": {"deleted": [{"line_no": 2, "char_start": 19, "char_end": 59, "line": "    self._dump_root = tempfile.mktemp()\n"}, {"line_no": 3, "char_start": 59, "char_end": 88, "line": "    os.mkdir(self._dump_root)\n"}], "added": [{"line_no": 2, "char_start": 19, "char_end": 59, "line": "    self._dump_root = tempfile.mkdtemp()\n"}]}, "char_changes": {"deleted": [{"char_start": 58, "char_end": 88, "chars": "\n    os.mkdir(self._dump_root)"}], "added": [{"char_start": 52, "char_end": 53, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/9b1fe8f31ee1788208d8d6b7385382e436c5e1d7", "file_name": "debug_data_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp` to create directories.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do: just a name change\n\nPiperOrigin-RevId: 420370858\nChange-Id: I44a0849d161132eacd4f3881fdb615e09c0f02a2", "description": "Write a Python function called `setUp` that initializes a temporary directory for storing data."}
{"func_name": "ConfigureJMeterMojo::extractConfigSettings", "func_src_before": "    private void extractConfigSettings(Artifact artifact) throws MojoExecutionException {// NOSONAR\n        try (JarFile configSettings = new JarFile(artifact.getFile())) {\n            Enumeration<JarEntry> entries = configSettings.entries();\n            while (entries.hasMoreElements()) {\n                JarEntry jarFileEntry = entries.nextElement();\n                // Only interested in files in the /bin directory that are not properties files\n                if (!jarFileEntry.isDirectory() && jarFileEntry.getName().startsWith(\"bin\") && !jarFileEntry.getName().endsWith(\".properties\")) {\n                    //FIXME add a test to check directory creation with multiple child directories\n                    Files.createDirectories(jmeterDirectoryPath.resolve(new File(jarFileEntry.getName()).getParentFile().getPath()));\n                    Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName()));\n                }\n            }\n        } catch (IOException e) {\n            throw new MojoExecutionException(e.getMessage(), e);\n        }\n    }", "func_src_after": "    private void extractConfigSettings(Artifact artifact) throws MojoExecutionException {// NOSONAR\n        try (JarFile configSettings = new JarFile(artifact.getFile())) {\n            Enumeration<JarEntry> entries = configSettings.entries();\n            while (entries.hasMoreElements()) {\n                JarEntry jarFileEntry = entries.nextElement();\n                // Only interested in files in the /bin directory that are not properties files\n                if (!jarFileEntry.isDirectory() && jarFileEntry.getName().startsWith(\"bin\") && !jarFileEntry.getName().endsWith(\".properties\")) {\n                    //FIXME add a test to check directory creation with multiple child directories\n                    Files.createDirectories(jmeterDirectoryPath.resolve(new File(jarFileEntry.getName()).getParentFile().getPath()));\n                    final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n                        throw new RuntimeException(\"Bad zip entry\");\n                    }\n                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath);\n                }\n            }\n        } catch (IOException e) {\n            throw new MojoExecutionException(e.getMessage(), e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 10, "char_start": 829, "char_end": 959, "line": "                    Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName()));\n"}], "added": [{"line_no": 10, "char_start": 829, "char_end": 928, "line": "                    final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n"}, {"line_no": 11, "char_start": 928, "char_end": 1025, "line": "                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n"}, {"line_no": 12, "char_start": 1025, "char_end": 1094, "line": "                        throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 13, "char_start": 1094, "char_end": 1116, "line": "                    }\n"}, {"line_no": 14, "char_start": 1116, "char_end": 1206, "line": "                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath);\n"}]}, "char_changes": {"deleted": [{"char_start": 849, "char_end": 956, "chars": "Files.copy(configSettings.getInputStream(jarFileEntry), jmeterDirectoryPath.resolve(jarFileEntry.getName())"}], "added": [{"char_start": 849, "char_end": 1203, "chars": "final Path zipEntryPath = jmeterDirectoryPath.resolve(jarFileEntry.getName());\n                    if (!zipEntryPath.normalize().startsWith(jmeterDirectoryPath.normalize())) {\n                        throw new RuntimeException(\"Bad zip entry\");\n                    }\n                    Files.copy(configSettings.getInputStream(jarFileEntry),zipEntryPath"}]}, "commit_link": "github.com/jmeter-maven-plugin/jmeter-maven-plugin/commit/caa11b14b535be82797b8796cceb89234c5b382b", "file_name": "ConfigureJMeterMojo.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to extract non-properties files from the /bin directory within a JAR file and copy them to a specified directory."}
{"func_name": "PropertiesWidget::loadTorrentInfos", "func_src_before": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(m_torrent->comment()));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(m_torrent->creator());\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "func_src_after": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(Utils::String::toHtmlEscaped(m_torrent->comment())));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(Utils::String::toHtmlEscaped(m_torrent->creator()));\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "commit_link": "github.com/qbittorrent/qBittorrent/commit/6ca3e4f094da0a0017cb2d483ec1db6176bb0b16", "file_name": "src/gui/properties/propertieswidget.cpp", "vul_type": "cwe-079", "description": "Write a C++ function in the PropertiesWidget class that loads and displays torrent information using the BitTorrent::TorrentHandle object."}
{"func_name": "_inject_net_into_fs", "func_src_before": "def _inject_net_into_fs(net, fs, execute=None):\n    \"\"\"Inject /etc/network/interfaces into the filesystem rooted at fs.\n\n    net is the contents of /etc/network/interfaces.\n    \"\"\"\n    netdir = os.path.join(os.path.join(fs, 'etc'), 'network')\n    utils.execute('mkdir', '-p', netdir, run_as_root=True)\n    utils.execute('chown', 'root:root', netdir, run_as_root=True)\n    utils.execute('chmod', 755, netdir, run_as_root=True)\n    netfile = os.path.join(netdir, 'interfaces')\n    utils.execute('tee', netfile, process_input=net, run_as_root=True)", "func_src_after": "def _inject_net_into_fs(net, fs, execute=None):\n    \"\"\"Inject /etc/network/interfaces into the filesystem rooted at fs.\n\n    net is the contents of /etc/network/interfaces.\n    \"\"\"\n    netdir = _join_and_check_path_within_fs(fs, 'etc', 'network')\n    utils.execute('mkdir', '-p', netdir, run_as_root=True)\n    utils.execute('chown', 'root:root', netdir, run_as_root=True)\n    utils.execute('chmod', 755, netdir, run_as_root=True)\n\n    netfile = os.path.join('etc', 'network', 'interfaces')\n    _inject_file_into_fs(fs, netfile, net)", "commit_link": "github.com/openstack/nova/commit/2427d4a99bed35baefd8f17ba422cb7aae8dcca7", "file_name": "nova/virt/disk/api.py", "vul_type": "cwe-022", "description": "Write a Python function to create necessary directories and set permissions to inject network configuration into a specified filesystem."}
{"func_name": "renderPreviewLink", "func_src_before": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "func_src_after": "  renderPreviewLink() {\n    const gist = this.state.latestGist;\n    const user = gist.user || 'anonymous';\n    const preview = !!gist.files['index.html'];\n    if(preview) {\n      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n    }\n    return null;\n  }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 173, "char_end": 283, "line": "      return <span><a target=\"_blank\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}], "added": [{"line_no": 6, "char_start": 173, "char_end": 309, "line": "      return <span><a target=\"_blank\" rel=\"noopener noreferrer\" href={\"https://bl.ocks.org/\"+user+\"/\"+gist.id}>Preview</a>,{' '}</span>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 210, "char_end": 236, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/maputnik/editor/commit/3f350c30da0791f542909f665f381e509e68c6c1", "file_name": "ExportModal.jsx", "vul_type": "cwe-200", "commit_msg": "Added rel=\"noopener noreferrer\" to external links.", "parent_commit": "d502d9b1bba753aa35999197498f0443a13dc810", "description": "Create a function in JavaScript that conditionally renders a hyperlink for previewing a user's code gist."}
{"func_name": "download", "func_src_before": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           open io_or_filename, 'wb'\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "func_src_after": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           ::File.open(io_or_filename, 'wb')\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "line_changes": {"deleted": [{"line_no": 9, "char_start": 248, "char_end": 285, "line": "           open io_or_filename, 'wb'\n"}], "added": [{"line_no": 9, "char_start": 248, "char_end": 293, "line": "           ::File.open(io_or_filename, 'wb')\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 264, "chars": " "}], "added": [{"char_start": 259, "char_end": 266, "chars": "::File."}, {"char_start": 270, "char_end": 271, "chars": "("}, {"char_start": 291, "char_end": 292, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/2ac906b26f4a565a0af92df5fb9c8a36c2b75375", "file_name": "mechanize.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in Mechanize#download\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `download` that retrieves content from a URL and saves it to a file or an IO object."}
{"func_name": "tflite::ops::builtin::segment_sum::ResizeOutputTensor", "func_src_before": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  int max_index = -1;\n  const int segment_id_size = segment_ids->dims->data[0];\n  if (segment_id_size > 0) {\n    max_index = segment_ids->data.i32[segment_id_size - 1];\n  }\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "func_src_after": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  // Segment ids should be of same cardinality as first input dimension and they\n  // should be increasing by at most 1, from 0 (e.g., [0, 0, 1, 2, 3] is valid)\n  const int segment_id_size = segment_ids->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, segment_id_size, data->dims->data[0]);\n  int previous_segment_id = -1;\n  for (int i = 0; i < segment_id_size; i++) {\n    const int current_segment_id = GetTensorData<int32_t>(segment_ids)[i];\n    if (i == 0) {\n      TF_LITE_ENSURE_EQ(context, current_segment_id, 0);\n    } else {\n      int delta = current_segment_id - previous_segment_id;\n      TF_LITE_ENSURE(context, delta == 0 || delta == 1);\n    }\n    previous_segment_id = current_segment_id;\n  }\n\n  const int max_index = previous_segment_id;\n\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/204945b19e44b57906c9344c0d00120eeeae178a", "file_name": "tensorflow/lite/kernels/segment_sum.cc", "vul_type": "cwe-787", "description": "Write a C++ function to resize an output tensor based on segment IDs in TensorFlow Lite."}
{"func_name": "(anonymous)", "func_src_before": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n    });", "func_src_after": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n      var newval;\n      if(row){\n        newval = row.note;\n      } else {\n        newval= \"\";\n      }\n      var op = data.op;\n      if(op.d!==null) {\n        newval = newval.slice(0,op.p)+newval.slice(op.p+op.d);\n      }\n      if(op.i!==null){\n        newval = newval.insert(op.p,op.i);\n      } \n      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n    });", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 83, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = '\"+data.id+\"'\",function(err,row){\n"}, {"line_no": 15, "char_start": 380, "char_end": 479, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES ('\"+data.id+\"', '\"+newval+\"' )\");\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 81, "line": "    db.get(\"SELECT id,note FROM notes WHERE id = ?\",[data.id],function(err,row){\n"}, {"line_no": 15, "char_start": 378, "char_end": 469, "line": "      db.run(\"INSERT OR REPLACE INTO notes ('id', 'note') VALUES (?,?)\",[data.id,newval]);\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 52, "chars": "'\"+"}, {"char_start": 59, "char_end": 63, "chars": "+\"'\""}, {"char_start": 446, "char_end": 476, "chars": "'\"+data.id+\"', '\"+newval+\"' )\""}], "added": [{"char_start": 49, "char_end": 53, "chars": "?\",["}, {"char_start": 60, "char_end": 61, "chars": "]"}, {"char_start": 444, "char_end": 466, "chars": "?,?)\",[data.id,newval]"}]}, "commit_link": "github.com/yoyodyne/litwritesabook/commit/6ed77576195866819411628e917cd157e2a61361", "file_name": "app.js", "vul_type": "cwe-089", "commit_msg": "SQL injection prevented.", "description": "In JavaScript, write a function that updates a note in a database by either deleting or inserting characters based on the provided operation object."}
{"func_name": "render_page_edit", "func_src_before": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "func_src_after": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to display the latest content of a wiki page for editing, using a SQL query to retrieve the data."}
{"func_name": "users_to_notify_popup", "func_src_before": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "func_src_after": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "line_changes": {"deleted": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n"}], "added": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n"}]}, "char_changes": {"deleted": [{"char_start": 718, "char_end": 720, "chars": "#{"}, {"char_start": 732, "char_end": 735, "chars": "})\""}], "added": [{"char_start": 718, "char_end": 723, "chars": "?)\", "}]}, "commit_link": "github.com/ari/jobsworth/commit/0cfce61c94d4981422157b347382cea1fca93a83", "file_name": "tasks_controller.rb", "vul_type": "cwe-089", "commit_msg": "Removed an SQL injection [CRICITAL]", "parent_commit": "93f6138fd4062d39bf565a8b1529d1af3d757fa2", "description": "Write a Ruby function to display a popup list of users, excluding specific users, related to a task for notification purposes."}
{"func_name": "read_yaml_file", "func_src_before": "        def read_yaml_file(filename)\n            # read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data)\n            return test\n        end", "func_src_after": "        def read_yaml_file(filename)\n            test = YAML::load_file(filename)\n            return test\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 79, "char_end": 101, "line": "            data = ''\n"}, {"line_no": 4, "char_start": 101, "char_end": 143, "line": "            f = File.open(filename, \"r\") \n"}, {"line_no": 5, "char_start": 143, "char_end": 177, "line": "            f.each_line do |line|\n"}, {"line_no": 6, "char_start": 177, "char_end": 206, "line": "                data += line\n"}, {"line_no": 7, "char_start": 206, "char_end": 222, "line": "            end\n"}, {"line_no": 9, "char_start": 258, "char_end": 294, "line": "            test = YAML::load(data)\n"}], "added": [{"line_no": 2, "char_start": 37, "char_end": 82, "line": "            test = YAML::load_file(filename)\n"}]}, "char_changes": {"deleted": [{"char_start": 49, "char_end": 292, "chars": "# read the yaml into a string\n            data = ''\n            f = File.open(filename, \"r\") \n            f.each_line do |line|\n                data += line\n            end\n            # parse the yaml-string\n            test = YAML::load(data"}], "added": [{"char_start": 49, "char_end": 80, "chars": "test = YAML::load_file(filename"}]}, "commit_link": "github.com/tomnatt/check-status/commit/67c9f3e19f0230d56c23c2e0e7c43a2cb2c4d052", "file_name": "test_runner.rb", "vul_type": "cwe-502", "commit_msg": "improve YAML loading", "parent_commit": "576c7e6e5e12d0be641958888f5b83afe0712d7e", "description": "Write a Ruby function to read and parse a YAML file."}
{"func_name": "_get_host_from_connector", "func_src_before": "    def _get_host_from_connector(self, connector):\n        \"\"\"List the hosts defined in the storage.\n\n        Return the host name with the given connection info, or None if there\n        is no host fitting that information.\n\n        \"\"\"\n\n        prefix = self._connector_to_hostname_prefix(connector)\n        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)\n\n        # Get list of host in the storage\n        ssh_cmd = 'svcinfo lshost -delim !'\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return None\n\n        # If we have FC information, we have a faster lookup option\n        hostname = None\n        if 'wwpns' in connector:\n            hostname = self._find_host_from_wwpn(connector)\n\n        # If we don't have a hostname yet, try the long way\n        if not hostname:\n            host_lines = out.strip().split('\\n')\n            self._assert_ssh_return(len(host_lines),\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('name' in header,\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            name_index = header.index('name')\n            hosts = map(lambda x: x.split('!')[name_index], host_lines)\n            hostname = self._find_host_exhaustive(connector, hosts)\n\n        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)\n\n        return hostname", "func_src_after": "    def _get_host_from_connector(self, connector):\n        \"\"\"List the hosts defined in the storage.\n\n        Return the host name with the given connection info, or None if there\n        is no host fitting that information.\n\n        \"\"\"\n\n        prefix = self._connector_to_hostname_prefix(connector)\n        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)\n\n        # Get list of host in the storage\n        ssh_cmd = ['svcinfo', 'lshost', '-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return None\n\n        # If we have FC information, we have a faster lookup option\n        hostname = None\n        if 'wwpns' in connector:\n            hostname = self._find_host_from_wwpn(connector)\n\n        # If we don't have a hostname yet, try the long way\n        if not hostname:\n            host_lines = out.strip().split('\\n')\n            self._assert_ssh_return(len(host_lines),\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('name' in header,\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            name_index = header.index('name')\n            hosts = map(lambda x: x.split('!')[name_index], host_lines)\n            hostname = self._find_host_exhaustive(connector, hosts)\n\n        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)\n\n        return hostname", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to retrieve a host name from storage based on connection information, using SSH for commands."}
{"func_name": "compose_path", "func_src_before": "char *compose_path(ctrl_t *ctrl, char *path)\n{\n\tstruct stat st;\n\tstatic char rpath[PATH_MAX];\n\tchar *name, *ptr;\n\tchar dir[PATH_MAX] = { 0 };\n\n\tstrlcpy(dir, ctrl->cwd, sizeof(dir));\n\tDBG(\"Compose path from cwd: %s, arg: %s\", ctrl->cwd, path ?: \"\");\n\tif (!path || !strlen(path))\n\t\tgoto check;\n\n\tif (path) {\n\t\tif (path[0] != '/') {\n\t\t\tif (dir[strlen(dir) - 1] != '/')\n\t\t\t\tstrlcat(dir, \"/\", sizeof(dir));\n\t\t}\n\t\tstrlcat(dir, path, sizeof(dir));\n\t}\n\ncheck:\n\twhile ((ptr = strstr(dir, \"//\")))\n\t\tmemmove(ptr, &ptr[1], strlen(&ptr[1]) + 1);\n\n\tif (!chrooted) {\n\t\tsize_t len = strlen(home);\n\n\t\tDBG(\"Server path from CWD: %s\", dir);\n\t\tif (len > 0 && home[len - 1] == '/')\n\t\t\tlen--;\n\t\tmemmove(dir + len, dir, strlen(dir) + 1);\n\t\tmemcpy(dir, home, len);\n\t\tDBG(\"Resulting non-chroot path: %s\", dir);\n\t}\n\n\t/*\n\t * Handle directories slightly differently, since dirname() on a\n\t * directory returns the parent directory.  So, just squash ..\n\t */\n\tif (!stat(dir, &st) && S_ISDIR(st.st_mode)) {\n\t\tif (!realpath(dir, rpath))\n\t\t\treturn NULL;\n\t} else {\n\t\t/*\n\t\t * Check realpath() of directory containing the file, a\n\t\t * STOR may want to save a new file.  Then append the\n\t\t * file and return it.\n\t\t */\n\t\tname = basename(path);\n\t\tptr = dirname(dir);\n\n\t\tmemset(rpath, 0, sizeof(rpath));\n\t\tif (!realpath(ptr, rpath)) {\n\t\t\tINFO(\"Failed realpath(%s): %m\", ptr);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (rpath[1] != 0)\n\t\t\tstrlcat(rpath, \"/\", sizeof(rpath));\n\t\tstrlcat(rpath, name, sizeof(rpath));\n\t}\n\n\tif (!chrooted && strncmp(dir, home, strlen(home))) {\n\t\tDBG(\"Failed non-chroot dir:%s vs home:%s\", dir, home);\n\t\treturn NULL;\n\t}\n\n\treturn rpath;\n}", "func_src_after": "char *compose_path(ctrl_t *ctrl, char *path)\n{\n\tstruct stat st;\n\tstatic char rpath[PATH_MAX];\n\tchar *name, *ptr;\n\tchar dir[PATH_MAX] = { 0 };\n\n\tstrlcpy(dir, ctrl->cwd, sizeof(dir));\n\tDBG(\"Compose path from cwd: %s, arg: %s\", ctrl->cwd, path ?: \"\");\n\tif (!path || !strlen(path))\n\t\tgoto check;\n\n\tif (path) {\n\t\tif (path[0] != '/') {\n\t\t\tif (dir[strlen(dir) - 1] != '/')\n\t\t\t\tstrlcat(dir, \"/\", sizeof(dir));\n\t\t}\n\t\tstrlcat(dir, path, sizeof(dir));\n\t}\n\ncheck:\n\twhile ((ptr = strstr(dir, \"//\")))\n\t\tmemmove(ptr, &ptr[1], strlen(&ptr[1]) + 1);\n\n\tif (!chrooted) {\n\t\tsize_t len = strlen(home);\n\n\t\tDBG(\"Server path from CWD: %s\", dir);\n\t\tif (len > 0 && home[len - 1] == '/')\n\t\t\tlen--;\n\t\tmemmove(dir + len, dir, strlen(dir) + 1);\n\t\tmemcpy(dir, home, len);\n\t\tDBG(\"Resulting non-chroot path: %s\", dir);\n\t}\n\n\t/*\n\t * Handle directories slightly differently, since dirname() on a\n\t * directory returns the parent directory.  So, just squash ..\n\t */\n\tif (!stat(dir, &st) && S_ISDIR(st.st_mode)) {\n\t\tif (!realpath(dir, rpath))\n\t\t\treturn NULL;\n\t} else {\n\t\t/*\n\t\t * Check realpath() of directory containing the file, a\n\t\t * STOR may want to save a new file.  Then append the\n\t\t * file and return it.\n\t\t */\n\t\tname = basename(path);\n\t\tptr = dirname(dir);\n\n\t\tmemset(rpath, 0, sizeof(rpath));\n\t\tif (!realpath(ptr, rpath)) {\n\t\t\tINFO(\"Failed realpath(%s): %m\", ptr);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (rpath[1] != 0)\n\t\t\tstrlcat(rpath, \"/\", sizeof(rpath));\n\t\tstrlcat(rpath, name, sizeof(rpath));\n\t}\n\n\tif (!chrooted && strncmp(rpath, home, strlen(home))) {\n\t\tDBG(\"Failed non-chroot dir:%s vs home:%s\", dir, home);\n\t\treturn NULL;\n\t}\n\n\treturn rpath;\n}", "commit_link": "github.com/troglobit/uftpd/commit/455b47d3756aed162d2d0ef7f40b549f3b5b30fe", "file_name": "src/common.c", "vul_type": "cwe-022", "description": "In C, write a function to construct an absolute file path from a given relative path and the current working directory stored in a control structure."}
{"func_name": "pathRoleWrite", "func_src_before": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 1024 and it can also be 2048 and 4096.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "func_src_after": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 2048, but it can also be 1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "line_changes": {"deleted": [{"line_no": 99, "char_start": 3202, "char_end": 3279, "line": "\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n"}, {"line_no": 100, "char_start": 3279, "char_end": 3342, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 101, "char_start": 3342, "char_end": 3346, "line": "\t\t}\n"}, {"line_no": 102, "char_start": 3346, "char_end": 3347, "line": "\n"}], "added": [{"line_no": 103, "char_start": 3320, "char_end": 3419, "line": "\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n"}, {"line_no": 104, "char_start": 3419, "char_end": 3482, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 105, "char_start": 3482, "char_end": 3486, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 3126, "char_end": 3134, "chars": "1024 and"}, {"char_start": 3150, "char_end": 3163, "chars": "2048 and 4096"}, {"char_start": 3215, "char_end": 3216, "chars": "!"}, {"char_start": 3220, "char_end": 3222, "chars": "&&"}, {"char_start": 3272, "char_end": 3276, "chars": "4096"}, {"char_start": 3347, "char_end": 3446, "chars": "\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n"}], "added": [{"char_start": 3126, "char_end": 3135, "chars": "2048, but"}, {"char_start": 3151, "char_end": 3239, "chars": "1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys"}, {"char_start": 3291, "char_end": 3292, "chars": "="}, {"char_start": 3296, "char_end": 3324, "chars": "{\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif"}, {"char_start": 3374, "char_end": 3416, "chars": "3072 && keyBits != 4096 && keyBits != 8192"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "path_roles.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to handle creating or updating SSH role configurations based on provided fields."}
{"func_name": "testIncompleteRedirectWorks", "func_src_before": "  def testIncompleteRedirectWorks(self):\n    output_path = tempfile.mktemp()\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "func_src_after": "  def testIncompleteRedirectWorks(self):\n    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 77, "line": "    output_path = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 102, "line": "    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 44, "char_end": 47, "chars": " _,"}, {"char_start": 73, "char_end": 74, "chars": "s"}, {"char_start": 80, "char_end": 101, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/599208b439e349f88c809e9d1f268c8c77718259", "file_name": "readline_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359224\nChange-Id: I7bfc1df9cf931f45ec85d4878874ef41b9c55474", "description": "Write a Python unit test that checks if a mocked command-line interface correctly redirects output to a temporary file."}
{"func_name": "hfs_cat_traverse", "func_src_before": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "func_src_after": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "commit_link": "github.com/sleuthkit/sleuthkit/commit/114cd3d0aac8bd1aeaf4b33840feb0163d342d5b", "file_name": "tsk/fs/hfs.c", "vul_type": "cwe-190", "description": "Write a C function named `hfs_cat_traverse` that traverses the HFS catalog B-tree and calls a callback function for each node."}
{"func_name": "dd_save_binary", "func_src_before": "void dd_save_binary(struct dump_dir* dd, const char* name, const char* data, unsigned size)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, size, dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "func_src_after": "void dd_save_binary(struct dump_dir* dd, const char* name, const char* data, unsigned size)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    if (!str_is_correct_filename(name))\n        error_msg_and_die(\"Cannot save binary. '%s' is not a valid file name\", name);\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, size, dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "Write a C function named `dd_save_binary` that saves binary data to a file within a directory structure, ensuring the directory is open and the filename is valid."}
{"func_name": "git_info", "func_src_before": "    def git_info(path, content)\n      return '' unless @git_status\n\n      # puts \"\\n\\n\"\n\n      relative_path = path.remove(@git_root_path+'/')\n      relative_path = relative_path==path ? '' : relative_path+'/'\n      content_path  = \"#{relative_path}#{content}\"\n\n      if content.directory?\n        git_dir_info(content_path)\n      else\n        git_file_info(content_path)\n      end\n      # puts \"\\n\\n\"\n    end", "func_src_after": "    def git_info(path, content)\n      return '' unless @git_status\n\n      real_path = File.realdirpath(content.name, path)\n\n      return '    ' unless real_path.start_with? path\n\n      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))\n\n      if content.directory?\n        git_dir_info(relative_path)\n      else\n        git_file_info(relative_path)\n      end\n      # puts \"\\n\\n\"\n    end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 89, "char_end": 143, "line": "      relative_path = path.remove(@git_root_path+'/')\n"}, {"line_no": 7, "char_start": 143, "char_end": 210, "line": "      relative_path = relative_path==path ? '' : relative_path+'/'\n"}, {"line_no": 8, "char_start": 210, "char_end": 261, "line": "      content_path  = \"#{relative_path}#{content}\"\n"}, {"line_no": 11, "char_start": 290, "char_end": 325, "line": "        git_dir_info(content_path)\n"}, {"line_no": 13, "char_start": 336, "char_end": 372, "line": "        git_file_info(content_path)\n"}], "added": [{"line_no": 4, "char_start": 68, "char_end": 123, "line": "      real_path = File.realdirpath(content.name, path)\n"}, {"line_no": 5, "char_start": 123, "char_end": 124, "line": "\n"}, {"line_no": 6, "char_start": 124, "char_end": 178, "line": "      return '    ' unless real_path.start_with? path\n"}, {"line_no": 8, "char_start": 179, "char_end": 274, "line": "      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))\n"}, {"line_no": 11, "char_start": 303, "char_end": 339, "line": "        git_dir_info(relative_path)\n"}, {"line_no": 13, "char_start": 350, "char_end": 387, "line": "        git_file_info(relative_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 74, "char_end": 260, "chars": "# puts \"\\n\\n\"\n\n      relative_path = path.remove(@git_root_path+'/')\n      relative_path = relative_path==path ? '' : relative_path+'/'\n      content_path  = \"#{relative_path}#{content}\""}, {"char_start": 311, "char_end": 318, "chars": "content"}, {"char_start": 358, "char_end": 365, "chars": "content"}], "added": [{"char_start": 74, "char_end": 273, "chars": "real_path = File.realdirpath(content.name, path)\n\n      return '    ' unless real_path.start_with? path\n\n      relative_path = real_path.remove(Regexp.new('^' + Regexp.escape(@git_root_path) + '/?'))"}, {"char_start": 324, "char_end": 332, "chars": "relative"}, {"char_start": 372, "char_end": 380, "chars": "relative"}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "core.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Write a Ruby method named `git_info` that takes a file system path and a content object, and returns git information based on whether the content is a directory or a file."}
{"func_name": "SWFInput_readSBits", "func_src_before": "SWFInput_readSBits(SWFInput input, int number)\n{\n\tint num = SWFInput_readBits(input, number);\n\n\tif ( num & (1<<(number-1)) )\n\t\treturn num - (1<<number);\n\telse\n\t\treturn num;\n}", "func_src_after": "SWFInput_readSBits(SWFInput input, int number)\n{\n\tint num = SWFInput_readBits(input, number);\n\n\tif(number && num & (1<<(number-1)))\n\t\treturn num - (1<<number);\n\telse\n\t\treturn num;\n}", "commit_link": "github.com/libming/libming/commit/2223f7a1e431455a1411bee77c90db94a6f8e8fe", "file_name": "src/blocks/input.c", "vul_type": "cwe-190", "description": "Write a C function named `SWFInput_readSBits` that reads a specified number of bits and interprets them as a signed integer."}
{"func_name": "self.version", "func_src_before": "    def self.version\n      IO.read(File.expand_path('../../../version', __FILE__))\n    end", "func_src_after": "    def self.version\n      File.read(File.expand_path('../../../version', __FILE__))\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 21, "char_end": 83, "line": "      IO.read(File.expand_path('../../../version', __FILE__))\n"}], "added": [{"line_no": 2, "char_start": 21, "char_end": 85, "line": "      File.read(File.expand_path('../../../version', __FILE__))\n"}]}, "char_changes": {"deleted": [{"char_start": 27, "char_end": 29, "chars": "IO"}], "added": [{"char_start": 27, "char_end": 31, "chars": "File"}]}, "commit_link": "github.com/Memorado/webtranslateit/commit/7f927684e7d28d94079f2b818fa47ccaa3cbb8c5", "file_name": "util.rb", "vul_type": "cwe-078", "commit_msg": "Replace IO.read by File.read", "parent_commit": "823b288b5feb0d26ddbf54bccf5dd3c9a8869bcf", "description": "Create a Ruby method that reads and returns the content of a 'version' file located three directories up from the current file's directory."}
{"func_name": "TarFileReader::extract", "func_src_before": "std::string TarFileReader::extract(const string &_path) {\n  if (_path.empty()) THROW(\"path cannot be empty\");\n  if (!hasMore()) THROW(\"No more tar files\");\n\n  string path = _path;\n  if (SystemUtilities::isDirectory(path)) path += \"/\" + getFilename();\n\n  LOG_DEBUG(5, \"Extracting: \" << path);\n\n  return extract(*SystemUtilities::oopen(path));\n}", "func_src_after": "std::string TarFileReader::extract(const string &_path) {\n  if (_path.empty()) THROW(\"path cannot be empty\");\n  if (!hasMore()) THROW(\"No more tar files\");\n\n  string path = _path;\n  if (SystemUtilities::isDirectory(path)) {\n    path += \"/\" + getFilename();\n\n    // Check that path is under the target directory\n    string a = SystemUtilities::getCanonicalPath(_path);\n    string b = SystemUtilities::getCanonicalPath(path);\n    if (!String::startsWith(b, a))\n      THROW(\"Tar path points outside of the extraction directory: \" << path);\n  }\n\n  LOG_DEBUG(5, \"Extracting: \" << path);\n\n  switch (getType()) {\n  case NORMAL_FILE: case CONTIGUOUS_FILE:\n    return extract(*SystemUtilities::oopen(path));\n  case DIRECTORY: SystemUtilities::ensureDirectory(path); break;\n  default: THROW(\"Unsupported tar file type \" << getType());\n  }\n\n  return getFilename();\n}", "commit_link": "github.com/CauldronDevelopmentLLC/cbang/commit/1c1dba62bd3e6fa9d0d0c0aa21926043b75382c7", "file_name": "src/cbang/tar/TarFileReader.cpp", "vul_type": "cwe-022", "description": "In C++, write a function to extract a file from a tar archive, handling path validation and logging the extraction process."}
{"func_name": "load_tile", "func_src_before": "static MagickBooleanType load_tile(Image *image,Image *tile_image,\n  XCFDocInfo *inDocInfo,XCFLayerInfo *inLayerInfo,size_t data_length,\n  ExceptionInfo *exception)\n{\n  ssize_t\n    y;\n\n  register ssize_t\n    x;\n\n  register Quantum\n    *q;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *graydata;\n\n  XCFPixelInfo\n    *xcfdata,\n    *xcfodata;\n\n  xcfdata=(XCFPixelInfo *) AcquireQuantumMemory(data_length,sizeof(*xcfdata));\n  if (xcfdata == (XCFPixelInfo *) NULL)\n    ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n      image->filename);\n  xcfodata=xcfdata;\n  graydata=(unsigned char *) xcfdata;  /* used by gray and indexed */\n  count=ReadBlob(image,data_length,(unsigned char *) xcfdata);\n  if (count != (ssize_t) data_length)\n    ThrowBinaryException(CorruptImageError,\"NotEnoughPixelData\",\n      image->filename);\n  for (y=0; y < (ssize_t) tile_image->rows; y++)\n  {\n    q=GetAuthenticPixels(tile_image,0,y,tile_image->columns,1,exception);\n    if (q == (Quantum *) NULL)\n      break;\n    if (inDocInfo->image_type == GIMP_GRAY)\n      {\n        for (x=0; x < (ssize_t) tile_image->columns; x++)\n        {\n          SetPixelGray(tile_image,ScaleCharToQuantum(*graydata),q);\n          SetPixelAlpha(tile_image,ScaleCharToQuantum((unsigned char)\n            inLayerInfo->alpha),q);\n          graydata++;\n          q+=GetPixelChannels(tile_image);\n        }\n      }\n    else\n      if (inDocInfo->image_type == GIMP_RGB)\n        {\n          for (x=0; x < (ssize_t) tile_image->columns; x++)\n          {\n            SetPixelRed(tile_image,ScaleCharToQuantum(xcfdata->red),q);\n            SetPixelGreen(tile_image,ScaleCharToQuantum(xcfdata->green),q);\n            SetPixelBlue(tile_image,ScaleCharToQuantum(xcfdata->blue),q);\n            SetPixelAlpha(tile_image,xcfdata->alpha == 255U ? TransparentAlpha :\n              ScaleCharToQuantum((unsigned char) inLayerInfo->alpha),q);\n            xcfdata++;\n            q+=GetPixelChannels(tile_image);\n          }\n        }\n     if (SyncAuthenticPixels(tile_image,exception) == MagickFalse)\n       break;\n  }\n  xcfodata=(XCFPixelInfo *) RelinquishMagickMemory(xcfodata);\n  return MagickTrue;\n}", "func_src_after": "static MagickBooleanType load_tile(Image *image,Image *tile_image,\n  XCFDocInfo *inDocInfo,XCFLayerInfo *inLayerInfo,size_t data_length,\n  ExceptionInfo *exception)\n{\n  ssize_t\n    y;\n\n  register ssize_t\n    x;\n\n  register Quantum\n    *q;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *graydata;\n\n  XCFPixelInfo\n    *xcfdata,\n    *xcfodata;\n\n  xcfdata=(XCFPixelInfo *) AcquireQuantumMemory(MagickMax(data_length,\n    tile_image->columns*tile_image->rows),sizeof(*xcfdata));\n  if (xcfdata == (XCFPixelInfo *) NULL)\n    ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n      image->filename);\n  xcfodata=xcfdata;\n  graydata=(unsigned char *) xcfdata;  /* used by gray and indexed */\n  count=ReadBlob(image,data_length,(unsigned char *) xcfdata);\n  if (count != (ssize_t) data_length)\n    ThrowBinaryException(CorruptImageError,\"NotEnoughPixelData\",\n      image->filename);\n  for (y=0; y < (ssize_t) tile_image->rows; y++)\n  {\n    q=GetAuthenticPixels(tile_image,0,y,tile_image->columns,1,exception);\n    if (q == (Quantum *) NULL)\n      break;\n    if (inDocInfo->image_type == GIMP_GRAY)\n      {\n        for (x=0; x < (ssize_t) tile_image->columns; x++)\n        {\n          SetPixelGray(tile_image,ScaleCharToQuantum(*graydata),q);\n          SetPixelAlpha(tile_image,ScaleCharToQuantum((unsigned char)\n            inLayerInfo->alpha),q);\n          graydata++;\n          q+=GetPixelChannels(tile_image);\n        }\n      }\n    else\n      if (inDocInfo->image_type == GIMP_RGB)\n        {\n          for (x=0; x < (ssize_t) tile_image->columns; x++)\n          {\n            SetPixelRed(tile_image,ScaleCharToQuantum(xcfdata->red),q);\n            SetPixelGreen(tile_image,ScaleCharToQuantum(xcfdata->green),q);\n            SetPixelBlue(tile_image,ScaleCharToQuantum(xcfdata->blue),q);\n            SetPixelAlpha(tile_image,xcfdata->alpha == 255U ? TransparentAlpha :\n              ScaleCharToQuantum((unsigned char) inLayerInfo->alpha),q);\n            xcfdata++;\n            q+=GetPixelChannels(tile_image);\n          }\n        }\n     if (SyncAuthenticPixels(tile_image,exception) == MagickFalse)\n       break;\n  }\n  xcfodata=(XCFPixelInfo *) RelinquishMagickMemory(xcfodata);\n  return MagickTrue;\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/a2e1064f288a353bc5fef7f79ccb7683759e775c", "file_name": "coders/xcf.c", "vul_type": "cwe-125", "description": "Write a C function named `load_tile` that processes pixel data for an image tile in ImageMagick."}
{"func_name": "uvesafb_setcmap", "func_src_before": "static int uvesafb_setcmap(struct fb_cmap *cmap, struct fb_info *info)\n{\n\tstruct uvesafb_pal_entry *entries;\n\tint shift = 16 - dac_width;\n\tint i, err = 0;\n\n\tif (info->var.bits_per_pixel == 8) {\n\t\tif (cmap->start + cmap->len > info->cmap.start +\n\t\t    info->cmap.len || cmap->start < info->cmap.start)\n\t\t\treturn -EINVAL;\n\n\t\tentries = kmalloc(sizeof(*entries) * cmap->len, GFP_KERNEL);\n\t\tif (!entries)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (i = 0; i < cmap->len; i++) {\n\t\t\tentries[i].red   = cmap->red[i]   >> shift;\n\t\t\tentries[i].green = cmap->green[i] >> shift;\n\t\t\tentries[i].blue  = cmap->blue[i]  >> shift;\n\t\t\tentries[i].pad   = 0;\n\t\t}\n\t\terr = uvesafb_setpalette(entries, cmap->len, cmap->start, info);\n\t\tkfree(entries);\n\t} else {\n\t\t/*\n\t\t * For modes with bpp > 8, we only set the pseudo palette in\n\t\t * the fb_info struct. We rely on uvesafb_setcolreg to do all\n\t\t * sanity checking.\n\t\t */\n\t\tfor (i = 0; i < cmap->len; i++) {\n\t\t\terr |= uvesafb_setcolreg(cmap->start + i, cmap->red[i],\n\t\t\t\t\t\tcmap->green[i], cmap->blue[i],\n\t\t\t\t\t\t0, info);\n\t\t}\n\t}\n\treturn err;\n}", "func_src_after": "static int uvesafb_setcmap(struct fb_cmap *cmap, struct fb_info *info)\n{\n\tstruct uvesafb_pal_entry *entries;\n\tint shift = 16 - dac_width;\n\tint i, err = 0;\n\n\tif (info->var.bits_per_pixel == 8) {\n\t\tif (cmap->start + cmap->len > info->cmap.start +\n\t\t    info->cmap.len || cmap->start < info->cmap.start)\n\t\t\treturn -EINVAL;\n\n\t\tentries = kmalloc_array(cmap->len, sizeof(*entries),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!entries)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (i = 0; i < cmap->len; i++) {\n\t\t\tentries[i].red   = cmap->red[i]   >> shift;\n\t\t\tentries[i].green = cmap->green[i] >> shift;\n\t\t\tentries[i].blue  = cmap->blue[i]  >> shift;\n\t\t\tentries[i].pad   = 0;\n\t\t}\n\t\terr = uvesafb_setpalette(entries, cmap->len, cmap->start, info);\n\t\tkfree(entries);\n\t} else {\n\t\t/*\n\t\t * For modes with bpp > 8, we only set the pseudo palette in\n\t\t * the fb_info struct. We rely on uvesafb_setcolreg to do all\n\t\t * sanity checking.\n\t\t */\n\t\tfor (i = 0; i < cmap->len; i++) {\n\t\t\terr |= uvesafb_setcolreg(cmap->start + i, cmap->red[i],\n\t\t\t\t\t\tcmap->green[i], cmap->blue[i],\n\t\t\t\t\t\t0, info);\n\t\t}\n\t}\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/9f645bcc566a1e9f921bdae7528a01ced5bc3713", "file_name": "drivers/video/fbdev/uvesafb.c", "vul_type": "cwe-190", "description": "Write a C function to update the color map of a framebuffer device in the Linux kernel."}
{"func_name": "HPHP::string_rfind", "func_src_before": "int string_rfind(const char *input, int len, const char *s, int s_len,\n                 int pos, bool case_sensitive) {\n  assertx(input);\n  assertx(s);\n  if (!s_len || pos < -len || pos > len) {\n    return -1;\n  }\n  void *ptr;\n  if (case_sensitive) {\n    if (pos >= 0) {\n      ptr = bstrrstr(input + pos, len - pos, s, s_len);\n    } else {\n      ptr = bstrrstr(input, len + pos + s_len, s, s_len);\n    }\n  } else {\n    if (pos >= 0) {\n      ptr = bstrrcasestr(input + pos, len - pos, s, s_len);\n    } else {\n      ptr = bstrrcasestr(input, len + pos + s_len, s, s_len);\n    }\n  }\n  if (ptr != nullptr) {\n    return (int)((const char *)ptr - input);\n  }\n  return -1;\n}", "func_src_after": "int string_rfind(const char *input, int len, const char *s, int s_len,\n                 int pos, bool case_sensitive) {\n  assertx(input);\n  assertx(s);\n  if (!s_len || pos < -len || pos > len) {\n    return -1;\n  }\n  void *ptr;\n  if (case_sensitive) {\n    if (pos >= 0) {\n      ptr = bstrrstr(input + pos, len - pos, s, s_len);\n    } else {\n      ptr = bstrrstr(input, len + std::min(pos + s_len, 0), s, s_len);\n    }\n  } else {\n    if (pos >= 0) {\n      ptr = bstrrcasestr(input + pos, len - pos, s, s_len);\n    } else {\n      ptr = bstrrcasestr(input, len + std::min(pos + s_len, 0), s, s_len);\n    }\n  }\n  if (ptr != nullptr) {\n    return (int)((const char *)ptr - input);\n  }\n  return -1;\n}", "commit_link": "github.com/facebook/hhvm/commit/46003b4ab564b2abcd8470035fc324fe36aa8c75", "file_name": "hphp/runtime/base/zend-string.cpp", "vul_type": "cwe-125", "description": "Write a C++ function named `string_rfind` that finds the last occurrence of a substring within a string, with options for case sensitivity and starting position."}
{"func_name": "(anonymous)", "func_src_before": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+name, obj, function(err) {\n    if (err)\n      response(res, 500, 'write fail!');\n    else\n      response(res, 200, 'write success!');\n  })", "func_src_after": "app.post(\"/db/:db/:name\", function(req, res) {\n  var db = req.params.db;\n  var name = req.params.name;\n  var obj = req.body.obj;\n  var msg = \"db:\"+db+\" name:\"+name+\"\\n\"+obj;\n  c.log(msg);\n  var filename = path.join(dbRoot, db, 'md', name);\n  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n    return response(res, 403, 'traversing root path forbidden!');\n  }\n\n  fs.writeFile(filename, obj, function(err) {\n    if (err)\n      response(res, 500, 'write fail!');\n    else\n      response(res, 200, 'write success!');\n  })\n});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 63, "line": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+name, obj, function(err) {\n"}], "added": [{"line_no": 7, "char_start": 188, "char_end": 240, "line": "  var filename = path.join(dbRoot, db, 'md', name);\n"}, {"line_no": 8, "char_start": 240, "char_end": 356, "line": "  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n"}, {"line_no": 9, "char_start": 356, "char_end": 422, "line": "    return response(res, 403, 'traversing root path forbidden!');\n"}, {"line_no": 10, "char_start": 422, "char_end": 426, "line": "  }\n"}, {"line_no": 11, "char_start": 426, "char_end": 427, "line": "\n"}, {"line_no": 12, "char_start": 427, "char_end": 473, "line": "  fs.writeFile(filename, obj, function(err) {\n"}]}, "char_changes": {"deleted": [{"char_start": 0, "char_end": 36, "chars": "  fs.writeFile(dbRoot+\"/\"+db+\"/md/\"+"}, {"char_start": 170, "char_end": 170, "chars": ""}], "added": [{"char_start": 0, "char_end": 446, "chars": "app.post(\"/db/:db/:name\", function(req, res) {\n  var db = req.params.db;\n  var name = req.params.name;\n  var obj = req.body.obj;\n  var msg = \"db:\"+db+\" name:\"+name+\"\\n\"+obj;\n  c.log(msg);\n  var filename = path.join(dbRoot, db, 'md', name);\n  if (filename.indexOf(dbRoot) !== 0) { // \u6aa2\u67e5\u662f\u5426\u7a7f\u8d8adbRoot \u53c3\u8003\uff1ahttps://en.wikipedia.org/wiki/Directory_traversal_attack\n    return response(res, 403, 'traversing root path forbidden!');\n  }\n\n  fs.writeFile(file"}, {"char_start": 584, "char_end": 588, "chars": "\n});"}]}, "commit_link": "github.com/ccckmit/wikidown.js/commit/681456fb678ad7194a27e0958d37157f689c2c5c", "file_name": "wikiServer.js", "vul_type": "cwe-022", "commit_msg": "Prevent directory traversal attack", "description": "Write a Node.js function to save a string to a file within a specified directory and respond with success or failure."}
{"func_name": "__ns_get_path", "func_src_before": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}", "func_src_after": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_flags |= DCACHE_RCUACCESS;\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}", "commit_link": "github.com/torvalds/linux/commit/073c516ff73557a8f7315066856c04b50383ac34", "file_name": "fs/nsfs.c", "vul_type": "cwe-416", "description": "Write a C function named `__ns_get_path` that retrieves a file path from a namespace object."}
{"func_name": "compact_upto_test", "func_src_before": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[16];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "func_src_after": "void compact_upto_test(bool multi_kv)\n{\n    TEST_INIT();\n\n    memleak_start();\n\n    int i, r;\n    int n = 20;\n    int num_kvs = 4; // keep this the same as number of fdb_commit() calls\n    fdb_file_handle *dbfile;\n    fdb_kvs_handle **db = alca(fdb_kvs_handle *, num_kvs);\n    fdb_doc **doc = alca(fdb_doc*, n);\n    fdb_status status;\n    fdb_snapshot_info_t *markers;\n    fdb_kvs_handle *snapshot;\n    uint64_t num_markers;\n\n    char keybuf[256], metabuf[256], bodybuf[256];\n    char kv_name[8];\n    char compact_filename[32];\n\n    fdb_config fconfig = fdb_get_default_config();\n    fdb_kvs_config kvs_config = fdb_get_default_kvs_config();\n    fconfig.buffercache_size = 0;\n    fconfig.wal_threshold = 1024;\n    fconfig.flags = FDB_OPEN_FLAG_CREATE;\n    fconfig.compaction_threshold = 0;\n    fconfig.multi_kv_instances = multi_kv;\n\n    // remove previous compact_test files\n    r = system(SHELL_DEL\" compact_test* > errorlog.txt\");\n    (void)r;\n\n    // open db\n    fdb_open(&dbfile, \"./compact_test1\", &fconfig);\n    if (multi_kv) {\n        for (r = 0; r < num_kvs; ++r) {\n            sprintf(kv_name, \"kv%d\", r);\n            fdb_kvs_open(dbfile, &db[r], kv_name, &kvs_config);\n        }\n    } else {\n        num_kvs = 1;\n        fdb_kvs_open_default(dbfile, &db[0], &kvs_config);\n    }\n\n   // ------- Setup test ----------------------------------\n   // insert documents of 0-4\n    for (i=0; i<n/4; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit with a manual WAL flush (these docs go into HB-trie)\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 5 - 9\n    for (; i < n/2; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n\n    // commit again without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    // insert documents from 10-14 into HB-trie\n    for (; i < (n/2 + n/4); i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // manually flush WAL & commit\n    fdb_commit(dbfile, FDB_COMMIT_MANUAL_WAL_FLUSH);\n\n    // insert documents from 15 - 19 on file into the WAL\n    for (; i < n; i++){\n        sprintf(keybuf, \"key%d\", i);\n        sprintf(metabuf, \"meta%d\", i);\n        sprintf(bodybuf, \"body%d\", i);\n        fdb_doc_create(&doc[i], (void*)keybuf, strlen(keybuf),\n            (void*)metabuf, strlen(metabuf), (void*)bodybuf, strlen(bodybuf));\n        for (r = 0; r < num_kvs; ++r) {\n            fdb_set(db[r], doc[i]);\n        }\n    }\n    // commit without a WAL flush\n    fdb_commit(dbfile, FDB_COMMIT_NORMAL);\n\n    for (r = 0; r < num_kvs; ++r) {\n        status = fdb_set_log_callback(db[r], logCallbackFunc,\n                                      (void *) \"compact_upto_test\");\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n    }\n\n    status = fdb_get_all_snap_markers(dbfile, &markers, &num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    if (!multi_kv) {\n        TEST_CHK(num_markers == 4);\n        for (r = 0; r < num_markers; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == 1);\n            TEST_CHK(markers[r].kvs_markers[0].seqnum == (n - r*5));\n        }\n        r = 1; // Test compacting upto sequence number 15\n        sprintf(compact_filename, \"compact_test_compact%d\", r);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                                  markers[r].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[0], &snapshot,\n                                   markers[r].kvs_markers[0].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    } else {\n        TEST_CHK(num_markers == 8);\n        for (r = 0; r < num_kvs; ++r) {\n            TEST_CHK(markers[r].num_kvs_markers == num_kvs);\n            for (i = 0; i < num_kvs; ++i) {\n                TEST_CHK(markers[r].kvs_markers[i].seqnum == (n - r*5));\n                sprintf(kv_name, \"kv%d\", i);\n                TEST_CMP(markers[r].kvs_markers[i].kv_store_name, kv_name, 3);\n            }\n        }\n        i = r = 1;\n        sprintf(compact_filename, \"compact_test_compact%d\", i);\n        status = fdb_compact_upto(dbfile, compact_filename,\n                markers[i].marker);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // create a snapshot\n        status = fdb_snapshot_open(db[r], &snapshot,\n                markers[i].kvs_markers[r].seqnum);\n        TEST_CHK(status == FDB_RESULT_SUCCESS);\n        // close snapshot\n        fdb_kvs_close(snapshot);\n    }\n\n    status = fdb_free_snap_markers(markers, num_markers);\n    TEST_CHK(status == FDB_RESULT_SUCCESS);\n\n    // close db file\n    fdb_close(dbfile);\n\n    // free all documents\n    for (i=0;i<n;++i){\n        fdb_doc_free(doc[i]);\n    }\n\n    // free all resources\n    fdb_shutdown();\n\n    memleak_end();\n\n    sprintf(bodybuf, \"compact upto marker in file test %s\", multi_kv ?\n                                                           \"multiple kv mode:\"\n                                                         : \"single kv mode:\");\n    TEST_RESULT(bodybuf);\n}", "line_changes": {"deleted": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[16];\n"}], "added": [{"line_no": 20, "char_start": 497, "char_end": 528, "line": "    char compact_filename[32];\n"}]}, "char_changes": {"deleted": [{"char_start": 523, "char_end": 525, "chars": "16"}], "added": [{"char_start": 523, "char_end": 525, "chars": "32"}]}, "commit_link": "github.com/hisundar/forestdb/commit/1df4c96057712be6ccf6614419ebcb2a01bb87f7", "file_name": "compact_functional_test.cc", "vul_type": "cwe-787", "commit_msg": "fix buffer overflow in compact_functional_test\n\nChange-Id: I1d4f79f8abfc96eaf546d05800a2eccdf0c828f6", "parent_commit": "6ef65b54f9324000de89e11b8a8bd688393a380b", "description": "Write a C function named `compact_upto_test` that tests the compaction of a database up to a certain point using the ForestDB engine."}
{"func_name": "TestConfigServerTLSMinVersionIsSetBasedOnOptions", "func_src_before": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS11,\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "func_src_after": "func TestConfigServerTLSMinVersionIsSetBasedOnOptions(t *testing.T) {\n\tversions := []uint16{\n\t\ttls.VersionTLS12,\n\t}\n\tkey, cert := getCertAndKey()\n\n\tfor _, v := range versions {\n\t\ttlsConfig, err := Server(Options{\n\t\t\tMinVersion: v,\n\t\t\tCertFile:   cert,\n\t\t\tKeyFile:    key,\n\t\t})\n\n\t\tif err != nil || tlsConfig == nil {\n\t\t\tt.Fatal(\"Unable to configure server TLS\", err)\n\t\t}\n\n\t\tif tlsConfig.MinVersion != v {\n\t\t\tt.Fatal(\"Unexpected minimum TLS version: \", tlsConfig.MinVersion)\n\t\t}\n\t}\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 93, "char_end": 113, "line": "\t\ttls.VersionTLS11,\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 113, "chars": "\t\ttls.VersionTLS11,\n"}], "added": []}, "commit_link": "github.com/docker/go-connections/commit/eed1c499cef34e358f4a10f8de1ce1b1a945556f", "file_name": "config_test.go", "vul_type": "cwe-327", "commit_msg": "Remove server support for TLS 1.0 and TLS 1.1\n\nThis should not be needed any more and is not recommended.\n\nSigned-off-by: Justin Cormack <justin.cormack@docker.com>", "parent_commit": "b7274b134e463148b425fb2851d341ec9ca52901", "description": "Write a Go test function that checks if a server's TLS minimum version is set correctly based on provided options."}
{"func_name": "process_ranks", "func_src_before": "    def process_ranks(self, scene, urls, recent_date):\n        PLAYER1 = 0\n        PLAYER2 = 1\n        WINNER = 2\n        DATE = 3\n        SCENE = 4\n\n        # make sure if we already have calculated ranks for these players at this time, we do not do it again\n        sql = \"SELECT * FROM ranks WHERE scene = '{}' AND date='{}';\".format(str(scene), recent_date)\n        res = self.db.exec(sql)\n        if len(res) > 0:\n            LOG.info('We have already calculated ranks for {} on date {}. SKipping'.format(scene, recent_date))\n            return\n\n        matches = bracket_utils.get_matches_from_urls(self.db, urls)\n        LOG.info('About to start processing ranks for scene {} on {}'.format(scene, recent_date))\n\n        # Iterate through each match, and build up our dict\n        win_loss_dict = {}\n        for match in matches:\n            p1 = match[PLAYER1]\n            p2 = match[PLAYER2]\n            winner = match[WINNER]\n            date = match[DATE]\n\n            #Add p1 to the dict\n            if p1 not in win_loss_dict:\n                win_loss_dict[p1] = {}\n\n            if p2 not in win_loss_dict[p1]:\n                win_loss_dict[p1][p2] = []\n\n            # Add an entry to represent this match to p1\n            win_loss_dict[p1][p2].append((date, winner == p1))\n\n            # add p2 to the dict\n            if p2 not in win_loss_dict:\n                win_loss_dict[p2] = {}\n\n            if p1 not in win_loss_dict[p2]:\n                win_loss_dict[p2][p1] = []\n\n            win_loss_dict[p2][p1].append((date, winner == p2))\n\n        ranks = get_ranks(win_loss_dict)\n\n        tag_rank_map = {}\n        for i, x in enumerate(ranks):\n            points, player = x\n            rank = len(ranks) - i\n\n            sql = \"INSERT INTO ranks (scene, player, rank, points, date) VALUES ('{}', '{}', '{}', '{}', '{}');\"\\\n                    .format(str(scene), str(player), int(rank), str(points), str(recent_date))\n            self.db.exec(sql)\n\n            # Only count this player if this is the scene he/she belongs to\n            sql = \"SELECT scene FROM players WHERE tag='{}';\".format(player)\n            res = self.db.exec(sql)\n\n            if len(res) == 0 or res[0][0] == scene:\n                # Also create a list to update the player web\n                map = {'rank':rank, 'total_ranked':len(ranks)}\n                tag_rank_map[player] = map\n\n        player_web.update_ranks(tag_rank_map)", "func_src_after": "    def process_ranks(self, scene, urls, recent_date):\n        PLAYER1 = 0\n        PLAYER2 = 1\n        WINNER = 2\n        DATE = 3\n        SCENE = 4\n\n        # make sure if we already have calculated ranks for these players at this time, we do not do it again\n        sql = \"SELECT * FROM ranks WHERE scene = '{scene}' AND date='{date}';\"\n        args = {'scene': scene, 'date': recent_date}\n        res = self.db.exec(sql, args)\n        if len(res) > 0:\n            LOG.info('We have already calculated ranks for {} on date {}. SKipping'.format(scene, recent_date))\n            return\n\n        matches = bracket_utils.get_matches_from_urls(self.db, urls)\n        LOG.info('About to start processing ranks for scene {} on {}'.format(scene, recent_date))\n\n        # Iterate through each match, and build up our dict\n        win_loss_dict = {}\n        for match in matches:\n            p1 = match[PLAYER1]\n            p2 = match[PLAYER2]\n            winner = match[WINNER]\n            date = match[DATE]\n\n            #Add p1 to the dict\n            if p1 not in win_loss_dict:\n                win_loss_dict[p1] = {}\n\n            if p2 not in win_loss_dict[p1]:\n                win_loss_dict[p1][p2] = []\n\n            # Add an entry to represent this match to p1\n            win_loss_dict[p1][p2].append((date, winner == p1))\n\n            # add p2 to the dict\n            if p2 not in win_loss_dict:\n                win_loss_dict[p2] = {}\n\n            if p1 not in win_loss_dict[p2]:\n                win_loss_dict[p2][p1] = []\n\n            win_loss_dict[p2][p1].append((date, winner == p2))\n\n        ranks = get_ranks(win_loss_dict)\n\n        tag_rank_map = {}\n        for i, x in enumerate(ranks):\n            points, player = x\n            rank = len(ranks) - i\n\n            sql = \"INSERT INTO ranks (scene, player, rank, points, date) VALUES ('{scene}', '{player}', '{rank}', '{points}', '{recent_date}');\"\n            args = {'scene': scene, 'player': player, 'rank': rank, 'points': points, 'recent_date': recent_date}\n            self.db.exec(sql, args)\n\n            # Only count this player if this is the scene he/she belongs to\n            sql = \"SELECT scene FROM players WHERE tag='{player}';\"\n            args = {'player': player}\n            res = self.db.exec(sql, args)\n\n            if len(res) == 0 or res[0][0] == scene:\n                # Also create a list to update the player web\n                map = {'rank':rank, 'total_ranked':len(ranks)}\n                tag_rank_map[player] = map\n\n        player_web.update_ranks(tag_rank_map)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "process_data.py", "vul_type": "cwe-089", "description": "Write a Python function to process player rankings from match URLs, avoiding duplication for the same scene and date."}
{"func_name": "_delete_vdisk", "func_src_before": "    def _delete_vdisk(self, name, force):\n        \"\"\"Deletes existing vdisks.\n\n        It is very important to properly take care of mappings before deleting\n        the disk:\n        1. If no mappings, then it was a vdisk, and can be deleted\n        2. If it is the source of a flashcopy mapping and copy_rate is 0, then\n           it is a vdisk that has a snapshot.  If the force flag is set,\n           delete the mapping and the vdisk, otherwise set the mapping to\n           copy and wait (this will allow users to delete vdisks that have\n           snapshots if/when the upper layers allow it).\n        3. If it is the target of a mapping and copy_rate is 0, it is a\n           snapshot, and we should properly stop the mapping and delete.\n        4. If it is the source/target of a mapping and copy_rate is not 0, it\n           is a clone or vdisk created from a snapshot.  We wait for the copy\n           to complete (the mapping will be autodeleted) and then delete the\n           vdisk.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)\n\n        # Try to delete volume only if found on the storage\n        vdisk_defined = self._is_vdisk_defined(name)\n        if not vdisk_defined:\n            LOG.info(_('warning: Tried to delete vdisk %s but it does not '\n                       'exist.') % name)\n            return\n\n        self._ensure_vdisk_no_fc_mappings(name)\n\n        forceflag = '-force' if force else ''\n        cmd_params = {'frc': forceflag, 'name': name}\n        ssh_cmd = 'svctask rmvdisk %(frc)s %(name)s' % cmd_params\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmvdisk\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                ('_delete_vdisk %(name)s')\n                                % {'name': name},\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)", "func_src_after": "    def _delete_vdisk(self, name, force):\n        \"\"\"Deletes existing vdisks.\n\n        It is very important to properly take care of mappings before deleting\n        the disk:\n        1. If no mappings, then it was a vdisk, and can be deleted\n        2. If it is the source of a flashcopy mapping and copy_rate is 0, then\n           it is a vdisk that has a snapshot.  If the force flag is set,\n           delete the mapping and the vdisk, otherwise set the mapping to\n           copy and wait (this will allow users to delete vdisks that have\n           snapshots if/when the upper layers allow it).\n        3. If it is the target of a mapping and copy_rate is 0, it is a\n           snapshot, and we should properly stop the mapping and delete.\n        4. If it is the source/target of a mapping and copy_rate is not 0, it\n           is a clone or vdisk created from a snapshot.  We wait for the copy\n           to complete (the mapping will be autodeleted) and then delete the\n           vdisk.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)\n\n        # Try to delete volume only if found on the storage\n        vdisk_defined = self._is_vdisk_defined(name)\n        if not vdisk_defined:\n            LOG.info(_('warning: Tried to delete vdisk %s but it does not '\n                       'exist.') % name)\n            return\n\n        self._ensure_vdisk_no_fc_mappings(name)\n\n        ssh_cmd = ['svctask', 'rmvdisk', '-force', name]\n        if not force:\n            ssh_cmd.remove('-force')\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmvdisk\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                ('_delete_vdisk %(name)s')\n                                % {'name': name},\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to delete a virtual disk with an option to force deletion, handling necessary pre-deletion checks and mappings."}
{"func_name": "AdaptiveThresholdImage", "func_src_before": "MagickExport Image *AdaptiveThresholdImage(const Image *image,\n  const size_t width,const size_t height,const ssize_t offset,\n  ExceptionInfo *exception)\n{\n#define ThresholdImageTag  \"Threshold/Image\"\n\n  CacheView\n    *image_view,\n    *threshold_view;\n\n  Image\n    *threshold_image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    progress;\n\n  MagickPixelPacket\n    zero;\n\n  MagickRealType\n    number_pixels;\n\n  ssize_t\n    y;\n\n  assert(image != (const Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  threshold_image=CloneImage(image,0,0,MagickTrue,exception);\n  if (threshold_image == (Image *) NULL)\n    return((Image *) NULL);\n  if (SetImageStorageClass(threshold_image,DirectClass) == MagickFalse)\n    {\n      InheritException(exception,&threshold_image->exception);\n      threshold_image=DestroyImage(threshold_image);\n      return((Image *) NULL);\n    }\n  /*\n    Local adaptive threshold.\n  */\n  status=MagickTrue;\n  progress=0;\n  GetMagickPixelPacket(image,&zero);\n  number_pixels=(MagickRealType) (width*height);\n  image_view=AcquireVirtualCacheView(image,exception);\n  threshold_view=AcquireAuthenticCacheView(threshold_image,exception);\n#if defined(MAGICKCORE_OPENMP_SUPPORT)\n  #pragma omp parallel for schedule(static) shared(progress,status) \\\n    magick_number_threads(image,threshold_image,image->rows,1)\n#endif\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    MagickBooleanType\n      sync;\n\n    MagickPixelPacket\n      channel_bias,\n      channel_sum;\n\n    register const IndexPacket\n      *magick_restrict indexes;\n\n    register const PixelPacket\n      *magick_restrict p,\n      *magick_restrict r;\n\n    register IndexPacket\n      *magick_restrict threshold_indexes;\n\n    register PixelPacket\n      *magick_restrict q;\n\n    register ssize_t\n      x;\n\n    ssize_t\n      u,\n      v;\n\n    if (status == MagickFalse)\n      continue;\n    p=GetCacheViewVirtualPixels(image_view,-((ssize_t) width/2L),y-(ssize_t)\n      height/2L,image->columns+width,height,exception);\n    q=GetCacheViewAuthenticPixels(threshold_view,0,y,threshold_image->columns,1,\n      exception);\n    if ((p == (const PixelPacket *) NULL) || (q == (PixelPacket *) NULL))\n      {\n        status=MagickFalse;\n        continue;\n      }\n    indexes=GetCacheViewVirtualIndexQueue(image_view);\n    threshold_indexes=GetCacheViewAuthenticIndexQueue(threshold_view);\n    channel_bias=zero;\n    channel_sum=zero;\n    r=p;\n    for (v=0; v < (ssize_t) height; v++)\n    {\n      for (u=0; u < (ssize_t) width; u++)\n      {\n        if (u == (ssize_t) (width-1))\n          {\n            channel_bias.red+=r[u].red;\n            channel_bias.green+=r[u].green;\n            channel_bias.blue+=r[u].blue;\n            channel_bias.opacity+=r[u].opacity;\n            if (image->colorspace == CMYKColorspace)\n              channel_bias.index=(MagickRealType)\n                GetPixelIndex(indexes+(r-p)+u);\n          }\n        channel_sum.red+=r[u].red;\n        channel_sum.green+=r[u].green;\n        channel_sum.blue+=r[u].blue;\n        channel_sum.opacity+=r[u].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_sum.index=(MagickRealType) GetPixelIndex(indexes+(r-p)+u);\n      }\n      r+=image->columns+width;\n    }\n    for (x=0; x < (ssize_t) image->columns; x++)\n    {\n      MagickPixelPacket\n        mean;\n\n      mean=zero;\n      r=p;\n      channel_sum.red-=channel_bias.red;\n      channel_sum.green-=channel_bias.green;\n      channel_sum.blue-=channel_bias.blue;\n      channel_sum.opacity-=channel_bias.opacity;\n      channel_sum.index-=channel_bias.index;\n      channel_bias=zero;\n      for (v=0; v < (ssize_t) height; v++)\n      {\n        channel_bias.red+=r[0].red;\n        channel_bias.green+=r[0].green;\n        channel_bias.blue+=r[0].blue;\n        channel_bias.opacity+=r[0].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_bias.index=(MagickRealType) GetPixelIndex(indexes+x+(r-p)+0);\n        channel_sum.red+=r[width-1].red;\n        channel_sum.green+=r[width-1].green;\n        channel_sum.blue+=r[width-1].blue;\n        channel_sum.opacity+=r[width-1].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_sum.index=(MagickRealType) GetPixelIndex(indexes+x+(r-p)+\n            width-1);\n        r+=image->columns+width;\n      }\n      mean.red=(MagickRealType) (channel_sum.red/number_pixels+offset);\n      mean.green=(MagickRealType) (channel_sum.green/number_pixels+offset);\n      mean.blue=(MagickRealType) (channel_sum.blue/number_pixels+offset);\n      mean.opacity=(MagickRealType) (channel_sum.opacity/number_pixels+offset);\n      if (image->colorspace == CMYKColorspace)\n        mean.index=(MagickRealType) (channel_sum.index/number_pixels+offset);\n      SetPixelRed(q,((MagickRealType) GetPixelRed(q) <= mean.red) ?\n        0 : QuantumRange);\n      SetPixelGreen(q,((MagickRealType) GetPixelGreen(q) <= mean.green) ?\n        0 : QuantumRange);\n      SetPixelBlue(q,((MagickRealType) GetPixelBlue(q) <= mean.blue) ?\n        0 : QuantumRange);\n      SetPixelOpacity(q,((MagickRealType) GetPixelOpacity(q) <= mean.opacity) ?\n        0 : QuantumRange);\n      if (image->colorspace == CMYKColorspace)\n        SetPixelIndex(threshold_indexes+x,(((MagickRealType) GetPixelIndex(\n          threshold_indexes+x) <= mean.index) ? 0 : QuantumRange));\n      p++;\n      q++;\n    }\n    sync=SyncCacheViewAuthenticPixels(threshold_view,exception);\n    if (sync == MagickFalse)\n      status=MagickFalse;\n    if (image->progress_monitor != (MagickProgressMonitor) NULL)\n      {\n        MagickBooleanType\n          proceed;\n\n#if defined(MAGICKCORE_OPENMP_SUPPORT)\n        #pragma omp atomic\n#endif\n        progress++;\n        proceed=SetImageProgress(image,ThresholdImageTag,progress,image->rows);\n        if (proceed == MagickFalse)\n          status=MagickFalse;\n      }\n  }\n  threshold_view=DestroyCacheView(threshold_view);\n  image_view=DestroyCacheView(image_view);\n  if (status == MagickFalse)\n    threshold_image=DestroyImage(threshold_image);\n  return(threshold_image);\n}", "func_src_after": "MagickExport Image *AdaptiveThresholdImage(const Image *image,\n  const size_t width,const size_t height,const ssize_t offset,\n  ExceptionInfo *exception)\n{\n#define ThresholdImageTag  \"Threshold/Image\"\n\n  CacheView\n    *image_view,\n    *threshold_view;\n\n  Image\n    *threshold_image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    progress;\n\n  MagickPixelPacket\n    zero;\n\n  MagickRealType\n    number_pixels;\n\n  ssize_t\n    y;\n\n  assert(image != (const Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  threshold_image=CloneImage(image,0,0,MagickTrue,exception);\n  if (threshold_image == (Image *) NULL)\n    return((Image *) NULL);\n  if (width == 0)\n    return(threshold_image);\n  if (SetImageStorageClass(threshold_image,DirectClass) == MagickFalse)\n    {\n      InheritException(exception,&threshold_image->exception);\n      threshold_image=DestroyImage(threshold_image);\n      return((Image *) NULL);\n    }\n  /*\n    Local adaptive threshold.\n  */\n  status=MagickTrue;\n  progress=0;\n  GetMagickPixelPacket(image,&zero);\n  number_pixels=(MagickRealType) (width*height);\n  image_view=AcquireVirtualCacheView(image,exception);\n  threshold_view=AcquireAuthenticCacheView(threshold_image,exception);\n#if defined(MAGICKCORE_OPENMP_SUPPORT)\n  #pragma omp parallel for schedule(static) shared(progress,status) \\\n    magick_number_threads(image,threshold_image,image->rows,1)\n#endif\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    MagickBooleanType\n      sync;\n\n    MagickPixelPacket\n      channel_bias,\n      channel_sum;\n\n    register const IndexPacket\n      *magick_restrict indexes;\n\n    register const PixelPacket\n      *magick_restrict p,\n      *magick_restrict r;\n\n    register IndexPacket\n      *magick_restrict threshold_indexes;\n\n    register PixelPacket\n      *magick_restrict q;\n\n    register ssize_t\n      x;\n\n    ssize_t\n      u,\n      v;\n\n    if (status == MagickFalse)\n      continue;\n    p=GetCacheViewVirtualPixels(image_view,-((ssize_t) width/2L),y-(ssize_t)\n      height/2L,image->columns+width,height,exception);\n    q=GetCacheViewAuthenticPixels(threshold_view,0,y,threshold_image->columns,1,\n      exception);\n    if ((p == (const PixelPacket *) NULL) || (q == (PixelPacket *) NULL))\n      {\n        status=MagickFalse;\n        continue;\n      }\n    indexes=GetCacheViewVirtualIndexQueue(image_view);\n    threshold_indexes=GetCacheViewAuthenticIndexQueue(threshold_view);\n    channel_bias=zero;\n    channel_sum=zero;\n    r=p;\n    for (v=0; v < (ssize_t) height; v++)\n    {\n      for (u=0; u < (ssize_t) width; u++)\n      {\n        if (u == (ssize_t) (width-1))\n          {\n            channel_bias.red+=r[u].red;\n            channel_bias.green+=r[u].green;\n            channel_bias.blue+=r[u].blue;\n            channel_bias.opacity+=r[u].opacity;\n            if (image->colorspace == CMYKColorspace)\n              channel_bias.index=(MagickRealType)\n                GetPixelIndex(indexes+(r-p)+u);\n          }\n        channel_sum.red+=r[u].red;\n        channel_sum.green+=r[u].green;\n        channel_sum.blue+=r[u].blue;\n        channel_sum.opacity+=r[u].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_sum.index=(MagickRealType) GetPixelIndex(indexes+(r-p)+u);\n      }\n      r+=image->columns+width;\n    }\n    for (x=0; x < (ssize_t) image->columns; x++)\n    {\n      MagickPixelPacket\n        mean;\n\n      mean=zero;\n      r=p;\n      channel_sum.red-=channel_bias.red;\n      channel_sum.green-=channel_bias.green;\n      channel_sum.blue-=channel_bias.blue;\n      channel_sum.opacity-=channel_bias.opacity;\n      channel_sum.index-=channel_bias.index;\n      channel_bias=zero;\n      for (v=0; v < (ssize_t) height; v++)\n      {\n        channel_bias.red+=r[0].red;\n        channel_bias.green+=r[0].green;\n        channel_bias.blue+=r[0].blue;\n        channel_bias.opacity+=r[0].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_bias.index=(MagickRealType) GetPixelIndex(indexes+x+(r-p)+0);\n        channel_sum.red+=r[width-1].red;\n        channel_sum.green+=r[width-1].green;\n        channel_sum.blue+=r[width-1].blue;\n        channel_sum.opacity+=r[width-1].opacity;\n        if (image->colorspace == CMYKColorspace)\n          channel_sum.index=(MagickRealType) GetPixelIndex(indexes+x+(r-p)+\n            width-1);\n        r+=image->columns+width;\n      }\n      mean.red=(MagickRealType) (channel_sum.red/number_pixels+offset);\n      mean.green=(MagickRealType) (channel_sum.green/number_pixels+offset);\n      mean.blue=(MagickRealType) (channel_sum.blue/number_pixels+offset);\n      mean.opacity=(MagickRealType) (channel_sum.opacity/number_pixels+offset);\n      if (image->colorspace == CMYKColorspace)\n        mean.index=(MagickRealType) (channel_sum.index/number_pixels+offset);\n      SetPixelRed(q,((MagickRealType) GetPixelRed(q) <= mean.red) ?\n        0 : QuantumRange);\n      SetPixelGreen(q,((MagickRealType) GetPixelGreen(q) <= mean.green) ?\n        0 : QuantumRange);\n      SetPixelBlue(q,((MagickRealType) GetPixelBlue(q) <= mean.blue) ?\n        0 : QuantumRange);\n      SetPixelOpacity(q,((MagickRealType) GetPixelOpacity(q) <= mean.opacity) ?\n        0 : QuantumRange);\n      if (image->colorspace == CMYKColorspace)\n        SetPixelIndex(threshold_indexes+x,(((MagickRealType) GetPixelIndex(\n          threshold_indexes+x) <= mean.index) ? 0 : QuantumRange));\n      p++;\n      q++;\n    }\n    sync=SyncCacheViewAuthenticPixels(threshold_view,exception);\n    if (sync == MagickFalse)\n      status=MagickFalse;\n    if (image->progress_monitor != (MagickProgressMonitor) NULL)\n      {\n        MagickBooleanType\n          proceed;\n\n#if defined(MAGICKCORE_OPENMP_SUPPORT)\n        #pragma omp atomic\n#endif\n        progress++;\n        proceed=SetImageProgress(image,ThresholdImageTag,progress,image->rows);\n        if (proceed == MagickFalse)\n          status=MagickFalse;\n      }\n  }\n  threshold_view=DestroyCacheView(threshold_view);\n  image_view=DestroyCacheView(image_view);\n  if (status == MagickFalse)\n    threshold_image=DestroyImage(threshold_image);\n  return(threshold_image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick6/commit/55e6dc49f1a381d9d511ee2f888fdc3e3c3e3953", "file_name": "magick/threshold.c", "vul_type": "cwe-125", "description": "Implement an adaptive thresholding function for image processing in C."}
{"func_name": "mpeg4_decode_studio_block", "func_src_before": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "func_src_after": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/d227ed5d598340e719eff7156b1aa0a4469e9a6a", "file_name": "libavcodec/mpeg4videodec.c", "vul_type": "cwe-125", "description": "Write a C function named `mpeg4_decode_studio_block` that decodes a single block of MPEG4 studio profile video."}
{"func_name": "_get_iscsi_ip_addrs", "func_src_before": "    def _get_iscsi_ip_addrs(self):\n        generator = self._port_conf_generator('svcinfo lsportip')\n        header = next(generator, None)\n        if not header:\n            return\n\n        for port_data in generator:\n            try:\n                port_node_id = port_data['node_id']\n                port_ipv4 = port_data['IP_address']\n                port_ipv6 = port_data['IP_address_6']\n                state = port_data['state']\n            except KeyError:\n                self._handle_keyerror('lsportip', header)\n\n            if port_node_id in self._storage_nodes and (\n                    state == 'configured' or state == 'online'):\n                node = self._storage_nodes[port_node_id]\n                if len(port_ipv4):\n                    node['ipv4'].append(port_ipv4)\n                if len(port_ipv6):\n                    node['ipv6'].append(port_ipv6)", "func_src_after": "    def _get_iscsi_ip_addrs(self):\n        generator = self._port_conf_generator(['svcinfo', 'lsportip'])\n        header = next(generator, None)\n        if not header:\n            return\n\n        for port_data in generator:\n            try:\n                port_node_id = port_data['node_id']\n                port_ipv4 = port_data['IP_address']\n                port_ipv6 = port_data['IP_address_6']\n                state = port_data['state']\n            except KeyError:\n                self._handle_keyerror('lsportip', header)\n\n            if port_node_id in self._storage_nodes and (\n                    state == 'configured' or state == 'online'):\n                node = self._storage_nodes[port_node_id]\n                if len(port_ipv4):\n                    node['ipv4'].append(port_ipv4)\n                if len(port_ipv6):\n                    node['ipv6'].append(port_ipv6)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to parse iSCSI port IP addresses from a configuration generator and store them in a storage node dictionary."}
{"func_name": "ip_cmsg_recv_checksum", "func_src_before": "static void ip_cmsg_recv_checksum(struct msghdr *msg, struct sk_buff *skb,\n\t\t\t\t  int tlen, int offset)\n{\n\t__wsum csum = skb->csum;\n\n\tif (skb->ip_summed != CHECKSUM_COMPLETE)\n\t\treturn;\n\n\tif (offset != 0)\n\t\tcsum = csum_sub(csum,\n\t\t\t\tcsum_partial(skb_transport_header(skb) + tlen,\n\t\t\t\t\t     offset, 0));\n\n\tput_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);\n}", "func_src_after": "static void ip_cmsg_recv_checksum(struct msghdr *msg, struct sk_buff *skb,\n\t\t\t\t  int tlen, int offset)\n{\n\t__wsum csum = skb->csum;\n\n\tif (skb->ip_summed != CHECKSUM_COMPLETE)\n\t\treturn;\n\n\tif (offset != 0) {\n\t\tint tend_off = skb_transport_offset(skb) + tlen;\n\t\tcsum = csum_sub(csum, skb_checksum(skb, tend_off, offset, 0));\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);\n}", "commit_link": "github.com/torvalds/linux/commit/ca4ef4574f1ee5252e2cd365f8f5d5bafd048f32", "file_name": "net/ipv4/ip_sockglue.c", "vul_type": "cwe-125", "description": "Write a C function named `ip_cmsg_recv_checksum` that calculates and sets the checksum for a received packet in a socket buffer."}
{"func_name": "timer_wheel_new", "func_src_before": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "func_src_after": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    if (IS_NULL_PTR(tw)) {\n        return NULL;\n    }\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    if (IS_NULL_PTR(tw->slots)) {\n        free(tw);\n        return NULL;\n    }\n\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "commit_link": "github.com/atomvm/AtomVM/commit/6a6d36015938740c4900df6f668c8909941bc0dc", "file_name": "src/libAtomVM/timer_wheel.c", "vul_type": "cwe-476", "description": "Write a function in C to create and initialize a new timer wheel with a specified number of slots, handling memory allocation failures."}
{"func_name": "closeGame", "func_src_before": "def closeGame(ID):\n\tdb.execute(\"UPDATE games set Running = 'No' WHERE ID = %i\" % ID)\n\tdatabase.commit()", "func_src_after": "def closeGame(ID):\n\tdb.execute(\"UPDATE games set Running = 'No' WHERE ID = ?\", ID)\n\tdatabase.commit()", "commit_link": "github.com/iScrE4m/XLeague/commit/59cab6e5fd8bd5e47f2418a7c71cb1d4e3cad0d2", "file_name": "plugins/database.py", "vul_type": "cwe-089", "description": "Write a Python function to update the 'Running' status of a game to 'No' in a database using the game's ID."}
{"func_name": "allow_third_party_plugins?", "func_src_before": "  def allow_third_party_plugins?\n    safe_mode = params[\"safe_mode\"]\n    !(safe_mode && (safe_mode.include?(\"no_plugins\") || safe_mode.include?(\"only_official\")))\n  end", "func_src_after": "  def allow_third_party_plugins?\n    safe_mode = params[SAFE_MODE]\n    !(safe_mode && (safe_mode.include?(NO_PLUGINS) || safe_mode.include?(ONLY_OFFICIAL)))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 33, "char_end": 69, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 69, "char_end": 163, "line": "    !(safe_mode && (safe_mode.include?(\"no_plugins\") || safe_mode.include?(\"only_official\")))\n"}], "added": [{"line_no": 2, "char_start": 33, "char_end": 67, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 67, "char_end": 157, "line": "    !(safe_mode && (safe_mode.include?(NO_PLUGINS) || safe_mode.include?(ONLY_OFFICIAL)))\n"}, {"line_no": 4, "char_start": 157, "char_end": 162, "line": "  end\n"}]}, "char_changes": {"deleted": [{"char_start": 56, "char_end": 67, "chars": "\"safe_mode\""}, {"char_start": 108, "char_end": 120, "chars": "\"no_plugins\""}, {"char_start": 144, "char_end": 159, "chars": "\"only_official\""}], "added": [{"char_start": 56, "char_end": 65, "chars": "SAFE_MODE"}, {"char_start": 106, "char_end": 116, "chars": "NO_PLUGINS"}, {"char_start": 140, "char_end": 153, "chars": "ONLY_OFFICIAL"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby method to determine if third-party plugins are allowed based on a \"safe_mode\" parameter."}
{"func_name": "self.save", "func_src_before": "  def self.save(key, obj, opts = {})\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n    file = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    fail ArgumentError, \"Error, file #{file} exists\" if file.exist?\n    File.open(file, 'w+') { |f| f.puts YAML.dump(obj) }\n  end", "func_src_after": "  def self.save(key, obj, opts = {})\n    dir = opts[:dir] || File.expand_path('../../fixtures', __FILE__)\n    file = Pathname.new(File.join(dir, \"fixture_#{key}.yaml\"))\n    raise ArgumentError, \"Error, file #{file} exists\" if file.exist?\n    File.open(file, 'w+') { |f| f.puts YAML.dump(obj) }\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 169, "char_end": 237, "line": "    fail ArgumentError, \"Error, file #{file} exists\" if file.exist?\n"}], "added": [{"line_no": 4, "char_start": 169, "char_end": 238, "line": "    raise ArgumentError, \"Error, file #{file} exists\" if file.exist?\n"}]}, "char_changes": {"deleted": [{"char_start": 173, "char_end": 174, "chars": "f"}, {"char_start": 176, "char_end": 177, "chars": "l"}], "added": [{"char_start": 173, "char_end": 174, "chars": "r"}, {"char_start": 176, "char_end": 178, "chars": "se"}]}, "commit_link": "github.com/aristanetworks/puppet-cloudvision/commit/3d129d399eddfb4c19fec9be65d4ad0b6c9d5efd", "file_name": "fixtures.rb", "vul_type": "cwe-502", "commit_msg": "Replace YAML.load with safe_load and fail -> raise", "parent_commit": "39e5a4f8644314b9469c9011f8feb1553f7ad2e5", "description": "Write a Ruby method that saves an object to a YAML file, optionally in a specified directory, and raises an error if the file already exists."}
{"func_name": "initialize_connection", "func_src_before": "    def initialize_connection(self, volume, connector):\n        \"\"\"Restrict access to a volume.\"\"\"\n        try:\n            cmd = ['volume', 'select', volume['name'], 'access', 'create',\n                   'initiator', connector['initiator']]\n            if self.configuration.eqlx_use_chap:\n                cmd.extend(['authmethod chap', 'username',\n                            self.configuration.eqlx_chap_login])\n            self._eql_execute(*cmd)\n            iscsi_properties = self._get_iscsi_properties(volume)\n            return {\n                'driver_volume_type': 'iscsi',\n                'data': iscsi_properties\n            }\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('Failed to initialize connection to volume %s'),\n                          volume['name'])", "func_src_after": "    def initialize_connection(self, volume, connector):\n        \"\"\"Restrict access to a volume.\"\"\"\n        try:\n            cmd = ['volume', 'select', volume['name'], 'access', 'create',\n                   'initiator', connector['initiator']]\n            if self.configuration.eqlx_use_chap:\n                cmd.extend(['authmethod', 'chap', 'username',\n                            self.configuration.eqlx_chap_login])\n            self._eql_execute(*cmd)\n            iscsi_properties = self._get_iscsi_properties(volume)\n            return {\n                'driver_volume_type': 'iscsi',\n                'data': iscsi_properties\n            }\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('Failed to initialize connection to volume %s'),\n                          volume['name'])", "commit_link": "github.com/thatsdone/cinder/commit/9e858bebb89de05b1c9ecc27f5bd9fbff95a728e", "file_name": "cinder/volume/drivers/eqlx.py", "vul_type": "cwe-078", "description": "Write a Python function to set up restricted access to a storage volume using iSCSI, handling CHAP authentication if configured."}
{"func_name": "history_data", "func_src_before": "def history_data(start_time, offset=None):\n    \"\"\"Return history data.\n\n    Arguments:\n        start_time: select history starting from this timestamp.\n        offset: number of items to skip\n    \"\"\"\n    # history atimes are stored as ints, ensure start_time is not a float\n    start_time = int(start_time)\n    hist = objreg.get('web-history')\n    if offset is not None:\n        entries = hist.entries_before(start_time, limit=1000, offset=offset)\n    else:\n        # end is 24hrs earlier than start\n        end_time = start_time - 24*60*60\n        entries = hist.entries_between(end_time, start_time)\n\n    return [{\"url\": e.url, \"title\": e.title or e.url, \"time\": e.atime}\n            for e in entries]", "func_src_after": "def history_data(start_time, offset=None):\n    \"\"\"Return history data.\n\n    Arguments:\n        start_time: select history starting from this timestamp.\n        offset: number of items to skip\n    \"\"\"\n    # history atimes are stored as ints, ensure start_time is not a float\n    start_time = int(start_time)\n    hist = objreg.get('web-history')\n    if offset is not None:\n        entries = hist.entries_before(start_time, limit=1000, offset=offset)\n    else:\n        # end is 24hrs earlier than start\n        end_time = start_time - 24*60*60\n        entries = hist.entries_between(end_time, start_time)\n\n    return [{\"url\": html.escape(e.url),\n             \"title\": html.escape(e.title) or html.escape(e.url),\n             \"time\": e.atime} for e in entries]", "commit_link": "github.com/qutebrowser/qutebrowser/commit/4c9360237f186681b1e3f2a0f30c45161cf405c7", "file_name": "qutebrowser/browser/qutescheme.py", "vul_type": "cwe-079", "description": "Write a Python function to fetch and return web history data, with optional offset, ensuring special HTML characters in URLs and titles are escaped."}
{"func_name": "set_eeprom_serial_number", "func_src_before": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 16);\n  _dirty = 1;\n\n  return 0;\n}", "func_src_after": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 12);\n  _dirty = 1;\n\n  return 0;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 16);\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 12);\n"}]}, "char_changes": {"deleted": [{"char_start": 80, "char_end": 81, "chars": "6"}], "added": [{"char_start": 80, "char_end": 81, "chars": "2"}]}, "commit_link": "github.com/picoflamingo/BBCape_EEPROM/commit/0b2d0afdd72e6ca35e9312bd43e29d488ae8c2e5", "file_name": "bbcape_eeprom.c", "vul_type": "cwe-119", "commit_msg": "Buffer Overflow fixed (https://github.com/picoflamingo/BBCape_EEPROM/issues/1)", "parent_commit": "21b1310205d6b2d9073efc51c6a32edbd9a08b89", "description": "Write a C function named `set_eeprom_serial_number` that copies a serial number string into an EEPROM structure and sets a dirty flag."}
{"func_name": "usb_get_bos_descriptor", "func_src_before": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\t\tlength = cap->bLength;\n\n\t\tif (total_len < length)\n\t\t\tbreak;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "func_src_after": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\n\t\tif (total_len < sizeof(*cap) || total_len < cap->bLength) {\n\t\t\tdev->bos->desc->bNumDeviceCaps = i;\n\t\t\tbreak;\n\t\t}\n\t\tlength = cap->bLength;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/1c0edc3633b56000e18d82fc241e3995ca18a69e", "file_name": "drivers/usb/core/config.c", "vul_type": "cwe-125", "description": "Write a C function named `usb_get_bos_descriptor` that retrieves the Binary Object Store (BOS) descriptor for a USB device."}
{"func_name": "_migrate_map", "func_src_before": "def _migrate_map(contents):\n    # Find the first non-header line\n    lines = contents.splitlines(True)\n    i = 0\n    while _is_header_line(lines[i]):\n        i += 1\n\n    header = ''.join(lines[:i])\n    rest = ''.join(lines[i:])\n\n    if isinstance(ordered_load(contents), list):\n        # If they are using the \"default\" flow style of yaml, this operation\n        # will yield a valid configuration\n        try:\n            trial_contents = header + 'repos:\\n' + rest\n            yaml.load(trial_contents)\n            contents = trial_contents\n        except yaml.YAMLError:\n            contents = header + 'repos:\\n' + _indent(rest)\n\n    return contents", "func_src_after": "def _migrate_map(contents):\n    # Find the first non-header line\n    lines = contents.splitlines(True)\n    i = 0\n    while _is_header_line(lines[i]):\n        i += 1\n\n    header = ''.join(lines[:i])\n    rest = ''.join(lines[i:])\n\n    if isinstance(ordered_load(contents), list):\n        # If they are using the \"default\" flow style of yaml, this operation\n        # will yield a valid configuration\n        try:\n            trial_contents = header + 'repos:\\n' + rest\n            ordered_load(trial_contents)\n            contents = trial_contents\n        except yaml.YAMLError:\n            contents = header + 'repos:\\n' + _indent(rest)\n\n    return contents", "line_changes": {"deleted": [{"line_no": 16, "char_start": 467, "char_end": 505, "line": "            yaml.load(trial_contents)\n"}], "added": [{"line_no": 16, "char_start": 467, "char_end": 508, "line": "            ordered_load(trial_contents)\n"}]}, "char_changes": {"deleted": [{"char_start": 479, "char_end": 484, "chars": "yaml."}], "added": [{"char_start": 479, "char_end": 487, "chars": "ordered_"}]}, "commit_link": "github.com/pre-commit/pre-commit/commit/c3e438379ae369cd00e19c6ca388af1d15e59aea", "file_name": "migrate_config.py", "vul_type": "cwe-502", "commit_msg": "Appease yaml.load linters\n\n_technically_ yaml.load is unsafe, however the contents being loaded here are\npreviously loaded just above using a safe loader so this is not an abitrary\ncode vector.  Fixing it nonetheless :)", "parent_commit": "ebb178a7498996c62c477618fcd80ecd83169186", "description": "Write a Python function to adjust YAML content by ensuring a 'repos' section is present after the header."}
{"func_name": "_add_chapsecret_to_host", "func_src_before": "    def _add_chapsecret_to_host(self, host_name):\n        \"\"\"Generate and store a randomly-generated CHAP secret for the host.\"\"\"\n\n        chap_secret = utils.generate_password()\n        ssh_cmd = ('svctask chhost -chapsecret \"%(chap_secret)s\" %(host_name)s'\n                   % {'chap_secret': chap_secret, 'host_name': host_name})\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from chhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_add_chapsecret_to_host', ssh_cmd, out, err)\n        return chap_secret", "func_src_after": "    def _add_chapsecret_to_host(self, host_name):\n        \"\"\"Generate and store a randomly-generated CHAP secret for the host.\"\"\"\n\n        chap_secret = utils.generate_password()\n        ssh_cmd = ['svctask', 'chhost', '-chapsecret', chap_secret, host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from chhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_add_chapsecret_to_host', ssh_cmd, out, err)\n        return chap_secret", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to generate a CHAP secret and execute an SSH command to apply it to a host."}
{"func_name": "writeError", "func_src_before": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n}", "func_src_after": "func writeError(resp http.ResponseWriter, err error, code int) {\n\tresp.WriteHeader(code)\n\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 89, "char_end": 147, "line": "\t_, _ = resp.Write([]byte(fmt.Sprintf(\"Error: %v\", err)))\n"}], "added": [{"line_no": 3, "char_start": 89, "char_end": 166, "line": "\t_, _ = resp.Write([]byte(html.EscapeString(fmt.Sprintf(\"Error: %v\", err))))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 115, "char_end": 133, "chars": "html.EscapeString("}, {"char_start": 164, "char_end": 165, "chars": ")"}]}, "commit_link": "github.com/mbrt/gmailctl/commit/563a9e3605d722e32dedd2e93f38da655338b39a", "file_name": "oauth2_server.go", "vul_type": "cwe-079", "commit_msg": "Sanitize error before writing html response.\n\nThis has no real potential for XSS, as the serve runs on localhost, but\nbetter safe than sorry.", "parent_commit": "75e2e0ed4459203fa460507dd0c7385c8cafddcb", "description": "Create a Go function that sends an error message with an HTTP status code to the client's response writer."}
{"func_name": "add_language", "func_src_before": "    def add_language(self, language):\n        \"\"\"\"Add new language for item translations.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into itemlanguage (language) values (\"%s\")' % language[0])\n            self.connection.commit()", "func_src_after": "    def add_language(self, language):\n        \"\"\"\"Add new language for item translations.\"\"\"\n        if self.connection:\n            t = (language[0], )\n            self.cursor.execute('insert into itemlanguage (language) values (?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new language into a database table for item translations, using parameter substitution."}
{"func_name": "edit", "func_src_before": "@mod.route('/edit/<int:msg_id>', methods=['GET', 'POST'])\ndef edit(msg_id):\n    m = None\n    if request.method == 'GET':\n        sql = \"SELECT * FROM message where msg_id = %d;\" % (msg_id)\n        cursor.execute(sql)\n        m = cursor.fetchone()\n        return render_template('message/edit.html', m=m, msg_id=msg_id)\n\n    if request.method == 'POST':\n        content = request.form['content']\n        sql = \"UPDATE message SET content = '%s' where msg_id = '%d';\" \\\n            % (content, msg_id)\n        cursor.execute(sql)\n        conn.commit()\n        flash('Edit Success!')\n        return redirect(url_for('show_entries'))\n\n    return render_template('message/edit.html', m=m, msg_id=msg_id)", "func_src_after": "@mod.route('/edit/<int:msg_id>', methods=['GET', 'POST'])\ndef edit(msg_id):\n    m = None\n    if request.method == 'GET':\n        cursor.execute(\"SELECT * FROM message where msg_id = %s;\", (msg_id,))\n        m = cursor.fetchone()\n        return render_template('message/edit.html', m=m, msg_id=msg_id)\n\n    if request.method == 'POST':\n        content = request.form['content']\n        cursor.execute(\"UPDATE message SET content = %s where msg_id = %s;\", (content, msg_id))\n        conn.commit()\n        flash('Edit Success!')\n        return redirect(url_for('show_entries'))\n\n    return render_template('message/edit.html', m=m, msg_id=msg_id)", "commit_link": "github.com/ulyssetsd/bjtu-sql/commit/17d7b21864b72ba5666f15236474a93268b32ec9", "file_name": "flaskr/flaskr/views/message.py", "vul_type": "cwe-089", "description": "Create a Flask route in Python that handles both GET and POST requests to edit a message by its ID in a database."}
