{"func_name": "do_file_clean", "func_src_before": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "func_src_after": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 127, "char_end": 171, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n"}], "added": [{"line_no": 6, "char_start": 127, "char_end": 177, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 168, "char_end": 174, "chars": ", 0664"}]}, "commit_link": "github.com/andikleen/mce-test/commit/4da149ab9462e1e8750e2db7a93fa88429eab91a", "file_name": "tinjpage.c", "vul_type": "cwe-732", "commit_msg": "mce-test: Add file attributes to open\n\nThis fixes a compile warning.\n\nopen(2) manpage says:\n... mode specifies the permissions to use in case a new\nfile is created. This argument must be  supplied  when\nO_CREAT is specified  in  flags; ...\n\nSigned-off-by: Thomas Renninger <trenn@suse.de>\nSigned-off-by: Chen Gong <gong.chen@linux.intel.com>", "parent_commit": "cfdf0d9f9564de49ec38c794fa4b151babea98f1", "description": "Write a C function to create a temporary file, map it into memory, perform read/write tests, and print a value from the mapped memory."}
{"func_name": "add_article_action", "func_src_before": "def add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = request.POST[\"notes\"]\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response", "func_src_after": "def add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = escape(request.POST[\"notes\"])\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response", "commit_link": "github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "file_name": "c3shop/frontpage/management/reservation_actions.py", "vul_type": "cwe-079", "description": "Write a Python function to handle adding single or multiple articles to a reservation, with redirection and error handling."}
{"func_name": "UiApplication::SecurityConfiguration::configure", "func_src_before": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n\n            http.headers().httpStrictTransportSecurity();\n            http.csrf().disable();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "func_src_after": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n            //http.csrf().disable();\n\n\n            http.headers().httpStrictTransportSecurity();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "line_changes": {"deleted": [{"line_no": 12, "char_start": 548, "char_end": 583, "line": "            http.csrf().disable();\n"}], "added": [{"line_no": 11, "char_start": 526, "char_end": 527, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 548, "char_end": 583, "chars": "            http.csrf().disable();\n"}], "added": [{"char_start": 489, "char_end": 527, "chars": "            //http.csrf().disable();\n\n"}]}, "commit_link": "github.com/barbaraisabelvieira/VulnerableDemoApp/commit/a64bbb524722f63134826a7d0424d71518a0aae6", "file_name": "UiApplication.java", "vul_type": "cwe-352", "commit_msg": "Fixing CSRF - enabled now", "parent_commit": "1faf96211c58f44a8510497e08d31757160c6367", "description": "Write a Java method using Spring Security to configure HTTP security, allowing unrestricted access to specific URL patterns and requiring authentication for all other requests."}
{"func_name": "openscript", "func_src_before": "openscript(\n    char_u\t*name,\n    int\t\tdirectly)\t/* when TRUE execute directly */\n{\n    if (curscript + 1 == NSCRIPT)\n    {\n\temsg(_(e_nesting));\n\treturn;\n    }\n#ifdef FEAT_EVAL\n    if (ignore_script)\n\t/* Not reading from script, also don't open one.  Warning message? */\n\treturn;\n#endif\n\n    if (scriptin[curscript] != NULL)\t/* already reading script */\n\t++curscript;\n\t\t\t\t/* use NameBuff for expanded name */\n    expand_env(name, NameBuff, MAXPATHL);\n    if ((scriptin[curscript] = mch_fopen((char *)NameBuff, READBIN)) == NULL)\n    {\n\tsemsg(_(e_notopen), name);\n\tif (curscript)\n\t    --curscript;\n\treturn;\n    }\n    if (save_typebuf() == FAIL)\n\treturn;\n\n    /*\n     * Execute the commands from the file right now when using \":source!\"\n     * after \":global\" or \":argdo\" or in a loop.  Also when another command\n     * follows.  This means the display won't be updated.  Don't do this\n     * always, \"make test\" would fail.\n     */\n    if (directly)\n    {\n\toparg_T\toa;\n\tint\toldcurscript;\n\tint\tsave_State = State;\n\tint\tsave_restart_edit = restart_edit;\n\tint\tsave_insertmode = p_im;\n\tint\tsave_finish_op = finish_op;\n\tint\tsave_msg_scroll = msg_scroll;\n\n\tState = NORMAL;\n\tmsg_scroll = FALSE;\t/* no msg scrolling in Normal mode */\n\trestart_edit = 0;\t/* don't go to Insert mode */\n\tp_im = FALSE;\t\t/* don't use 'insertmode' */\n\tclear_oparg(&oa);\n\tfinish_op = FALSE;\n\n\toldcurscript = curscript;\n\tdo\n\t{\n\t    update_topline_cursor();\t// update cursor position and topline\n\t    normal_cmd(&oa, FALSE);\t// execute one command\n\t    vpeekc();\t\t\t// check for end of file\n\t}\n\twhile (scriptin[oldcurscript] != NULL);\n\n\tState = save_State;\n\tmsg_scroll = save_msg_scroll;\n\trestart_edit = save_restart_edit;\n\tp_im = save_insertmode;\n\tfinish_op = save_finish_op;\n    }\n}", "func_src_after": "openscript(\n    char_u\t*name,\n    int\t\tdirectly)\t/* when TRUE execute directly */\n{\n    if (curscript + 1 == NSCRIPT)\n    {\n\temsg(_(e_nesting));\n\treturn;\n    }\n\n    // Disallow sourcing a file in the sandbox, the commands would be executed\n    // later, possibly outside of the sandbox.\n    if (check_secure())\n\treturn;\n\n#ifdef FEAT_EVAL\n    if (ignore_script)\n\t/* Not reading from script, also don't open one.  Warning message? */\n\treturn;\n#endif\n\n    if (scriptin[curscript] != NULL)\t/* already reading script */\n\t++curscript;\n\t\t\t\t/* use NameBuff for expanded name */\n    expand_env(name, NameBuff, MAXPATHL);\n    if ((scriptin[curscript] = mch_fopen((char *)NameBuff, READBIN)) == NULL)\n    {\n\tsemsg(_(e_notopen), name);\n\tif (curscript)\n\t    --curscript;\n\treturn;\n    }\n    if (save_typebuf() == FAIL)\n\treturn;\n\n    /*\n     * Execute the commands from the file right now when using \":source!\"\n     * after \":global\" or \":argdo\" or in a loop.  Also when another command\n     * follows.  This means the display won't be updated.  Don't do this\n     * always, \"make test\" would fail.\n     */\n    if (directly)\n    {\n\toparg_T\toa;\n\tint\toldcurscript;\n\tint\tsave_State = State;\n\tint\tsave_restart_edit = restart_edit;\n\tint\tsave_insertmode = p_im;\n\tint\tsave_finish_op = finish_op;\n\tint\tsave_msg_scroll = msg_scroll;\n\n\tState = NORMAL;\n\tmsg_scroll = FALSE;\t/* no msg scrolling in Normal mode */\n\trestart_edit = 0;\t/* don't go to Insert mode */\n\tp_im = FALSE;\t\t/* don't use 'insertmode' */\n\tclear_oparg(&oa);\n\tfinish_op = FALSE;\n\n\toldcurscript = curscript;\n\tdo\n\t{\n\t    update_topline_cursor();\t// update cursor position and topline\n\t    normal_cmd(&oa, FALSE);\t// execute one command\n\t    vpeekc();\t\t\t// check for end of file\n\t}\n\twhile (scriptin[oldcurscript] != NULL);\n\n\tState = save_State;\n\tmsg_scroll = save_msg_scroll;\n\trestart_edit = save_restart_edit;\n\tp_im = save_insertmode;\n\tfinish_op = save_finish_op;\n    }\n}", "commit_link": "github.com/vim/vim/commit/53575521406739cf20bbe4e384d88e7dca11f040", "file_name": "src/getchar.c", "vul_type": "cwe-078", "description": "In C, write a function `openscript` that takes a script name and a flag to execute directly, handling script nesting and environment expansion."}
{"func_name": "mpeg4video_probe", "func_src_before": "static int mpeg4video_probe(AVProbeData *probe_packet)\n{\n    uint32_t temp_buffer = -1;\n    int VO = 0, VOL = 0, VOP = 0, VISO = 0, res = 0;\n    int i;\n\n    for (i = 0; i < probe_packet->buf_size; i++) {\n        temp_buffer = (temp_buffer << 8) + probe_packet->buf[i];\n        if ((temp_buffer & 0xffffff00) != 0x100)\n            continue;\n\n        if (temp_buffer == VOP_START_CODE)\n            VOP++;\n        else if (temp_buffer == VISUAL_OBJECT_START_CODE)\n            VISO++;\n        else if (temp_buffer < 0x120)\n            VO++;\n        else if (temp_buffer < 0x130)\n            VOL++;\n        else if (!(0x1AF < temp_buffer && temp_buffer < 0x1B7) &&\n                 !(0x1B9 < temp_buffer && temp_buffer < 0x1C4))\n            res++;\n    }\n\n    if (VOP >= VISO && VOP >= VOL && VO >= VOL && VOL > 0 && res == 0)\n        return AVPROBE_SCORE_EXTENSION;\n    return 0;\n}", "func_src_after": "static int mpeg4video_probe(AVProbeData *probe_packet)\n{\n    uint32_t temp_buffer = -1;\n    int VO = 0, VOL = 0, VOP = 0, VISO = 0, res = 0;\n    int i;\n\n    for (i = 0; i < probe_packet->buf_size; i++) {\n        temp_buffer = (temp_buffer << 8) + probe_packet->buf[i];\n        if (temp_buffer & 0xfffffe00)\n            continue;\n        if (temp_buffer < 2)\n            continue;\n\n        if (temp_buffer == VOP_START_CODE)\n            VOP++;\n        else if (temp_buffer == VISUAL_OBJECT_START_CODE)\n            VISO++;\n        else if (temp_buffer >= 0x100 && temp_buffer < 0x120)\n            VO++;\n        else if (temp_buffer >= 0x120 && temp_buffer < 0x130)\n            VOL++;\n        else if (!(0x1AF < temp_buffer && temp_buffer < 0x1B7) &&\n                 !(0x1B9 < temp_buffer && temp_buffer < 0x1C4))\n            res++;\n    }\n\n    if (VOP >= VISO && VOP >= VOL && VO >= VOL && VOL > 0 && res == 0)\n        return AVPROBE_SCORE_EXTENSION;\n    return 0;\n}", "commit_link": "github.com/libav/libav/commit/e5b019725f53b79159931d3a7317107cbbfd0860", "file_name": "libavformat/m4vdec.c", "vul_type": "cwe-476", "description": "Write a C function named `mpeg4video_probe` that checks for MPEG-4 video signatures in a buffer and returns a score based on specific criteria."}
{"func_name": "add_empty_commit", "func_src_before": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        os.system(f'git clone {folder} {tmpdirname}')\n        os.chdir(tmpdirname)\n        os.system(f'git checkout -b {which_branch}')\n        os.system(\"git \" +\n                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n                  \"--allow-empty -m 'initial empty commit' \" +\n                  \"--author='submitty <submitty@example.com>'\")\n        os.system(f'git push origin {which_branch}')\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "func_src_after": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        subprocess.run(['git', 'clone', folder, tmpdirname])\n        os.chdir(tmpdirname)\n        subprocess.run(['git', 'checkout', '-b', which_branch])\n        subprocess.run(['git',\n            '-c', 'user.name=submitty',\n            '-c', 'user.email=submitty@example.com',\n            'commit', '--allow-empty',\n            '-m', 'initial empty commit',\n            '--author=submitty <submitty@example.com>'])\n        subprocess.run(['git', 'push', 'origin', which_branch])\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "line_changes": {"deleted": [{"line_no": 17, "char_start": 618, "char_end": 672, "line": "        os.system(f'git clone {folder} {tmpdirname}')\n"}, {"line_no": 19, "char_start": 701, "char_end": 754, "line": "        os.system(f'git checkout -b {which_branch}')\n"}, {"line_no": 20, "char_start": 754, "char_end": 781, "line": "        os.system(\"git \" +\n"}, {"line_no": 21, "char_start": 781, "char_end": 868, "line": "                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n"}, {"line_no": 22, "char_start": 868, "char_end": 931, "line": "                  \"--allow-empty -m 'initial empty commit' \" +\n"}, {"line_no": 23, "char_start": 931, "char_end": 995, "line": "                  \"--author='submitty <submitty@example.com>'\")\n"}, {"line_no": 24, "char_start": 995, "char_end": 1048, "line": "        os.system(f'git push origin {which_branch}')\n"}], "added": [{"line_no": 17, "char_start": 618, "char_end": 679, "line": "        subprocess.run(['git', 'clone', folder, tmpdirname])\n"}, {"line_no": 19, "char_start": 708, "char_end": 772, "line": "        subprocess.run(['git', 'checkout', '-b', which_branch])\n"}, {"line_no": 20, "char_start": 772, "char_end": 803, "line": "        subprocess.run(['git',\n"}, {"line_no": 21, "char_start": 803, "char_end": 843, "line": "            '-c', 'user.name=submitty',\n"}, {"line_no": 22, "char_start": 843, "char_end": 896, "line": "            '-c', 'user.email=submitty@example.com',\n"}, {"line_no": 23, "char_start": 896, "char_end": 935, "line": "            'commit', '--allow-empty',\n"}, {"line_no": 24, "char_start": 935, "char_end": 977, "line": "            '-m', 'initial empty commit',\n"}, {"line_no": 25, "char_start": 977, "char_end": 1034, "line": "            '--author=submitty <submitty@example.com>'])\n"}, {"line_no": 26, "char_start": 1034, "char_end": 1098, "line": "        subprocess.run(['git', 'push', 'origin', which_branch])\n"}]}, "char_changes": {"deleted": [{"char_start": 626, "char_end": 642, "chars": "os.system(f'git "}, {"char_start": 648, "char_end": 649, "chars": "{"}, {"char_start": 655, "char_end": 658, "chars": "} {"}, {"char_start": 668, "char_end": 670, "chars": "}'"}, {"char_start": 709, "char_end": 738, "chars": "os.system(f'git checkout -b {"}, {"char_start": 750, "char_end": 752, "chars": "}'"}, {"char_start": 762, "char_end": 785, "chars": "os.system(\"git \" +\n    "}, {"char_start": 797, "char_end": 803, "chars": "  \"-c "}, {"char_start": 821, "char_end": 825, "chars": " -c "}, {"char_start": 856, "char_end": 867, "chars": " commit \" +"}, {"char_start": 880, "char_end": 887, "chars": "      \""}, {"char_start": 900, "char_end": 903, "chars": " -m"}, {"char_start": 926, "char_end": 934, "chars": " \" +\n   "}, {"char_start": 946, "char_end": 950, "chars": "   \""}, {"char_start": 959, "char_end": 960, "chars": "'"}, {"char_start": 992, "char_end": 993, "chars": "\""}, {"char_start": 1003, "char_end": 1024, "chars": "os.system(f'git push "}, {"char_start": 1031, "char_end": 1032, "chars": "{"}, {"char_start": 1044, "char_end": 1046, "chars": "}'"}], "added": [{"char_start": 626, "char_end": 650, "chars": "subprocess.run(['git', '"}, {"char_start": 655, "char_end": 657, "chars": "',"}, {"char_start": 664, "char_end": 666, "chars": ", "}, {"char_start": 676, "char_end": 677, "chars": "]"}, {"char_start": 716, "char_end": 757, "chars": "subprocess.run(['git', 'checkout', '-b', "}, {"char_start": 769, "char_end": 770, "chars": "]"}, {"char_start": 780, "char_end": 802, "chars": "subprocess.run(['git',"}, {"char_start": 815, "char_end": 822, "chars": "'-c', '"}, {"char_start": 840, "char_end": 862, "chars": "',\n            '-c', '"}, {"char_start": 893, "char_end": 896, "chars": "',\n"}, {"char_start": 908, "char_end": 919, "chars": "'commit', '"}, {"char_start": 932, "char_end": 952, "chars": "',\n            '-m',"}, {"char_start": 975, "char_end": 976, "chars": ","}, {"char_start": 989, "char_end": 990, "chars": "'"}, {"char_start": 1031, "char_end": 1032, "chars": "]"}, {"char_start": 1042, "char_end": 1074, "chars": "subprocess.run(['git', 'push', '"}, {"char_start": 1080, "char_end": 1082, "chars": "',"}, {"char_start": 1095, "char_end": 1096, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to create an empty commit on a specified branch in a Git repository if the repository has no commits."}
{"func_name": "build_filter_params", "func_src_before": "  def build_filter_params\n    @conditions = \"state in('published', 'withdrawn')\"\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions += \" AND published = #{@search[:published].to_i}\"\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "func_src_after": "  def build_filter_params\n    @conditions = [\"state in('published', 'withdrawn')\"]\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions[0] += \" AND published_at LIKE ? \"\n        @conditions << \"%#{@search[:published_at]}%\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions[0] += \" AND user_id = ? \"\n        @conditions << @search[:user_id]\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions[0] += \" AND published = ? \"\n        @conditions << @search[:published]\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions[0] += \" AND categorizations.category_id = ? \"\n        @conditions << @search[:category]\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 81, "line": "    @conditions = \"state in('published', 'withdrawn')\"\n"}, {"line_no": 7, "char_start": 221, "char_end": 299, "line": "        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n"}, {"line_no": 11, "char_start": 368, "char_end": 434, "line": "        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n"}, {"line_no": 15, "char_start": 518, "char_end": 588, "line": "        @conditions += \" AND published = #{@search[:published].to_i}\"\n"}, {"line_no": 19, "char_start": 665, "char_end": 752, "line": "        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 83, "line": "    @conditions = [\"state in('published', 'withdrawn')\"]\n"}, {"line_no": 7, "char_start": 223, "char_end": 277, "line": "        @conditions[0] += \" AND published_at LIKE ? \"\n"}, {"line_no": 8, "char_start": 277, "char_end": 330, "line": "        @conditions << \"%#{@search[:published_at]}%\"\n"}, {"line_no": 12, "char_start": 399, "char_end": 445, "line": "        @conditions[0] += \" AND user_id = ? \"\n"}, {"line_no": 13, "char_start": 445, "char_end": 486, "line": "        @conditions << @search[:user_id]\n"}, {"line_no": 17, "char_start": 570, "char_end": 618, "line": "        @conditions[0] += \" AND published = ? \"\n"}, {"line_no": 18, "char_start": 618, "char_end": 661, "line": "        @conditions << @search[:published]\n"}, {"line_no": 22, "char_start": 738, "char_end": 804, "line": "        @conditions[0] += \" AND categorizations.category_id = ? \"\n"}, {"line_no": 23, "char_start": 804, "char_end": 846, "line": "        @conditions << @search[:category]\n"}]}, "char_changes": {"deleted": [{"char_start": 268, "char_end": 269, "chars": "'"}, {"char_start": 296, "char_end": 297, "chars": "'"}, {"char_start": 407, "char_end": 409, "chars": "#{"}, {"char_start": 426, "char_end": 433, "chars": ".to_i}\""}, {"char_start": 559, "char_end": 561, "chars": "#{"}, {"char_start": 580, "char_end": 587, "chars": ".to_i}\""}, {"char_start": 724, "char_end": 726, "chars": "#{"}, {"char_start": 744, "char_end": 751, "chars": ".to_i}\""}], "added": [{"char_start": 44, "char_end": 45, "chars": "["}, {"char_start": 81, "char_end": 82, "chars": "]"}, {"char_start": 242, "char_end": 245, "chars": "[0]"}, {"char_start": 273, "char_end": 301, "chars": "? \"\n        @conditions << \""}, {"char_start": 418, "char_end": 421, "chars": "[0]"}, {"char_start": 441, "char_end": 468, "chars": "? \"\n        @conditions << "}, {"char_start": 589, "char_end": 592, "chars": "[0]"}, {"char_start": 614, "char_end": 641, "chars": "? \"\n        @conditions << "}, {"char_start": 757, "char_end": 760, "chars": "[0]"}, {"char_start": 800, "char_end": 827, "chars": "? \"\n        @conditions << "}]}, "commit_link": "github.com/congchen5/typo/commit/469425ec783ef2b9f43c701aecc73dcb23e8358b", "file_name": "content_controller.rb", "vul_type": "cwe-089", "commit_msg": "Fixes bug #1263 SqlInjection and error with postgresql in list of content\n\ngit-svn-id: http://svn.typosphere.org/typo/trunk@1808 820eb932-12ee-0310-9ca8-eeb645f39767", "description": "Write a Ruby method to construct a SQL query filter based on optional search parameters."}
{"func_name": "mif_process_cmpt", "func_src_before": "static int mif_process_cmpt(mif_hdr_t *hdr, char *buf)\n{\n\tjas_tvparser_t *tvp;\n\tmif_cmpt_t *cmpt;\n\tint id;\n\n\tcmpt = 0;\n\ttvp = 0;\n\n\tif (!(cmpt = mif_cmpt_create())) {\n\t\tgoto error;\n\t}\n\tcmpt->tlx = 0;\n\tcmpt->tly = 0;\n\tcmpt->sampperx = 0;\n\tcmpt->samppery = 0;\n\tcmpt->width = 0;\n\tcmpt->height = 0;\n\tcmpt->prec = 0;\n\tcmpt->sgnd = -1;\n\tcmpt->data = 0;\n\n\tif (!(tvp = jas_tvparser_create(buf))) {\n\t\tgoto error;\n\t}\n\twhile (!(id = jas_tvparser_next(tvp))) {\n\t\tswitch (jas_taginfo_nonull(jas_taginfos_lookup(mif_tags,\n\t\t  jas_tvparser_gettag(tvp)))->id) {\n\t\tcase MIF_TLX:\n\t\t\tcmpt->tlx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_TLY:\n\t\t\tcmpt->tly = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_WIDTH:\n\t\t\tcmpt->width = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HEIGHT:\n\t\t\tcmpt->height = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HSAMP:\n\t\t\tcmpt->sampperx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_VSAMP:\n\t\t\tcmpt->samppery = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_PREC:\n\t\t\tcmpt->prec = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_SGND:\n\t\t\tcmpt->sgnd = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_DATA:\n\t\t\tif (!(cmpt->data = jas_strdup(jas_tvparser_getval(tvp)))) {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tjas_tvparser_destroy(tvp);\n\tif (!cmpt->sampperx || !cmpt->samppery) {\n\t\tgoto error;\n\t}\n\tif (mif_hdr_addcmpt(hdr, hdr->numcmpts, cmpt)) {\n\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tif (cmpt) {\n\t\tmif_cmpt_destroy(cmpt);\n\t}\n\tif (tvp) {\n\t\tjas_tvparser_destroy(tvp);\n\t}\n\treturn -1;\n}", "func_src_after": "static int mif_process_cmpt(mif_hdr_t *hdr, char *buf)\n{\n\tjas_tvparser_t *tvp;\n\tmif_cmpt_t *cmpt;\n\tint id;\n\n\tcmpt = 0;\n\ttvp = 0;\n\n\tif (!(cmpt = mif_cmpt_create())) {\n\t\tgoto error;\n\t}\n\tcmpt->tlx = 0;\n\tcmpt->tly = 0;\n\tcmpt->sampperx = 0;\n\tcmpt->samppery = 0;\n\tcmpt->width = 0;\n\tcmpt->height = 0;\n\tcmpt->prec = 0;\n\tcmpt->sgnd = -1;\n\tcmpt->data = 0;\n\n\tif (!(tvp = jas_tvparser_create(buf))) {\n\t\tgoto error;\n\t}\n\twhile (!(id = jas_tvparser_next(tvp))) {\n\t\tswitch (jas_taginfo_nonull(jas_taginfos_lookup(mif_tags,\n\t\t  jas_tvparser_gettag(tvp)))->id) {\n\t\tcase MIF_TLX:\n\t\t\tcmpt->tlx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_TLY:\n\t\t\tcmpt->tly = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_WIDTH:\n\t\t\tcmpt->width = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HEIGHT:\n\t\t\tcmpt->height = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HSAMP:\n\t\t\tcmpt->sampperx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_VSAMP:\n\t\t\tcmpt->samppery = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_PREC:\n\t\t\tcmpt->prec = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_SGND:\n\t\t\tcmpt->sgnd = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_DATA:\n\t\t\tif (!(cmpt->data = jas_strdup(jas_tvparser_getval(tvp)))) {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!cmpt->sampperx || !cmpt->samppery) {\n\t\tgoto error;\n\t}\n\tif (mif_hdr_addcmpt(hdr, hdr->numcmpts, cmpt)) {\n\t\tgoto error;\n\t}\n\tjas_tvparser_destroy(tvp);\n\treturn 0;\n\nerror:\n\tif (cmpt) {\n\t\tmif_cmpt_destroy(cmpt);\n\t}\n\tif (tvp) {\n\t\tjas_tvparser_destroy(tvp);\n\t}\n\treturn -1;\n}", "commit_link": "github.com/mdadams/jasper/commit/df5d2867e8004e51e18b89865bc4aa69229227b3", "file_name": "src/libjasper/mif/mif_cod.c", "vul_type": "cwe-416", "description": "Write a C function to parse component information from a buffer and add it to a MIF header, handling errors appropriately."}
{"func_name": "gps_tracker", "func_src_before": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "func_src_after": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - pos - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "commit_link": "github.com/aircrack-ng/aircrack-ng/commit/ff70494dd389ba570dbdbf36f217c28d4381c6b5/", "file_name": "src/airodump-ng.c", "vul_type": "cwe-787", "description": "Write a C function named `gps_tracker` that connects to a GPS daemon on localhost and reads GPS coordinates in a loop."}
{"func_name": "ReadMATImage", "func_src_before": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "func_src_after": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\n    quantum_info=DestroyQuantumInfo(quantum_info);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/b173a352397877775c51c9a0e9d59eb6ce24c455", "file_name": "coders/mat.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a MATLAB image file in ImageMagick."}
{"func_name": "PHYSICALPATH_FUNC", "func_src_before": "PHYSICALPATH_FUNC(mod_alias_physical_handler) {\n\tplugin_data *p = p_d;\n\tint uri_len, basedir_len;\n\tchar *uri_ptr;\n\tsize_t k;\n\n\tif (buffer_is_empty(con->physical.path)) return HANDLER_GO_ON;\n\n\tmod_alias_patch_connection(srv, con, p);\n\n\t/* not to include the tailing slash */\n\tbasedir_len = buffer_string_length(con->physical.basedir);\n\tif ('/' == con->physical.basedir->ptr[basedir_len-1]) --basedir_len;\n\turi_len = buffer_string_length(con->physical.path) - basedir_len;\n\turi_ptr = con->physical.path->ptr + basedir_len;\n\n\tfor (k = 0; k < p->conf.alias->used; k++) {\n\t\tdata_string *ds = (data_string *)p->conf.alias->data[k];\n\t\tint alias_len = buffer_string_length(ds->key);\n\n\t\tif (alias_len > uri_len) continue;\n\t\tif (buffer_is_empty(ds->key)) continue;\n\n\t\tif (0 == (con->conf.force_lowercase_filenames ?\n\t\t\t\t\tstrncasecmp(uri_ptr, ds->key->ptr, alias_len) :\n\t\t\t\t\tstrncmp(uri_ptr, ds->key->ptr, alias_len))) {\n\t\t\t/* matched */\n\n\t\t\tbuffer_copy_buffer(con->physical.basedir, ds->value);\n\t\t\tbuffer_copy_buffer(srv->tmp_buf, ds->value);\n\t\t\tbuffer_append_string(srv->tmp_buf, uri_ptr + alias_len);\n\t\t\tbuffer_copy_buffer(con->physical.path, srv->tmp_buf);\n\n\t\t\treturn HANDLER_GO_ON;\n\t\t}\n\t}\n\n\t/* not found */\n\treturn HANDLER_GO_ON;\n}", "func_src_after": "PHYSICALPATH_FUNC(mod_alias_physical_handler) {\n\tplugin_data *p = p_d;\n\tint uri_len, basedir_len;\n\tchar *uri_ptr;\n\tsize_t k;\n\n\tif (buffer_is_empty(con->physical.path)) return HANDLER_GO_ON;\n\n\tmod_alias_patch_connection(srv, con, p);\n\n\t/* not to include the tailing slash */\n\tbasedir_len = buffer_string_length(con->physical.basedir);\n\tif ('/' == con->physical.basedir->ptr[basedir_len-1]) --basedir_len;\n\turi_len = buffer_string_length(con->physical.path) - basedir_len;\n\turi_ptr = con->physical.path->ptr + basedir_len;\n\n\tfor (k = 0; k < p->conf.alias->used; k++) {\n\t\tdata_string *ds = (data_string *)p->conf.alias->data[k];\n\t\tint alias_len = buffer_string_length(ds->key);\n\n\t\tif (alias_len > uri_len) continue;\n\t\tif (buffer_is_empty(ds->key)) continue;\n\n\t\tif (0 == (con->conf.force_lowercase_filenames ?\n\t\t\t\t\tstrncasecmp(uri_ptr, ds->key->ptr, alias_len) :\n\t\t\t\t\tstrncmp(uri_ptr, ds->key->ptr, alias_len))) {\n\t\t\t/* matched */\n\n\t\t\t/* check for path traversal in url-path following alias if key\n\t\t\t * does not end in slash, but replacement value ends in slash */\n\t\t\tif (uri_ptr[alias_len] == '.') {\n\t\t\t\tchar *s = uri_ptr + alias_len + 1;\n\t\t\t\tif (*s == '.') ++s;\n\t\t\t\tif (*s == '/' || *s == '\\0') {\n\t\t\t\t\tsize_t vlen = buffer_string_length(ds->value);\n\t\t\t\t\tif (0 != alias_len && ds->key->ptr[alias_len-1] != '/'\n\t\t\t\t\t    && 0 != vlen && ds->value->ptr[vlen-1] == '/') {\n\t\t\t\t\t\tcon->http_status = 403;\n\t\t\t\t\t\treturn HANDLER_FINISHED;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbuffer_copy_buffer(con->physical.basedir, ds->value);\n\t\t\tbuffer_copy_buffer(srv->tmp_buf, ds->value);\n\t\t\tbuffer_append_string(srv->tmp_buf, uri_ptr + alias_len);\n\t\t\tbuffer_copy_buffer(con->physical.path, srv->tmp_buf);\n\n\t\t\treturn HANDLER_GO_ON;\n\t\t}\n\t}\n\n\t/* not found */\n\treturn HANDLER_GO_ON;\n}", "commit_link": "github.com/lighttpd/lighttpd1.4/commit/2105dae0f9d7a964375ce681e53cb165375f84c1", "file_name": "src/mod_alias.c", "vul_type": "cwe-022", "description": "Write a C function named `mod_alias_physical_handler` that rewrites URL paths based on predefined aliases and handles path traversal security."}
{"func_name": "top_proxies", "func_src_before": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id='\"+proxy_id+\"' LIMIT 1\"\n        cur.execute(query)\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as='\"+proxy_id+\"'\"\n        cur.execute(query)\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "func_src_after": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id=%s LIMIT 1\"\n        cur.execute(query, (proxy_id,))\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as=%s\"\n        cur.execute(query, (proxy_id,))\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/top_proxies' that queries a PostgreSQL database to retrieve and return a JSON list of top proxy voters and their details."}
{"func_name": "AuthenticationConfig::configure", "func_src_before": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .csrf()\n\t\t        .disable()\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "func_src_after": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 106, "line": "\t\t        .csrf()\n"}, {"line_no": 5, "char_start": 106, "char_end": 127, "line": "\t\t        .disable()\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 88, "char_end": 127, "chars": "\t\t        .csrf()\n\t\t        .disable()\n"}], "added": []}, "commit_link": "github.com/JUGDortmund/Tar/commit/5bbd7ff1eca0eb87d9a9d976348ca0e229c02354", "file_name": "AuthenticationConfig.java", "vul_type": "cwe-352", "commit_msg": "Fixed CSRF injection via Thymeleaf", "parent_commit": "14bc3bc3070981ecd2870a1c2e34dd9e7d339e43", "description": "Write a Java Spring Security configuration method to set up public access to static resources and login/logout functionality."}
{"func_name": "check_clus_chain", "func_src_before": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tclus_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %u bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "func_src_after": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tuint64_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %\"\n\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 136, "char_end": 162, "line": "\tclus_t count, max_count;\n"}, {"line_no": 32, "char_start": 888, "char_end": 917, "line": "\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 48, "char_start": 1333, "char_end": 1379, "line": "\t\t\t\t\t\"the other file. truncated to %u bytes\",\n"}, {"line_no": 64, "char_start": 1783, "char_end": 1813, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 65, "char_start": 1813, "char_end": 1827, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 66, "char_start": 1827, "char_end": 1852, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 78, "char_start": 2116, "char_end": 2146, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 79, "char_start": 2146, "char_end": 2160, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 80, "char_start": 2160, "char_end": 2185, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 98, "char_start": 2496, "char_end": 2524, "line": "\t\t\t\"truncates to %u bytes\",\n"}], "added": [{"line_no": 5, "char_start": 136, "char_end": 164, "line": "\tuint64_t count, max_count;\n"}, {"line_no": 32, "char_start": 890, "char_end": 928, "line": "\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n"}, {"line_no": 48, "char_start": 1344, "char_end": 1382, "line": "\t\t\t\t\t\"the other file. truncated to %\"\n"}, {"line_no": 49, "char_start": 1382, "char_end": 1404, "line": "\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 65, "char_start": 1808, "char_end": 1830, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 66, "char_start": 1830, "char_end": 1853, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 67, "char_start": 1853, "char_end": 1886, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 79, "char_start": 2150, "char_end": 2172, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 80, "char_start": 2172, "char_end": 2195, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 81, "char_start": 2195, "char_end": 2228, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 99, "char_start": 2539, "char_end": 2576, "line": "\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 141, "chars": "clus"}, {"char_start": 907, "char_end": 908, "chars": "u"}, {"char_start": 1369, "char_end": 1370, "chars": "u"}, {"char_start": 1803, "char_end": 1804, "chars": "u"}, {"char_start": 1826, "char_end": 1833, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2136, "char_end": 2137, "chars": "u"}, {"char_start": 2159, "char_end": 2166, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2514, "char_end": 2515, "chars": "u"}], "added": [{"char_start": 137, "char_end": 143, "chars": "uint64"}, {"char_start": 909, "char_end": 919, "chars": "\" PRIu64 \""}, {"char_start": 1380, "char_end": 1395, "chars": "\"\n\t\t\t\t\tPRIu64 \""}, {"char_start": 1828, "char_end": 1844, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 1866, "char_end": 1867, "chars": " "}, {"char_start": 2170, "char_end": 2186, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 2208, "char_end": 2209, "chars": " "}, {"char_start": 2557, "char_end": 2567, "chars": "\" PRIu64 \""}]}, "commit_link": "github.com/exfatprogs/exfatprogs/commit/c1f48157c38df8d958ab81012abae2470750a785", "file_name": "fsck.c", "vul_type": "cwe-190", "commit_msg": "fsck: fix integer overflow in calculating size\n\nThe size must be 64-bit integer\n\nSigned-off-by: Hyunchul Lee <hyc.lee@gmail.com>", "parent_commit": "edf7a39b7252f4b63915292420aaa63a750f22d9", "description": "Write a C function to validate and repair the cluster chain of a file in an exFAT file system."}
{"func_name": "test_feature_tags", "func_src_before": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "func_src_after": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 139, "char_end": 183, "line": "    mapper_yaml = yaml.load(mapper_content)\n"}], "added": [{"line_no": 5, "char_start": 139, "char_end": 207, "line": "    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 181, "char_end": 205, "chars": ", Loader=yaml.BaseLoader"}]}, "commit_link": "github.com/NetworkManager/NetworkManager-ci/commit/e8a0a1686315dc988b3600ce80ff3953cccb7b4b", "file_name": "test.py", "vul_type": "cwe-502", "commit_msg": "nmci/test: avoid deprecation warning for yaml.load()\n\nAvoids:\n\n  nmci/test.py::test_feature_tags\n    /TMP/NetworkManager-ci/nmci/test.py:411: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n      mapper_yaml = yaml.load(mapper_content)\n\n  -- Docs: https://docs.pytest.org/en/stable/warnings.html", "parent_commit": "5e80ca2fac5f2fa62e0daccc6703b69694a97f51", "description": "Write a Python function to validate feature tags against various criteria, including version tags, bugzilla tags, and a tag registry."}
{"func_name": "build_board", "func_src_before": "def build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\" % game)\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)", "func_src_after": "def build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\", (game,))\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)", "commit_link": "github.com/russ-lewis/ttt_-_python_cgi/commit/6096f43fd4b2d91211eec4614b7960c0816900da", "file_name": "cgi/common.py", "vul_type": "cwe-089", "description": "In Python, write a function to construct a tic-tac-toe board from a database, tracking the next player's turn based on move history."}
{"func_name": "edit_coordinator", "func_src_before": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "func_src_after": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json_for_html(),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "commit_link": "github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "file_name": "apps/oozie/src/oozie/views/editor2.py", "vul_type": "cwe-079", "description": "In Python, write a view function `edit_coordinator` that checks access permissions, fetches coordinator and workflow details, and renders them to a template."}
{"func_name": "addKey", "func_src_before": "def addKey(client):\n\t\"\"\"Adds a new key with the specified name and contents.\n\tReturns an error if a key with the specified name already exists.\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal CREATED\n\n\tvalidateClient(client)\n\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateNewKeyData(token_data)\n\n\t# Use 'x' flag so we can throw an error if a key with this name already exists\n\ttry:\n\t\twith open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:\n\t\t\tf.write(token_data['key'])\n\texcept FileExistsError:\n\t\traise FoxlockError(BAD_REQUEST, \"Key '%s' already exists\" % token_data['name'])\n\n\treturn 'Key successfully created', CREATED", "func_src_after": "def addKey(client):\n\t\"\"\"Adds a new key with the specified name and contents.\n\tReturns an error if a key with the specified name already exists.\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal CREATED\n\n\tvalidateClient(client)\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateNewKeyData(token_data)\n\tvalidateKeyName(token_data['name'])\n\n\t# Use 'x' flag so we can throw an error if a key with this name already exists\n\ttry:\n\t\twith open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:\n\t\t\tf.write(token_data['key'])\n\texcept FileExistsError:\n\t\traise FoxlockError(BAD_REQUEST, \"Key '%s' already exists\" % token_data['name'])\n\n\treturn 'Key successfully created', CREATED", "commit_link": "github.com/Mimickal/FoxLock/commit/7c665e556987f4e2c1a75e143a1e80ae066ad833", "file_name": "impl.py", "vul_type": "cwe-022", "description": "Write a Python function named `addKey` that creates a new key file for a client if it doesn't already exist, or raises an error if the key file already exists."}
{"func_name": "enc_untrusted_read", "func_src_before": "ssize_t enc_untrusted_read(int fd, void *buf, size_t count) {\n  return static_cast<ssize_t>(EnsureInitializedAndDispatchSyscall(\n      asylo::system_call::kSYS_read, fd, buf, count));\n}", "func_src_after": "ssize_t enc_untrusted_read(int fd, void *buf, size_t count) {\n  ssize_t ret = static_cast<ssize_t>(EnsureInitializedAndDispatchSyscall(\n      asylo::system_call::kSYS_read, fd, buf, count));\n  if (ret != -1 && ret > count) {\n    ::asylo::primitives::TrustedPrimitives::BestEffortAbort(\n        \"enc_untrusted_read: read result exceeds requested\");\n  }\n  return ret;\n}", "commit_link": "github.com/google/asylo/commit/b1d120a2c7d7446d2cc58d517e20a1b184b82200", "file_name": "asylo/platform/host_call/trusted/host_calls.cc", "vul_type": "cwe-125", "description": "Write a C++ function named `enc_untrusted_read` that wraps a syscall for reading from a file descriptor, with error handling for reading more bytes than requested."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to display all records from a specified MySQL table in an HTTP response."}
{"func_name": "filter_session_io", "func_src_before": "filter_session_io(struct io *io, int evt, void *arg)\n{\n\tstruct filter_session *fs = arg;\n\tchar *line = NULL;\n\tssize_t len;\n\n\tlog_trace(TRACE_IO, \"filter session: %p: %s %s\", fs, io_strevent(evt),\n\t    io_strio(io));\n\n\tswitch (evt) {\n\tcase IO_DATAIN:\n\tnextline:\n\t\tline = io_getline(fs->io, &len);\n\t\t/* No complete line received */\n\t\tif (line == NULL)\n\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\n\tcase IO_DISCONNECTED:\n\t\tio_free(fs->io);\n\t\tfs->io = NULL;\n\t\tbreak;\n\t}\n}", "func_src_after": "filter_session_io(struct io *io, int evt, void *arg)\n{\n\tstruct filter_session *fs = arg;\n\tchar *line = NULL;\n\tssize_t len;\n\n\tlog_trace(TRACE_IO, \"filter session: %p: %s %s\", fs, io_strevent(evt),\n\t    io_strio(io));\n\n\tswitch (evt) {\n\tcase IO_DATAIN:\n\tnextline:\n\t\tline = io_getline(fs->io, &len);\n\t\t/* No complete line received */\n\t\tif (line == NULL)\n\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\t}\n}", "commit_link": "github.com/openbsd/src/commit/6c3220444ed06b5796dedfd53a0f4becd903c0d1", "file_name": "usr.sbin/smtpd/lka_filter.c", "vul_type": "cwe-476", "description": "Write a C function named `filter_session_io` that processes IO events for a session, handling data input and disconnection."}
{"func_name": "testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError", "func_src_before": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    unrelated_source_path = tempfile.mktemp()\n    with open(unrelated_source_path, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "func_src_after": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    fd, unrelated_source_path = tempfile.mkstemp()\n    with open(fd, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 111, "char_end": 157, "line": "    unrelated_source_path = tempfile.mktemp()\n"}, {"line_no": 4, "char_start": 157, "char_end": 216, "line": "    with open(unrelated_source_path, \"wt\") as source_file:\n"}], "added": [{"line_no": 3, "char_start": 111, "char_end": 162, "line": "    fd, unrelated_source_path = tempfile.mkstemp()\n"}, {"line_no": 4, "char_start": 162, "char_end": 202, "line": "    with open(fd, \"wt\") as source_file:\n"}]}, "char_changes": {"deleted": [{"char_start": 171, "char_end": 192, "chars": "unrelated_source_path"}], "added": [{"char_start": 114, "char_end": 118, "chars": " fd,"}, {"char_start": 154, "char_end": 155, "chars": "s"}, {"char_start": 176, "char_end": 178, "chars": "fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/3752cc4c6ba6b69f04f857c6047adde9e8487bd6", "file_name": "source_utils_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359237\nChange-Id: I7fa45e888deff612ca53a4f8610cfad8f28e9671", "description": "Write a Python function to test that calling a source annotation utility with an unrelated source file does not produce an error, and clean up the file afterwards."}
{"func_name": "DefaultArchiveExtractor::extract", "func_src_before": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "func_src_after": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory, entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1467, "char_end": 1580, "line": "                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n"}, {"line_no": 55, "char_start": 2986, "char_end": 3102, "line": "                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n"}], "added": [{"line_no": 26, "char_start": 1467, "char_end": 1562, "line": "                        final File destPath = new File(destinationDirectory, entry.getName());\n"}, {"line_no": 27, "char_start": 1562, "char_end": 1657, "line": "                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n"}, {"line_no": 28, "char_start": 1657, "char_end": 1730, "line": "                            throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1756, "line": "                        }\n"}, {"line_no": 58, "char_start": 3162, "char_end": 3260, "line": "                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n"}]}, "char_changes": {"deleted": [{"char_start": 1542, "char_end": 1579, "chars": " + File.separator + entry.getName());"}, {"char_start": 3061, "char_end": 3080, "chars": " + File.separator +"}], "added": [{"char_start": 1542, "char_end": 1755, "chars": ", entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }"}, {"char_start": 3237, "char_end": 3238, "chars": ","}]}, "commit_link": "github.com/eirslett/frontend-maven-plugin/commit/9f53f7617b41d89cbef1a29342c6b6b7441fa78a", "file_name": "ArchiveExtractor.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java method to extract files from an archive (ZIP, MSI, or TAR.GZ) to a specified directory."}
{"func_name": "MultiPartInputFile::Data::chunkOffsetReconstruction", "func_src_before": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber> static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}", "func_src_after": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber>= static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}", "commit_link": "github.com/AcademySoftwareFoundation/openexr/commit/8b5370c688a7362673c3a5256d93695617a4cd9a", "file_name": "OpenEXR/IlmImf/ImfMultiPartInputFile.cpp", "vul_type": "cwe-787", "description": "Write a C++ function to reconstruct chunk offset tables from an OpenEXR file stream, handling exceptions silently."}
{"func_name": "set_eeprom_serial_number", "func_src_before": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 16);\n  _dirty = 1;\n\n  return 0;\n}", "func_src_after": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 12);\n  _dirty = 1;\n\n  return 0;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 16);\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 12);\n"}]}, "char_changes": {"deleted": [{"char_start": 80, "char_end": 81, "chars": "6"}], "added": [{"char_start": 80, "char_end": 81, "chars": "2"}]}, "commit_link": "github.com/picoflamingo/BBCape_EEPROM/commit/0b2d0afdd72e6ca35e9312bd43e29d488ae8c2e5", "file_name": "bbcape_eeprom.c", "vul_type": "cwe-119", "commit_msg": "Buffer Overflow fixed (https://github.com/picoflamingo/BBCape_EEPROM/issues/1)", "parent_commit": "21b1310205d6b2d9073efc51c6a32edbd9a08b89", "description": "Write a C function named `set_eeprom_serial_number` that copies a serial number string into an EEPROM structure and sets a dirty flag."}
{"func_name": "send_message", "func_src_before": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "func_src_after": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id = %s\"\"\", sender)\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "commit_link": "github.com/libracore/erpnext/commit/9acb885e60f77cd4e9ea8c98bdc39c18abcac731", "file_name": "erpnext/templates/utils.py", "vul_type": "cwe-089", "description": "Write a Python function in Frappe that handles incoming messages by creating leads, opportunities, and communications."}
{"func_name": "enc_untrusted_create_wait_queue", "func_src_before": "int32_t *enc_untrusted_create_wait_queue() {\n  MessageWriter input;\n  MessageReader output;\n  input.Push<uint64_t>(sizeof(int32_t));\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);\n  return queue;\n}", "func_src_after": "int32_t *enc_untrusted_create_wait_queue() {\n  MessageWriter input;\n  MessageReader output;\n  input.Push<uint64_t>(sizeof(int32_t));\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  if (!TrustedPrimitives::IsOutsideEnclave(queue, sizeof(int32_t))) {\n    TrustedPrimitives::BestEffortAbort(\n        \"enc_untrusted_create_wait_queue: queue should be in untrusted memory\");\n  }\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);\n  return queue;\n}", "commit_link": "github.com/google/asylo/commit/a37fb6a0e7daf30134dbbf357c9a518a1026aa02", "file_name": "asylo/platform/host_call/trusted/concurrency.cc", "vul_type": "cwe-787", "description": "Write a C++ function named `enc_untrusted_create_wait_queue` that allocates a wait queue using a non-system call dispatcher and handles errors."}
{"func_name": "tid_num_to_tag_nums", "func_src_before": "    def tid_num_to_tag_nums(self, tid_num):\n        ''' Returns list of the associated tag_nums to the given tid_num. '''\n\n        q = \"SELECT tag FROM tid_tag WHERE tid = '\" + str(tid_num) + \"'\"\n        self.query(q)\n        return [i[0] for i in self.c.fetchall()]", "func_src_after": "    def tid_num_to_tag_nums(self, tid_num):\n        ''' Returns list of the associated tag_nums to the given tid_num. '''\n\n        q = \"SELECT tag FROM tid_tag WHERE tid = ?\"\n        self.query(q, tid_num)\n        return [i[0] for i in self.c.fetchall()]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves a list of tags associated with a given ID from a database."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2184, "char_end": 2228, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2356, "char_end": 2431, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2399, "char_end": 2430, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/8c9aada477cbaa791b3fc9b1c5526d9960d10e5c", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function to initialize an SSL connection using pyOpenSSL, handling client certificates and setting SSL context options."}
{"func_name": "ImagingPcxDecode", "func_src_before": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if ((state->xsize * state->bits + 7) / 8 > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;\n\n    for (;;) {\n\n\tif (bytes < 1)\n\t    return ptr - buf;\n\n\tif ((*ptr & 0xC0) == 0xC0) {\n\n\t    /* Run */\n\t    if (bytes < 2)\n\t\treturn ptr - buf;\n\n\t    n = ptr[0] & 0x3F;\n\n\t    while (n > 0) {\n\t\tif (state->x >= state->bytes) {\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    break;\n\t\t}\n\t\tstate->buffer[state->x++] = ptr[1];\n\t\tn--;\n\t    }\n\n\t    ptr += 2; bytes -= 2;\n\n\t} else {\n\n\t    /* Literal */\n\t    state->buffer[state->x++] = ptr[0];\n\t    ptr++; bytes--;\n\n\t}\n\n\tif (state->x >= state->bytes) {\n        if (state->bytes % state->xsize && state->bytes > state->xsize) {\n            int bands = state->bytes / state->xsize;\n            int stride = state->bytes / bands;\n            int i;\n            for (i=1; i< bands; i++) {  // note -- skipping first band\n                memmove(&state->buffer[i*state->xsize],\n                        &state->buffer[i*stride],\n                        state->xsize);\n            }\n        }\n\t    /* Got a full line, unpack it */\n\t    state->shuffle((UINT8*) im->image[state->y + state->yoff] +\n\t\t\t   state->xoff * im->pixelsize, state->buffer,\n\t\t\t   state->xsize);\n\n\t    state->x = 0;\n\n\t    if (++state->y >= state->ysize) {\n\t\t/* End of file (errcode = 0) */\n\t\treturn -1;\n\t    }\n\t}\n\n    }\n}", "func_src_after": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if ((state->xsize * state->bits + 7) / 8 > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;\n\n    for (;;) {\n\n\tif (bytes < 1)\n\t    return ptr - buf;\n\n\tif ((*ptr & 0xC0) == 0xC0) {\n\n\t    /* Run */\n\t    if (bytes < 2)\n\t\treturn ptr - buf;\n\n\t    n = ptr[0] & 0x3F;\n\n\t    while (n > 0) {\n\t\tif (state->x >= state->bytes) {\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    break;\n\t\t}\n\t\tstate->buffer[state->x++] = ptr[1];\n\t\tn--;\n\t    }\n\n\t    ptr += 2; bytes -= 2;\n\n\t} else {\n\n\t    /* Literal */\n\t    state->buffer[state->x++] = ptr[0];\n\t    ptr++; bytes--;\n\n\t}\n\n\tif (state->x >= state->bytes) {\n        if (state->bytes % state->xsize && state->bytes > state->xsize) {\n            int bands = state->bytes / state->xsize;\n            int stride = state->bytes / bands;\n            int i;\n            for (i=1; i< bands; i++) {  // note -- skipping first band\n                memmove(&state->buffer[i*state->xsize],\n                        &state->buffer[i*stride],\n                        state->xsize);\n            }\n        }\n\t    /* Got a full line, unpack it */\n\t    state->shuffle((UINT8*) im->image[state->y + state->yoff] +\n\t\t\t   state->xoff * im->pixelsize, state->buffer,\n\t\t\t   state->xsize);\n\n\t    state->x = 0;\n\n\t    if (++state->y >= state->ysize) {\n\t\t/* End of file (errcode = 0) */\n\t\treturn -1;\n\t    }\n\t}\n\n    }\n}", "commit_link": "github.com/python-pillow/Pillow/commit/6a83e4324738bb0452fbe8074a995b1c73f08de7#diff-9478f2787e3ae9668a15123b165c23ac", "file_name": "src/libImaging/PcxDecode.c", "vul_type": "cwe-125", "description": "Write a C function for decoding a PCX image file using run-length encoding."}
{"func_name": "tflite::ops::builtin::segment_sum::ResizeOutputTensor", "func_src_before": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  int max_index = -1;\n  const int segment_id_size = segment_ids->dims->data[0];\n  if (segment_id_size > 0) {\n    max_index = segment_ids->data.i32[segment_id_size - 1];\n  }\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "func_src_after": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  // Segment ids should be of same cardinality as first input dimension and they\n  // should be increasing by at most 1, from 0 (e.g., [0, 0, 1, 2, 3] is valid)\n  const int segment_id_size = segment_ids->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, segment_id_size, data->dims->data[0]);\n  int previous_segment_id = -1;\n  for (int i = 0; i < segment_id_size; i++) {\n    const int current_segment_id = GetTensorData<int32_t>(segment_ids)[i];\n    if (i == 0) {\n      TF_LITE_ENSURE_EQ(context, current_segment_id, 0);\n    } else {\n      int delta = current_segment_id - previous_segment_id;\n      TF_LITE_ENSURE(context, delta == 0 || delta == 1);\n    }\n    previous_segment_id = current_segment_id;\n  }\n\n  const int max_index = previous_segment_id;\n\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/204945b19e44b57906c9344c0d00120eeeae178a", "file_name": "tensorflow/lite/kernels/segment_sum.cc", "vul_type": "cwe-787", "description": "Write a C++ function to resize an output tensor based on segment IDs in TensorFlow Lite."}
{"func_name": "errorf", "func_src_before": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode(v.name))\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "func_src_after": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode())\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1027, "char_end": 1064, "line": "\t\tglog.Infof(v.DumpByteCode(v.name))\n"}], "added": [{"line_no": 26, "char_start": 1027, "char_end": 1058, "line": "\t\tglog.Infof(v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 1055, "char_end": 1061, "chars": "v.name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "vm.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that logs a formatted runtime error and VM state for a virtual machine object."}
{"func_name": "isValidAdmToken", "func_src_before": "def isValidAdmToken(adm_token):\n    conn, c = connectDB()\n    req = \"SELECT *  from {} where adm_token='{}'\".format(CFG(\"admintoken_table_name\"), adm_token)\n    answer = bool(queryOne(c, req))\n    closeDB(conn)\n    return answer", "func_src_after": "def isValidAdmToken(adm_token):\n    conn, c = connectDB()\n    req = \"SELECT *  from {} where adm_token=?\".format(CFG(\"admintoken_table_name\"))\n    answer = bool(queryOne(c, req, (adm_token,)))\n    closeDB(conn)\n    return answer", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function to check the validity of an admin token against a database."}
{"func_name": "updateDevice", "func_src_before": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "func_src_after": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\t*pp = p->next;\t/* remove \"p\" from the list */\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "commit_link": "github.com/miniupnp/miniupnp/commit/cd506a67e174a45c6a202eff182a712955ed6d6f", "file_name": "minissdpd/minissdpd.c", "vul_type": "cwe-416", "description": "In C, write a function `updateDevice` to manage a device list by updating an existing device or adding a new one based on provided headers and a timestamp."}
{"func_name": "ipv4_pktinfo_prepare", "func_src_before": "void ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb)\n{\n\tstruct in_pktinfo *pktinfo = PKTINFO_SKB_CB(skb);\n\tbool prepare = (inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO) ||\n\t\t       ipv6_sk_rxinfo(sk);\n\n\tif (prepare && skb_rtable(skb)) {\n\t\t/* skb->cb is overloaded: prior to this point it is IP{6}CB\n\t\t * which has interface index (iif) as the first member of the\n\t\t * underlying inet{6}_skb_parm struct. This code then overlays\n\t\t * PKTINFO_SKB_CB and in_pktinfo also has iif as the first\n\t\t * element so the iif is picked up from the prior IPCB. If iif\n\t\t * is the loopback interface, then return the sending interface\n\t\t * (e.g., process binds socket to eth0 for Tx which is\n\t\t * redirected to loopback in the rtable/dst).\n\t\t */\n\t\tif (pktinfo->ipi_ifindex == LOOPBACK_IFINDEX)\n\t\t\tpktinfo->ipi_ifindex = inet_iif(skb);\n\n\t\tpktinfo->ipi_spec_dst.s_addr = fib_compute_spec_dst(skb);\n\t} else {\n\t\tpktinfo->ipi_ifindex = 0;\n\t\tpktinfo->ipi_spec_dst.s_addr = 0;\n\t}\n\tskb_dst_drop(skb);\n}", "func_src_after": "void ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb)\n{\n\tstruct in_pktinfo *pktinfo = PKTINFO_SKB_CB(skb);\n\tbool prepare = (inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO) ||\n\t\t       ipv6_sk_rxinfo(sk);\n\n\tif (prepare && skb_rtable(skb)) {\n\t\t/* skb->cb is overloaded: prior to this point it is IP{6}CB\n\t\t * which has interface index (iif) as the first member of the\n\t\t * underlying inet{6}_skb_parm struct. This code then overlays\n\t\t * PKTINFO_SKB_CB and in_pktinfo also has iif as the first\n\t\t * element so the iif is picked up from the prior IPCB. If iif\n\t\t * is the loopback interface, then return the sending interface\n\t\t * (e.g., process binds socket to eth0 for Tx which is\n\t\t * redirected to loopback in the rtable/dst).\n\t\t */\n\t\tif (pktinfo->ipi_ifindex == LOOPBACK_IFINDEX)\n\t\t\tpktinfo->ipi_ifindex = inet_iif(skb);\n\n\t\tpktinfo->ipi_spec_dst.s_addr = fib_compute_spec_dst(skb);\n\t} else {\n\t\tpktinfo->ipi_ifindex = 0;\n\t\tpktinfo->ipi_spec_dst.s_addr = 0;\n\t}\n\t/* We need to keep the dst for __ip_options_echo()\n\t * We could restrict the test to opt.ts_needtime || opt.srr,\n\t * but the following is good enough as IP options are not often used.\n\t */\n\tif (unlikely(IPCB(skb)->opt.optlen))\n\t\tskb_dst_force(skb);\n\telse\n\t\tskb_dst_drop(skb);\n}", "commit_link": "github.com/torvalds/linux/commit/34b2cef20f19c87999fff3da4071e66937db9644", "file_name": "net/ipv4/ip_sockglue.c", "vul_type": "cwe-476", "description": "Write a C function named `ipv4_pktinfo_prepare` that prepares packet information for an IPv4 socket in a Linux kernel environment."}
{"func_name": "(anonymous)", "func_src_before": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().append(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "func_src_after": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().text(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "line_changes": {"deleted": [{"line_no": 15, "char_start": 590, "char_end": 653, "line": "            $currentRow.find('a.twist').parent().append(text);\n"}], "added": [{"line_no": 15, "char_start": 590, "char_end": 651, "line": "            $currentRow.find('a.twist').parent().text(text);\n"}]}, "char_changes": {"deleted": [{"char_start": 639, "char_end": 645, "chars": "append"}], "added": [{"char_start": 639, "char_end": 643, "chars": "text"}]}, "commit_link": "github.com/eethomas/eucalyptus/commit/e6133f9af0199452f7dd51bf6ad146e73809046f", "file_name": "eucatable.js", "vul_type": "cwe-079", "commit_msg": "XSS Additional Fix: html(..) to text(..) in eucatable and sgroup JS files", "description": "In JavaScript, write a function to handle custom interactions with table rows, including expanding details and selecting rows with checkboxes."}
{"func_name": "glyph_cache_put", "func_src_before": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index > glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "func_src_after": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index >= glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/c0fd449ec0870b050d350d6d844b1ea6dad4bc7d", "file_name": "libfreerdp/cache/glyph.c", "vul_type": "cwe-125", "description": "Write a C function named `glyph_cache_put` that stores a glyph in a glyph cache, ensuring the cache ID and index are within valid bounds, and logs the operation."}
{"func_name": "update_institutions", "func_src_before": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES ('%s')\" % current_institution\n            sqlite.execute(sql)\n            conn.commit()", "func_src_after": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES (?)\"\n            sqlite.execute(sql, (current_institution,))\n            conn.commit()", "commit_link": "github.com/miku/siskin/commit/7fa398d2fea72bf2e8b4808f75df4b3d35ae959a", "file_name": "bin/solrcheckup.py", "vul_type": "cwe-089", "description": "Write a Python function to update an institution table by comparing old and new institution records, sending notifications for discrepancies, and inserting new records."}
{"func_name": "_drain_to_working_set", "func_src_before": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with open(self.working_set_filename, 'wb') as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        line = base64.b64encode(pickle.dumps({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }))\n                        work_file.write(line)\n                        work_file.write(b'\\n')\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )", "func_src_after": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        pickle.dump({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }, work_file)\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )\n\n                pickle.dump('eof', work_file)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 365, "char_end": 434, "line": "            with open(self.working_set_filename, 'wb') as work_file:\n"}, {"line_no": 27, "char_start": 839, "char_end": 902, "line": "                        line = base64.b64encode(pickle.dumps({\n"}, {"line_no": 34, "char_start": 1228, "char_end": 1256, "line": "                        }))\n"}, {"line_no": 35, "char_start": 1256, "char_end": 1302, "line": "                        work_file.write(line)\n"}, {"line_no": 36, "char_start": 1302, "char_end": 1349, "line": "                        work_file.write(b'\\n')\n"}], "added": [{"line_no": 12, "char_start": 365, "char_end": 456, "line": "            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n"}, {"line_no": 27, "char_start": 861, "char_end": 899, "line": "                        pickle.dump({\n"}, {"line_no": 34, "char_start": 1225, "char_end": 1263, "line": "                        }, work_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 863, "char_end": 887, "chars": "line = base64.b64encode("}, {"char_start": 898, "char_end": 899, "chars": "s"}, {"char_start": 1253, "char_end": 1347, "chars": "))\n                        work_file.write(line)\n                        work_file.write(b'\\n'"}], "added": [{"char_start": 382, "char_end": 387, "chars": "gzip."}, {"char_start": 423, "char_end": 440, "chars": ", compresslevel=1"}, {"char_start": 1250, "char_end": 1261, "chars": ", work_file"}, {"char_start": 2534, "char_end": 2581, "chars": "\n\n                pickle.dump('eof', work_file)"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to export a dataset to a file, with optional data filtering and deletion after export."}
{"func_name": "keyctl_read_key", "func_src_before": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "func_src_after": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\tif (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {\n\t\tret = -ENOKEY;\n\t\tgoto error2;\n\t}\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/37863c43b2c6464f252862bf2e9768264e961678", "file_name": "security/keys/keyctl.c", "vul_type": "cwe-476", "description": "Write a C function named `keyctl_read_key` that reads data from a specified key into a buffer, handling permissions and errors."}
{"func_name": "ParseMPLSLabelStack", "func_src_before": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.Atoi(elem)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i < 0 || i > ((1<<20)-1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "func_src_after": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.ParseUint(elem, 10, 32)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i > ((1 << 20) - 1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 205, "char_end": 236, "line": "\t\ti, err := strconv.Atoi(elem)\n"}, {"line_no": 12, "char_start": 270, "char_end": 302, "line": "\t\tif i < 0 || i > ((1<<20)-1) {\n"}], "added": [{"line_no": 8, "char_start": 205, "char_end": 249, "line": "\t\ti, err := strconv.ParseUint(elem, 10, 32)\n"}, {"line_no": 12, "char_start": 283, "char_end": 310, "line": "\t\tif i > ((1 << 20) - 1) {\n"}]}, "char_changes": {"deleted": [{"char_start": 225, "char_end": 234, "chars": "Atoi(elem"}, {"char_start": 274, "char_end": 283, "chars": " i < 0 ||"}, {"char_start": 296, "char_end": 297, "chars": "-"}], "added": [{"char_start": 225, "char_end": 247, "chars": "ParseUint(elem, 10, 32"}, {"char_start": 295, "char_end": 296, "chars": " "}, {"char_start": 298, "char_end": 299, "chars": " "}, {"char_start": 302, "char_end": 305, "chars": " - "}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse a string of MPLS labels separated by slashes into a label stack, returning an error for invalid formats."}
{"func_name": "get_monthly_ranks_for_scene", "func_src_before": "def get_monthly_ranks_for_scene(db, scene, tag):\n\n    sql = \"SELECT date, rank FROM ranks WHERE scene='{}' AND player='{}'\".format(scene, tag)\n    res = db.exec(sql)\n\n    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]\n\n    # Build up a dict of {date: rank}\n    ranks = {}\n    for r in res:\n        ranks[r[0]] = r[1]\n\n    return ranks", "func_src_after": "def get_monthly_ranks_for_scene(db, scene, tag):\n\n    sql = \"SELECT date, rank FROM ranks WHERE scene='{scene}' AND player='{tag}'\"\n    args = {'scene': scene, 'tag': tag}\n    res = db.exec(sql, args)\n\n    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]\n\n    # Build up a dict of {date: rank}\n    ranks = {}\n    for r in res:\n        ranks[r[0]] = r[1]\n\n    return ranks", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "bracket_utils.py", "vul_type": "cwe-089", "description": "Write a Python function to retrieve a dictionary of dates and ranks for a specific scene and player from a database, ensuring the player was active in the previous month."}
{"func_name": "create_or_update_repo", "func_src_before": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "func_src_after": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "line_changes": {"deleted": [{"line_no": 14, "char_start": 430, "char_end": 500, "line": "        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n"}], "added": [{"line_no": 14, "char_start": 430, "char_end": 516, "line": "        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n"}]}, "char_changes": {"deleted": [{"char_start": 438, "char_end": 472, "chars": "os.system(f'git symbolic-ref HEAD "}], "added": [{"char_start": 438, "char_end": 487, "chars": "subprocess.run(['git', 'symbolic-ref', 'HEAD', f'"}, {"char_start": 513, "char_end": 514, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to manage a git repository by creating or updating it and setting file permissions."}
{"func_name": "store_versioninfo_gnu_verdef", "func_src_before": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && ((char *)defs + i < end); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || vstart + sizeof (Elf_(Verdaux)) > end) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || vstart + sizeof(Elf_(Verdaux)) > end) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "func_src_after": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && (end - (char *)defs > i); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1 || (char *)UINTPTR_MAX - vstart < vdaux) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "commit_link": "github.com/radare/radare2/commit/62e39f34b2705131a2d08aff0c2e542c6a52cf0e", "file_name": "libr/bin/format/elf/elf.c", "vul_type": "cwe-476", "description": "Write a C function named `store_versioninfo_gnu_verdef` that processes GNU version definition sections from an ELF binary object."}
{"func_name": "parse_playlist", "func_src_before": "static int parse_playlist(HLSContext *c, const char *url,\n                          struct playlist *pls, AVIOContext *in)\n{\n    int ret = 0, is_segment = 0, is_variant = 0;\n    int64_t duration = 0;\n    enum KeyType key_type = KEY_NONE;\n    uint8_t iv[16] = \"\";\n    int has_iv = 0;\n    char key[MAX_URL_SIZE] = \"\";\n    char line[MAX_URL_SIZE];\n    const char *ptr;\n    int close_in = 0;\n    int64_t seg_offset = 0;\n    int64_t seg_size = -1;\n    uint8_t *new_url = NULL;\n    struct variant_info variant_info;\n    char tmp_str[MAX_URL_SIZE];\n    struct segment *cur_init_section = NULL;\n\n    if (!in) {\n#if 1\n        AVDictionary *opts = NULL;\n        close_in = 1;\n        /* Some HLS servers don't like being sent the range header */\n        av_dict_set(&opts, \"seekable\", \"0\", 0);\n\n        // broker prior HTTP options that should be consistent across requests\n        av_dict_set(&opts, \"user-agent\", c->user_agent, 0);\n        av_dict_set(&opts, \"cookies\", c->cookies, 0);\n        av_dict_set(&opts, \"headers\", c->headers, 0);\n\n        ret = avio_open2(&in, url, AVIO_FLAG_READ,\n                         c->interrupt_callback, &opts);\n        av_dict_free(&opts);\n        if (ret < 0)\n            return ret;\n#else\n        ret = open_in(c, &in, url);\n        if (ret < 0)\n            return ret;\n        close_in = 1;\n#endif\n    }\n\n    if (av_opt_get(in, \"location\", AV_OPT_SEARCH_CHILDREN, &new_url) >= 0)\n        url = new_url;\n\n    read_chomp_line(in, line, sizeof(line));\n    if (strcmp(line, \"#EXTM3U\")) {\n        ret = AVERROR_INVALIDDATA;\n        goto fail;\n    }\n\n    if (pls) {\n        free_segment_list(pls);\n        pls->finished = 0;\n        pls->type = PLS_TYPE_UNSPECIFIED;\n    }\n    while (!avio_feof(in)) {\n        read_chomp_line(in, line, sizeof(line));\n        if (av_strstart(line, \"#EXT-X-STREAM-INF:\", &ptr)) {\n            is_variant = 1;\n            memset(&variant_info, 0, sizeof(variant_info));\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_variant_args,\n                               &variant_info);\n        } else if (av_strstart(line, \"#EXT-X-KEY:\", &ptr)) {\n            struct key_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_key_args,\n                               &info);\n            key_type = KEY_NONE;\n            has_iv = 0;\n            if (!strcmp(info.method, \"AES-128\"))\n                key_type = KEY_AES_128;\n            if (!strcmp(info.method, \"SAMPLE-AES\"))\n                key_type = KEY_SAMPLE_AES;\n            if (!strncmp(info.iv, \"0x\", 2) || !strncmp(info.iv, \"0X\", 2)) {\n                ff_hex_to_data(iv, info.iv + 2);\n                has_iv = 1;\n            }\n            av_strlcpy(key, info.uri, sizeof(key));\n        } else if (av_strstart(line, \"#EXT-X-MEDIA:\", &ptr)) {\n            struct rendition_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_rendition_args,\n                               &info);\n            new_rendition(c, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-TARGETDURATION:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->target_duration = atoi(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-MEDIA-SEQUENCE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->start_seq_no = atoi(ptr);\n        } else if (av_strstart(line, \"#EXT-X-PLAYLIST-TYPE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            if (!strcmp(ptr, \"EVENT\"))\n                pls->type = PLS_TYPE_EVENT;\n            else if (!strcmp(ptr, \"VOD\"))\n                pls->type = PLS_TYPE_VOD;\n        } else if (av_strstart(line, \"#EXT-X-MAP:\", &ptr)) {\n            struct init_section_info info = {{0}};\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_init_section_args,\n                               &info);\n            cur_init_section = new_init_section(pls, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-ENDLIST\", &ptr)) {\n            if (pls)\n                pls->finished = 1;\n        } else if (av_strstart(line, \"#EXTINF:\", &ptr)) {\n            is_segment = 1;\n            duration   = atof(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-BYTERANGE:\", &ptr)) {\n            seg_size = atoi(ptr);\n            ptr = strchr(ptr, '@');\n            if (ptr)\n                seg_offset = atoi(ptr+1);\n        } else if (av_strstart(line, \"#\", NULL)) {\n            continue;\n        } else if (line[0]) {\n            if (is_variant) {\n                if (!new_variant(c, &variant_info, line, url)) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                is_variant = 0;\n            }\n            if (is_segment) {\n                struct segment *seg;\n                if (!pls) {\n                    if (!new_variant(c, 0, url, NULL)) {\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                    pls = c->playlists[c->n_playlists - 1];\n                }\n                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    AV_WB32(seg->iv + 12, seq);\n                }\n\n                if (key_type != KEY_NONE) {\n                    ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, key);\n                    seg->key = av_strdup(tmp_str);\n                    if (!seg->key) {\n                        av_free(seg);\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                } else {\n                    seg->key = NULL;\n                }\n\n                ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, line);\n                seg->url = av_strdup(tmp_str);\n                if (!seg->url) {\n                    av_free(seg->key);\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {\n                    seg->url_offset = seg_offset;\n                    seg_offset += seg_size;\n                    seg_size = -1;\n                } else {\n                    seg->url_offset = 0;\n                    seg_offset = 0;\n                }\n\n                seg->init_section = cur_init_section;\n            }\n        }\n    }\n    if (pls)\n        pls->last_load_time = av_gettime_relative();\n\nfail:\n    av_free(new_url);\n    if (close_in)\n        avio_close(in);\n    return ret;\n}", "func_src_after": "static int parse_playlist(HLSContext *c, const char *url,\n                          struct playlist *pls, AVIOContext *in)\n{\n    int ret = 0, is_segment = 0, is_variant = 0;\n    int64_t duration = 0;\n    enum KeyType key_type = KEY_NONE;\n    uint8_t iv[16] = \"\";\n    int has_iv = 0;\n    char key[MAX_URL_SIZE] = \"\";\n    char line[MAX_URL_SIZE];\n    const char *ptr;\n    int close_in = 0;\n    int64_t seg_offset = 0;\n    int64_t seg_size = -1;\n    uint8_t *new_url = NULL;\n    struct variant_info variant_info;\n    char tmp_str[MAX_URL_SIZE];\n    struct segment *cur_init_section = NULL;\n\n    if (!in) {\n#if 1\n        AVDictionary *opts = NULL;\n        close_in = 1;\n        /* Some HLS servers don't like being sent the range header */\n        av_dict_set(&opts, \"seekable\", \"0\", 0);\n\n        // broker prior HTTP options that should be consistent across requests\n        av_dict_set(&opts, \"user-agent\", c->user_agent, 0);\n        av_dict_set(&opts, \"cookies\", c->cookies, 0);\n        av_dict_set(&opts, \"headers\", c->headers, 0);\n\n        ret = avio_open2(&in, url, AVIO_FLAG_READ,\n                         c->interrupt_callback, &opts);\n        av_dict_free(&opts);\n        if (ret < 0)\n            return ret;\n#else\n        ret = open_in(c, &in, url);\n        if (ret < 0)\n            return ret;\n        close_in = 1;\n#endif\n    }\n\n    if (av_opt_get(in, \"location\", AV_OPT_SEARCH_CHILDREN, &new_url) >= 0)\n        url = new_url;\n\n    read_chomp_line(in, line, sizeof(line));\n    if (strcmp(line, \"#EXTM3U\")) {\n        ret = AVERROR_INVALIDDATA;\n        goto fail;\n    }\n\n    if (pls) {\n        free_segment_list(pls);\n        pls->finished = 0;\n        pls->type = PLS_TYPE_UNSPECIFIED;\n    }\n    while (!avio_feof(in)) {\n        read_chomp_line(in, line, sizeof(line));\n        if (av_strstart(line, \"#EXT-X-STREAM-INF:\", &ptr)) {\n            is_variant = 1;\n            memset(&variant_info, 0, sizeof(variant_info));\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_variant_args,\n                               &variant_info);\n        } else if (av_strstart(line, \"#EXT-X-KEY:\", &ptr)) {\n            struct key_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_key_args,\n                               &info);\n            key_type = KEY_NONE;\n            has_iv = 0;\n            if (!strcmp(info.method, \"AES-128\"))\n                key_type = KEY_AES_128;\n            if (!strcmp(info.method, \"SAMPLE-AES\"))\n                key_type = KEY_SAMPLE_AES;\n            if (!strncmp(info.iv, \"0x\", 2) || !strncmp(info.iv, \"0X\", 2)) {\n                ff_hex_to_data(iv, info.iv + 2);\n                has_iv = 1;\n            }\n            av_strlcpy(key, info.uri, sizeof(key));\n        } else if (av_strstart(line, \"#EXT-X-MEDIA:\", &ptr)) {\n            struct rendition_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_rendition_args,\n                               &info);\n            new_rendition(c, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-TARGETDURATION:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->target_duration = atoi(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-MEDIA-SEQUENCE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->start_seq_no = atoi(ptr);\n        } else if (av_strstart(line, \"#EXT-X-PLAYLIST-TYPE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            if (!strcmp(ptr, \"EVENT\"))\n                pls->type = PLS_TYPE_EVENT;\n            else if (!strcmp(ptr, \"VOD\"))\n                pls->type = PLS_TYPE_VOD;\n        } else if (av_strstart(line, \"#EXT-X-MAP:\", &ptr)) {\n            struct init_section_info info = {{0}};\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_init_section_args,\n                               &info);\n            cur_init_section = new_init_section(pls, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-ENDLIST\", &ptr)) {\n            if (pls)\n                pls->finished = 1;\n        } else if (av_strstart(line, \"#EXTINF:\", &ptr)) {\n            is_segment = 1;\n            duration   = atof(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-BYTERANGE:\", &ptr)) {\n            seg_size = atoi(ptr);\n            ptr = strchr(ptr, '@');\n            if (ptr)\n                seg_offset = atoi(ptr+1);\n        } else if (av_strstart(line, \"#\", NULL)) {\n            continue;\n        } else if (line[0]) {\n            if (is_variant) {\n                if (!new_variant(c, &variant_info, line, url)) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                is_variant = 0;\n            }\n            if (is_segment) {\n                struct segment *seg;\n                if (!pls) {\n                    if (!new_variant(c, 0, url, NULL)) {\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                    pls = c->playlists[c->n_playlists - 1];\n                }\n                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    AV_WB32(seg->iv + 12, seq);\n                }\n\n                if (key_type != KEY_NONE) {\n                    ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, key);\n                    seg->key = av_strdup(tmp_str);\n                    if (!seg->key) {\n                        av_free(seg);\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                } else {\n                    seg->key = NULL;\n                }\n\n                ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, line);\n                seg->url = av_strdup(tmp_str);\n                if (!seg->url) {\n                    av_free(seg->key);\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                if (duration < 0.001 * AV_TIME_BASE) {\n                    duration = 0.001 * AV_TIME_BASE;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {\n                    seg->url_offset = seg_offset;\n                    seg_offset += seg_size;\n                    seg_size = -1;\n                } else {\n                    seg->url_offset = 0;\n                    seg_offset = 0;\n                }\n\n                seg->init_section = cur_init_section;\n            }\n        }\n    }\n    if (pls)\n        pls->last_load_time = av_gettime_relative();\n\nfail:\n    av_free(new_url);\n    if (close_in)\n        avio_close(in);\n    return ret;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/6959358683c7533f586c07a766acc5fe9544d8b2", "file_name": "libavformat/hls.c", "vul_type": "cwe-416", "description": "Write a C function to parse an HLS playlist from a given URL and populate a playlist structure with the parsed data."}
{"func_name": "decode_frame", "func_src_before": "static int decode_frame(AVCodecContext *avctx, void *data,\n                        int *got_frame, AVPacket *avpkt)\n{\n    EXRContext *s = avctx->priv_data;\n    ThreadFrame frame = { .f = data };\n    AVFrame *picture = data;\n    uint8_t *ptr;\n\n    int i, y, ret, ymax;\n    int planes;\n    int out_line_size;\n    int nb_blocks;   /* nb scanline or nb tile */\n    uint64_t start_offset_table;\n    uint64_t start_next_scanline;\n    PutByteContext offset_table_writer;\n\n    bytestream2_init(&s->gb, avpkt->data, avpkt->size);\n\n    if ((ret = decode_header(s, picture)) < 0)\n        return ret;\n\n    switch (s->pixel_type) {\n    case EXR_FLOAT:\n    case EXR_HALF:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            } else {\n                /* todo: change this when a floating point pixel format with luma with alpha is implemented */\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRPF32;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAYF32;\n            }\n        }\n        break;\n    case EXR_UINT:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGBA64;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_YA16;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGB48;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAY16;\n            }\n        }\n        break;\n    default:\n        av_log(avctx, AV_LOG_ERROR, \"Missing channel list.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if (s->apply_trc_type != AVCOL_TRC_UNSPECIFIED)\n        avctx->color_trc = s->apply_trc_type;\n\n    switch (s->compression) {\n    case EXR_RAW:\n    case EXR_RLE:\n    case EXR_ZIP1:\n        s->scan_lines_per_block = 1;\n        break;\n    case EXR_PXR24:\n    case EXR_ZIP16:\n        s->scan_lines_per_block = 16;\n        break;\n    case EXR_PIZ:\n    case EXR_B44:\n    case EXR_B44A:\n        s->scan_lines_per_block = 32;\n        break;\n    default:\n        avpriv_report_missing_feature(avctx, \"Compression %d\", s->compression);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    /* Verify the xmin, xmax, ymin and ymax before setting the actual image size.\n     * It's possible for the data window can larger or outside the display window */\n    if (s->xmin > s->xmax  || s->ymin > s->ymax ||\n        s->ydelta == 0xFFFFFFFF || s->xdelta == 0xFFFFFFFF) {\n        av_log(avctx, AV_LOG_ERROR, \"Wrong or missing size information.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if ((ret = ff_set_dimensions(avctx, s->w, s->h)) < 0)\n        return ret;\n\n    s->desc          = av_pix_fmt_desc_get(avctx->pix_fmt);\n    if (!s->desc)\n        return AVERROR_INVALIDDATA;\n\n    if (s->desc->flags & AV_PIX_FMT_FLAG_FLOAT) {\n        planes           = s->desc->nb_components;\n        out_line_size    = avctx->width * 4;\n    } else {\n        planes           = 1;\n        out_line_size    = avctx->width * 2 * s->desc->nb_components;\n    }\n\n    if (s->is_tile) {\n        nb_blocks = ((s->xdelta + s->tile_attr.xSize - 1) / s->tile_attr.xSize) *\n        ((s->ydelta + s->tile_attr.ySize - 1) / s->tile_attr.ySize);\n    } else { /* scanline */\n        nb_blocks = (s->ydelta + s->scan_lines_per_block - 1) /\n        s->scan_lines_per_block;\n    }\n\n    if ((ret = ff_thread_get_buffer(avctx, &frame, 0)) < 0)\n        return ret;\n\n    if (bytestream2_get_bytes_left(&s->gb)/8 < nb_blocks)\n        return AVERROR_INVALIDDATA;\n\n    // check offset table and recreate it if need\n    if (!s->is_tile && bytestream2_peek_le64(&s->gb) == 0) {\n        av_log(s->avctx, AV_LOG_DEBUG, \"recreating invalid scanline offset table\\n\");\n\n        start_offset_table = bytestream2_tell(&s->gb);\n        start_next_scanline = start_offset_table + nb_blocks * 8;\n        bytestream2_init_writer(&offset_table_writer, &avpkt->data[start_offset_table], nb_blocks * 8);\n\n        for (y = 0; y < nb_blocks; y++) {\n            /* write offset of prev scanline in offset table */\n            bytestream2_put_le64(&offset_table_writer, start_next_scanline);\n\n            /* get len of next scanline */\n            bytestream2_seek(&s->gb, start_next_scanline + 4, SEEK_SET);/* skip line number */\n            start_next_scanline += (bytestream2_get_le32(&s->gb) + 8);\n        }\n        bytestream2_seek(&s->gb, start_offset_table, SEEK_SET);\n    }\n\n    // save pointer we are going to use in decode_block\n    s->buf      = avpkt->data;\n    s->buf_size = avpkt->size;\n\n    // Zero out the start if ymin is not 0\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i];\n        for (y = 0; y < s->ymin; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    s->picture = picture;\n\n    avctx->execute2(avctx, decode_block, s->thread_data, NULL, nb_blocks);\n\n    ymax = FFMAX(0, s->ymax + 1);\n    // Zero out the end if ymax+1 is not h\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i] + (ymax * picture->linesize[i]);\n        for (y = ymax; y < avctx->height; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    picture->pict_type = AV_PICTURE_TYPE_I;\n    *got_frame = 1;\n\n    return avpkt->size;\n}", "func_src_after": "static int decode_frame(AVCodecContext *avctx, void *data,\n                        int *got_frame, AVPacket *avpkt)\n{\n    EXRContext *s = avctx->priv_data;\n    ThreadFrame frame = { .f = data };\n    AVFrame *picture = data;\n    uint8_t *ptr;\n\n    int i, y, ret, ymax;\n    int planes;\n    int out_line_size;\n    int nb_blocks;   /* nb scanline or nb tile */\n    uint64_t start_offset_table;\n    uint64_t start_next_scanline;\n    PutByteContext offset_table_writer;\n\n    bytestream2_init(&s->gb, avpkt->data, avpkt->size);\n\n    if ((ret = decode_header(s, picture)) < 0)\n        return ret;\n\n    switch (s->pixel_type) {\n    case EXR_FLOAT:\n    case EXR_HALF:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            } else {\n                /* todo: change this when a floating point pixel format with luma with alpha is implemented */\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRPF32;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAYF32;\n            }\n        }\n        break;\n    case EXR_UINT:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGBA64;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_YA16;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGB48;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAY16;\n            }\n        }\n        break;\n    default:\n        av_log(avctx, AV_LOG_ERROR, \"Missing channel list.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if (s->apply_trc_type != AVCOL_TRC_UNSPECIFIED)\n        avctx->color_trc = s->apply_trc_type;\n\n    switch (s->compression) {\n    case EXR_RAW:\n    case EXR_RLE:\n    case EXR_ZIP1:\n        s->scan_lines_per_block = 1;\n        break;\n    case EXR_PXR24:\n    case EXR_ZIP16:\n        s->scan_lines_per_block = 16;\n        break;\n    case EXR_PIZ:\n    case EXR_B44:\n    case EXR_B44A:\n        s->scan_lines_per_block = 32;\n        break;\n    default:\n        avpriv_report_missing_feature(avctx, \"Compression %d\", s->compression);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    /* Verify the xmin, xmax, ymin and ymax before setting the actual image size.\n     * It's possible for the data window can larger or outside the display window */\n    if (s->xmin > s->xmax  || s->ymin > s->ymax ||\n        s->ydelta == 0xFFFFFFFF || s->xdelta == 0xFFFFFFFF) {\n        av_log(avctx, AV_LOG_ERROR, \"Wrong or missing size information.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if ((ret = ff_set_dimensions(avctx, s->w, s->h)) < 0)\n        return ret;\n\n    s->desc          = av_pix_fmt_desc_get(avctx->pix_fmt);\n    if (!s->desc)\n        return AVERROR_INVALIDDATA;\n\n    if (s->desc->flags & AV_PIX_FMT_FLAG_FLOAT) {\n        planes           = s->desc->nb_components;\n        out_line_size    = avctx->width * 4;\n    } else {\n        planes           = 1;\n        out_line_size    = avctx->width * 2 * s->desc->nb_components;\n    }\n\n    if (s->is_tile) {\n        nb_blocks = ((s->xdelta + s->tile_attr.xSize - 1) / s->tile_attr.xSize) *\n        ((s->ydelta + s->tile_attr.ySize - 1) / s->tile_attr.ySize);\n    } else { /* scanline */\n        nb_blocks = (s->ydelta + s->scan_lines_per_block - 1) /\n        s->scan_lines_per_block;\n    }\n\n    if ((ret = ff_thread_get_buffer(avctx, &frame, 0)) < 0)\n        return ret;\n\n    if (bytestream2_get_bytes_left(&s->gb)/8 < nb_blocks)\n        return AVERROR_INVALIDDATA;\n\n    // check offset table and recreate it if need\n    if (!s->is_tile && bytestream2_peek_le64(&s->gb) == 0) {\n        av_log(s->avctx, AV_LOG_DEBUG, \"recreating invalid scanline offset table\\n\");\n\n        start_offset_table = bytestream2_tell(&s->gb);\n        start_next_scanline = start_offset_table + nb_blocks * 8;\n        bytestream2_init_writer(&offset_table_writer, &avpkt->data[start_offset_table], nb_blocks * 8);\n\n        for (y = 0; y < nb_blocks; y++) {\n            /* write offset of prev scanline in offset table */\n            bytestream2_put_le64(&offset_table_writer, start_next_scanline);\n\n            /* get len of next scanline */\n            bytestream2_seek(&s->gb, start_next_scanline + 4, SEEK_SET);/* skip line number */\n            start_next_scanline += (bytestream2_get_le32(&s->gb) + 8);\n        }\n        bytestream2_seek(&s->gb, start_offset_table, SEEK_SET);\n    }\n\n    // save pointer we are going to use in decode_block\n    s->buf      = avpkt->data;\n    s->buf_size = avpkt->size;\n\n    // Zero out the start if ymin is not 0\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i];\n        for (y = 0; y < FFMIN(s->ymin, s->h); y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    s->picture = picture;\n\n    avctx->execute2(avctx, decode_block, s->thread_data, NULL, nb_blocks);\n\n    ymax = FFMAX(0, s->ymax + 1);\n    // Zero out the end if ymax+1 is not h\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i] + (ymax * picture->linesize[i]);\n        for (y = ymax; y < avctx->height; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    picture->pict_type = AV_PICTURE_TYPE_I;\n    *got_frame = 1;\n\n    return avpkt->size;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/3e5959b3457f7f1856d997261e6ac672bba49e8b", "file_name": "libavcodec/exr.c", "vul_type": "cwe-787", "description": "Write a C function named `decode_frame` for decoding video frames in an FFmpeg codec context."}
{"func_name": "load_blacklist!", "func_src_before": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n    @blacklist = YAML::load(blacklist_path)\n  end", "func_src_after": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n    @blacklist = YAML.load_file(blacklist_path)\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 178, "char_end": 273, "line": "    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n"}, {"line_no": 6, "char_start": 273, "char_end": 317, "line": "    @blacklist = YAML::load(blacklist_path)\n"}], "added": [{"line_no": 5, "char_start": 178, "char_end": 262, "line": "    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n"}, {"line_no": 6, "char_start": 262, "char_end": 310, "line": "    @blacklist = YAML.load_file(blacklist_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 211, "chars": "File.read("}, {"char_start": 271, "char_end": 272, "chars": ")"}, {"char_start": 294, "char_end": 296, "chars": "::"}], "added": [{"char_start": 283, "char_end": 284, "chars": "."}, {"char_start": 288, "char_end": 293, "chars": "_file"}]}, "commit_link": "github.com/episko/blacklist_validator/commit/76255a46f62a8cd082d5a6fb133d2c0b6c2438d6", "file_name": "blacklist_validator.rb", "vul_type": "cwe-502", "commit_msg": "Fixed YAML loading Rails file", "parent_commit": "1aaf965676d2ec5fb505d50c169d0029a411f84a", "description": "Write a Ruby method to load a YAML blacklist configuration file, with a fallback to a default path if the Rails root is not defined."}
{"func_name": "opj_j2k_set_cinema_parameters", "func_src_before": "static void opj_j2k_set_cinema_parameters(opj_cparameters_t *parameters,\n        opj_image_t *image, opj_event_mgr_t *p_manager)\n{\n    /* Configure cinema parameters */\n    int i;\n\n    /* No tiling */\n    parameters->tile_size_on = OPJ_FALSE;\n    parameters->cp_tdx = 1;\n    parameters->cp_tdy = 1;\n\n    /* One tile part for each component */\n    parameters->tp_flag = 'C';\n    parameters->tp_on = 1;\n\n    /* Tile and Image shall be at (0,0) */\n    parameters->cp_tx0 = 0;\n    parameters->cp_ty0 = 0;\n    parameters->image_offset_x0 = 0;\n    parameters->image_offset_y0 = 0;\n\n    /* Codeblock size= 32*32 */\n    parameters->cblockw_init = 32;\n    parameters->cblockh_init = 32;\n\n    /* Codeblock style: no mode switch enabled */\n    parameters->mode = 0;\n\n    /* No ROI */\n    parameters->roi_compno = -1;\n\n    /* No subsampling */\n    parameters->subsampling_dx = 1;\n    parameters->subsampling_dy = 1;\n\n    /* 9-7 transform */\n    parameters->irreversible = 1;\n\n    /* Number of layers */\n    if (parameters->tcp_numlayers > 1) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"1 single quality layer\"\n                      \"-> Number of layers forced to 1 (rather than %d)\\n\"\n                      \"-> Rate of the last layer (%3.1f) will be used\",\n                      parameters->tcp_numlayers,\n                      parameters->tcp_rates[parameters->tcp_numlayers - 1]);\n        parameters->tcp_rates[0] = parameters->tcp_rates[parameters->tcp_numlayers - 1];\n        parameters->tcp_numlayers = 1;\n    }\n\n    /* Resolution levels */\n    switch (parameters->rsiz) {\n    case OPJ_PROFILE_CINEMA_2K:\n        if (parameters->numresolution > 6) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-3 (2k dc profile) requires:\\n\"\n                          \"Number of decomposition levels <= 5\\n\"\n                          \"-> Number of decomposition levels forced to 5 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 6;\n        }\n        break;\n    case OPJ_PROFILE_CINEMA_4K:\n        if (parameters->numresolution < 2) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 1 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 1;\n        } else if (parameters->numresolution > 7) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 6 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 7;\n        }\n        break;\n    default :\n        break;\n    }\n\n    /* Precincts */\n    parameters->csty |= 0x01;\n    parameters->res_spec = parameters->numresolution - 1;\n    for (i = 0; i < parameters->res_spec; i++) {\n        parameters->prcw_init[i] = 256;\n        parameters->prch_init[i] = 256;\n    }\n\n    /* The progression order shall be CPRL */\n    parameters->prog_order = OPJ_CPRL;\n\n    /* Progression order changes for 4K, disallowed for 2K */\n    if (parameters->rsiz == OPJ_PROFILE_CINEMA_4K) {\n        parameters->numpocs = (OPJ_UINT32)opj_j2k_initialise_4K_poc(parameters->POC,\n                              parameters->numresolution);\n    } else {\n        parameters->numpocs = 0;\n    }\n\n    /* Limited bit-rate */\n    parameters->cp_disto_alloc = 1;\n    if (parameters->max_cs_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_cs_size > OPJ_CINEMA_24_CS) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1302083 bytes.\\n\");\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n    }\n\n    if (parameters->max_comp_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_comp_size > OPJ_CINEMA_24_COMP) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1041666 bytes.\\n\");\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n    }\n\n    parameters->tcp_rates[0] = (OPJ_FLOAT32)(image->numcomps * image->comps[0].w *\n                               image->comps[0].h * image->comps[0].prec) /\n                               (OPJ_FLOAT32)(((OPJ_UINT32)parameters->max_cs_size) * 8 * image->comps[0].dx *\n                                       image->comps[0].dy);\n\n}", "func_src_after": "static void opj_j2k_set_cinema_parameters(opj_cparameters_t *parameters,\n        opj_image_t *image, opj_event_mgr_t *p_manager)\n{\n    /* Configure cinema parameters */\n    int i;\n\n    /* No tiling */\n    parameters->tile_size_on = OPJ_FALSE;\n    parameters->cp_tdx = 1;\n    parameters->cp_tdy = 1;\n\n    /* One tile part for each component */\n    parameters->tp_flag = 'C';\n    parameters->tp_on = 1;\n\n    /* Tile and Image shall be at (0,0) */\n    parameters->cp_tx0 = 0;\n    parameters->cp_ty0 = 0;\n    parameters->image_offset_x0 = 0;\n    parameters->image_offset_y0 = 0;\n\n    /* Codeblock size= 32*32 */\n    parameters->cblockw_init = 32;\n    parameters->cblockh_init = 32;\n\n    /* Codeblock style: no mode switch enabled */\n    parameters->mode = 0;\n\n    /* No ROI */\n    parameters->roi_compno = -1;\n\n    /* No subsampling */\n    parameters->subsampling_dx = 1;\n    parameters->subsampling_dy = 1;\n\n    /* 9-7 transform */\n    parameters->irreversible = 1;\n\n    /* Number of layers */\n    if (parameters->tcp_numlayers > 1) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"1 single quality layer\"\n                      \"-> Number of layers forced to 1 (rather than %d)\\n\"\n                      \"-> Rate of the last layer (%3.1f) will be used\",\n                      parameters->tcp_numlayers,\n                      parameters->tcp_rates[parameters->tcp_numlayers - 1]);\n        parameters->tcp_rates[0] = parameters->tcp_rates[parameters->tcp_numlayers - 1];\n        parameters->tcp_numlayers = 1;\n    }\n\n    /* Resolution levels */\n    switch (parameters->rsiz) {\n    case OPJ_PROFILE_CINEMA_2K:\n        if (parameters->numresolution > 6) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-3 (2k dc profile) requires:\\n\"\n                          \"Number of decomposition levels <= 5\\n\"\n                          \"-> Number of decomposition levels forced to 5 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 6;\n        }\n        break;\n    case OPJ_PROFILE_CINEMA_4K:\n        if (parameters->numresolution < 2) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 1 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 1;\n        } else if (parameters->numresolution > 7) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 6 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 7;\n        }\n        break;\n    default :\n        break;\n    }\n\n    /* Precincts */\n    parameters->csty |= 0x01;\n    if (parameters->numresolution == 1) {\n        parameters->res_spec = 1;\n        parameters->prcw_init[0] = 128;\n        parameters->prch_init[0] = 128;\n    } else {\n        parameters->res_spec = parameters->numresolution - 1;\n        for (i = 0; i < parameters->res_spec; i++) {\n            parameters->prcw_init[i] = 256;\n            parameters->prch_init[i] = 256;\n        }\n    }\n\n    /* The progression order shall be CPRL */\n    parameters->prog_order = OPJ_CPRL;\n\n    /* Progression order changes for 4K, disallowed for 2K */\n    if (parameters->rsiz == OPJ_PROFILE_CINEMA_4K) {\n        parameters->numpocs = (OPJ_UINT32)opj_j2k_initialise_4K_poc(parameters->POC,\n                              parameters->numresolution);\n    } else {\n        parameters->numpocs = 0;\n    }\n\n    /* Limited bit-rate */\n    parameters->cp_disto_alloc = 1;\n    if (parameters->max_cs_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_cs_size > OPJ_CINEMA_24_CS) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1302083 bytes.\\n\");\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n    }\n\n    if (parameters->max_comp_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_comp_size > OPJ_CINEMA_24_COMP) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1041666 bytes.\\n\");\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n    }\n\n    parameters->tcp_rates[0] = (OPJ_FLOAT32)(image->numcomps * image->comps[0].w *\n                               image->comps[0].h * image->comps[0].prec) /\n                               (OPJ_FLOAT32)(((OPJ_UINT32)parameters->max_cs_size) * 8 * image->comps[0].dx *\n                                       image->comps[0].dy);\n\n}", "commit_link": "github.com/uclouvain/openjpeg/commit/4241ae6fbbf1de9658764a80944dc8108f2b4154", "file_name": "src/lib/openjp2/j2k.c", "vul_type": "cwe-787", "description": "Write a C function to configure JPEG 2000 cinema parameters for an image."}
{"func_name": "_download_file", "func_src_before": "    @staticmethod\n    def _download_file(bucket, filename, local_dir):\n        key = bucket.get_key(filename)\n        local_filename = os.path.join(local_dir, filename)\n        key.get_contents_to_filename(local_filename)\n        return local_filename", "func_src_after": "    @staticmethod\n    def _download_file(bucket, filename, local_dir):\n        key = bucket.get_key(filename)\n        local_filename = os.path.join(local_dir, os.path.basename(filename))\n        key.get_contents_to_filename(local_filename)\n        return local_filename", "commit_link": "github.com/openstack/nova/commit/76363226bd8533256f7795bba358d7f4b8a6c9e6", "file_name": "nova/image/s3.py", "vul_type": "cwe-022", "description": "Write a Python method to download a file from an S3 bucket to a local directory."}
{"func_name": "WriteTIFFImage", "func_src_before": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\n    if (image->exception.severity > ErrorException)\n      break;\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    (void) TIFFWriteDirectory(tiff);\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(image->exception.severity > ErrorException ? MagickFalse : MagickTrue);\n}", "func_src_after": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    if (TIFFWriteDirectory(tiff) == 0)\n      {\n        status=MagickFalse;\n        break;\n      }\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick6/commit/3c53413eb544cc567309b4c86485eae43e956112", "file_name": "coders/tiff.c", "vul_type": "cwe-125", "description": "Write a function in C to save an image as a TIFF file."}
{"func_name": "get", "func_src_before": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError as ex:\n      self.response.write(ex.message)\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(gae_users.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "func_src_after": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError:\n      self.response.write('invalid build id')\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(self.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "commit_link": "github.com/asdfghjjklllllaaa/infra/commit/2f39f3df54fb79b56744f00bcf97583b3807851f", "file_name": "appengine/cr-buildbucket/handlers.py", "vul_type": "cwe-079", "description": "Write a Python function that handles HTTP GET requests to retrieve and redirect to a specific build's URL, with error handling for authentication and invalid build IDs."}
{"func_name": "(anonymous)", "func_src_before": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "func_src_after": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1141, "char_end": 1234, "line": "                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n"}], "added": [{"line_no": 30, "char_start": 1141, "char_end": 1260, "line": "                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1205, "char_end": 1231, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5e310be94fd9adfaa058c7abe3ab2515b16f128", "file_name": "ArticleView.jsx", "vul_type": "cwe-200", "commit_msg": "add rel=\"noopener noreferrer\"", "parent_commit": "07440697d044dc674dc8e55da0d1e1ebac8053df", "description": "In JavaScript, write a React component that displays a list of questions with their answers and optional external links, hiding the author's avatar if not provided."}
{"func_name": "HPHP::SimpleParser::handleBackslash", "func_src_before": "  bool handleBackslash(signed char& out) {\n    char ch = *p++;\n    switch (ch) {\n      case 0: return false;\n      case '\"': out = ch; return true;\n      case '\\\\': out = ch; return true;\n      case '/': out = ch; return true;\n      case 'b': out = '\\b'; return true;\n      case 'f': out = '\\f'; return true;\n      case 'n': out = '\\n'; return true;\n      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          auto const ch2 = *p++;\n          auto const dch3 = dehexchar(*p++);\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(ch1 != '0' || ch2 != '0' || dch3 < 0 || dch4 < 0)) {\n            return false;\n          }\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {\n            auto const hexv = dehexchar(*p++);\n            if (hexv < 0) return false; // includes check for end of string\n            u16cp <<= 4;\n            u16cp |= hexv;\n          }\n          if (u16cp > 0x7f) {\n            return false;\n          } else {\n            out = u16cp;\n            return true;\n          }\n        }\n      }\n      default: return false;\n    }\n  }", "func_src_after": "  bool handleBackslash(signed char& out) {\n    char ch = *p++;\n    switch (ch) {\n      case 0: return false;\n      case '\"': out = ch; return true;\n      case '\\\\': out = ch; return true;\n      case '/': out = ch; return true;\n      case 'b': out = '\\b'; return true;\n      case 'f': out = '\\f'; return true;\n      case 'n': out = '\\n'; return true;\n      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          if (UNLIKELY(ch1 != '0')) return false;\n          auto const ch2 = *p++;\n          if (UNLIKELY(ch2 != '0')) return false;\n          auto const dch3 = dehexchar(*p++);\n          if (UNLIKELY(dch3 < 0)) return false;\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(dch4 < 0)) return false;\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {\n            auto const hexv = dehexchar(*p++);\n            if (hexv < 0) return false; // includes check for end of string\n            u16cp <<= 4;\n            u16cp |= hexv;\n          }\n          if (u16cp > 0x7f) {\n            return false;\n          } else {\n            out = u16cp;\n            return true;\n          }\n        }\n      }\n      default: return false;\n    }\n  }", "commit_link": "github.com/facebook/hhvm/commit/b3679121bb3c7017ff04b4c08402ffff5cf59b13", "file_name": "hphp/runtime/ext/json/JSON_parser.cpp", "vul_type": "cwe-125", "description": "Write a C++ function to process escape sequences in a JSON string and convert them to their corresponding characters."}
{"func_name": "WidocoUtils::unZipIt", "func_src_before": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "func_src_after": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder, fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 315, "char_end": 386, "line": "\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n"}], "added": [{"line_no": 11, "char_start": 315, "char_end": 368, "line": "\t\t\t\tFile newFile = new File(outputFolder, fileName);\n"}, {"line_no": 12, "char_start": 368, "char_end": 434, "line": "\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n"}, {"line_no": 13, "char_start": 434, "char_end": 484, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 484, "char_end": 490, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 355, "char_end": 385, "chars": " + File.separator + fileName);"}], "added": [{"char_start": 355, "char_end": 489, "chars": ", fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}"}]}, "commit_link": "github.com/dgarijo/Widoco/commit/f2279b76827f32190adfa9bd5229b7d5a147fa92", "file_name": "WidocoUtils.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to unzip a resource into a specified output folder."}
{"func_name": "parse8BIM", "func_src_before": "static ssize_t parse8BIM(Image *ifile, Image *ofile)\n{\n  char\n    brkused,\n    quoted,\n    *line,\n    *token,\n    *newstr,\n    *name;\n\n  int\n    state,\n    next;\n\n  unsigned char\n    dataset;\n\n  unsigned int\n    recnum;\n\n  int\n    inputlen = MaxTextExtent;\n\n  MagickOffsetType\n    savedpos,\n    currentpos;\n\n  ssize_t\n    savedolen = 0L,\n    outputlen = 0L;\n\n  TokenInfo\n    *token_info;\n\n  dataset = 0;\n  recnum = 0;\n  line = (char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*line));\n  if (line == (char *) NULL)\n    return(-1);\n  newstr = name = token = (char *) NULL;\n  savedpos = 0;\n  token_info=AcquireTokenInfo();\n  while (super_fgets(&line,&inputlen,ifile)!=NULL)\n  {\n    state=0;\n    next=0;\n\n    token=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*token));\n    if (token == (char *) NULL)\n      break;\n    newstr=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*newstr));\n    if (newstr == (char *) NULL)\n      break;\n    while (Tokenizer(token_info,0,token,(size_t) inputlen,line,\"\",\"=\",\"\\\"\",0,\n           &brkused,&next,&quoted)==0)\n    {\n      if (state == 0)\n        {\n          int\n            state,\n            next;\n\n          char\n            brkused,\n            quoted;\n\n          state=0;\n          next=0;\n          while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"#\",\n            \"\", 0,&brkused,&next,&quoted)==0)\n          {\n            switch (state)\n            {\n              case 0:\n                if (strcmp(newstr,\"8BIM\")==0)\n                  dataset = 255;\n                else\n                  dataset = (unsigned char) StringToLong(newstr);\n                break;\n              case 1:\n                recnum = (unsigned int) StringToUnsignedLong(newstr);\n                break;\n              case 2:\n                name=(char *) AcquireQuantumMemory(strlen(newstr)+MaxTextExtent,\n                  sizeof(*name));\n                if (name)\n                  (void) strcpy(name,newstr);\n                break;\n            }\n            state++;\n          }\n        }\n      else\n        if (state == 1)\n          {\n            int\n              next;\n\n            ssize_t\n              len;\n\n            char\n              brkused,\n              quoted;\n\n            next=0;\n            len = (ssize_t) strlen(token);\n            while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"&\",\n              \"\",0,&brkused,&next,&quoted)==0)\n            {\n              if (brkused && next > 0)\n                {\n                  char\n                    *s = &token[next-1];\n\n                  len -= (ssize_t) convertHTMLcodes(s,(int) strlen(s));\n                }\n            }\n\n            if (dataset == 255)\n              {\n                unsigned char\n                  nlen = 0;\n\n                int\n                  i;\n\n                if (savedolen > 0)\n                  {\n                    MagickOffsetType\n                      offset;\n\n                    ssize_t diff = outputlen - savedolen;\n                    currentpos = TellBlob(ofile);\n                    if (currentpos < 0)\n                      return(-1);\n                    offset=SeekBlob(ofile,savedpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n                    offset=SeekBlob(ofile,currentpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    savedolen = 0L;\n                  }\n                if (outputlen & 1)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                (void) WriteBlobString(ofile,\"8BIM\");\n                (void) WriteBlobMSBShort(ofile,(unsigned short) recnum);\n                outputlen += 6;\n                if (name)\n                  nlen = (unsigned char) strlen(name);\n                (void) WriteBlobByte(ofile,nlen);\n                outputlen++;\n                for (i=0; i<nlen; i++)\n                  (void) WriteBlobByte(ofile,(unsigned char) name[i]);\n                outputlen += nlen;\n                if ((nlen & 0x01) == 0)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                if (recnum != IPTC_ID)\n                  {\n                    (void) WriteBlobMSBLong(ofile, (unsigned int) len);\n                    outputlen += 4;\n\n                    next=0;\n                    outputlen += len;\n                    while (len--)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n\n                    if (outputlen & 1)\n                      {\n                        (void) WriteBlobByte(ofile,0x00);\n                        outputlen++;\n                      }\n                  }\n                else\n                  {\n                    /* patch in a fake length for now and fix it later */\n                    savedpos = TellBlob(ofile);\n                    if (savedpos < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,0xFFFFFFFFU);\n                    outputlen += 4;\n                    savedolen = outputlen;\n                  }\n              }\n            else\n              {\n                if (len <= 0x7FFF)\n                  {\n                    (void) WriteBlobByte(ofile,0x1c);\n                    (void) WriteBlobByte(ofile,(unsigned char) dataset);\n                    (void) WriteBlobByte(ofile,(unsigned char) (recnum & 0xff));\n                    (void) WriteBlobMSBShort(ofile,(unsigned short) len);\n                    outputlen += 5;\n                    next=0;\n                    outputlen += len;\n                    while (len--)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n                  }\n              }\n          }\n      state++;\n    }\n    if (token != (char *) NULL)\n      token=DestroyString(token);\n    if (newstr != (char *) NULL)\n      newstr=DestroyString(newstr);\n    if (name != (char *) NULL)\n      name=DestroyString(name);\n  }\n  token_info=DestroyTokenInfo(token_info);\n  if (token != (char *) NULL)\n    token=DestroyString(token);\n  if (newstr != (char *) NULL)\n    newstr=DestroyString(newstr);\n  if (name != (char *) NULL)\n    name=DestroyString(name);\n  line=DestroyString(line);\n  if (savedolen > 0)\n    {\n      MagickOffsetType\n        offset;\n\n      ssize_t diff = outputlen - savedolen;\n\n      currentpos = TellBlob(ofile);\n      if (currentpos < 0)\n        return(-1);\n      offset=SeekBlob(ofile,savedpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n      offset=SeekBlob(ofile,currentpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      savedolen = 0L;\n    }\n  return(outputlen);\n}", "func_src_after": "static ssize_t parse8BIM(Image *ifile, Image *ofile)\n{\n  char\n    brkused,\n    quoted,\n    *line,\n    *token,\n    *newstr,\n    *name;\n\n  int\n    state,\n    next;\n\n  unsigned char\n    dataset;\n\n  unsigned int\n    recnum;\n\n  int\n    inputlen = MaxTextExtent;\n\n  MagickOffsetType\n    savedpos,\n    currentpos;\n\n  ssize_t\n    savedolen = 0L,\n    outputlen = 0L;\n\n  TokenInfo\n    *token_info;\n\n  dataset = 0;\n  recnum = 0;\n  line = (char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*line));\n  if (line == (char *) NULL)\n    return(-1);\n  newstr = name = token = (char *) NULL;\n  savedpos = 0;\n  token_info=AcquireTokenInfo();\n  while (super_fgets(&line,&inputlen,ifile)!=NULL)\n  {\n    state=0;\n    next=0;\n\n    token=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*token));\n    if (token == (char *) NULL)\n      break;\n    newstr=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*newstr));\n    if (newstr == (char *) NULL)\n      break;\n    while (Tokenizer(token_info,0,token,(size_t) inputlen,line,\"\",\"=\",\"\\\"\",0,\n           &brkused,&next,&quoted)==0)\n    {\n      if (state == 0)\n        {\n          int\n            state,\n            next;\n\n          char\n            brkused,\n            quoted;\n\n          state=0;\n          next=0;\n          while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"#\",\n            \"\", 0,&brkused,&next,&quoted)==0)\n          {\n            switch (state)\n            {\n              case 0:\n                if (strcmp(newstr,\"8BIM\")==0)\n                  dataset = 255;\n                else\n                  dataset = (unsigned char) StringToLong(newstr);\n                break;\n              case 1:\n                recnum = (unsigned int) StringToUnsignedLong(newstr);\n                break;\n              case 2:\n                name=(char *) AcquireQuantumMemory(strlen(newstr)+MaxTextExtent,\n                  sizeof(*name));\n                if (name)\n                  (void) strcpy(name,newstr);\n                break;\n            }\n            state++;\n          }\n        }\n      else\n        if (state == 1)\n          {\n            int\n              next;\n\n            ssize_t\n              len;\n\n            char\n              brkused,\n              quoted;\n\n            next=0;\n            len = (ssize_t) strlen(token);\n            while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"&\",\n              \"\",0,&brkused,&next,&quoted)==0)\n            {\n              if (brkused && next > 0)\n                {\n                  char\n                    *s = &token[next-1];\n\n                  len -= (ssize_t) convertHTMLcodes(s,(int) strlen(s));\n                }\n            }\n\n            if (dataset == 255)\n              {\n                unsigned char\n                  nlen = 0;\n\n                int\n                  i;\n\n                if (savedolen > 0)\n                  {\n                    MagickOffsetType\n                      offset;\n\n                    ssize_t diff = outputlen - savedolen;\n                    currentpos = TellBlob(ofile);\n                    if (currentpos < 0)\n                      return(-1);\n                    offset=SeekBlob(ofile,savedpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n                    offset=SeekBlob(ofile,currentpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    savedolen = 0L;\n                  }\n                if (outputlen & 1)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                (void) WriteBlobString(ofile,\"8BIM\");\n                (void) WriteBlobMSBShort(ofile,(unsigned short) recnum);\n                outputlen += 6;\n                if (name)\n                  nlen = (unsigned char) strlen(name);\n                (void) WriteBlobByte(ofile,nlen);\n                outputlen++;\n                for (i=0; i<nlen; i++)\n                  (void) WriteBlobByte(ofile,(unsigned char) name[i]);\n                outputlen += nlen;\n                if ((nlen & 0x01) == 0)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                if (recnum != IPTC_ID)\n                  {\n                    (void) WriteBlobMSBLong(ofile, (unsigned int) len);\n                    outputlen += 4;\n\n                    next=0;\n                    outputlen += len;\n                    while (len-- > 0)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n\n                    if (outputlen & 1)\n                      {\n                        (void) WriteBlobByte(ofile,0x00);\n                        outputlen++;\n                      }\n                  }\n                else\n                  {\n                    /* patch in a fake length for now and fix it later */\n                    savedpos = TellBlob(ofile);\n                    if (savedpos < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,0xFFFFFFFFU);\n                    outputlen += 4;\n                    savedolen = outputlen;\n                  }\n              }\n            else\n              {\n                if (len <= 0x7FFF)\n                  {\n                    (void) WriteBlobByte(ofile,0x1c);\n                    (void) WriteBlobByte(ofile,(unsigned char) dataset);\n                    (void) WriteBlobByte(ofile,(unsigned char) (recnum & 0xff));\n                    (void) WriteBlobMSBShort(ofile,(unsigned short) len);\n                    outputlen += 5;\n                    next=0;\n                    outputlen += len;\n                    while (len-- > 0)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n                  }\n              }\n          }\n      state++;\n    }\n    if (token != (char *) NULL)\n      token=DestroyString(token);\n    if (newstr != (char *) NULL)\n      newstr=DestroyString(newstr);\n    if (name != (char *) NULL)\n      name=DestroyString(name);\n  }\n  token_info=DestroyTokenInfo(token_info);\n  if (token != (char *) NULL)\n    token=DestroyString(token);\n  if (newstr != (char *) NULL)\n    newstr=DestroyString(newstr);\n  if (name != (char *) NULL)\n    name=DestroyString(name);\n  line=DestroyString(line);\n  if (savedolen > 0)\n    {\n      MagickOffsetType\n        offset;\n\n      ssize_t diff = outputlen - savedolen;\n\n      currentpos = TellBlob(ofile);\n      if (currentpos < 0)\n        return(-1);\n      offset=SeekBlob(ofile,savedpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n      offset=SeekBlob(ofile,currentpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      savedolen = 0L;\n    }\n  return(outputlen);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/97c9f438a9b3454d085895f4d1f66389fd22a0fb", "file_name": "coders/meta.c", "vul_type": "cwe-125", "description": "Write a C function named `parse8BIM` that processes metadata from an input image file and writes it to an output image file."}
{"func_name": "cx24116_send_diseqc_msg", "func_src_before": "static int cx24116_send_diseqc_msg(struct dvb_frontend *fe,\n\tstruct dvb_diseqc_master_cmd *d)\n{\n\tstruct cx24116_state *state = fe->demodulator_priv;\n\tint i, ret;\n\n\t/* Dump DiSEqC message */\n\tif (debug) {\n\t\tprintk(KERN_INFO \"cx24116: %s(\", __func__);\n\t\tfor (i = 0 ; i < d->msg_len ;) {\n\t\t\tprintk(KERN_INFO \"0x%02x\", d->msg[i]);\n\t\t\tif (++i < d->msg_len)\n\t\t\t\tprintk(KERN_INFO \", \");\n\t\t}\n\t\tprintk(\") toneburst=%d\\n\", toneburst);\n\t}\n\n\t/* Validate length */\n\tif (d->msg_len > (CX24116_ARGLEN - CX24116_DISEQC_MSGOFS))\n\t\treturn -EINVAL;\n\n\t/* DiSEqC message */\n\tfor (i = 0; i < d->msg_len; i++)\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];\n\n\t/* DiSEqC message length */\n\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN] = d->msg_len;\n\n\t/* Command length */\n\tstate->dsec_cmd.len = CX24116_DISEQC_MSGOFS +\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN];\n\n\t/* DiSEqC toneburst */\n\tif (toneburst == CX24116_DISEQC_MESGCACHE)\n\t\t/* Message is cached */\n\t\treturn 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONEOFF)\n\t\t/* Message is sent without burst */\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] = 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONECACHE) {\n\t\t/*\n\t\t * Message is sent with derived else cached burst\n\t\t *\n\t\t * WRITE PORT GROUP COMMAND 38\n\t\t *\n\t\t * 0/A/A: E0 10 38 F0..F3\n\t\t * 1/B/B: E0 10 38 F4..F7\n\t\t * 2/C/A: E0 10 38 F8..FB\n\t\t * 3/D/B: E0 10 38 FC..FF\n\t\t *\n\t\t * databyte[3]= 8421:8421\n\t\t *              ABCD:WXYZ\n\t\t *              CLR :SET\n\t\t *\n\t\t *              WX= PORT SELECT 0..3    (X=TONEBURST)\n\t\t *              Y = VOLTAGE             (0=13V, 1=18V)\n\t\t *              Z = BAND                (0=LOW, 1=HIGH(22K))\n\t\t */\n\t\tif (d->msg_len >= 4 && d->msg[2] == 0x38)\n\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] =\n\t\t\t\t((d->msg[3] & 4) >> 2);\n\t\tif (debug)\n\t\t\tdprintk(\"%s burst=%d\\n\", __func__,\n\t\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST]);\n\t}\n\n\t/* Wait for LNB ready */\n\tret = cx24116_wait_for_lnb(fe);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t/* Wait for voltage/min repeat delay */\n\tmsleep(100);\n\n\t/* Command */\n\tret = cx24116_cmd_execute(fe, &state->dsec_cmd);\n\tif (ret != 0)\n\t\treturn ret;\n\t/*\n\t * Wait for send\n\t *\n\t * Eutelsat spec:\n\t * >15ms delay          + (XXX determine if FW does this, see set_tone)\n\t *  13.5ms per byte     +\n\t * >15ms delay          +\n\t *  12.5ms burst        +\n\t * >15ms delay            (XXX determine if FW does this, see set_tone)\n\t */\n\tmsleep((state->dsec_cmd.args[CX24116_DISEQC_MSGLEN] << 4) +\n\t\t((toneburst == CX24116_DISEQC_TONEOFF) ? 30 : 60));\n\n\treturn 0;\n}", "func_src_after": "static int cx24116_send_diseqc_msg(struct dvb_frontend *fe,\n\tstruct dvb_diseqc_master_cmd *d)\n{\n\tstruct cx24116_state *state = fe->demodulator_priv;\n\tint i, ret;\n\n\t/* Validate length */\n\tif (d->msg_len > sizeof(d->msg))\n                return -EINVAL;\n\n\t/* Dump DiSEqC message */\n\tif (debug) {\n\t\tprintk(KERN_INFO \"cx24116: %s(\", __func__);\n\t\tfor (i = 0 ; i < d->msg_len ;) {\n\t\t\tprintk(KERN_INFO \"0x%02x\", d->msg[i]);\n\t\t\tif (++i < d->msg_len)\n\t\t\t\tprintk(KERN_INFO \", \");\n\t\t}\n\t\tprintk(\") toneburst=%d\\n\", toneburst);\n\t}\n\n\t/* DiSEqC message */\n\tfor (i = 0; i < d->msg_len; i++)\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];\n\n\t/* DiSEqC message length */\n\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN] = d->msg_len;\n\n\t/* Command length */\n\tstate->dsec_cmd.len = CX24116_DISEQC_MSGOFS +\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN];\n\n\t/* DiSEqC toneburst */\n\tif (toneburst == CX24116_DISEQC_MESGCACHE)\n\t\t/* Message is cached */\n\t\treturn 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONEOFF)\n\t\t/* Message is sent without burst */\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] = 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONECACHE) {\n\t\t/*\n\t\t * Message is sent with derived else cached burst\n\t\t *\n\t\t * WRITE PORT GROUP COMMAND 38\n\t\t *\n\t\t * 0/A/A: E0 10 38 F0..F3\n\t\t * 1/B/B: E0 10 38 F4..F7\n\t\t * 2/C/A: E0 10 38 F8..FB\n\t\t * 3/D/B: E0 10 38 FC..FF\n\t\t *\n\t\t * databyte[3]= 8421:8421\n\t\t *              ABCD:WXYZ\n\t\t *              CLR :SET\n\t\t *\n\t\t *              WX= PORT SELECT 0..3    (X=TONEBURST)\n\t\t *              Y = VOLTAGE             (0=13V, 1=18V)\n\t\t *              Z = BAND                (0=LOW, 1=HIGH(22K))\n\t\t */\n\t\tif (d->msg_len >= 4 && d->msg[2] == 0x38)\n\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] =\n\t\t\t\t((d->msg[3] & 4) >> 2);\n\t\tif (debug)\n\t\t\tdprintk(\"%s burst=%d\\n\", __func__,\n\t\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST]);\n\t}\n\n\t/* Wait for LNB ready */\n\tret = cx24116_wait_for_lnb(fe);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t/* Wait for voltage/min repeat delay */\n\tmsleep(100);\n\n\t/* Command */\n\tret = cx24116_cmd_execute(fe, &state->dsec_cmd);\n\tif (ret != 0)\n\t\treturn ret;\n\t/*\n\t * Wait for send\n\t *\n\t * Eutelsat spec:\n\t * >15ms delay          + (XXX determine if FW does this, see set_tone)\n\t *  13.5ms per byte     +\n\t * >15ms delay          +\n\t *  12.5ms burst        +\n\t * >15ms delay            (XXX determine if FW does this, see set_tone)\n\t */\n\tmsleep((state->dsec_cmd.args[CX24116_DISEQC_MSGLEN] << 4) +\n\t\t((toneburst == CX24116_DISEQC_TONEOFF) ? 30 : 60));\n\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/1fa2337a315a2448c5434f41e00d56b01a22283c", "file_name": "drivers/media/dvb-frontends/cx24116.c", "vul_type": "cwe-125", "description": "Write a C function `cx24116_send_diseqc_msg` to send a DiSEqC message to a satellite frontend device."}
{"func_name": "parse_hid_report_descriptor", "func_src_before": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i];\n\n\t\t/* Skip over prefix */\n\t\ti++;\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = PREF_SIZE(prefix);\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tsize = 4;\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "func_src_after": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i++];\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = (1U << PREF_SIZE(prefix)) >> 1;\n\t\tif (i + size > length) {\n\t\t\tdev_err(ddev,\n\t\t\t\t\"Not enough data (need %d, have %d)\\n\",\n\t\t\t\ti + size, length);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/a50829479f58416a013a4ccca791336af3c584c7", "file_name": "drivers/input/tablet/gtco.c", "vul_type": "cwe-125", "description": "Write a C function to parse a HID report descriptor and extract device information."}
{"func_name": "handle_message", "func_src_before": "    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            #TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)", "func_src_after": "    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        headers = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            # TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip'],\n                       'source': input['_zops_source']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)", "commit_link": "github.com/zetaops/zengine/commit/52eafbee90f8ddf78be0c7452828d49423246851", "file_name": "zengine/wf_daemon.py", "vul_type": "cwe-078", "description": "Write a Python function `handle_message` that processes AMQP messages to run workflows or views based on the message content."}
{"func_name": "init_settings", "func_src_before": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = jinja_env_options if jinja_env_options else {}\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "func_src_after": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = {\"autoescape\": True}\n        jenv_opt.update(jinja_env_options if jinja_env_options else {})\n\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "commit_link": "github.com/ipython/ipython/commit/3ab41641cf6fce3860c73d5cf4645aa12e1e5892", "file_name": "IPython/html/notebookapp.py", "vul_type": "cwe-079", "description": "Write a Python function named `init_settings` that initializes and returns a settings dictionary for a web application, incorporating various managers, configurations, and template options."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/aburchette/territory-manager-mean/commit/24620016541089cc0ca316a0dec32ee0db864d98", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Replaced SHA1 password hashing with more bcrypt", "parent_commit": "f944f0a464555f033f01413f24d1cd47ab412ae7", "description": "Write a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "flb_gzip_compress", "func_src_before": "int flb_gzip_compress(void *in_data, size_t in_len,\n                      void **out_data, size_t *out_len)\n{\n    int flush;\n    int status;\n    int footer_start;\n    uint8_t *pb;\n    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n    out_size = in_len + 32;\n    out_buf = flb_malloc(out_size);\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }\n\n    /* Initialize streaming buffer context */\n    memset(&strm, '\\0', sizeof(strm));\n    strm.zalloc    = Z_NULL;\n    strm.zfree     = Z_NULL;\n    strm.opaque    = Z_NULL;\n    strm.next_in   = in_data;\n    strm.avail_in  = in_len;\n    strm.total_out = 0;\n\n    /* Deflate mode */\n    deflateInit2(&strm, Z_DEFAULT_COMPRESSION,\n                 Z_DEFLATED, -Z_DEFAULT_WINDOW_BITS, 9, Z_DEFAULT_STRATEGY);\n\n    /*\n     * Miniz don't support GZip format directly, instead we will:\n     *\n     * - append manual GZip magic bytes\n     * - deflate raw content\n     * - append manual CRC32 data\n     */\n    gzip_header(out_buf);\n\n    /* Header offset */\n    pb = (uint8_t *) out_buf + FLB_GZIP_HEADER_OFFSET;\n\n    flush = Z_NO_FLUSH;\n    while (1) {\n        strm.next_out  = pb + strm.total_out;\n        strm.avail_out = out_size - (pb - (uint8_t *) out_buf);\n\n        if (strm.avail_in == 0) {\n            flush = Z_FINISH;\n        }\n\n        status = deflate(&strm, flush);\n        if (status == Z_STREAM_END) {\n            break;\n        }\n        else if (status != Z_OK) {\n            deflateEnd(&strm);\n            return -1;\n        }\n    }\n\n    if (deflateEnd(&strm) != Z_OK) {\n        flb_free(out_buf);\n        return -1;\n    }\n    *out_len = strm.total_out;\n\n    /* Construct the gzip checksum (CRC32 footer) */\n    footer_start = FLB_GZIP_HEADER_OFFSET + *out_len;\n    pb = (uint8_t *) out_buf + footer_start;\n\n    crc = mz_crc32(MZ_CRC32_INIT, in_data, in_len);\n    *pb++ = crc & 0xFF;\n    *pb++ = (crc >> 8) & 0xFF;\n    *pb++ = (crc >> 16) & 0xFF;\n    *pb++ = (crc >> 24) & 0xFF;\n    *pb++ = in_len & 0xFF;\n    *pb++ = (in_len >> 8) & 0xFF;\n    *pb++ = (in_len >> 16) & 0xFF;\n    *pb++ = (in_len >> 24) & 0xFF;\n\n    /* Set the real buffer size for the caller */\n    *out_len += FLB_GZIP_HEADER_OFFSET + 8;\n    *out_data = out_buf;\n\n    return 0;\n}", "func_src_after": "int flb_gzip_compress(void *in_data, size_t in_len,\n                      void **out_data, size_t *out_len)\n{\n    int flush;\n    int status;\n    int footer_start;\n    uint8_t *pb;\n    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n\n    /*\n     * GZIP relies on an algorithm with worst-case expansion\n     * of 5 bytes per 32KB data. This means we need to create a variable\n     * length output, that depends on the input length.\n     * See RFC 1951 for details.\n     */\n    int max_input_expansion = ((int)(in_len / 32000) + 1) * 5;\n\n    /*\n     * Max compressed size is equal to sum of:\n     *   10 byte header\n     *   8 byte foot\n     *   max input expansion\n     *   size of input\n     */\n    out_size = 10 + 8 + max_input_expansion + in_len;\n    out_buf = flb_malloc(out_size);\n\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }\n\n    /* Initialize streaming buffer context */\n    memset(&strm, '\\0', sizeof(strm));\n    strm.zalloc    = Z_NULL;\n    strm.zfree     = Z_NULL;\n    strm.opaque    = Z_NULL;\n    strm.next_in   = in_data;\n    strm.avail_in  = in_len;\n    strm.total_out = 0;\n\n    /* Deflate mode */\n    deflateInit2(&strm, Z_DEFAULT_COMPRESSION,\n                 Z_DEFLATED, -Z_DEFAULT_WINDOW_BITS, 9, Z_DEFAULT_STRATEGY);\n\n    /*\n     * Miniz don't support GZip format directly, instead we will:\n     *\n     * - append manual GZip magic bytes\n     * - deflate raw content\n     * - append manual CRC32 data\n     */\n    gzip_header(out_buf);\n\n    /* Header offset */\n    pb = (uint8_t *) out_buf + FLB_GZIP_HEADER_OFFSET;\n\n    flush = Z_NO_FLUSH;\n    while (1) {\n        strm.next_out  = pb + strm.total_out;\n        strm.avail_out = out_size - (pb - (uint8_t *) out_buf);\n\n        if (strm.avail_in == 0) {\n            flush = Z_FINISH;\n        }\n\n        status = deflate(&strm, flush);\n        if (status == Z_STREAM_END) {\n            break;\n        }\n        else if (status != Z_OK) {\n            deflateEnd(&strm);\n            return -1;\n        }\n    }\n\n    if (deflateEnd(&strm) != Z_OK) {\n        flb_free(out_buf);\n        return -1;\n    }\n    *out_len = strm.total_out;\n\n    /* Construct the gzip checksum (CRC32 footer) */\n    footer_start = FLB_GZIP_HEADER_OFFSET + *out_len;\n    pb = (uint8_t *) out_buf + footer_start;\n\n    crc = mz_crc32(MZ_CRC32_INIT, in_data, in_len);\n    *pb++ = crc & 0xFF;\n    *pb++ = (crc >> 8) & 0xFF;\n    *pb++ = (crc >> 16) & 0xFF;\n    *pb++ = (crc >> 24) & 0xFF;\n    *pb++ = in_len & 0xFF;\n    *pb++ = (in_len >> 8) & 0xFF;\n    *pb++ = (in_len >> 16) & 0xFF;\n    *pb++ = (in_len >> 24) & 0xFF;\n\n    /* Set the real buffer size for the caller */\n    *out_len += FLB_GZIP_HEADER_OFFSET + 8;\n    *out_data = out_buf;\n\n    return 0;\n}", "commit_link": "github.com/fluent/fluent-bit/commit/cadff53c093210404aed01c4cf586adb8caa07af", "file_name": "src/flb_gzip.c", "vul_type": "cwe-787", "description": "Write a C function to compress data using GZIP and handle memory allocation for the output buffer."}
{"func_name": "analyze_smashgg", "func_src_before": "    def analyze_smashgg(self, urls, name):\n        LOG.info('we are about to analyze scene {} with {} brackets'.format(name, len(urls)))\n        for url in urls:\n            # Before we process this URL, check to see if we already have\n            sql = \"SELECT * FROM analyzed where base_url='{}'\".format(url)\n            res = self.db.exec(sql)\n            if len(res) == 0:\n\n                display_name = bracket_utils.get_display_base(url)\n\n                # We don't care about doubles tournaments\n                if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                    LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                    continue\n\n                LOG.info('About to process pro bracket {}'.format(url))\n                self.data_processor.process(url, name, display_name)\n            else:\n                LOG.info(\"Skpping pro bracket because it has already been analyzed: {}\".format(url))", "func_src_after": "    def analyze_smashgg(self, urls, name):\n        LOG.info('we are about to analyze scene {} with {} brackets'.format(name, len(urls)))\n        for url in urls:\n            # Before we process this URL, check to see if we already have\n            sql = \"SELECT * FROM analyzed where base_url='{url}'\"\n            args = {'url':url}\n            res = self.db.exec(sql, args)\n            if len(res) == 0:\n\n                display_name = bracket_utils.get_display_base(url)\n\n                # We don't care about doubles tournaments\n                if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                    LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                    continue\n\n                LOG.info('About to process pro bracket {}'.format(url))\n                self.data_processor.process(url, name, display_name)\n            else:\n                LOG.info(\"Skpping pro bracket because it has already been analyzed: {}\".format(url))", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "validURLs.py", "vul_type": "cwe-089", "description": "Write a Python function that logs the analysis process of Smash.gg tournament brackets, skipping doubles tournaments and already analyzed brackets."}
{"func_name": "get_request", "func_src_before": "        def get_request(self):\n            socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n                                     certfile=HttpsTestServerLayer.CERT_FILE,\n                                     cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                     server_side=True)\n            return socket, client_address", "func_src_after": "        def get_request(self):\n\n            # Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                certfile=HttpsTestServerLayer.CERT_FILE,\n                keyfile=HttpsTestServerLayer.CERT_FILE,\n                cafile=HttpsTestServerLayer.CACERT_FILE)\n\n            # Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket, server_side=True)\n\n            return socket, client_address", "line_changes": {"deleted": [{"line_no": 3, "char_start": 97, "char_end": 142, "line": "            socket = ssl.wrap_socket(socket,\n"}, {"line_no": 4, "char_start": 142, "char_end": 219, "line": "                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 5, "char_start": 219, "char_end": 297, "line": "                                     certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 6, "char_start": 297, "char_end": 363, "line": "                                     cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 363, "char_end": 443, "line": "                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n"}, {"line_no": 8, "char_start": 443, "char_end": 498, "line": "                                     server_side=True)\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 32, "line": "\n"}, {"line_no": 4, "char_start": 67, "char_end": 121, "line": "            context = ssl._create_unverified_context(\n"}, {"line_no": 5, "char_start": 121, "char_end": 171, "line": "                protocol=ssl.PROTOCOL_TLS_SERVER,\n"}, {"line_no": 6, "char_start": 171, "char_end": 216, "line": "                cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 216, "char_end": 254, "line": "                check_hostname=False,\n"}, {"line_no": 8, "char_start": 254, "char_end": 303, "line": "                purpose=ssl.Purpose.CLIENT_AUTH,\n"}, {"line_no": 9, "char_start": 303, "char_end": 360, "line": "                certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 10, "char_start": 360, "char_end": 416, "line": "                keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 11, "char_start": 416, "char_end": 473, "line": "                cafile=HttpsTestServerLayer.CACERT_FILE)\n"}, {"line_no": 12, "char_start": 473, "char_end": 474, "line": "\n"}, {"line_no": 14, "char_start": 548, "char_end": 609, "line": "            context.minimum_version = ssl.TLSVersion.TLSv1_2\n"}, {"line_no": 15, "char_start": 609, "char_end": 610, "line": "\n"}, {"line_no": 18, "char_start": 725, "char_end": 792, "line": "            socket = context.wrap_socket(socket, server_side=True)\n"}, {"line_no": 19, "char_start": 792, "char_end": 793, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 43, "char_end": 182, "chars": "socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     key"}, {"char_start": 235, "char_end": 240, "chars": "     "}, {"char_start": 257, "char_end": 260, "chars": "ert"}, {"char_start": 295, "char_end": 296, "chars": ","}, {"char_start": 309, "char_end": 479, "chars": "                         cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                    "}], "added": [{"char_start": 31, "char_end": 32, "chars": "\n"}, {"char_start": 44, "char_end": 323, "chars": "# Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                cert"}, {"char_start": 376, "char_end": 416, "chars": "keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"char_start": 433, "char_end": 434, "chars": "a"}, {"char_start": 460, "char_end": 462, "chars": "CA"}, {"char_start": 471, "char_end": 473, "chars": ")\n"}, {"char_start": 486, "char_end": 773, "chars": "# Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket,"}, {"char_start": 792, "char_end": 793, "chars": "\n"}]}, "commit_link": "github.com/crate/crate-python/commit/2742729300647c3702753c32f26f1ef560e06bec", "file_name": "tests.py", "vul_type": "cwe-327", "commit_msg": "Tests: Stop using deprecated `ssl.wrap_socket`\n\nUse `context.wrap_socket` instead. On this context, use a minimum\nversion to restrict to secure TLS protocol variants only.\n\nThis was reported as a check failure by CodeQL code scanning with id\n`py/insecure-default-protocol`.\n\nhttps://github.com/github/codeql/blob/main/python/ql/src/Security/CWE-327/InsecureDefaultProtocol.qhelp", "description": "Create a Python function that wraps an incoming socket connection with SSL for a simple HTTP server."}
{"func_name": "init", "func_src_before": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = document.location.href;\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "func_src_after": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = escapePath(document.location.href);\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "line_changes": {"deleted": [{"line_no": 172, "char_start": 10731, "char_end": 10761, "line": "\t\th = document.location.href;\n"}], "added": [{"line_no": 172, "char_start": 10731, "char_end": 10773, "line": "\t\th = escapePath(document.location.href);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 10737, "char_end": 10748, "chars": "escapePath("}, {"char_start": 10770, "char_end": 10771, "chars": ")"}]}, "commit_link": "github.com/jaffa-projects/jaffa-framework/commit/f9241bf1b4e4f06fc9778b2314664b58f4fbe309", "file_name": "tiny_mce_src.js", "vul_type": "cwe-079", "commit_msg": "Coverity CWE79 (DOM XSS) new TinyMCE vulnerability fix", "description": "Write a JavaScript function to initialize a WYSIWYG editor with given settings."}
{"func_name": "timer_wheel_new", "func_src_before": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "func_src_after": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    if (IS_NULL_PTR(tw)) {\n        return NULL;\n    }\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    if (IS_NULL_PTR(tw->slots)) {\n        free(tw);\n        return NULL;\n    }\n\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "commit_link": "github.com/atomvm/AtomVM/commit/6a6d36015938740c4900df6f668c8909941bc0dc", "file_name": "src/libAtomVM/timer_wheel.c", "vul_type": "cwe-476", "description": "Write a function in C to create and initialize a new timer wheel with a specified number of slots, handling memory allocation failures."}
{"func_name": "PlayerGeneric::~PlayerGeneric", "func_src_before": "PlayerGeneric::~PlayerGeneric()\n{\n\tif (mixer)\n\t\tdelete mixer;\n\n\tif (player)\n\t{\n\t\tif (mixer->isActive() && !mixer->isDeviceRemoved(player))\n\t\t\tmixer->removeDevice(player);\n\t\tdelete player;\n\t}\n\n\tdelete[] audioDriverName;\n\t\n\tdelete listener;\n}", "func_src_after": "PlayerGeneric::~PlayerGeneric()\n{\n\n\tif (player)\n\t{\n\t\tif (mixer && mixer->isActive() && !mixer->isDeviceRemoved(player))\n\t\t\tmixer->removeDevice(player);\n\t\tdelete player;\n\t}\n\t\n\tif (mixer)\n\t\tdelete mixer;\n\n\tdelete[] audioDriverName;\n\t\n\tdelete listener;\n}", "commit_link": "github.com/milkytracker/MilkyTracker/commit/7afd55c42ad80d01a339197a2d8b5461d214edaf", "file_name": "src/milkyplay/PlayerGeneric.cpp", "vul_type": "cwe-416", "description": "Write a C++ destructor for a class named `PlayerGeneric` that cleans up memory by deleting member pointers, ensuring that a `mixer` object is properly deactivated and devices are removed before deletion."}
{"func_name": "save_page_edit", "func_src_before": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "func_src_after": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "In Python, write a Flask endpoint to save edited content for a given page, creating the page in the database if it doesn't exist."}
{"func_name": "ZipMisc::unzip", "func_src_before": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [], "added": [{"line_no": 6, "char_start": 321, "char_end": 407, "line": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n"}, {"line_no": 7, "char_start": 407, "char_end": 457, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 8, "char_start": 457, "char_end": 463, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 321, "char_end": 463, "chars": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n"}]}, "commit_link": "github.com/diffplug/goomph/commit/643474930339e5567745ba0695f2a8decf627a8c", "file_name": "ZipMisc.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "fe2083196f0d0a75885aea402baaa972254173d5", "description": "Write a Java function to extract the contents of a ZIP file to a specified directory."}
{"func_name": "load", "func_src_before": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "func_src_after": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return ::File.open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 65, "char_end": 125, "line": "        return open(input, 'r') { |io| load(io, *options) }\n"}], "added": [{"line_no": 3, "char_start": 65, "char_end": 132, "line": "        return ::File.open(input, 'r') { |io| load(io, *options) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 80, "char_end": 87, "chars": "::File."}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/aae0b13514a1a0caf93b1cf233733c50e679069a", "file_name": "cookie_jar.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in CookieJar\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `load` that takes an input and optional parameters to process file loading with format options."}
{"func_name": "quiz", "func_src_before": "@app.route('/quiz')\ndef quiz():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = '%s' order by random() limit 1;\" % varga)\n            rows = cur.fetchall();\n\n            artha = rows[0][\"artha\"];\n            cur.execute(\"select pada from pada where varga = '%s' and artha = '%s' order by id\" % (varga, artha));\n            paryaya = cur.fetchall();\n\n            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)\n    finally:\n        con.close()", "func_src_after": "@app.route('/quiz')\ndef quiz():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = ? order by random() limit 1;\", [varga])\n            rows = cur.fetchall();\n\n            artha = rows[0][\"artha\"];\n            cur.execute(\"select pada from pada where varga = ? and artha = ? order by id\", [varga, artha]);\n            paryaya = cur.fetchall();\n\n            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)\n    finally:\n        con.close()", "commit_link": "github.com/aupasana/amara-quiz/commit/6ceb5dc8ec38b4a3f1399e578ab970f7e3354922", "file_name": "docker/app.py", "vul_type": "cwe-089", "description": "Create a Python Flask endpoint that fetches a random quiz question and its multiple choices from a database based on a query parameter."}
{"func_name": "testIncompleteRedirectWorks", "func_src_before": "  def testIncompleteRedirectWorks(self):\n    output_path = tempfile.mktemp()\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "func_src_after": "  def testIncompleteRedirectWorks(self):\n    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 77, "line": "    output_path = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 102, "line": "    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 44, "char_end": 47, "chars": " _,"}, {"char_start": 73, "char_end": 74, "chars": "s"}, {"char_start": 80, "char_end": 101, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/599208b439e349f88c809e9d1f268c8c77718259", "file_name": "readline_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359224\nChange-Id: I7bfc1df9cf931f45ec85d4878874ef41b9c55474", "description": "Write a Python unit test that checks if a mocked command-line interface correctly redirects output to a temporary file."}
{"func_name": "output", "func_src_before": "    def output\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "func_src_after": "    def output\n      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{intercom_settings_json};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 86, "char_end": 164, "line": "  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n"}], "added": [{"line_no": 2, "char_start": 15, "char_end": 112, "line": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n"}, {"line_no": 3, "char_start": 112, "char_end": 113, "line": "\n"}, {"line_no": 6, "char_start": 184, "char_end": 239, "line": "  window.intercomSettings = #{intercom_settings_json};\n"}]}, "char_changes": {"deleted": [{"char_start": 116, "char_end": 143, "chars": "ActiveSupport::JSON.encode("}, {"char_start": 160, "char_end": 161, "chars": ")"}], "added": [{"char_start": 15, "char_end": 113, "chars": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n"}, {"char_start": 231, "char_end": 236, "chars": "_json"}]}, "commit_link": "github.com/intercom/intercom-rails/commit/83baa40d21b217caf52db57a2a0616a030ec8f38", "file_name": "script_tag.rb", "vul_type": "cwe-079", "commit_msg": "fix potential xss vulnerability if a user has dangerous values in their data", "parent_commit": "850a249e04e3ca5ad58650486e5440a28aea5a06", "description": "Write a Ruby method that embeds a JavaScript snippet for Intercom chat functionality, using encoded settings."}
{"func_name": "calibrate_cpuinfo", "func_src_before": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[10];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "func_src_after": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[11];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[10];\n"}], "added": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[11];\n"}]}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 94, "chars": "0"}], "added": [{"char_start": 93, "char_end": 94, "chars": "1"}]}, "commit_link": "github.com/01org/numatop/commit/0a01f1a06227b2c3fa599ac048a5bf3dda3cefdd", "file_name": "os_util.c", "vul_type": "cwe-787", "commit_msg": "calibrate_cpuinfo: ensure we do not overflow buffer unit\n\ncppcheck found a potential buffer overflow:\n\n[common/os/os_util.c:212]: (error) Width 10 given in format string\n  (no. 2) is larger than destination buffer 'unit[10]', use %9s to\n  prevent overflowing it.\n\nIncrease unit array to 11 bytes.\n\nSigned-off-by: Colin Ian King <colin.king@canonical.com>", "parent_commit": "f1a317da5a98219932d43c9848438983ff8cd55d", "description": "Write a C function named `calibrate_cpuinfo` that reads the CPU frequency from a file and calculates the number of nanoseconds per clock cycle and the clock frequency in Hz."}
{"func_name": "testDebugDumpDir_nonexistentDumpRoot", "func_src_before": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")", "func_src_after": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")", "line_changes": {"deleted": [{"line_no": 3, "char_start": 110, "char_end": 167, "line": "      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")\n"}], "added": [{"line_no": 3, "char_start": 110, "char_end": 168, "line": "      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 151, "char_end": 152, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/9b1fe8f31ee1788208d8d6b7385382e436c5e1d7", "file_name": "debug_data_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp` to create directories.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do: just a name change\n\nPiperOrigin-RevId: 420370858\nChange-Id: I44a0849d161132eacd4f3881fdb615e09c0f02a2", "description": "Write a Python unit test that expects an IOError when trying to create a DebugDumpDir with a non-existent directory."}
{"func_name": "smprintf", "func_src_before": "smprintf(const char *fmt, ...)\n{\n\tva_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0';\n\tif (asprintf(&ret, \"%s\", tmp) < 0)\n\t\treturn NULL;\n\n\tva_end(fmtargs);\n\treturn ret;\n}", "func_src_after": "smprintf(const char *fmt, ...)\n{\n\t/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt);\n\tif (vasprintf(&ret, fmt, ap) < 0)\n\t\treturn NULL;\n\n\tva_end(ap);\n\treturn ret;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 33, "char_end": 51, "line": "\tva_list fmtargs;\n"}, {"line_no": 4, "char_start": 51, "char_end": 67, "line": "\tchar tmp[120];\n"}, {"line_no": 7, "char_start": 87, "char_end": 112, "line": "\tva_start(fmtargs, fmt);\n"}, {"line_no": 8, "char_start": 112, "char_end": 157, "line": "\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n"}, {"line_no": 9, "char_start": 157, "char_end": 183, "line": "\ttmp[sizeof(tmp)] = '\\0';\n"}, {"line_no": 10, "char_start": 183, "char_end": 219, "line": "\tif (asprintf(&ret, \"%s\", tmp) < 0)\n"}, {"line_no": 13, "char_start": 235, "char_end": 253, "line": "\tva_end(fmtargs);\n"}], "added": [{"line_no": 3, "char_start": 33, "char_end": 66, "line": "\t/* FIXME: This code should have\n"}, {"line_no": 4, "char_start": 66, "char_end": 101, "line": "\tbound checks, it is vulnerable to\n"}, {"line_no": 5, "char_start": 101, "char_end": 122, "line": "\tbuffer overflows */\n"}, {"line_no": 6, "char_start": 122, "char_end": 135, "line": "\tva_list ap;\n"}, {"line_no": 9, "char_start": 155, "char_end": 175, "line": "\tva_start(ap, fmt);\n"}, {"line_no": 10, "char_start": 175, "char_end": 210, "line": "\tif (vasprintf(&ret, fmt, ap) < 0)\n"}, {"line_no": 13, "char_start": 226, "char_end": 239, "line": "\tva_end(ap);\n"}]}, "char_changes": {"deleted": [{"char_start": 34, "char_end": 181, "chars": "va_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0'"}, {"char_start": 203, "char_end": 211, "chars": "\"%s\", tm"}, {"char_start": 243, "char_end": 250, "chars": "fmtargs"}], "added": [{"char_start": 34, "char_end": 173, "chars": "/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt)"}, {"char_start": 180, "char_end": 181, "chars": "v"}, {"char_start": 196, "char_end": 202, "chars": "fmt, a"}, {"char_start": 234, "char_end": 236, "chars": "ap"}]}, "commit_link": "github.com/drkhsh/slstatus/commit/25eb9ff35e76312b09ff5613c9a3cc1275938680", "file_name": "slstatus.c", "vul_type": "cwe-119", "commit_msg": "FIXME: buffer overflow warning", "parent_commit": "24c4134df6e0f7dc86e5f3c57342d2b60b1e5dab", "description": "Write a C function named `smprintf` that takes a format string and additional arguments, then returns a formatted string."}
{"func_name": "customization_disabled?", "func_src_before": "  def customization_disabled?\n    safe_mode = params[\"safe_mode\"]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n  end", "func_src_after": "  def customization_disabled?\n    safe_mode = params[SAFE_MODE]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 30, "char_end": 66, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 66, "char_end": 152, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n"}], "added": [{"line_no": 2, "char_start": 30, "char_end": 64, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 64, "char_end": 148, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n"}]}, "char_changes": {"deleted": [{"char_start": 53, "char_end": 64, "chars": "\"safe_mode\""}, {"char_start": 138, "char_end": 149, "chars": "\"no_custom\""}], "added": [{"char_start": 53, "char_end": 62, "chars": "SAFE_MODE"}, {"char_start": 136, "char_end": 145, "chars": "NO_CUSTOM"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby function named `customization_disabled?` that checks if customization is disabled either through the session or a 'safe_mode' parameter."}
{"func_name": "Get", "func_src_before": "func Get(key string, lat string, long string, time string) *Forecast {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\treturn &f\n}", "func_src_after": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &f, nil\n}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 71, "line": "func Get(key string, lat string, long string, time string) *Forecast {\n"}, {"line_no": 6, "char_start": 135, "char_end": 191, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n"}, {"line_no": 8, "char_start": 201, "char_end": 270, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n"}, {"line_no": 12, "char_start": 298, "char_end": 392, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n"}, {"line_no": 18, "char_start": 482, "char_end": 499, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 26, "char_start": 633, "char_end": 650, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 29, "char_start": 654, "char_end": 665, "line": "\treturn &f\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 80, "line": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n"}, {"line_no": 6, "char_start": 144, "char_end": 200, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n"}, {"line_no": 8, "char_start": 210, "char_end": 279, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n"}, {"line_no": 12, "char_start": 307, "char_end": 403, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n"}, {"line_no": 18, "char_start": 493, "char_end": 511, "line": "\t\treturn nil, err\n"}, {"line_no": 26, "char_start": 645, "char_end": 663, "line": "\t\treturn nil, err\n"}, {"line_no": 29, "char_start": 667, "char_end": 683, "line": "\treturn &f, nil\n"}]}, "char_changes": {"deleted": [{"char_start": 187, "char_end": 189, "chars": "ca"}, {"char_start": 266, "char_end": 268, "chars": "ca"}, {"char_start": 349, "char_end": 352, "chars": "tru"}, {"char_start": 359, "char_end": 391, "chars": "susceptible to man-in-the-middle"}, {"char_start": 484, "char_end": 494, "chars": "log.Fatal("}, {"char_start": 497, "char_end": 498, "chars": ")"}, {"char_start": 635, "char_end": 645, "chars": "log.Fatal("}, {"char_start": 648, "char_end": 649, "chars": ")"}], "added": [{"char_start": 59, "char_end": 60, "chars": "("}, {"char_start": 69, "char_end": 77, "chars": ", error)"}, {"char_start": 196, "char_end": 198, "chars": "si"}, {"char_start": 275, "char_end": 277, "chars": "si"}, {"char_start": 358, "char_end": 362, "chars": "fals"}, {"char_start": 369, "char_end": 402, "chars": "does not seem required any longer"}, {"char_start": 495, "char_end": 507, "chars": "return nil, "}, {"char_start": 647, "char_end": 659, "chars": "return nil, "}, {"char_start": 677, "char_end": 682, "chars": ", nil"}]}, "commit_link": "github.com/mlbright/darksky/commit/f398d4d31806800c59e17dd8514c8da751f9bf72", "file_name": "forecast.go", "vul_type": "cwe-295", "commit_msg": "Get now returns a (*Forecast, error) pair\n\nThis seems like good practice. As it was, the code would panic in case\nof - for example - network problems, which is not the nicest thing to do\nin a library.\n\nAdditionally, InsecureSkipVerify has been set to false.", "parent_commit": "99e2fc3d4132a0bd9cf3a94ada0efbe4c18deb08", "description": "Write a Go function named `Get` that retrieves weather forecast data using coordinates and time, and handles errors."}
{"func_name": "_remove_volume_from_volume_set", "func_src_before": "    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run('removevvset -f %s %s' % (vvs_name, volume_name), None)", "func_src_after": "    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run(['removevvset', '-f', vvs_name, volume_name])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to execute a command-line instruction that removes a volume from a volume set."}
{"func_name": "test_get_iscsi_ip", "func_src_before": "    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = 'showport'\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = 'showport -iscsi'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = 'showport -iscsiname'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = 'showvlun -a -host fakehost'\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = 'showvlun -a -showcols Port'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')", "func_src_after": "    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands to retrieve iSCSI IP information and asserts the expected IP address."}
{"func_name": "core_anal_bytes", "func_src_before": "static void core_anal_bytes(RCore *core, const ut8 *buf, int len, int nops, int fmt) {\n\tint stacksize = r_config_get_i (core->config, \"esil.stack.depth\");\n\tbool iotrap = r_config_get_i (core->config, \"esil.iotrap\");\n\tbool romem = r_config_get_i (core->config, \"esil.romem\");\n\tbool stats = r_config_get_i (core->config, \"esil.stats\");\n\tbool be = core->print->big_endian;\n\tbool use_color = core->print->flags & R_PRINT_FLAGS_COLOR;\n\tcore->parser->relsub = r_config_get_i (core->config, \"asm.relsub\");\n\tint ret, i, j, idx, size;\n\tconst char *color = \"\";\n\tconst char *esilstr;\n\tconst char *opexstr;\n\tRAnalHint *hint;\n\tRAnalEsil *esil = NULL;\n\tRAsmOp asmop;\n\tRAnalOp op = {0};\n\tut64 addr;\n\tbool isFirst = true;\n\tunsigned int addrsize = r_config_get_i (core->config, \"esil.addr.size\");\n\tint totalsize = 0;\n\n\t// Variables required for setting up ESIL to REIL conversion\n\tif (use_color) {\n\t\tcolor = core->cons->pal.label;\n\t}\n\tswitch (fmt) {\n\tcase 'j':\n\t\tr_cons_printf (\"[\");\n\t\tbreak;\n\tcase 'r':\n\t\t// Setup for ESIL to REIL conversion\n\t\tesil = r_anal_esil_new (stacksize, iotrap, addrsize);\n\t\tif (!esil) {\n\t\t\treturn;\n\t\t}\n\t\tr_anal_esil_to_reil_setup (esil, core->anal, romem, stats);\n\t\tr_anal_esil_set_pc (esil, core->offset);\n\t\tbreak;\n\t}\n\tfor (i = idx = ret = 0; idx < len && (!nops || (nops && i < nops)); i++, idx += ret) {\n\t\taddr = core->offset + idx;\n\t\t// TODO: use more anal hints\n\t\thint = r_anal_hint_get (core->anal, addr);\n\t\tr_asm_set_pc (core->assembler, addr);\n\t\t(void)r_asm_disassemble (core->assembler, &asmop, buf + idx, len - idx);\n\t\tret = r_anal_op (core->anal, &op, core->offset + idx, buf + idx, len - idx, R_ANAL_OP_MASK_ESIL);\n\t\tesilstr = R_STRBUF_SAFEGET (&op.esil);\n\t\topexstr = R_STRBUF_SAFEGET (&op.opex);\n\t\tchar *mnem = strdup (r_asm_op_get_asm (&asmop));\n\t\tchar *sp = strchr (mnem, ' ');\n\t\tif (sp) {\n\t\t\t*sp = 0;\n\t\t\tif (op.prefix) {\n\t\t\t\tchar *arg = strdup (sp + 1);\n\t\t\t\tchar *sp = strchr (arg, ' ');\n\t\t\t\tif (sp) {\n\t\t\t\t\t*sp = 0;\n\t\t\t\t}\n\t\t\t\tfree (mnem);\n\t\t\t\tmnem = arg;\n\t\t\t}\n\t\t}\n\t\tif (ret < 1 && fmt != 'd') {\n\t\t\teprintf (\"Oops at 0x%08\" PFMT64x \" (\", core->offset + idx);\n\t\t\tfor (i = idx, j = 0; i < core->blocksize && j < 3; ++i, ++j) {\n\t\t\t\teprintf (\"%02x \", buf[i]);\n\t\t\t}\n\t\t\teprintf (\"...)\\n\");\n\t\t\tfree (mnem);\n\t\t\tbreak;\n\t\t}\n\t\tsize = (hint && hint->size)? hint->size: op.size;\n\t\tif (fmt == 'd') {\n\t\t\tchar *opname = strdup (r_asm_op_get_asm (&asmop));\n\t\t\tif (opname) {\n\t\t\t\tr_str_split (opname, ' ');\n\t\t\t\tchar *d = r_asm_describe (core->assembler, opname);\n\t\t\t\tif (d && *d) {\n\t\t\t\t\tr_cons_printf (\"%s: %s\\n\", opname, d);\n\t\t\t\t\tfree (d);\n\t\t\t\t} else {\n\t\t\t\t\teprintf (\"Unknown opcode\\n\");\n\t\t\t\t}\n\t\t\t\tfree (opname);\n\t\t\t}\n\t\t} else if (fmt == 'e') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \" %s\\n\", color, core->offset + idx, esilstr);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \" %s\\n\", core->offset + idx, esilstr);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (fmt == 's') {\n\t\t\ttotalsize += op.size;\n\t\t} else if (fmt == 'r') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \"\\n\", color, core->offset + idx);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\t\t}\n\t\t\t\tr_anal_esil_parse (esil, esilstr);\n\t\t\t\tr_anal_esil_dumpstack (esil);\n\t\t\t\tr_anal_esil_stack_free (esil);\n\t\t\t}\n\t\t} else if (fmt == 'j') {\n\t\t\tif (isFirst) {\n\t\t\t\tisFirst = false;\n\t\t\t} else {\n\t\t\t\tr_cons_print (\",\");\n\t\t\t}\n\t\t\tr_cons_printf (\"{\\\"opcode\\\":\\\"%s\\\",\", r_asm_op_get_asm (&asmop));\n\t\t\t{\n\t\t\t\tchar strsub[128] = { 0 };\n\t\t\t\t// pc+33\n\t\t\t\tr_parse_varsub (core->parser, NULL,\n\t\t\t\t\tcore->offset + idx,\n\t\t\t\t\tasmop.size, r_asm_op_get_asm (&asmop),\n\t\t\t\t\tstrsub, sizeof (strsub));\n\t\t\t\t{\n\t\t\t\t\tut64 killme = UT64_MAX;\n\t\t\t\t\tif (r_io_read_i (core->io, op.ptr, &killme, op.refptr, be)) {\n\t\t\t\t\t\tcore->parser->relsub_addr = killme;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// 0x33->sym.xx\n\t\t\t\tchar *p = strdup (strsub);\n\t\t\t\tif (p) {\n\t\t\t\t\tr_parse_filter (core->parser, addr, core->flags, p,\n\t\t\t\t\t\t\tstrsub, sizeof (strsub), be);\n\t\t\t\t\tfree (p);\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"disasm\\\":\\\"%s\\\",\", strsub);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"mnemonic\\\":\\\"%s\\\",\", mnem);\n\t\t\tif (hint && hint->opcode) {\n\t\t\t\tr_cons_printf (\"\\\"ophint\\\":\\\"%s\\\",\", hint->opcode);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"sign\\\":%s,\", r_str_bool (op.sign));\n\t\t\tr_cons_printf (\"\\\"prefix\\\":%\" PFMT64u \",\", op.prefix);\n\t\t\tr_cons_printf (\"\\\"id\\\":%d,\", op.id);\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tr_cons_printf (\"\\\"opex\\\":%s,\", opexstr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"addr\\\":%\" PFMT64u \",\", core->offset + idx);\n\t\t\tr_cons_printf (\"\\\"bytes\\\":\\\"\");\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\",\");\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"val\\\": %\" PFMT64u \",\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"ptr\\\": %\" PFMT64u \",\", op.ptr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"size\\\": %d,\", size);\n\t\t\tr_cons_printf (\"\\\"type\\\": \\\"%s\\\",\",\n\t\t\t\tr_anal_optype_to_string (op.type));\n\t\t\tif (op.reg) {\n\t\t\t\tr_cons_printf (\"\\\"reg\\\": \\\"%s\\\",\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tr_cons_printf (\"\\\"ireg\\\": \\\"%s\\\",\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tr_cons_printf (\"\\\"scale\\\":%d,\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"jump\\\":%\" PFMT64u \",\", op.jump);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tr_cons_printf (\"\\\"refptr\\\":%d,\", op.refptr);\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"fail\\\":%\" PFMT64u \",\", op.fail);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"cycles\\\":%d,\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tr_cons_printf (\"\\\"failcycles\\\":%d,\", op.failcycles);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"delay\\\":%d,\", op.delay);\n\t\t\t{\n\t\t\t\tconst char *p = r_anal_stackop_tostring (op.stackop);\n\t\t\t\tif (p && *p && strcmp (p, \"null\"))\n\t\t\t\t\tr_cons_printf (\"\\\"stack\\\":\\\"%s\\\",\", p);\n\t\t\t}\n\t\t\tif (op.stackptr) {\n\t\t\t\tr_cons_printf (\"\\\"stackptr\\\":%d,\", op.stackptr);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)\n\t\t\t\t\t? r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tr_cons_printf (\"\\\"cond\\\":\\\"%s\\\",\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"family\\\":\\\"%s\\\"}\", r_anal_op_family_to_string (op.family));\n\t\t} else {\n#define printline(k, fmt, arg)\\\n\t{ \\\n\t\tif (use_color)\\\n\t\t\tr_cons_printf (\"%s%s: \" Color_RESET, color, k);\\\n\t\telse\\\n\t\t\tr_cons_printf (\"%s: \", k);\\\n\t\tif (fmt) r_cons_printf (fmt, arg);\\\n\t}\n\t\t\tprintline (\"address\", \"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\tprintline (\"opcode\", \"%s\\n\", r_asm_op_get_asm (&asmop));\n\t\t\tprintline (\"mnemonic\", \"%s\\n\", mnem);\n\t\t\tif (hint) {\n\t\t\t\tif (hint->opcode) {\n\t\t\t\t\tprintline (\"ophint\", \"%s\\n\", hint->opcode);\n\t\t\t\t}\n#if 0\n\t\t\t\t// addr should not override core->offset + idx.. its silly\n\t\t\t\tif (hint->addr != UT64_MAX) {\n\t\t\t\t\tprintline (\"addr\", \"0x%08\" PFMT64x \"\\n\", (hint->addr + idx));\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tprintline (\"prefix\", \"%\" PFMT64u \"\\n\", op.prefix);\n\t\t\tprintline (\"id\", \"%d\\n\", op.id);\n#if 0\n// no opex here to avoid lot of tests broken..and having json in here is not much useful imho\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tprintline (\"opex\", \"%s\\n\", opexstr);\n\t\t\t}\n#endif\n\t\t\tprintline (\"bytes\", NULL, 0);\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_newline ();\n\t\t\tif (op.val != UT64_MAX)\n\t\t\t\tprintline (\"val\", \"0x%08\" PFMT64x \"\\n\", op.val);\n\t\t\tif (op.ptr != UT64_MAX)\n\t\t\t\tprintline (\"ptr\", \"0x%08\" PFMT64x \"\\n\", op.ptr);\n\t\t\tif (op.refptr != -1)\n\t\t\t\tprintline (\"refptr\", \"%d\\n\", op.refptr);\n\t\t\tprintline (\"size\", \"%d\\n\", size);\n\t\t\tprintline (\"sign\", \"%s\\n\", r_str_bool (op.sign));\n\t\t\tprintline (\"type\", \"%s\\n\", r_anal_optype_to_string (op.type));\n\t\t\tprintline (\"cycles\", \"%d\\n\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tprintline (\"failcycles\", \"%d\\n\", op.failcycles);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *t2 = r_anal_optype_to_string (op.type2);\n\t\t\t\tif (t2 && strcmp (t2, \"null\")) {\n\t\t\t\t\tprintline (\"type2\", \"%s\\n\", t2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (op.reg) {\n\t\t\t\tprintline (\"reg\", \"%s\\n\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tprintline (\"ireg\", \"%s\\n\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tprintline (\"scale\", \"%d\\n\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tprintline (\"jump\", \"0x%08\" PFMT64x \"\\n\", op.jump);\n\t\t\t}\n\t\t\tif (op.direction != 0) {\n\t\t\t\tconst char * dir = op.direction == 1 ? \"read\"\n\t\t\t\t\t: op.direction == 2 ? \"write\"\n\t\t\t\t\t: op.direction == 4 ? \"exec\"\n\t\t\t\t\t: op.direction == 8 ? \"ref\": \"none\";\n\t\t\t\tprintline (\"direction\", \"%s\\n\", dir);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tprintline (\"fail\", \"0x%08\" PFMT64x \"\\n\", op.fail);\n\t\t\t}\n\t\t\tif (op.delay) {\n\t\t\t\tprintline (\"delay\", \"%d\\n\", op.delay);\n\t\t\t}\n\t\t\tprintline (\"stack\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)?  r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tprintline (\"cond\", \"%s\\n\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tprintline (\"family\", \"%s\\n\", r_anal_op_family_to_string (op.family));\n\t\t\tprintline (\"stackop\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\tif (op.stackptr) {\n\t\t\t\tprintline (\"stackptr\", \"%\"PFMT64u\"\\n\", op.stackptr);\n\t\t\t}\n\t\t}\n\t\t//r_cons_printf (\"false: 0x%08\"PFMT64x\"\\n\", core->offset+idx);\n\t\t//free (hint);\n\t\tfree (mnem);\n\t\tr_anal_hint_free (hint);\n\t\tr_anal_op_fini (&op);\n\t}\n\tr_anal_op_fini (&op);\n\tif (fmt == 'j') {\n\t\tr_cons_printf (\"]\");\n\t\tr_cons_newline ();\n\t} else if (fmt == 's') {\n\t\tr_cons_printf (\"%d\\n\", totalsize);\n\t}\n\tr_anal_esil_free (esil);\n}", "func_src_after": "static void core_anal_bytes(RCore *core, const ut8 *buf, int len, int nops, int fmt) {\n\tint stacksize = r_config_get_i (core->config, \"esil.stack.depth\");\n\tbool iotrap = r_config_get_i (core->config, \"esil.iotrap\");\n\tbool romem = r_config_get_i (core->config, \"esil.romem\");\n\tbool stats = r_config_get_i (core->config, \"esil.stats\");\n\tbool be = core->print->big_endian;\n\tbool use_color = core->print->flags & R_PRINT_FLAGS_COLOR;\n\tcore->parser->relsub = r_config_get_i (core->config, \"asm.relsub\");\n\tint ret, i, j, idx, size;\n\tconst char *color = \"\";\n\tconst char *esilstr;\n\tconst char *opexstr;\n\tRAnalHint *hint;\n\tRAnalEsil *esil = NULL;\n\tRAsmOp asmop;\n\tRAnalOp op = {0};\n\tut64 addr;\n\tbool isFirst = true;\n\tunsigned int addrsize = r_config_get_i (core->config, \"esil.addr.size\");\n\tint totalsize = 0;\n\n\t// Variables required for setting up ESIL to REIL conversion\n\tif (use_color) {\n\t\tcolor = core->cons->pal.label;\n\t}\n\tswitch (fmt) {\n\tcase 'j':\n\t\tr_cons_printf (\"[\");\n\t\tbreak;\n\tcase 'r':\n\t\t// Setup for ESIL to REIL conversion\n\t\tesil = r_anal_esil_new (stacksize, iotrap, addrsize);\n\t\tif (!esil) {\n\t\t\treturn;\n\t\t}\n\t\tr_anal_esil_to_reil_setup (esil, core->anal, romem, stats);\n\t\tr_anal_esil_set_pc (esil, core->offset);\n\t\tbreak;\n\t}\n\tfor (i = idx = ret = 0; idx < len && (!nops || (nops && i < nops)); i++, idx += ret) {\n\t\taddr = core->offset + idx;\n\t\t// TODO: use more anal hints\n\t\thint = r_anal_hint_get (core->anal, addr);\n\t\tr_asm_set_pc (core->assembler, addr);\n\t\t(void)r_asm_disassemble (core->assembler, &asmop, buf + idx, len - idx);\n\t\tret = r_anal_op (core->anal, &op, core->offset + idx, buf + idx, len - idx, R_ANAL_OP_MASK_ESIL);\n\t\tesilstr = R_STRBUF_SAFEGET (&op.esil);\n\t\topexstr = R_STRBUF_SAFEGET (&op.opex);\n\t\tchar *mnem = strdup (r_asm_op_get_asm (&asmop));\n\t\tchar *sp = strchr (mnem, ' ');\n\t\tif (sp) {\n\t\t\t*sp = 0;\n\t\t\tif (op.prefix) {\n\t\t\t\tchar *arg = strdup (sp + 1);\n\t\t\t\tchar *sp = strchr (arg, ' ');\n\t\t\t\tif (sp) {\n\t\t\t\t\t*sp = 0;\n\t\t\t\t}\n\t\t\t\tfree (mnem);\n\t\t\t\tmnem = arg;\n\t\t\t}\n\t\t}\n\t\tif (ret < 1 && fmt != 'd') {\n\t\t\teprintf (\"Oops at 0x%08\" PFMT64x \" (\", core->offset + idx);\n\t\t\tfor (i = idx, j = 0; i < core->blocksize && j < 3; ++i, ++j) {\n\t\t\t\teprintf (\"%02x \", buf[i]);\n\t\t\t}\n\t\t\teprintf (\"...)\\n\");\n\t\t\tfree (mnem);\n\t\t\tbreak;\n\t\t}\n\t\tsize = (hint && hint->size)? hint->size: op.size;\n\t\tif (fmt == 'd') {\n\t\t\tchar *opname = strdup (r_asm_op_get_asm (&asmop));\n\t\t\tif (opname) {\n\t\t\t\tr_str_split (opname, ' ');\n\t\t\t\tchar *d = r_asm_describe (core->assembler, opname);\n\t\t\t\tif (d && *d) {\n\t\t\t\t\tr_cons_printf (\"%s: %s\\n\", opname, d);\n\t\t\t\t\tfree (d);\n\t\t\t\t} else {\n\t\t\t\t\teprintf (\"Unknown opcode\\n\");\n\t\t\t\t}\n\t\t\t\tfree (opname);\n\t\t\t}\n\t\t} else if (fmt == 'e') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \" %s\\n\", color, core->offset + idx, esilstr);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \" %s\\n\", core->offset + idx, esilstr);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (fmt == 's') {\n\t\t\ttotalsize += op.size;\n\t\t} else if (fmt == 'r') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \"\\n\", color, core->offset + idx);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\t\t}\n\t\t\t\tr_anal_esil_parse (esil, esilstr);\n\t\t\t\tr_anal_esil_dumpstack (esil);\n\t\t\t\tr_anal_esil_stack_free (esil);\n\t\t\t}\n\t\t} else if (fmt == 'j') {\n\t\t\tif (isFirst) {\n\t\t\t\tisFirst = false;\n\t\t\t} else {\n\t\t\t\tr_cons_print (\",\");\n\t\t\t}\n\t\t\tr_cons_printf (\"{\\\"opcode\\\":\\\"%s\\\",\", r_asm_op_get_asm (&asmop));\n\t\t\t{\n\t\t\t\tchar strsub[128] = { 0 };\n\t\t\t\t// pc+33\n\t\t\t\tr_parse_varsub (core->parser, NULL,\n\t\t\t\t\tcore->offset + idx,\n\t\t\t\t\tasmop.size, r_asm_op_get_asm (&asmop),\n\t\t\t\t\tstrsub, sizeof (strsub));\n\t\t\t\t{\n\t\t\t\t\tut64 killme = UT64_MAX;\n\t\t\t\t\tif (r_io_read_i (core->io, op.ptr, &killme, op.refptr, be)) {\n\t\t\t\t\t\tcore->parser->relsub_addr = killme;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// 0x33->sym.xx\n\t\t\t\tchar *p = strdup (strsub);\n\t\t\t\tif (p) {\n\t\t\t\t\tr_parse_filter (core->parser, addr, core->flags, p,\n\t\t\t\t\t\t\tstrsub, sizeof (strsub), be);\n\t\t\t\t\tfree (p);\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"disasm\\\":\\\"%s\\\",\", strsub);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"mnemonic\\\":\\\"%s\\\",\", mnem);\n\t\t\tif (hint && hint->opcode) {\n\t\t\t\tr_cons_printf (\"\\\"ophint\\\":\\\"%s\\\",\", hint->opcode);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"sign\\\":%s,\", r_str_bool (op.sign));\n\t\t\tr_cons_printf (\"\\\"prefix\\\":%\" PFMT64u \",\", op.prefix);\n\t\t\tr_cons_printf (\"\\\"id\\\":%d,\", op.id);\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tr_cons_printf (\"\\\"opex\\\":%s,\", opexstr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"addr\\\":%\" PFMT64u \",\", core->offset + idx);\n\t\t\tr_cons_printf (\"\\\"bytes\\\":\\\"\");\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\",\");\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"val\\\": %\" PFMT64u \",\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"ptr\\\": %\" PFMT64u \",\", op.ptr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"size\\\": %d,\", size);\n\t\t\tr_cons_printf (\"\\\"type\\\": \\\"%s\\\",\",\n\t\t\t\tr_anal_optype_to_string (op.type));\n\t\t\tif (op.reg) {\n\t\t\t\tr_cons_printf (\"\\\"reg\\\": \\\"%s\\\",\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tr_cons_printf (\"\\\"ireg\\\": \\\"%s\\\",\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tr_cons_printf (\"\\\"scale\\\":%d,\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"jump\\\":%\" PFMT64u \",\", op.jump);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tr_cons_printf (\"\\\"refptr\\\":%d,\", op.refptr);\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"fail\\\":%\" PFMT64u \",\", op.fail);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"cycles\\\":%d,\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tr_cons_printf (\"\\\"failcycles\\\":%d,\", op.failcycles);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"delay\\\":%d,\", op.delay);\n\t\t\t{\n\t\t\t\tconst char *p = r_anal_stackop_tostring (op.stackop);\n\t\t\t\tif (p && *p && strcmp (p, \"null\"))\n\t\t\t\t\tr_cons_printf (\"\\\"stack\\\":\\\"%s\\\",\", p);\n\t\t\t}\n\t\t\tif (op.stackptr) {\n\t\t\t\tr_cons_printf (\"\\\"stackptr\\\":%d,\", op.stackptr);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)\n\t\t\t\t\t? r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tr_cons_printf (\"\\\"cond\\\":\\\"%s\\\",\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"family\\\":\\\"%s\\\"}\", r_anal_op_family_to_string (op.family));\n\t\t} else {\n#define printline(k, fmt, arg)\\\n\t{ \\\n\t\tif (use_color)\\\n\t\t\tr_cons_printf (\"%s%s: \" Color_RESET, color, k);\\\n\t\telse\\\n\t\t\tr_cons_printf (\"%s: \", k);\\\n\t\tif (fmt) r_cons_printf (fmt, arg);\\\n\t}\n\t\t\tprintline (\"address\", \"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\tprintline (\"opcode\", \"%s\\n\", r_asm_op_get_asm (&asmop));\n\t\t\tprintline (\"mnemonic\", \"%s\\n\", mnem);\n\t\t\tif (hint) {\n\t\t\t\tif (hint->opcode) {\n\t\t\t\t\tprintline (\"ophint\", \"%s\\n\", hint->opcode);\n\t\t\t\t}\n#if 0\n\t\t\t\t// addr should not override core->offset + idx.. its silly\n\t\t\t\tif (hint->addr != UT64_MAX) {\n\t\t\t\t\tprintline (\"addr\", \"0x%08\" PFMT64x \"\\n\", (hint->addr + idx));\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tprintline (\"prefix\", \"%\" PFMT64u \"\\n\", op.prefix);\n\t\t\tprintline (\"id\", \"%d\\n\", op.id);\n#if 0\n// no opex here to avoid lot of tests broken..and having json in here is not much useful imho\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tprintline (\"opex\", \"%s\\n\", opexstr);\n\t\t\t}\n#endif\n\t\t\tprintline (\"bytes\", NULL, 0);\n\t\t\tint minsz = R_MIN (len, size);\n\t\t\tminsz = R_MAX (minsz, 0);\n\t\t\tfor (j = 0; j < minsz; j++) {\n\t\t\t\tut8 ch = ((j + idx - 1) > minsz)? 0xff: buf[j + idx];\n\t\t\t\tr_cons_printf (\"%02x\", ch);\n\t\t\t}\n\t\t\tr_cons_newline ();\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tprintline (\"val\", \"0x%08\" PFMT64x \"\\n\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tprintline (\"ptr\", \"0x%08\" PFMT64x \"\\n\", op.ptr);\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tprintline (\"refptr\", \"%d\\n\", op.refptr);\n\t\t\t}\n\t\t\tprintline (\"size\", \"%d\\n\", size);\n\t\t\tprintline (\"sign\", \"%s\\n\", r_str_bool (op.sign));\n\t\t\tprintline (\"type\", \"%s\\n\", r_anal_optype_to_string (op.type));\n\t\t\tprintline (\"cycles\", \"%d\\n\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tprintline (\"failcycles\", \"%d\\n\", op.failcycles);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *t2 = r_anal_optype_to_string (op.type2);\n\t\t\t\tif (t2 && strcmp (t2, \"null\")) {\n\t\t\t\t\tprintline (\"type2\", \"%s\\n\", t2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (op.reg) {\n\t\t\t\tprintline (\"reg\", \"%s\\n\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tprintline (\"ireg\", \"%s\\n\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tprintline (\"scale\", \"%d\\n\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tprintline (\"jump\", \"0x%08\" PFMT64x \"\\n\", op.jump);\n\t\t\t}\n\t\t\tif (op.direction != 0) {\n\t\t\t\tconst char * dir = op.direction == 1 ? \"read\"\n\t\t\t\t\t: op.direction == 2 ? \"write\"\n\t\t\t\t\t: op.direction == 4 ? \"exec\"\n\t\t\t\t\t: op.direction == 8 ? \"ref\": \"none\";\n\t\t\t\tprintline (\"direction\", \"%s\\n\", dir);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tprintline (\"fail\", \"0x%08\" PFMT64x \"\\n\", op.fail);\n\t\t\t}\n\t\t\tif (op.delay) {\n\t\t\t\tprintline (\"delay\", \"%d\\n\", op.delay);\n\t\t\t}\n\t\t\tprintline (\"stack\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)?  r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tprintline (\"cond\", \"%s\\n\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tprintline (\"family\", \"%s\\n\", r_anal_op_family_to_string (op.family));\n\t\t\tprintline (\"stackop\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\tif (op.stackptr) {\n\t\t\t\tprintline (\"stackptr\", \"%\"PFMT64u\"\\n\", op.stackptr);\n\t\t\t}\n\t\t}\n\t\t//r_cons_printf (\"false: 0x%08\"PFMT64x\"\\n\", core->offset+idx);\n\t\t//free (hint);\n\t\tfree (mnem);\n\t\tr_anal_hint_free (hint);\n\t\tr_anal_op_fini (&op);\n\t}\n\tr_anal_op_fini (&op);\n\tif (fmt == 'j') {\n\t\tr_cons_printf (\"]\");\n\t\tr_cons_newline ();\n\t} else if (fmt == 's') {\n\t\tr_cons_printf (\"%d\\n\", totalsize);\n\t}\n\tr_anal_esil_free (esil);\n}", "commit_link": "github.com/radare/radare2/commit/a1bc65c3db593530775823d6d7506a457ed95267", "file_name": "libr/core/cmd_anal.c", "vul_type": "cwe-125", "description": "Write a C function named `core_anal_bytes` that analyzes a buffer of bytes for disassembly and ESIL (Evaluable Strings Intermediate Language) conversion in Radare2."}
{"func_name": "save", "func_src_before": "async def save(request):\n    # TODO csrf\n    data = await request.post()\n    item = Item(data['src'])\n\n    # Update name\n    new_src = data.get('new_src')\n    if new_src and new_src != data['src']:\n        # don't need to worry about html unquote\n        shutil.move(item.abspath, settings.STORAGE_DIR + new_src)\n        old_backup_abspath = item.backup_abspath\n        item = Item(new_src)\n        if os.path.isfile(old_backup_abspath):\n            shutil.move(old_backup_abspath, item.backup_abspath)\n\n    # Update meta\n    for field in item.FORM:\n        # TODO handle .repeatable (keywords)\n        item.meta[field] = [data.get(field, '')]\n\n    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):\n        shutil.copyfile(item.abspath, item.backup_abspath)\n\n    # WISHLIST don't write() if nothing changed\n    item.meta.write()\n\n    return web.Response(\n        status=200,\n        body=json.dumps(item.get_form_fields()).encode('utf8'),\n        content_type='application/json',\n    )", "func_src_after": "async def save(request):\n    # TODO csrf\n    data = await request.post()\n    item = Item(data['src'])\n\n    # Update name\n    new_src = data.get('new_src')\n    if new_src:\n        new_abspath = os.path.abspath(settings.STORAGE_DIR + new_src)\n        if not new_abspath.startswith(settings.STORAGE_DIR):\n            return web.Response(status=400, body=b'Invalid Request')\n\n        if new_abspath != item.abspath:\n            shutil.move(item.abspath, new_abspath)\n            old_backup_abspath = item.backup_abspath\n            item = Item(new_src)\n            if os.path.isfile(old_backup_abspath):\n                shutil.move(old_backup_abspath, item.backup_abspath)\n\n    # Update meta\n    for field in item.FORM:\n        # TODO handle .repeatable (keywords)\n        item.meta[field] = [data.get(field, '')]\n\n    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):\n        shutil.copyfile(item.abspath, item.backup_abspath)\n\n    # WISHLIST don't write() if nothing changed\n    item.meta.write()\n\n    return web.Response(\n        status=200,\n        body=json.dumps(item.get_form_fields()).encode('utf8'),\n        content_type='application/json',\n    )", "commit_link": "github.com/crccheck/gallery-cms/commit/60dec5c580a779ae27824ed54cb113eca25afdc0", "file_name": "gallery/gallery.py", "vul_type": "cwe-022", "description": "Write a Python function to update an item's name and metadata from a POST request."}
{"func_name": "get", "func_src_before": "    def get(self, path):\n        return static_file(path, self.get_base_path())", "func_src_after": "    def get(self, path):\n        path = self.sanitize_path(path)\n        base_paths = self.get_base_paths()\n        if hasattr(base_paths, 'split'):\n            # String, so go simple\n            base_path = base_paths\n        else:\n            base_path = self.get_first_base(base_paths, path)\n        return static_file(path, base_path)", "commit_link": "github.com/foxbunny/seagull/commit/1fb790712fe0c1d1957b31e34a8e0e6593af87a7", "file_name": "seagull/routes/app.py", "vul_type": "cwe-022", "description": "Write a Python function named `get` that retrieves a static file from a base path, which may involve sanitizing the path and handling multiple base paths."}
{"func_name": "add_post", "func_src_before": "def add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"insert into posts values ('%s')\" % content)\n  conn.commit()\n  conn.close()", "func_src_after": "def add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  one_post = content\n  cursor.execute(\"insert into posts values (%s)\", (one_post,))\n  conn.commit()\n  conn.close()", "commit_link": "github.com/paulc1600/DB-API-Forum/commit/069700fb4beec79182fff3c556e9cccce3230d6f", "file_name": "forumdb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new post into a forum database using psycopg2."}
{"func_name": "__init__", "func_src_before": "    def __init__(self,p):\n        p = pickle.loads(p)\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "func_src_after": "    def __init__(self,p):\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 54, "line": "        p = pickle.loads(p)\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 26, "char_end": 54, "chars": "        p = pickle.loads(p)\n"}], "added": []}, "commit_link": "github.com/ellisk42/TikZ/commit/66ab87a1b9a4129fe6f2bc7645a17899f35c9c8b", "file_name": "noTraceBaseline.py", "vul_type": "cwe-502", "commit_msg": "fixed bug in pickle loading", "parent_commit": "c5bb3ad10611b779342b1866cad8961b4a3287ba", "description": "Write a Python class initializer that tokenizes a serialized program, generates an image from it, and checks for serialization consistency."}
{"func_name": "fiber_switch", "func_src_before": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  if (resume && c->status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (c->status == MRB_FIBER_RUNNING || c->status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (c->status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  mrb->c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  if (c->status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    if (len >= c->stend - c->stack) {\n      mrb_raise(mrb, E_FIBER_ERROR, \"too many arguments to fiber\");\n    }\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n  fiber_switch_context(mrb, c);\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "func_src_after": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  enum mrb_fiber_state status;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  status = c->status;\n  if (resume && status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (status == MRB_FIBER_RUNNING || status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  old_c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  fiber_switch_context(mrb, c);\n  if (status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    mrb_stack_extend(mrb, len+2); /* for receiver and (optional) block */\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "commit_link": "github.com/mruby/mruby/commit/778500563a9f7ceba996937dc886bd8cde29b42b", "file_name": "mrbgems/mruby-fiber/src/fiber.c", "vul_type": "cwe-125", "description": "Write a C function `fiber_switch` for the MRuby language that handles fiber resumption, argument passing, and optional VM execution."}
{"func_name": "WebSecurityConfig::configure", "func_src_before": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "func_src_after": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "line_changes": {"deleted": [{"line_no": 21, "char_start": 1381, "char_end": 1407, "line": "\t\thttp.csrf().disable();\r\n"}, {"line_no": 22, "char_start": 1407, "char_end": 1409, "line": "\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1335, "char_end": 1409, "chars": "\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n"}], "added": []}, "commit_link": "github.com/fred4jupiter/fredbet/commit/0470f1a9250316ca637ca55faea70fa0a4f2680a", "file_name": "WebSecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "enabled CSRF protection, because thymeleaf adds a form param for that\nautomatically", "parent_commit": "966f4c3fa53f7be32da840a85587565b82cb0948", "description": "Write a Java method using Spring Security to configure HTTP security, including authorization for specific paths and user roles, form login, logout, and remember-me functionality."}
{"func_name": "ReadPSDChannel", "func_src_before": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      mask->matte=MagickFalse;\n      channel_image=mask;\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "func_src_after": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      if (mask != (Image *) NULL)\n        {\n          mask->matte=MagickFalse;\n          channel_image=mask;\n        }\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/7f2dc7a1afc067d0c89f12c82bcdec0445fb1b94", "file_name": "coders/psd.c", "vul_type": "cwe-476", "description": "In C, write a function to read a channel from a PSD file, handling different compression types and optional masks."}
{"func_name": "delete", "func_src_before": "    @jwt_required\n    def delete(self, user_id):\n        \"\"\" Deletes user with the corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from users where user_id = '{user_id}'\"\"\")", "func_src_after": "    @jwt_required\n    def delete(self, user_id):\n        \"\"\" Deletes user with the corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from users where user_id = %s\"\"\", (user_id, ))", "commit_link": "github.com/sgosal2/tiger-boards-backend/commit/4670109dd613df2f2fe7e8403ebd149df2b55485", "file_name": "apis/users.py", "vul_type": "cwe-089", "description": "Create a Python function with JWT authentication that deletes a user from the database by their user_id."}
{"func_name": "create_basename_core", "func_src_before": "def create_basename_core(basename):\n    try:\n        basename = basename.casefold()\n    except Exception:\n        basename = basename.lower()\n\n    basename = basename.replace(' ', '-')\n    basename = re.sub(r'<[^>]*>', r'', basename)\n    basename = re.sub(r'[^a-z0-9\\-]', r'', basename)\n    basename = re.sub(r'\\-\\-', r'-', basename)\n    basename = urllib.parse.quote_plus(basename)\n\n    return basename", "func_src_after": "def create_basename_core(basename):\n    try:\n        basename = basename.casefold()\n    except Exception:\n        basename = basename.lower()\n\n    basename = re.sub(r'[ \\./]', r'-', basename)\n    basename = re.sub(r'<[^>]*>', r'', basename)\n    basename = re.sub(r'[^a-z0-9\\-]', r'', basename)\n    basename = re.sub(r'\\-\\-', r'-', basename)\n    basename = urllib.parse.quote_plus(basename)\n\n    return basename", "commit_link": "github.com/syegulalp/mercury/commit/3f7c7442fa49aec37577dbdb47ce11a848e7bd03", "file_name": "MeTal/core/utils.py", "vul_type": "cwe-022", "description": "Write a Python function to sanitize and encode a string for use as a URL basename."}
{"func_name": "SessionAttributesManager::doGet", "func_src_before": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n    response.getWriter().append(message);\n  }", "func_src_after": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n  }", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1712, "char_end": 1754, "line": "    response.getWriter().append(message);\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1712, "char_end": 1754, "chars": "    response.getWriter().append(message);\n"}], "added": []}, "commit_link": "github.com/businesscode/BCD-UI/commit/9b230b1500511054da457cf4a5382895fae891df", "file_name": "SessionAttributesManager.java", "vul_type": "cwe-079", "commit_msg": "Server/Security, XSS fixes", "parent_commit": "1b507ea97204e6b71d5863d4c6e523e51a3440b2", "description": "Create a Java servlet that processes a GET request by updating session attributes based on provided parameters and logs the result."}
{"func_name": "(anonymous)", "func_src_before": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n    });", "func_src_after": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n    });", "line_changes": {"deleted": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n"}], "added": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n"}]}, "char_changes": {"deleted": [{"char_start": 821, "char_end": 825, "chars": "html"}], "added": [{"char_start": 821, "char_end": 825, "chars": "text"}]}, "commit_link": "github.com/edmundoa/graylog2-server/commit/a88cae99955cd0ccdd5d99a1c6d506029eb15c60", "file_name": "streamrules.js", "vul_type": "cwe-079", "commit_msg": "use .text() not .html() in stream rule editor to prevent DOM XSS\n\nfixes #543", "description": "In JavaScript, write a jQuery event handler that updates text in a modal based on user input and selection changes, with special handling for an \"inverted\" checkbox state."}
{"func_name": "self.open", "func_src_before": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url, \"rb\", options) do |file|\n        read(file, ext)\n      end\n    end", "func_src_after": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n      end\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 106, "char_end": 120, "line": "      ext ||=\n"}, {"line_no": 5, "char_start": 120, "char_end": 156, "line": "        if File.exist?(path_or_url)\n"}, {"line_no": 6, "char_start": 156, "char_end": 192, "line": "          File.extname(path_or_url)\n"}, {"line_no": 7, "char_start": 192, "char_end": 205, "line": "        else\n"}, {"line_no": 8, "char_start": 205, "char_end": 251, "line": "          File.extname(URI(path_or_url).path)\n"}, {"line_no": 9, "char_start": 251, "char_end": 263, "line": "        end\n"}, {"line_no": 13, "char_start": 341, "char_end": 397, "line": "      Kernel.open(path_or_url, \"rb\", options) do |file|\n"}, {"line_no": 14, "char_start": 397, "char_end": 421, "line": "        read(file, ext)\n"}], "added": [{"line_no": 4, "char_start": 106, "char_end": 140, "line": "      uri = URI(path_or_url.to_s)\n"}, {"line_no": 6, "char_start": 141, "char_end": 178, "line": "      ext ||= File.extname(uri.path)\n"}, {"line_no": 9, "char_start": 255, "char_end": 308, "line": "      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n"}, {"line_no": 10, "char_start": 308, "char_end": 361, "line": "        uri.open(options) { |file| read(file, ext) }\n"}, {"line_no": 11, "char_start": 361, "char_end": 372, "line": "      else\n"}, {"line_no": 12, "char_start": 372, "char_end": 442, "line": "        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 370, "chars": "ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url"}, {"char_start": 387, "char_end": 389, "chars": "do"}, {"char_start": 396, "char_end": 404, "chars": "\n       "}], "added": [{"char_start": 112, "char_end": 398, "chars": "uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s"}, {"char_start": 415, "char_end": 416, "chars": "{"}, {"char_start": 439, "char_end": 441, "chars": " }"}]}, "commit_link": "github.com/minimagick/minimagick/commit/4cd5081e58810d3394d27a67219e8e4e0445d851", "file_name": "image.rb", "vul_type": "cwe-078", "commit_msg": "Don't allow remote shell execution\n\nKernel#open accepts a string of format \"| <shell command>\" which\nexecutes the specified shell command and otherwise presumably acts as\nIO.popen. The open-uri standard library overrides Kernel#open to also\naccept URLs.\n\nHowever, the overridden Kernel#open just delegates to URI#open, so we\nswitch to using that directly and avoid the remote shell execution\nvulnerability. For files we just use File.open, which should have the\nsame behaviour as Kernel#open.", "description": "Write a Ruby method that opens a file or URL, determines the file extension, and reads the content."}
{"func_name": "PropertiesWidget::loadTorrentInfos", "func_src_before": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(m_torrent->comment()));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(m_torrent->creator());\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "func_src_after": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(Utils::String::toHtmlEscaped(m_torrent->comment())));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(Utils::String::toHtmlEscaped(m_torrent->creator()));\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "commit_link": "github.com/qbittorrent/qBittorrent/commit/6ca3e4f094da0a0017cb2d483ec1db6176bb0b16", "file_name": "src/gui/properties/propertieswidget.cpp", "vul_type": "cwe-079", "description": "Write a C++ function in the PropertiesWidget class that loads and displays torrent information using the BitTorrent::TorrentHandle object."}
{"func_name": "generatePrivateKey", "func_src_before": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "func_src_after": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "line_changes": {"deleted": [], "added": [{"line_no": 22, "char_start": 887, "char_end": 909, "line": "\t\tif keyBits < 2048 {\n"}, {"line_no": 23, "char_start": 909, "char_end": 1015, "line": "\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n"}, {"line_no": 24, "char_start": 1015, "char_end": 1019, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 363, "char_end": 1019, "chars": "\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "helpers.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to generate a private key of a specified type and size, optionally using a custom entropy source."}
{"func_name": "tile_by_id", "func_src_before": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "func_src_after": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "line_changes": {"deleted": [{"line_no": 15, "char_start": 334, "char_end": 403, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n"}, {"line_no": 17, "char_start": 445, "char_end": 562, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n"}], "added": [{"line_no": 15, "char_start": 334, "char_end": 404, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n"}, {"line_no": 17, "char_start": 446, "char_end": 564, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n"}]}, "char_changes": {"deleted": [{"char_start": 391, "char_end": 392, "chars": "'"}, {"char_start": 394, "char_end": 401, "chars": "'\" % id"}, {"char_start": 508, "char_end": 509, "chars": "'"}, {"char_start": 511, "char_end": 512, "chars": "'"}, {"char_start": 555, "char_end": 560, "chars": " % id"}], "added": [{"char_start": 393, "char_end": 402, "chars": "\", (id, )"}, {"char_start": 554, "char_end": 562, "chars": ", (id, )"}]}, "commit_link": "github.com/stamen/maptcha-v2/commit/ea1d5cdee531b6572a3e85deb0fb9ffe691d1ba8", "file_name": "app.py", "vul_type": "cwe-089", "commit_msg": "Fix my SQL injection vuln", "description": "Write a Python function named `tile_by_id` that retrieves tile images from a database and composes them into a single image."}
{"func_name": "tid_to_tid_num", "func_src_before": "    def tid_to_tid_num(self, tid):\n        ''' Returns tid_num, given tid. '''\n\n        q = \"SELECT rowid FROM tids WHERE tid = '\" + tid + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]", "func_src_after": "    def tid_to_tid_num(self, tid):\n        ''' Returns tid_num, given tid. '''\n\n        q = \"SELECT rowid FROM tids WHERE tid = ?\"\n        self.query(q, tid)\n        return self.c.fetchone()[0]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves the numerical ID (rowid) for a given text identifier (tid) from a database table named 'tids'."}
{"func_name": "ResponseParser::parse", "func_src_before": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "func_src_after": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 133, "char_end": 134, "line": "\n"}], "added": [{"line_no": 5, "char_start": 207, "char_end": 208, "line": "\n"}, {"line_no": 8, "char_start": 351, "char_end": 436, "line": "\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n"}, {"line_no": 9, "char_start": 436, "char_end": 481, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 11, "char_start": 517, "char_end": 553, "line": "\t\t\tfactory.setXIncludeAware(false);\n"}, {"line_no": 12, "char_start": 553, "char_end": 598, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 14, "char_start": 655, "char_end": 656, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 133, "char_end": 134, "chars": "\n"}, {"char_start": 208, "char_end": 243, "chars": "\t\t\tfactory.setNamespaceAware(true);"}], "added": [{"char_start": 207, "char_end": 655, "chars": "\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n"}]}, "commit_link": "github.com/timroes/aXMLRPC/commit/ad6615b3ec41353e614f6ea5fdd5b046442a832b", "file_name": "ResponseParser.java", "vul_type": "cwe-611", "commit_msg": "Fix CWE-611\n\nThis commit fixes the issue described on\nhttps://cwe.mitre.org/data/definitions/611.html\n\ntest", "parent_commit": "2e59d03c961607d32bc9dc5e7aba931338907603", "description": "Write a Java function to parse an XMLRPC response from an InputStream and handle debug mode."}
{"func_name": "mrb_obj_clone", "func_src_before": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags = mrb_obj_ptr(self)->flags;\n\n  return clone;\n}", "func_src_after": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags |= mrb_obj_ptr(self)->flags & MRB_FLAG_IS_FROZEN;\n\n  return clone;\n}", "commit_link": "github.com/mruby/mruby/commit/55edae0226409de25e59922807cb09acb45731a2", "file_name": "src/kernel.c", "vul_type": "cwe-476", "description": "Write a function in C for the MRuby engine that clones an object, handling immediate values and singleton classes, and preserving the original object's frozen state."}
{"func_name": "Cipher::blowfishECB", "func_src_before": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction)\n        temp2 = byteToB64(temp2);\n\n    return temp2;\n}", "func_src_after": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        // ECB Blowfish encodes in blocks of 12 chars, so anything else is malformed input\n        if ((temp.length() % 12) != 0)\n            return cipherText;\n\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction) {\n        // Sanity check\n        if ((temp2.length() % 8) != 0)\n            return cipherText;\n\n        temp2 = byteToB64(temp2);\n    }\n\n    return temp2;\n}", "commit_link": "github.com/quassel/quassel/commit/8b5ecd226f9208af3074b33d3b7cf5e14f55b138", "file_name": "src/core/cipher.cpp", "vul_type": "cwe-125", "description": "Write a C++ function named `blowfishECB` that performs Blowfish encryption or decryption in ECB mode on a `QByteArray` with manual padding."}
{"func_name": "_create_database", "func_src_before": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "func_src_after": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(\n            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n        )\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "line_changes": {"deleted": [{"line_no": 11, "char_start": 275, "char_end": 362, "line": "        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n"}], "added": [{"line_no": 11, "char_start": 275, "char_end": 299, "line": "        cursor.execute(\n"}, {"line_no": 12, "char_start": 299, "char_end": 385, "line": "            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n"}, {"line_no": 13, "char_start": 385, "char_end": 395, "line": "        )\n"}]}, "char_changes": {"deleted": [{"char_start": 353, "char_end": 360, "chars": "'{db}'\""}], "added": [{"char_start": 298, "char_end": 311, "chars": "\n            "}, {"char_start": 366, "char_end": 393, "chars": "?\", (db,)  # nosec\n        "}]}, "commit_link": "github.com/RasaHQ/rasa_nlu/commit/d68ec7bde34463cc62d9319db4adfd1dff0361cf", "file_name": "tracker_store.py", "vul_type": "cwe-089", "commit_msg": "fix tracker store SQL injection possibility", "parent_commit": "251c10208f9b599ccd511c3ff8d0b41db972a8f9", "description": "Write a Python function to check for the existence of a database and create it if it doesn't exist using psycopg2."}
{"func_name": "vault_decrypt", "func_src_before": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)", "func_src_after": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    aes = do_crypto_setup(mp)\n    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 70, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 70, "char_end": 122, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 4, "char_start": 122, "char_end": 190, "line": "    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 80, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 80, "char_end": 147, "line": "    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 120, "chars": "iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv"}, {"char_start": 146, "char_end": 147, "chars": "d"}, {"char_start": 149, "char_end": 150, "chars": "3"}], "added": [{"char_start": 54, "char_end": 55, "chars": "a"}, {"char_start": 60, "char_end": 78, "chars": "do_crypto_setup(mp"}, {"char_start": 104, "char_end": 105, "chars": "a"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_decrypt` that decrypts a list of ciphertexts using a provided master password and a fixed-size vault."}
{"func_name": "searchAll", "func_src_before": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "func_src_after": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n    searchStr = escape(searchStr);\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "line_changes": {"deleted": [], "added": [{"line_no": 5, "char_start": 189, "char_end": 224, "line": "    searchStr = escape(searchStr);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 188, "char_end": 223, "chars": "\n    searchStr = escape(searchStr);"}]}, "commit_link": "github.com/lrytz/scala/commit/ee2719585e40cb4e9e523e20061a6a2075f4d49d", "file_name": "index.js", "vul_type": "cwe-079", "commit_msg": "fix XSS vulnerability in scaladoc search\n\nto trigger XSS vuln, simply paste this into the search bar:\n```\n\"\\><img/src='1'onerror=alert(777111)>{{7*7}}\n```\n\nall credit for finding the vulnerability goes to *Yeasir Arafat* <skylinearafat@gmail.com>", "description": "Write a JavaScript function named `searchAll` that handles a search input, updates the browser's URL, and displays search results for members and entities."}
{"func_name": "read_body", "func_src_before": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      open @file_path, 'rb' do |io|\n        yield io.read\n      end\n    end\n  end", "func_src_after": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      ::File.open(@file_path, 'rb') do |io|\n        yield io.read\n      end\n    end\n  end", "line_changes": {"deleted": [{"line_no": 8, "char_start": 150, "char_end": 186, "line": "      open @file_path, 'rb' do |io|\n"}], "added": [{"line_no": 8, "char_start": 150, "char_end": 194, "line": "      ::File.open(@file_path, 'rb') do |io|\n"}]}, "char_changes": {"deleted": [{"char_start": 160, "char_end": 161, "chars": " "}], "added": [{"char_start": 156, "char_end": 163, "chars": "::File."}, {"char_start": 167, "char_end": 168, "chars": "("}, {"char_start": 184, "char_end": 185, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/63f8779e49664d5e95fae8d42d04c8e373162b3c", "file_name": "file_response.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in FileResponse#read_body\n\nAlso add general test coverage for FileResponse#read_body\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `read_body` that yields the contents of a file or directory if it exists, otherwise raises an error."}
{"func_name": "_cliq_run", "func_src_before": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cliq_arg_strings = []\n        for k, v in cliq_args.items():\n            cliq_arg_strings.append(\" %s=%s\" % (k, v))\n        cmd = verb + ''.join(cliq_arg_strings)\n\n        return self._run_ssh(cmd, check_exit_code)", "func_src_after": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cmd_list = [verb]\n        for k, v in cliq_args.items():\n            cmd_list.append(\"%s=%s\" % (k, v))\n\n        return self._run_ssh(cmd_list, check_exit_code)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp_lefthand.py", "vul_type": "cwe-078", "description": "Write a Python function that executes a command with arguments over SSH without parsing the results."}
{"func_name": "decode_pointer_field", "func_src_before": "static bool checkreturn decode_pointer_field(pb_istream_t *stream, pb_wire_type_t wire_type, pb_field_iter_t *field)\n{\n#ifndef PB_ENABLE_MALLOC\n    PB_UNUSED(wire_type);\n    PB_UNUSED(field);\n    PB_RETURN_ERROR(stream, \"no malloc support\");\n#else\n    switch (PB_HTYPE(field->type))\n    {\n        case PB_HTYPE_REQUIRED:\n        case PB_HTYPE_OPTIONAL:\n        case PB_HTYPE_ONEOF:\n            if (!check_wire_type(wire_type, field))\n                PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n            if (PB_LTYPE_IS_SUBMSG(field->type) && *(void**)field->pField != NULL)\n            {\n                /* Duplicate field, have to release the old allocation first. */\n                /* FIXME: Does this work correctly for oneofs? */\n                pb_release_single_field(field);\n            }\n        \n            if (PB_HTYPE(field->type) == PB_HTYPE_ONEOF)\n            {\n                *(pb_size_t*)field->pSize = field->tag;\n            }\n\n            if (PB_LTYPE(field->type) == PB_LTYPE_STRING ||\n                PB_LTYPE(field->type) == PB_LTYPE_BYTES)\n            {\n                /* pb_dec_string and pb_dec_bytes handle allocation themselves */\n                field->pData = field->pField;\n                return decode_basic_field(stream, field);\n            }\n            else\n            {\n                if (!allocate_field(stream, field->pField, field->data_size, 1))\n                    return false;\n                \n                field->pData = *(void**)field->pField;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n    \n        case PB_HTYPE_REPEATED:\n            if (wire_type == PB_WT_STRING\n                && PB_LTYPE(field->type) <= PB_LTYPE_LAST_PACKABLE)\n            {\n                /* Packed array, multiple items come in at once. */\n                bool status = true;\n                pb_size_t *size = (pb_size_t*)field->pSize;\n                size_t allocated_size = *size;\n                pb_istream_t substream;\n                \n                if (!pb_make_string_substream(stream, &substream))\n                    return false;\n                \n                while (substream.bytes_left)\n                {\n                    if ((size_t)*size + 1 > allocated_size)\n                    {\n                        /* Allocate more storage. This tries to guess the\n                         * number of remaining entries. Round the division\n                         * upwards. */\n                        allocated_size += (substream.bytes_left - 1) / field->data_size + 1;\n                        \n                        if (!allocate_field(&substream, field->pField, field->data_size, allocated_size))\n                        {\n                            status = false;\n                            break;\n                        }\n                    }\n\n                    /* Decode the array entry */\n                    field->pData = *(char**)field->pField + field->data_size * (*size);\n                    initialize_pointer_field(field->pData, field);\n                    if (!decode_basic_field(&substream, field))\n                    {\n                        status = false;\n                        break;\n                    }\n                    \n                    if (*size == PB_SIZE_MAX)\n                    {\n#ifndef PB_NO_ERRMSG\n                        stream->errmsg = \"too many array entries\";\n#endif\n                        status = false;\n                        break;\n                    }\n                    \n                    (*size)++;\n                }\n                if (!pb_close_string_substream(stream, &substream))\n                    return false;\n                \n                return status;\n            }\n            else\n            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                pb_size_t *size = (pb_size_t*)field->pSize;\n\n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                if (!check_wire_type(wire_type, field))\n                    PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n                (*size)++;\n                if (!allocate_field(stream, field->pField, field->data_size, *size))\n                    return false;\n            \n                field->pData = *(char**)field->pField + field->data_size * (*size - 1);\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n\n        default:\n            PB_RETURN_ERROR(stream, \"invalid field type\");\n    }\n#endif\n}", "func_src_after": "static bool checkreturn decode_pointer_field(pb_istream_t *stream, pb_wire_type_t wire_type, pb_field_iter_t *field)\n{\n#ifndef PB_ENABLE_MALLOC\n    PB_UNUSED(wire_type);\n    PB_UNUSED(field);\n    PB_RETURN_ERROR(stream, \"no malloc support\");\n#else\n    switch (PB_HTYPE(field->type))\n    {\n        case PB_HTYPE_REQUIRED:\n        case PB_HTYPE_OPTIONAL:\n        case PB_HTYPE_ONEOF:\n            if (!check_wire_type(wire_type, field))\n                PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n            if (PB_LTYPE_IS_SUBMSG(field->type) && *(void**)field->pField != NULL)\n            {\n                /* Duplicate field, have to release the old allocation first. */\n                /* FIXME: Does this work correctly for oneofs? */\n                pb_release_single_field(field);\n            }\n        \n            if (PB_HTYPE(field->type) == PB_HTYPE_ONEOF)\n            {\n                *(pb_size_t*)field->pSize = field->tag;\n            }\n\n            if (PB_LTYPE(field->type) == PB_LTYPE_STRING ||\n                PB_LTYPE(field->type) == PB_LTYPE_BYTES)\n            {\n                /* pb_dec_string and pb_dec_bytes handle allocation themselves */\n                field->pData = field->pField;\n                return decode_basic_field(stream, field);\n            }\n            else\n            {\n                if (!allocate_field(stream, field->pField, field->data_size, 1))\n                    return false;\n                \n                field->pData = *(void**)field->pField;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n    \n        case PB_HTYPE_REPEATED:\n            if (wire_type == PB_WT_STRING\n                && PB_LTYPE(field->type) <= PB_LTYPE_LAST_PACKABLE)\n            {\n                /* Packed array, multiple items come in at once. */\n                bool status = true;\n                pb_size_t *size = (pb_size_t*)field->pSize;\n                size_t allocated_size = *size;\n                pb_istream_t substream;\n                \n                if (!pb_make_string_substream(stream, &substream))\n                    return false;\n                \n                while (substream.bytes_left)\n                {\n                    if ((size_t)*size + 1 > allocated_size)\n                    {\n                        /* Allocate more storage. This tries to guess the\n                         * number of remaining entries. Round the division\n                         * upwards. */\n                        allocated_size += (substream.bytes_left - 1) / field->data_size + 1;\n                        \n                        if (!allocate_field(&substream, field->pField, field->data_size, allocated_size))\n                        {\n                            status = false;\n                            break;\n                        }\n                    }\n\n                    /* Decode the array entry */\n                    field->pData = *(char**)field->pField + field->data_size * (*size);\n                    initialize_pointer_field(field->pData, field);\n                    if (!decode_basic_field(&substream, field))\n                    {\n                        status = false;\n                        break;\n                    }\n                    \n                    if (*size == PB_SIZE_MAX)\n                    {\n#ifndef PB_NO_ERRMSG\n                        stream->errmsg = \"too many array entries\";\n#endif\n                        status = false;\n                        break;\n                    }\n                    \n                    (*size)++;\n                }\n                if (!pb_close_string_substream(stream, &substream))\n                    return false;\n                \n                return status;\n            }\n            else\n            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                pb_size_t *size = (pb_size_t*)field->pSize;\n\n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                if (!check_wire_type(wire_type, field))\n                    PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n                if (!allocate_field(stream, field->pField, field->data_size, (size_t)(*size + 1)))\n                    return false;\n            \n                field->pData = *(char**)field->pField + field->data_size * (*size);\n                (*size)++;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n\n        default:\n            PB_RETURN_ERROR(stream, \"invalid field type\");\n    }\n#endif\n}", "commit_link": "github.com/nanopb/nanopb/commit/45582f1f97f49e2abfdba1463d1e1027682d9856", "file_name": "pb_decode.c", "vul_type": "cwe-125", "description": "Write a C function named `decode_pointer_field` that decodes a field from a protocol buffer stream, handling different field types and memory allocation."}
{"func_name": "JUtil::unpackJar", "func_src_before": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "func_src_after": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "line_changes": {"deleted": [], "added": [{"line_no": 9, "char_start": 301, "char_end": 377, "line": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n"}, {"line_no": 10, "char_start": 377, "char_end": 430, "line": "\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 11, "char_start": 430, "char_end": 439, "line": "\t\t\t\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 301, "char_end": 439, "chars": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n"}]}, "commit_link": "github.com/LEMS/jLEMS/commit/8c224637d7d561076364a9e3c2c375daeaf463dc", "file_name": "JUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to extract the contents of a JAR file to a specified directory, excluding META-INF directory."}
{"func_name": "get_task_ioprio", "func_src_before": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\nout:\n\treturn ret;\n}", "func_src_after": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\ttask_lock(p);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\n\ttask_unlock(p);\nout:\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/8ba8682107ee2ca3347354e018865d8e1967c5f4", "file_name": "block/ioprio.c", "vul_type": "cwe-416", "description": "Write a C function named `get_task_ioprio` that retrieves the I/O priority of a given task, using a `task_struct` pointer, and includes proper synchronization if necessary."}
{"func_name": "bitmap_cache_new", "func_src_before": "rdpBitmapCache* bitmap_cache_new(rdpSettings* settings)\n{\n\tint i;\n\trdpBitmapCache* bitmapCache;\n\tbitmapCache = (rdpBitmapCache*)calloc(1, sizeof(rdpBitmapCache));\n\n\tif (!bitmapCache)\n\t\treturn NULL;\n\n\tbitmapCache->settings = settings;\n\tbitmapCache->update = ((freerdp*)settings->instance)->update;\n\tbitmapCache->context = bitmapCache->update->context;\n\tbitmapCache->maxCells = settings->BitmapCacheV2NumCells;\n\tbitmapCache->cells = (BITMAP_V2_CELL*)calloc(bitmapCache->maxCells, sizeof(BITMAP_V2_CELL));\n\n\tif (!bitmapCache->cells)\n\t\tgoto fail;\n\n\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t{\n\t\tbitmapCache->cells[i].number = settings->BitmapCacheV2CellInfo[i].numEntries;\n\t\t/* allocate an extra entry for BITMAP_CACHE_WAITING_LIST_INDEX */\n\t\tbitmapCache->cells[i].entries =\n\t\t    (rdpBitmap**)calloc((bitmapCache->cells[i].number + 1), sizeof(rdpBitmap*));\n\n\t\tif (!bitmapCache->cells[i].entries)\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapCache;\nfail:\n\n\tif (bitmapCache->cells)\n\t{\n\t\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t\t\tfree(bitmapCache->cells[i].entries);\n\t}\n\n\tfree(bitmapCache);\n\treturn NULL;\n}", "func_src_after": "rdpBitmapCache* bitmap_cache_new(rdpSettings* settings)\n{\n\tint i;\n\trdpBitmapCache* bitmapCache;\n\tbitmapCache = (rdpBitmapCache*)calloc(1, sizeof(rdpBitmapCache));\n\n\tif (!bitmapCache)\n\t\treturn NULL;\n\n\tbitmapCache->settings = settings;\n\tbitmapCache->update = ((freerdp*)settings->instance)->update;\n\tbitmapCache->context = bitmapCache->update->context;\n\tbitmapCache->cells =\n\t    (BITMAP_V2_CELL*)calloc(settings->BitmapCacheV2NumCells, sizeof(BITMAP_V2_CELL));\n\n\tif (!bitmapCache->cells)\n\t\tgoto fail;\n\tbitmapCache->maxCells = settings->BitmapCacheV2NumCells;\n\n\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t{\n\t\tbitmapCache->cells[i].number = settings->BitmapCacheV2CellInfo[i].numEntries;\n\t\t/* allocate an extra entry for BITMAP_CACHE_WAITING_LIST_INDEX */\n\t\tbitmapCache->cells[i].entries =\n\t\t    (rdpBitmap**)calloc((bitmapCache->cells[i].number + 1), sizeof(rdpBitmap*));\n\n\t\tif (!bitmapCache->cells[i].entries)\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapCache;\nfail:\n\n\tif (bitmapCache->cells)\n\t{\n\t\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t\t\tfree(bitmapCache->cells[i].entries);\n\t}\n\n\tfree(bitmapCache);\n\treturn NULL;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/58dc36b3c883fd460199cedb6d30e58eba58298c", "file_name": "libfreerdp/cache/bitmap.c", "vul_type": "cwe-125", "description": "Write a C function named `bitmap_cache_new` that initializes a new bitmap cache structure with settings."}
{"func_name": "mrb_class_real", "func_src_before": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0)\n    return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n  }\n  return cl;\n}", "func_src_after": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0) return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n    if (cl == 0) return NULL;\n  }\n  return cl;\n}", "commit_link": "github.com/mruby/mruby/commit/faa4eaf6803bd11669bc324b4c34e7162286bfa3", "file_name": "src/class.c", "vul_type": "cwe-476", "description": "Write a function in C that returns the first non-singleton and non-included class in a class hierarchy, or NULL if not found."}
{"func_name": "test_create_host", "func_src_before": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = 'showhost -verbose fakehost'\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = ('createhost -persona 1 -domain (\\'OpenStack\\',) '\n                           'fakehost 123456789012345 123456789054321')\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "func_src_after": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands for creating and verifying a host in an HP 3PAR storage system."}
{"func_name": "SMB2_write", "func_src_before": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "func_src_after": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "commit_link": "github.com/torvalds/linux/commit/6a3eb3360667170988f8a6477f6686242061488a", "file_name": "fs/cifs/smb2pdu.c", "vul_type": "cwe-416", "description": "In C, write a function to perform an SMB2 write operation with error handling and encryption check."}
{"func_name": "_gd2GetHeader", "func_src_before": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "func_src_after": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tif (overflow2(sidx, nc)) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tif (cidx == NULL) {\n\t\t\tgoto fail1;\n\t\t}\n\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "commit_link": "github.com/php/php-src/commit/7722455726bec8c53458a32851d2a87982cf0eac", "file_name": "ext/gd/libgd/gd_gd2.c", "vul_type": "cwe-190", "description": "Write a C function to read and validate the header of a GD2 image file."}
{"func_name": "_create_3par_fibrechan_host", "func_src_before": "    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'\n                                   % (persona_id, domain,\n                                      hostname, \" \".join(wwn)), None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname", "func_src_after": "    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        command = ['createhost', '-persona', persona_id, '-domain', domain,\n                   hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_fc.py", "vul_type": "cwe-078", "description": "Write a Python function to create a 3PAR Fibre Channel host, handling potential WWN conflicts."}
{"func_name": "concat_opt_exact_str", "func_src_before": "concat_opt_exact_str(OptStr* to, UChar* s, UChar* end, OnigEncoding enc)\n{\n  int i, j, len;\n  UChar *p;\n\n  for (i = to->len, p = s; p < end && i < OPT_EXACT_MAXLEN; ) {\n    len = enclen(enc, p);\n    if (i + len > OPT_EXACT_MAXLEN) break;\n    for (j = 0; j < len && p < end; j++)\n      to->s[i++] = *p++;\n  }\n\n  to->len = i;\n\n  if (p >= end)\n    to->reach_end = 1;\n}", "func_src_after": "concat_opt_exact_str(OptStr* to, UChar* s, UChar* end, OnigEncoding enc)\n{\n  int i, j, len;\n  UChar *p;\n\n  for (i = to->len, p = s; p < end && i < OPT_EXACT_MAXLEN; ) {\n    len = enclen(enc, p);\n    if (i + len >= OPT_EXACT_MAXLEN) break;\n    for (j = 0; j < len && p < end; j++)\n      to->s[i++] = *p++;\n  }\n\n  to->len = i;\n\n  if (p >= end)\n    to->reach_end = 1;\n}", "commit_link": "github.com/kkos/oniguruma/commit/cbe9f8bd9cfc6c3c87a60fbae58fa1a85db59df0", "file_name": "src/regcomp.c", "vul_type": "cwe-787", "description": "Write a C function to concatenate a string to an OptStr structure, ensuring it doesn't exceed a maximum length."}
{"func_name": "exports.getBlockInfo", "func_src_before": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  var ignoremajor = \"\";\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n  }\n\n  // Build the command line\n  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;\n\n  // Run it\n  var aProc = exec(cmd, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "func_src_after": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    cmdArgs.push(\"--exclude\");\n    cmdArgs.push(options.ignoremajor);\n  }\n\n  // Build the command line\n  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n\n  // Run it\n  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 303, "char_end": 327, "line": "  var ignoremajor = \"\";\n"}, {"line_no": 13, "char_start": 390, "char_end": 452, "line": "    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n"}, {"line_no": 17, "char_start": 485, "char_end": 543, "line": "  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n"}, {"line_no": 18, "char_start": 543, "char_end": 616, "line": "      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n"}, {"line_no": 19, "char_start": 616, "char_end": 635, "line": "      ignoremajor;\n"}, {"line_no": 22, "char_start": 648, "char_end": 706, "line": "  var aProc = exec(cmd, function(error, stdout, stderr) {\n"}], "added": [{"line_no": 6, "char_start": 183, "char_end": 270, "line": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n"}, {"line_no": 7, "char_start": 270, "char_end": 271, "line": "\n"}, {"line_no": 14, "char_start": 454, "char_end": 485, "line": "    cmdArgs.push(\"--exclude\");\n"}, {"line_no": 15, "char_start": 485, "char_end": 524, "line": "    cmdArgs.push(options.ignoremajor);\n"}, {"line_no": 19, "char_start": 557, "char_end": 610, "line": "  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n"}, {"line_no": 22, "char_start": 623, "char_end": 694, "line": "  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 303, "char_end": 327, "chars": "  var ignoremajor = \"\";\n"}, {"char_start": 394, "char_end": 424, "chars": "ignoremajor = \" --exclude \" + "}, {"char_start": 443, "char_end": 449, "chars": ".join("}, {"char_start": 497, "char_end": 498, "chars": "("}, {"char_start": 538, "char_end": 634, "chars": ") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;"}, {"char_start": 666, "char_end": 670, "chars": "(cmd"}], "added": [{"char_start": 183, "char_end": 271, "chars": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n"}, {"char_start": 458, "char_end": 502, "chars": "cmdArgs.push(\"--exclude\");\n    cmdArgs.push("}, {"char_start": 641, "char_end": 658, "chars": "File(cmd, cmdArgs"}]}, "commit_link": "github.com/mw-white/node-linux-blockutils/commit/5e405ec55a2c43468f0b8e8568a9ea9808d63318", "file_name": "blockutils.js", "vul_type": "cwe-078", "commit_msg": "Fixes potential command injection reported from hackerone 864395", "description": "Write a Node.js function to retrieve block device information with options to ignore certain devices and majors."}
{"func_name": "ParseEthernetSegmentIdentifier", "func_src_before": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.Atoi(args[0])\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.Atoi(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "func_src_after": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 535, "char_end": 571, "line": "\t\ttyp, err := strconv.Atoi(args[0])\n"}, {"line_no": 46, "char_start": 1107, "char_end": 1141, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 62, "char_start": 1487, "char_end": 1521, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 80, "char_start": 1947, "char_end": 1981, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 90, "char_start": 2177, "char_end": 2212, "line": "\t\tas, err := strconv.Atoi(args[1])\n"}, {"line_no": 96, "char_start": 2353, "char_end": 2387, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}], "added": [{"line_no": 23, "char_start": 535, "char_end": 583, "line": "\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n"}, {"line_no": 46, "char_start": 1119, "char_end": 1166, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n"}, {"line_no": 62, "char_start": 1512, "char_end": 1559, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 80, "char_start": 1985, "char_end": 2032, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 90, "char_start": 2228, "char_end": 2276, "line": "\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n"}, {"line_no": 96, "char_start": 2417, "char_end": 2464, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}]}, "char_changes": {"deleted": [{"char_start": 557, "char_end": 561, "chars": "Atoi"}, {"char_start": 1127, "char_end": 1139, "chars": "Atoi(args[2]"}, {"char_start": 1507, "char_end": 1519, "chars": "Atoi(args[2]"}, {"char_start": 1967, "char_end": 1979, "chars": "Atoi(args[2]"}, {"char_start": 2198, "char_end": 2210, "chars": "Atoi(args[1]"}, {"char_start": 2373, "char_end": 2385, "chars": "Atoi(args[2]"}], "added": [{"char_start": 557, "char_end": 566, "chars": "ParseUint"}, {"char_start": 574, "char_end": 581, "chars": ", 10, 0"}, {"char_start": 1139, "char_end": 1164, "chars": "ParseUint(args[2], 10, 16"}, {"char_start": 1532, "char_end": 1557, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2005, "char_end": 2030, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2249, "char_end": 2274, "chars": "ParseUint(args[1], 10, 32"}, {"char_start": 2437, "char_end": 2462, "chars": "ParseUint(args[2], 10, 32"}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse an Ethernet Segment Identifier from command-line arguments."}
{"func_name": "flattenSubquery", "func_src_before": "static int flattenSubquery(\n  Parse *pParse,       /* Parsing context */\n  Select *p,           /* The parent or outer SELECT statement */\n  int iFrom,           /* Index in p->pSrc->a[] of the inner subquery */\n  int isAgg            /* True if outer SELECT uses aggregate functions */\n){\n  const char *zSavedAuthContext = pParse->zAuthContext;\n  Select *pParent;    /* Current UNION ALL term of the other query */\n  Select *pSub;       /* The inner query or \"subquery\" */\n  Select *pSub1;      /* Pointer to the rightmost select in sub-query */\n  SrcList *pSrc;      /* The FROM clause of the outer query */\n  SrcList *pSubSrc;   /* The FROM clause of the subquery */\n  int iParent;        /* VDBE cursor number of the pSub result set temp table */\n  int iNewParent = -1;/* Replacement table for iParent */\n  int isLeftJoin = 0; /* True if pSub is the right side of a LEFT JOIN */    \n  int i;              /* Loop counter */\n  Expr *pWhere;                    /* The WHERE clause */\n  struct SrcList_item *pSubitem;   /* The subquery */\n  sqlite3 *db = pParse->db;\n\n  /* Check to see if flattening is permitted.  Return 0 if not.\n  */\n  assert( p!=0 );\n  assert( p->pPrior==0 );\n  if( OptimizationDisabled(db, SQLITE_QueryFlattener) ) return 0;\n  pSrc = p->pSrc;\n  assert( pSrc && iFrom>=0 && iFrom<pSrc->nSrc );\n  pSubitem = &pSrc->a[iFrom];\n  iParent = pSubitem->iCursor;\n  pSub = pSubitem->pSelect;\n  assert( pSub!=0 );\n\n#ifndef SQLITE_OMIT_WINDOWFUNC\n  if( p->pWin || pSub->pWin ) return 0;                  /* Restriction (25) */\n#endif\n\n  pSubSrc = pSub->pSrc;\n  assert( pSubSrc );\n  /* Prior to version 3.1.2, when LIMIT and OFFSET had to be simple constants,\n  ** not arbitrary expressions, we allowed some combining of LIMIT and OFFSET\n  ** because they could be computed at compile-time.  But when LIMIT and OFFSET\n  ** became arbitrary expressions, we were forced to add restrictions (13)\n  ** and (14). */\n  if( pSub->pLimit && p->pLimit ) return 0;              /* Restriction (13) */\n  if( pSub->pLimit && pSub->pLimit->pRight ) return 0;   /* Restriction (14) */\n  if( (p->selFlags & SF_Compound)!=0 && pSub->pLimit ){\n    return 0;                                            /* Restriction (15) */\n  }\n  if( pSubSrc->nSrc==0 ) return 0;                       /* Restriction (7)  */\n  if( pSub->selFlags & SF_Distinct ) return 0;           /* Restriction (4)  */\n  if( pSub->pLimit && (pSrc->nSrc>1 || isAgg) ){\n     return 0;         /* Restrictions (8)(9) */\n  }\n  if( p->pOrderBy && pSub->pOrderBy ){\n     return 0;                                           /* Restriction (11) */\n  }\n  if( isAgg && pSub->pOrderBy ) return 0;                /* Restriction (16) */\n  if( pSub->pLimit && p->pWhere ) return 0;              /* Restriction (19) */\n  if( pSub->pLimit && (p->selFlags & SF_Distinct)!=0 ){\n     return 0;         /* Restriction (21) */\n  }\n  if( pSub->selFlags & (SF_Recursive) ){\n    return 0; /* Restrictions (22) */\n  }\n\n  /*\n  ** If the subquery is the right operand of a LEFT JOIN, then the\n  ** subquery may not be a join itself (3a). Example of why this is not\n  ** allowed:\n  **\n  **         t1 LEFT OUTER JOIN (t2 JOIN t3)\n  **\n  ** If we flatten the above, we would get\n  **\n  **         (t1 LEFT OUTER JOIN t2) JOIN t3\n  **\n  ** which is not at all the same thing.\n  **\n  ** If the subquery is the right operand of a LEFT JOIN, then the outer\n  ** query cannot be an aggregate. (3c)  This is an artifact of the way\n  ** aggregates are processed - there is no mechanism to determine if\n  ** the LEFT JOIN table should be all-NULL.\n  **\n  ** See also tickets #306, #350, and #3300.\n  */\n  if( (pSubitem->fg.jointype & JT_OUTER)!=0 ){\n    isLeftJoin = 1;\n    if( pSubSrc->nSrc>1 || isAgg || IsVirtual(pSubSrc->a[0].pTab) ){\n      /*  (3a)             (3c)     (3b) */\n      return 0;\n    }\n  }\n#ifdef SQLITE_EXTRA_IFNULLROW\n  else if( iFrom>0 && !isAgg ){\n    /* Setting isLeftJoin to -1 causes OP_IfNullRow opcodes to be generated for\n    ** every reference to any result column from subquery in a join, even\n    ** though they are not necessary.  This will stress-test the OP_IfNullRow \n    ** opcode. */\n    isLeftJoin = -1;\n  }\n#endif\n\n  /* Restriction (17): If the sub-query is a compound SELECT, then it must\n  ** use only the UNION ALL operator. And none of the simple select queries\n  ** that make up the compound SELECT are allowed to be aggregate or distinct\n  ** queries.\n  */\n  if( pSub->pPrior ){\n    if( pSub->pOrderBy ){\n      return 0;  /* Restriction (20) */\n    }\n    if( isAgg || (p->selFlags & SF_Distinct)!=0 || pSrc->nSrc!=1 ){\n      return 0; /* (17d1), (17d2), or (17d3) */\n    }\n    for(pSub1=pSub; pSub1; pSub1=pSub1->pPrior){\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Distinct );\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Aggregate );\n      assert( pSub->pSrc!=0 );\n      assert( pSub->pEList->nExpr==pSub1->pEList->nExpr );\n      if( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))!=0    /* (17b) */\n       || (pSub1->pPrior && pSub1->op!=TK_ALL)                 /* (17a) */\n       || pSub1->pSrc->nSrc<1                                  /* (17c) */\n      ){\n        return 0;\n      }\n      testcase( pSub1->pSrc->nSrc>1 );\n    }\n\n    /* Restriction (18). */\n    if( p->pOrderBy ){\n      int ii;\n      for(ii=0; ii<p->pOrderBy->nExpr; ii++){\n        if( p->pOrderBy->a[ii].u.x.iOrderByCol==0 ) return 0;\n      }\n    }\n  }\n\n  /* Ex-restriction (23):\n  ** The only way that the recursive part of a CTE can contain a compound\n  ** subquery is for the subquery to be one term of a join.  But if the\n  ** subquery is a join, then the flattening has already been stopped by\n  ** restriction (17d3)\n  */\n  assert( (p->selFlags & SF_Recursive)==0 || pSub->pPrior==0 );\n\n  /***** If we reach this point, flattening is permitted. *****/\n  SELECTTRACE(1,pParse,p,(\"flatten %u.%p from term %d\\n\",\n                   pSub->selId, pSub, iFrom));\n\n  /* Authorize the subquery */\n  pParse->zAuthContext = pSubitem->zName;\n  TESTONLY(i =) sqlite3AuthCheck(pParse, SQLITE_SELECT, 0, 0, 0);\n  testcase( i==SQLITE_DENY );\n  pParse->zAuthContext = zSavedAuthContext;\n\n  /* If the sub-query is a compound SELECT statement, then (by restrictions\n  ** 17 and 18 above) it must be a UNION ALL and the parent query must \n  ** be of the form:\n  **\n  **     SELECT <expr-list> FROM (<sub-query>) <where-clause> \n  **\n  ** followed by any ORDER BY, LIMIT and/or OFFSET clauses. This block\n  ** creates N-1 copies of the parent query without any ORDER BY, LIMIT or \n  ** OFFSET clauses and joins them to the left-hand-side of the original\n  ** using UNION ALL operators. In this case N is the number of simple\n  ** select statements in the compound sub-query.\n  **\n  ** Example:\n  **\n  **     SELECT a+1 FROM (\n  **        SELECT x FROM tab\n  **        UNION ALL\n  **        SELECT y FROM tab\n  **        UNION ALL\n  **        SELECT abs(z*2) FROM tab2\n  **     ) WHERE a!=5 ORDER BY 1\n  **\n  ** Transformed into:\n  **\n  **     SELECT x+1 FROM tab WHERE x+1!=5\n  **     UNION ALL\n  **     SELECT y+1 FROM tab WHERE y+1!=5\n  **     UNION ALL\n  **     SELECT abs(z*2)+1 FROM tab2 WHERE abs(z*2)+1!=5\n  **     ORDER BY 1\n  **\n  ** We call this the \"compound-subquery flattening\".\n  */\n  for(pSub=pSub->pPrior; pSub; pSub=pSub->pPrior){\n    Select *pNew;\n    ExprList *pOrderBy = p->pOrderBy;\n    Expr *pLimit = p->pLimit;\n    Select *pPrior = p->pPrior;\n    p->pOrderBy = 0;\n    p->pSrc = 0;\n    p->pPrior = 0;\n    p->pLimit = 0;\n    pNew = sqlite3SelectDup(db, p, 0);\n    p->pLimit = pLimit;\n    p->pOrderBy = pOrderBy;\n    p->pSrc = pSrc;\n    p->op = TK_ALL;\n    if( pNew==0 ){\n      p->pPrior = pPrior;\n    }else{\n      pNew->pPrior = pPrior;\n      if( pPrior ) pPrior->pNext = pNew;\n      pNew->pNext = p;\n      p->pPrior = pNew;\n      SELECTTRACE(2,pParse,p,(\"compound-subquery flattener\"\n                              \" creates %u as peer\\n\",pNew->selId));\n    }\n    if( db->mallocFailed ) return 1;\n  }\n\n  /* Begin flattening the iFrom-th entry of the FROM clause \n  ** in the outer query.\n  */\n  pSub = pSub1 = pSubitem->pSelect;\n\n  /* Delete the transient table structure associated with the\n  ** subquery\n  */\n  sqlite3DbFree(db, pSubitem->zDatabase);\n  sqlite3DbFree(db, pSubitem->zName);\n  sqlite3DbFree(db, pSubitem->zAlias);\n  pSubitem->zDatabase = 0;\n  pSubitem->zName = 0;\n  pSubitem->zAlias = 0;\n  pSubitem->pSelect = 0;\n\n  /* Defer deleting the Table object associated with the\n  ** subquery until code generation is\n  ** complete, since there may still exist Expr.pTab entries that\n  ** refer to the subquery even after flattening.  Ticket #3346.\n  **\n  ** pSubitem->pTab is always non-NULL by test restrictions and tests above.\n  */\n  if( ALWAYS(pSubitem->pTab!=0) ){\n    Table *pTabToDel = pSubitem->pTab;\n    if( pTabToDel->nTabRef==1 ){\n      Parse *pToplevel = sqlite3ParseToplevel(pParse);\n      pTabToDel->pNextZombie = pToplevel->pZombieTab;\n      pToplevel->pZombieTab = pTabToDel;\n    }else{\n      pTabToDel->nTabRef--;\n    }\n    pSubitem->pTab = 0;\n  }\n\n  /* The following loop runs once for each term in a compound-subquery\n  ** flattening (as described above).  If we are doing a different kind\n  ** of flattening - a flattening other than a compound-subquery flattening -\n  ** then this loop only runs once.\n  **\n  ** This loop moves all of the FROM elements of the subquery into the\n  ** the FROM clause of the outer query.  Before doing this, remember\n  ** the cursor number for the original outer query FROM element in\n  ** iParent.  The iParent cursor will never be used.  Subsequent code\n  ** will scan expressions looking for iParent references and replace\n  ** those references with expressions that resolve to the subquery FROM\n  ** elements we are now copying in.\n  */\n  for(pParent=p; pParent; pParent=pParent->pPrior, pSub=pSub->pPrior){\n    int nSubSrc;\n    u8 jointype = 0;\n    assert( pSub!=0 );\n    pSubSrc = pSub->pSrc;     /* FROM clause of subquery */\n    nSubSrc = pSubSrc->nSrc;  /* Number of terms in subquery FROM clause */\n    pSrc = pParent->pSrc;     /* FROM clause of the outer query */\n\n    if( pSrc ){\n      assert( pParent==p );  /* First time through the loop */\n      jointype = pSubitem->fg.jointype;\n    }else{\n      assert( pParent!=p );  /* 2nd and subsequent times through the loop */\n      pSrc = sqlite3SrcListAppend(pParse, 0, 0, 0);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* The subquery uses a single slot of the FROM clause of the outer\n    ** query.  If the subquery has more than one element in its FROM clause,\n    ** then expand the outer query to make space for it to hold all elements\n    ** of the subquery.\n    **\n    ** Example:\n    **\n    **    SELECT * FROM tabA, (SELECT * FROM sub1, sub2), tabB;\n    **\n    ** The outer query has 3 slots in its FROM clause.  One slot of the\n    ** outer query (the middle slot) is used by the subquery.  The next\n    ** block of code will expand the outer query FROM clause to 4 slots.\n    ** The middle slot is expanded to two slots in order to make space\n    ** for the two elements in the FROM clause of the subquery.\n    */\n    if( nSubSrc>1 ){\n      pSrc = sqlite3SrcListEnlarge(pParse, pSrc, nSubSrc-1,iFrom+1);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* Transfer the FROM clause terms from the subquery into the\n    ** outer query.\n    */\n    for(i=0; i<nSubSrc; i++){\n      sqlite3IdListDelete(db, pSrc->a[i+iFrom].pUsing);\n      assert( pSrc->a[i+iFrom].fg.isTabFunc==0 );\n      pSrc->a[i+iFrom] = pSubSrc->a[i];\n      iNewParent = pSubSrc->a[i].iCursor;\n      memset(&pSubSrc->a[i], 0, sizeof(pSubSrc->a[i]));\n    }\n    pSrc->a[iFrom].fg.jointype = jointype;\n  \n    /* Now begin substituting subquery result set expressions for \n    ** references to the iParent in the outer query.\n    ** \n    ** Example:\n    **\n    **   SELECT a+5, b*10 FROM (SELECT x*3 AS a, y+10 AS b FROM t1) WHERE a>b;\n    **   \\                     \\_____________ subquery __________/          /\n    **    \\_____________________ outer query ______________________________/\n    **\n    ** We look at every expression in the outer query and every place we see\n    ** \"a\" we substitute \"x*3\" and every place we see \"b\" we substitute \"y+10\".\n    */\n    if( pSub->pOrderBy ){\n      /* At this point, any non-zero iOrderByCol values indicate that the\n      ** ORDER BY column expression is identical to the iOrderByCol'th\n      ** expression returned by SELECT statement pSub. Since these values\n      ** do not necessarily correspond to columns in SELECT statement pParent,\n      ** zero them before transfering the ORDER BY clause.\n      **\n      ** Not doing this may cause an error if a subsequent call to this\n      ** function attempts to flatten a compound sub-query into pParent\n      ** (the only way this can happen is if the compound sub-query is\n      ** currently part of pSub->pSrc). See ticket [d11a6e908f].  */\n      ExprList *pOrderBy = pSub->pOrderBy;\n      for(i=0; i<pOrderBy->nExpr; i++){\n        pOrderBy->a[i].u.x.iOrderByCol = 0;\n      }\n      assert( pParent->pOrderBy==0 );\n      pParent->pOrderBy = pOrderBy;\n      pSub->pOrderBy = 0;\n    }\n    pWhere = pSub->pWhere;\n    pSub->pWhere = 0;\n    if( isLeftJoin>0 ){\n      sqlite3SetJoinExpr(pWhere, iNewParent);\n    }\n    pParent->pWhere = sqlite3ExprAnd(pParse, pWhere, pParent->pWhere);\n    if( db->mallocFailed==0 ){\n      SubstContext x;\n      x.pParse = pParse;\n      x.iTable = iParent;\n      x.iNewTable = iNewParent;\n      x.isLeftJoin = isLeftJoin;\n      x.pEList = pSub->pEList;\n      substSelect(&x, pParent, 0);\n    }\n  \n    /* The flattened query is a compound if either the inner or the\n    ** outer query is a compound. */\n    pParent->selFlags |= pSub->selFlags & SF_Compound;\n    assert( (pSub->selFlags & SF_Distinct)==0 ); /* restriction (17b) */\n  \n    /*\n    ** SELECT ... FROM (SELECT ... LIMIT a OFFSET b) LIMIT x OFFSET y;\n    **\n    ** One is tempted to try to add a and b to combine the limits.  But this\n    ** does not work if either limit is negative.\n    */\n    if( pSub->pLimit ){\n      pParent->pLimit = pSub->pLimit;\n      pSub->pLimit = 0;\n    }\n  }\n\n  /* Finially, delete what is left of the subquery and return\n  ** success.\n  */\n  sqlite3SelectDelete(db, pSub1);\n\n#if SELECTTRACE_ENABLED\n  if( sqlite3SelectTrace & 0x100 ){\n    SELECTTRACE(0x100,pParse,p,(\"After flattening:\\n\"));\n    sqlite3TreeViewSelect(0, p, 0);\n  }\n#endif\n\n  return 1;\n}", "func_src_after": "static int flattenSubquery(\n  Parse *pParse,       /* Parsing context */\n  Select *p,           /* The parent or outer SELECT statement */\n  int iFrom,           /* Index in p->pSrc->a[] of the inner subquery */\n  int isAgg            /* True if outer SELECT uses aggregate functions */\n){\n  const char *zSavedAuthContext = pParse->zAuthContext;\n  Select *pParent;    /* Current UNION ALL term of the other query */\n  Select *pSub;       /* The inner query or \"subquery\" */\n  Select *pSub1;      /* Pointer to the rightmost select in sub-query */\n  SrcList *pSrc;      /* The FROM clause of the outer query */\n  SrcList *pSubSrc;   /* The FROM clause of the subquery */\n  int iParent;        /* VDBE cursor number of the pSub result set temp table */\n  int iNewParent = -1;/* Replacement table for iParent */\n  int isLeftJoin = 0; /* True if pSub is the right side of a LEFT JOIN */    \n  int i;              /* Loop counter */\n  Expr *pWhere;                    /* The WHERE clause */\n  struct SrcList_item *pSubitem;   /* The subquery */\n  sqlite3 *db = pParse->db;\n\n  /* Check to see if flattening is permitted.  Return 0 if not.\n  */\n  assert( p!=0 );\n  assert( p->pPrior==0 );\n  if( OptimizationDisabled(db, SQLITE_QueryFlattener) ) return 0;\n  pSrc = p->pSrc;\n  assert( pSrc && iFrom>=0 && iFrom<pSrc->nSrc );\n  pSubitem = &pSrc->a[iFrom];\n  iParent = pSubitem->iCursor;\n  pSub = pSubitem->pSelect;\n  assert( pSub!=0 );\n\n#ifndef SQLITE_OMIT_WINDOWFUNC\n  if( p->pWin || pSub->pWin ) return 0;                  /* Restriction (25) */\n#endif\n\n  pSubSrc = pSub->pSrc;\n  assert( pSubSrc );\n  /* Prior to version 3.1.2, when LIMIT and OFFSET had to be simple constants,\n  ** not arbitrary expressions, we allowed some combining of LIMIT and OFFSET\n  ** because they could be computed at compile-time.  But when LIMIT and OFFSET\n  ** became arbitrary expressions, we were forced to add restrictions (13)\n  ** and (14). */\n  if( pSub->pLimit && p->pLimit ) return 0;              /* Restriction (13) */\n  if( pSub->pLimit && pSub->pLimit->pRight ) return 0;   /* Restriction (14) */\n  if( (p->selFlags & SF_Compound)!=0 && pSub->pLimit ){\n    return 0;                                            /* Restriction (15) */\n  }\n  if( pSubSrc->nSrc==0 ) return 0;                       /* Restriction (7)  */\n  if( pSub->selFlags & SF_Distinct ) return 0;           /* Restriction (4)  */\n  if( pSub->pLimit && (pSrc->nSrc>1 || isAgg) ){\n     return 0;         /* Restrictions (8)(9) */\n  }\n  if( p->pOrderBy && pSub->pOrderBy ){\n     return 0;                                           /* Restriction (11) */\n  }\n  if( isAgg && pSub->pOrderBy ) return 0;                /* Restriction (16) */\n  if( pSub->pLimit && p->pWhere ) return 0;              /* Restriction (19) */\n  if( pSub->pLimit && (p->selFlags & SF_Distinct)!=0 ){\n     return 0;         /* Restriction (21) */\n  }\n  if( pSub->selFlags & (SF_Recursive) ){\n    return 0; /* Restrictions (22) */\n  }\n\n  /*\n  ** If the subquery is the right operand of a LEFT JOIN, then the\n  ** subquery may not be a join itself (3a). Example of why this is not\n  ** allowed:\n  **\n  **         t1 LEFT OUTER JOIN (t2 JOIN t3)\n  **\n  ** If we flatten the above, we would get\n  **\n  **         (t1 LEFT OUTER JOIN t2) JOIN t3\n  **\n  ** which is not at all the same thing.\n  **\n  ** If the subquery is the right operand of a LEFT JOIN, then the outer\n  ** query cannot be an aggregate. (3c)  This is an artifact of the way\n  ** aggregates are processed - there is no mechanism to determine if\n  ** the LEFT JOIN table should be all-NULL.\n  **\n  ** See also tickets #306, #350, and #3300.\n  */\n  if( (pSubitem->fg.jointype & JT_OUTER)!=0 ){\n    isLeftJoin = 1;\n    if( pSubSrc->nSrc>1                   /* (3a) */\n     || isAgg                             /* (3b) */\n     || IsVirtual(pSubSrc->a[0].pTab)     /* (3c) */\n     || (p->selFlags & SF_Distinct)!=0    /* (3d) */\n    ){\n      return 0;\n    }\n  }\n#ifdef SQLITE_EXTRA_IFNULLROW\n  else if( iFrom>0 && !isAgg ){\n    /* Setting isLeftJoin to -1 causes OP_IfNullRow opcodes to be generated for\n    ** every reference to any result column from subquery in a join, even\n    ** though they are not necessary.  This will stress-test the OP_IfNullRow \n    ** opcode. */\n    isLeftJoin = -1;\n  }\n#endif\n\n  /* Restriction (17): If the sub-query is a compound SELECT, then it must\n  ** use only the UNION ALL operator. And none of the simple select queries\n  ** that make up the compound SELECT are allowed to be aggregate or distinct\n  ** queries.\n  */\n  if( pSub->pPrior ){\n    if( pSub->pOrderBy ){\n      return 0;  /* Restriction (20) */\n    }\n    if( isAgg || (p->selFlags & SF_Distinct)!=0 || pSrc->nSrc!=1 ){\n      return 0; /* (17d1), (17d2), or (17d3) */\n    }\n    for(pSub1=pSub; pSub1; pSub1=pSub1->pPrior){\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Distinct );\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Aggregate );\n      assert( pSub->pSrc!=0 );\n      assert( pSub->pEList->nExpr==pSub1->pEList->nExpr );\n      if( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))!=0    /* (17b) */\n       || (pSub1->pPrior && pSub1->op!=TK_ALL)                 /* (17a) */\n       || pSub1->pSrc->nSrc<1                                  /* (17c) */\n      ){\n        return 0;\n      }\n      testcase( pSub1->pSrc->nSrc>1 );\n    }\n\n    /* Restriction (18). */\n    if( p->pOrderBy ){\n      int ii;\n      for(ii=0; ii<p->pOrderBy->nExpr; ii++){\n        if( p->pOrderBy->a[ii].u.x.iOrderByCol==0 ) return 0;\n      }\n    }\n  }\n\n  /* Ex-restriction (23):\n  ** The only way that the recursive part of a CTE can contain a compound\n  ** subquery is for the subquery to be one term of a join.  But if the\n  ** subquery is a join, then the flattening has already been stopped by\n  ** restriction (17d3)\n  */\n  assert( (p->selFlags & SF_Recursive)==0 || pSub->pPrior==0 );\n\n  /***** If we reach this point, flattening is permitted. *****/\n  SELECTTRACE(1,pParse,p,(\"flatten %u.%p from term %d\\n\",\n                   pSub->selId, pSub, iFrom));\n\n  /* Authorize the subquery */\n  pParse->zAuthContext = pSubitem->zName;\n  TESTONLY(i =) sqlite3AuthCheck(pParse, SQLITE_SELECT, 0, 0, 0);\n  testcase( i==SQLITE_DENY );\n  pParse->zAuthContext = zSavedAuthContext;\n\n  /* If the sub-query is a compound SELECT statement, then (by restrictions\n  ** 17 and 18 above) it must be a UNION ALL and the parent query must \n  ** be of the form:\n  **\n  **     SELECT <expr-list> FROM (<sub-query>) <where-clause> \n  **\n  ** followed by any ORDER BY, LIMIT and/or OFFSET clauses. This block\n  ** creates N-1 copies of the parent query without any ORDER BY, LIMIT or \n  ** OFFSET clauses and joins them to the left-hand-side of the original\n  ** using UNION ALL operators. In this case N is the number of simple\n  ** select statements in the compound sub-query.\n  **\n  ** Example:\n  **\n  **     SELECT a+1 FROM (\n  **        SELECT x FROM tab\n  **        UNION ALL\n  **        SELECT y FROM tab\n  **        UNION ALL\n  **        SELECT abs(z*2) FROM tab2\n  **     ) WHERE a!=5 ORDER BY 1\n  **\n  ** Transformed into:\n  **\n  **     SELECT x+1 FROM tab WHERE x+1!=5\n  **     UNION ALL\n  **     SELECT y+1 FROM tab WHERE y+1!=5\n  **     UNION ALL\n  **     SELECT abs(z*2)+1 FROM tab2 WHERE abs(z*2)+1!=5\n  **     ORDER BY 1\n  **\n  ** We call this the \"compound-subquery flattening\".\n  */\n  for(pSub=pSub->pPrior; pSub; pSub=pSub->pPrior){\n    Select *pNew;\n    ExprList *pOrderBy = p->pOrderBy;\n    Expr *pLimit = p->pLimit;\n    Select *pPrior = p->pPrior;\n    p->pOrderBy = 0;\n    p->pSrc = 0;\n    p->pPrior = 0;\n    p->pLimit = 0;\n    pNew = sqlite3SelectDup(db, p, 0);\n    p->pLimit = pLimit;\n    p->pOrderBy = pOrderBy;\n    p->pSrc = pSrc;\n    p->op = TK_ALL;\n    if( pNew==0 ){\n      p->pPrior = pPrior;\n    }else{\n      pNew->pPrior = pPrior;\n      if( pPrior ) pPrior->pNext = pNew;\n      pNew->pNext = p;\n      p->pPrior = pNew;\n      SELECTTRACE(2,pParse,p,(\"compound-subquery flattener\"\n                              \" creates %u as peer\\n\",pNew->selId));\n    }\n    if( db->mallocFailed ) return 1;\n  }\n\n  /* Begin flattening the iFrom-th entry of the FROM clause \n  ** in the outer query.\n  */\n  pSub = pSub1 = pSubitem->pSelect;\n\n  /* Delete the transient table structure associated with the\n  ** subquery\n  */\n  sqlite3DbFree(db, pSubitem->zDatabase);\n  sqlite3DbFree(db, pSubitem->zName);\n  sqlite3DbFree(db, pSubitem->zAlias);\n  pSubitem->zDatabase = 0;\n  pSubitem->zName = 0;\n  pSubitem->zAlias = 0;\n  pSubitem->pSelect = 0;\n\n  /* Defer deleting the Table object associated with the\n  ** subquery until code generation is\n  ** complete, since there may still exist Expr.pTab entries that\n  ** refer to the subquery even after flattening.  Ticket #3346.\n  **\n  ** pSubitem->pTab is always non-NULL by test restrictions and tests above.\n  */\n  if( ALWAYS(pSubitem->pTab!=0) ){\n    Table *pTabToDel = pSubitem->pTab;\n    if( pTabToDel->nTabRef==1 ){\n      Parse *pToplevel = sqlite3ParseToplevel(pParse);\n      pTabToDel->pNextZombie = pToplevel->pZombieTab;\n      pToplevel->pZombieTab = pTabToDel;\n    }else{\n      pTabToDel->nTabRef--;\n    }\n    pSubitem->pTab = 0;\n  }\n\n  /* The following loop runs once for each term in a compound-subquery\n  ** flattening (as described above).  If we are doing a different kind\n  ** of flattening - a flattening other than a compound-subquery flattening -\n  ** then this loop only runs once.\n  **\n  ** This loop moves all of the FROM elements of the subquery into the\n  ** the FROM clause of the outer query.  Before doing this, remember\n  ** the cursor number for the original outer query FROM element in\n  ** iParent.  The iParent cursor will never be used.  Subsequent code\n  ** will scan expressions looking for iParent references and replace\n  ** those references with expressions that resolve to the subquery FROM\n  ** elements we are now copying in.\n  */\n  for(pParent=p; pParent; pParent=pParent->pPrior, pSub=pSub->pPrior){\n    int nSubSrc;\n    u8 jointype = 0;\n    assert( pSub!=0 );\n    pSubSrc = pSub->pSrc;     /* FROM clause of subquery */\n    nSubSrc = pSubSrc->nSrc;  /* Number of terms in subquery FROM clause */\n    pSrc = pParent->pSrc;     /* FROM clause of the outer query */\n\n    if( pSrc ){\n      assert( pParent==p );  /* First time through the loop */\n      jointype = pSubitem->fg.jointype;\n    }else{\n      assert( pParent!=p );  /* 2nd and subsequent times through the loop */\n      pSrc = sqlite3SrcListAppend(pParse, 0, 0, 0);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* The subquery uses a single slot of the FROM clause of the outer\n    ** query.  If the subquery has more than one element in its FROM clause,\n    ** then expand the outer query to make space for it to hold all elements\n    ** of the subquery.\n    **\n    ** Example:\n    **\n    **    SELECT * FROM tabA, (SELECT * FROM sub1, sub2), tabB;\n    **\n    ** The outer query has 3 slots in its FROM clause.  One slot of the\n    ** outer query (the middle slot) is used by the subquery.  The next\n    ** block of code will expand the outer query FROM clause to 4 slots.\n    ** The middle slot is expanded to two slots in order to make space\n    ** for the two elements in the FROM clause of the subquery.\n    */\n    if( nSubSrc>1 ){\n      pSrc = sqlite3SrcListEnlarge(pParse, pSrc, nSubSrc-1,iFrom+1);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* Transfer the FROM clause terms from the subquery into the\n    ** outer query.\n    */\n    for(i=0; i<nSubSrc; i++){\n      sqlite3IdListDelete(db, pSrc->a[i+iFrom].pUsing);\n      assert( pSrc->a[i+iFrom].fg.isTabFunc==0 );\n      pSrc->a[i+iFrom] = pSubSrc->a[i];\n      iNewParent = pSubSrc->a[i].iCursor;\n      memset(&pSubSrc->a[i], 0, sizeof(pSubSrc->a[i]));\n    }\n    pSrc->a[iFrom].fg.jointype = jointype;\n  \n    /* Now begin substituting subquery result set expressions for \n    ** references to the iParent in the outer query.\n    ** \n    ** Example:\n    **\n    **   SELECT a+5, b*10 FROM (SELECT x*3 AS a, y+10 AS b FROM t1) WHERE a>b;\n    **   \\                     \\_____________ subquery __________/          /\n    **    \\_____________________ outer query ______________________________/\n    **\n    ** We look at every expression in the outer query and every place we see\n    ** \"a\" we substitute \"x*3\" and every place we see \"b\" we substitute \"y+10\".\n    */\n    if( pSub->pOrderBy ){\n      /* At this point, any non-zero iOrderByCol values indicate that the\n      ** ORDER BY column expression is identical to the iOrderByCol'th\n      ** expression returned by SELECT statement pSub. Since these values\n      ** do not necessarily correspond to columns in SELECT statement pParent,\n      ** zero them before transfering the ORDER BY clause.\n      **\n      ** Not doing this may cause an error if a subsequent call to this\n      ** function attempts to flatten a compound sub-query into pParent\n      ** (the only way this can happen is if the compound sub-query is\n      ** currently part of pSub->pSrc). See ticket [d11a6e908f].  */\n      ExprList *pOrderBy = pSub->pOrderBy;\n      for(i=0; i<pOrderBy->nExpr; i++){\n        pOrderBy->a[i].u.x.iOrderByCol = 0;\n      }\n      assert( pParent->pOrderBy==0 );\n      pParent->pOrderBy = pOrderBy;\n      pSub->pOrderBy = 0;\n    }\n    pWhere = pSub->pWhere;\n    pSub->pWhere = 0;\n    if( isLeftJoin>0 ){\n      sqlite3SetJoinExpr(pWhere, iNewParent);\n    }\n    pParent->pWhere = sqlite3ExprAnd(pParse, pWhere, pParent->pWhere);\n    if( db->mallocFailed==0 ){\n      SubstContext x;\n      x.pParse = pParse;\n      x.iTable = iParent;\n      x.iNewTable = iNewParent;\n      x.isLeftJoin = isLeftJoin;\n      x.pEList = pSub->pEList;\n      substSelect(&x, pParent, 0);\n    }\n  \n    /* The flattened query is a compound if either the inner or the\n    ** outer query is a compound. */\n    pParent->selFlags |= pSub->selFlags & SF_Compound;\n    assert( (pSub->selFlags & SF_Distinct)==0 ); /* restriction (17b) */\n  \n    /*\n    ** SELECT ... FROM (SELECT ... LIMIT a OFFSET b) LIMIT x OFFSET y;\n    **\n    ** One is tempted to try to add a and b to combine the limits.  But this\n    ** does not work if either limit is negative.\n    */\n    if( pSub->pLimit ){\n      pParent->pLimit = pSub->pLimit;\n      pSub->pLimit = 0;\n    }\n  }\n\n  /* Finially, delete what is left of the subquery and return\n  ** success.\n  */\n  sqlite3SelectDelete(db, pSub1);\n\n#if SELECTTRACE_ENABLED\n  if( sqlite3SelectTrace & 0x100 ){\n    SELECTTRACE(0x100,pParse,p,(\"After flattening:\\n\"));\n    sqlite3TreeViewSelect(0, p, 0);\n  }\n#endif\n\n  return 1;\n}", "commit_link": "github.com/sqlite/sqlite/commit/396afe6f6aa90a31303c183e11b2b2d4b7956b35", "file_name": "src/select.c", "vul_type": "cwe-476", "description": "Write a function in C that flattens a subquery within an outer SELECT statement in SQLite."}
{"func_name": "write", "func_src_before": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "func_src_after": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "line_changes": {"deleted": [{"line_no": 25, "char_start": 932, "char_end": 1015, "line": "        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}], "added": [{"line_no": 25, "char_start": 932, "char_end": 1020, "line": "        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 945, "char_end": 950, "chars": "safe_"}]}, "commit_link": "github.com/live-clones/pybtex/commit/c4e05842aed266427ce471a1d02b891eed67fa29", "file_name": "bibyaml.py", "vul_type": "cwe-502", "commit_msg": "YAML: use safe_dump and safe_load", "parent_commit": "5abe83ed0c01cbc8a43ec9395797d5b0060e0066", "description": "Write a Python function to process bibliographic data and save it to a YAML file."}
{"func_name": "also_add", "func_src_before": "def also_add(name, also):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            INSERT INTO isalso(name,also) VALUES('{}','{}')\n            '''.format(name, also))\n        db.commit()\n        logger.debug('added to isalso name {} with value {}'.format(\n            name, also))\n        db.close()\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "func_src_after": "def also_add(name, also):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            INSERT INTO isalso(name,also) VALUES(%(name)s,%(also)s)\n            ''', (\n            name,\n            also,\n        ))\n        db.commit()\n        logger.debug('added to isalso name {} with value {}'.format(\n            name, also))\n        db.close()\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "commit_link": "github.com/tylarb/KarmaBoi-PCF/commit/c1d00a27d7f6b7eb6f15a3dacd4269654a32c10a", "file_name": "KarmaBoi/dbopts.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a name and an associated value into an 'isalso' database table and log the action."}
{"func_name": "multiSelect", "func_src_before": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "func_src_after": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  if( pParse->nErr ) goto multi_select_end;\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "commit_link": "github.com/sqlite/sqlite/commit/8428b3b437569338a9d1e10c4cd8154acbe33089", "file_name": "src/select.c", "vul_type": "cwe-476", "description": "Write a function in C for SQLite that handles the execution of compound SELECT queries with specific handling for UNION, INTERSECT, and EXCEPT operations."}
{"func_name": "lookup_assets", "func_src_before": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE '\"+start+\"%'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "func_src_after": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE %s\"\n    cur.execute(query, (start+'%',))\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to search for asset names starting with a given string using psycopg2 for database access."}
{"func_name": "make_canonical", "func_src_before": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "func_src_after": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            LY_CHECK_ERR_RETURN(strlen(module_name) + 1 + strlen(*value) > buf_len, LOGBUF(*value), -1);\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            LY_CHECK_ERR_RETURN(strlen(*value) > buf_len, LOGBUF(*value), -1);\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "commit_link": "github.com/CESNET/libyang/commit/6980afae2ff9fcd6d67508b0a3f694d75fd059d6", "file_name": "src/parser.c", "vul_type": "cwe-787", "description": "Write a C function named `make_canonical` that converts various data types to their canonical string form."}
{"func_name": "referrer_count", "func_src_before": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchone()\n\n    return jsonify(results)", "func_src_after": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer=%s\"\n    cur.execute(query, (account_id,))\n    results = cur.fetchone()\n\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to count referrer entries in a PostgreSQL database using an account ID parameter."}
{"func_name": "test_settings_path_skip_issue_909", "func_src_before": "def test_settings_path_skip_issue_909(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'skip =\\n'\n                                        '    file_to_be_skipped.py\\n'\n                                        'skip_glob =\\n'\n                                        '    *glob_skip*\\n')\n\n    base_dir.join('file_glob_skip.py').write('import os\\n'\n                                             '\\n'\n                                             'print(\"Hello World\")\\n'\n                                             '\\n'\n                                             'import sys\\n')\n    base_dir.join('file_to_be_skipped.py').write('import os\\n'\n                                                 '\\n'\n                                                 'print(\"Hello World\")'\n                                                 '\\n'\n                                                 'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors\n        check_output(['isort', '--check-only'])\n    results = check_output(['isort', '--check-only', '--settings-path=conf/.isort.cfg'])\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 2' in results.lower()", "func_src_after": "def test_settings_path_skip_issue_909(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'skip =\\n'\n                                        '    file_to_be_skipped.py\\n'\n                                        'skip_glob =\\n'\n                                        '    *glob_skip*\\n')\n\n    base_dir.join('file_glob_skip.py').write('import os\\n'\n                                             '\\n'\n                                             'print(\"Hello World\")\\n'\n                                             '\\n'\n                                             'import sys\\n')\n    base_dir.join('file_to_be_skipped.py').write('import os\\n'\n                                                 '\\n'\n                                                 'print(\"Hello World\")'\n                                                 '\\n'\n                                                 'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors\n        subprocess.run(['isort', '--check-only'], check=True)\n    result = subprocess.run(\n        ['isort', '--check-only', '--settings-path=conf/.isort.cfg'],\n        stdout=subprocess.PIPE,\n        check=True\n    )\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 2' in result.stdout.lower()", "commit_link": "github.com/timothycrosley/isort/commit/1ab38f4f7840a3c19bf961a24630a992a8373a76", "file_name": "test_isort.py", "vul_type": "cwe-078", "description": "Write a Python test function that creates a temporary directory structure to simulate skipping files with isort configuration and verifies the behavior."}
{"func_name": "usb_get_bos_descriptor", "func_src_before": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\t\tlength = cap->bLength;\n\n\t\tif (total_len < length)\n\t\t\tbreak;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "func_src_after": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\n\t\tif (total_len < sizeof(*cap) || total_len < cap->bLength) {\n\t\t\tdev->bos->desc->bNumDeviceCaps = i;\n\t\t\tbreak;\n\t\t}\n\t\tlength = cap->bLength;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/1c0edc3633b56000e18d82fc241e3995ca18a69e", "file_name": "drivers/usb/core/config.c", "vul_type": "cwe-125", "description": "Write a C function named `usb_get_bos_descriptor` that retrieves the Binary Object Store (BOS) descriptor for a USB device."}
{"func_name": "add_translationname", "func_src_before": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (item[0], trname[1], trname[2]))\n            self.connection.commit()", "func_src_after": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                t = (item[0], trname[1], trname[2], )\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (?, ?, ?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new translation into a database given an item name and translation details."}
{"func_name": "_bson_iter_next_internal", "func_src_before": "_bson_iter_next_internal (bson_iter_t *iter,    /* INOUT */\n                          uint32_t next_keylen, /* IN */\n                          const char **key,     /* OUT */\n                          uint32_t *bson_type,  /* OUT */\n                          bool *unsupported)    /* OUT */\n{\n   const uint8_t *data;\n   uint32_t o;\n   unsigned int len;\n\n   BSON_ASSERT (iter);\n\n   *unsupported = false;\n\n   if (!iter->raw) {\n      *key = NULL;\n      *bson_type = BSON_TYPE_EOD;\n      return false;\n   }\n\n   data = iter->raw;\n   len = iter->len;\n\n   iter->off = iter->next_off;\n   iter->type = iter->off;\n   iter->key = iter->off + 1;\n   iter->d1 = 0;\n   iter->d2 = 0;\n   iter->d3 = 0;\n   iter->d4 = 0;\n\n   if (next_keylen == 0) {\n      /* iterate from start to end of NULL-terminated key string */\n      for (o = iter->key; o < len; o++) {\n         if (!data[o]) {\n            iter->d1 = ++o;\n            goto fill_data_fields;\n         }\n      }\n   } else {\n      o = iter->key + next_keylen + 1;\n      iter->d1 = o;\n      goto fill_data_fields;\n   }\n\n   goto mark_invalid;\n\nfill_data_fields:\n\n   *key = bson_iter_key_unsafe (iter);\n   *bson_type = ITER_TYPE (iter);\n\n   switch (*bson_type) {\n   case BSON_TYPE_DATE_TIME:\n   case BSON_TYPE_DOUBLE:\n   case BSON_TYPE_INT64:\n   case BSON_TYPE_TIMESTAMP:\n      iter->next_off = o + 8;\n      break;\n   case BSON_TYPE_CODE:\n   case BSON_TYPE_SYMBOL:\n   case BSON_TYPE_UTF8: {\n      uint32_t l;\n\n      if ((o + 4) >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l > (len - (o + 4))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 4 + l;\n\n      /*\n       * Make sure the string length includes the NUL byte.\n       */\n      if (BSON_UNLIKELY ((l == 0) || (iter->next_off >= len))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      /*\n       * Make sure the last byte is a NUL byte.\n       */\n      if (BSON_UNLIKELY ((iter->raw + iter->d2)[l - 1] != '\\0')) {\n         iter->err_off = o + 4 + l - 1;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_BINARY: {\n      bson_subtype_t subtype;\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 5;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l >= (len - o)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      subtype = *(iter->raw + iter->d2);\n\n      if (subtype == BSON_SUBTYPE_BINARY_DEPRECATED) {\n         int32_t binary_len;\n\n         if (l < 4) {\n            iter->err_off = o;\n            goto mark_invalid;\n         }\n\n         /* subtype 2 has a redundant length header in the data */\n         memcpy (&binary_len, (iter->raw + iter->d3), sizeof (binary_len));\n         binary_len = BSON_UINT32_FROM_LE (binary_len);\n         if (binary_len + 4 != l) {\n            iter->err_off = iter->d3;\n            goto mark_invalid;\n         }\n      }\n\n      iter->next_off = o + 5 + l;\n   } break;\n   case BSON_TYPE_ARRAY:\n   case BSON_TYPE_DOCUMENT: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l > len) || (l > (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n   } break;\n   case BSON_TYPE_OID:\n      iter->next_off = o + 12;\n      break;\n   case BSON_TYPE_BOOL: {\n      char val;\n\n      if (iter->d1 >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&val, iter->raw + iter->d1, 1);\n      if (val != 0x00 && val != 0x01) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_REGEX: {\n      bool eor = false;\n      bool eoo = false;\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            iter->d2 = ++o;\n            eor = true;\n            break;\n         }\n      }\n\n      if (!eor) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            eoo = true;\n            break;\n         }\n      }\n\n      if (!eoo) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_DBPOINTER: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      /* Check valid string length. l counts '\\0' but not 4 bytes for itself. */\n      if (l == 0 || l > (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if (*(iter->raw + o + l + 3)) {\n         /* not null terminated */\n         iter->err_off = o + l + 3;\n         goto mark_invalid;\n      }\n\n      iter->d3 = o + 4 + l;\n      iter->next_off = o + 4 + l + 12;\n   } break;\n   case BSON_TYPE_CODEWSCOPE: {\n      uint32_t l;\n      uint32_t doclen;\n\n      if ((len < 19) || (o >= (len - 14))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 8;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l < 14) || (l >= (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n\n      if (iter->next_off >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d2, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l == 0 || l >= (len - o - 4 - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if ((o + 4 + 4 + l + 4) >= iter->next_off) {\n         iter->err_off = o + 4;\n         goto mark_invalid;\n      }\n\n      iter->d4 = o + 4 + 4 + l;\n      memcpy (&doclen, iter->raw + iter->d4, sizeof (doclen));\n      doclen = BSON_UINT32_FROM_LE (doclen);\n\n      if ((o + 4 + 4 + l + doclen) != iter->next_off) {\n         iter->err_off = o + 4 + 4 + l;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_INT32:\n      iter->next_off = o + 4;\n      break;\n   case BSON_TYPE_DECIMAL128:\n      iter->next_off = o + 16;\n      break;\n   case BSON_TYPE_MAXKEY:\n   case BSON_TYPE_MINKEY:\n   case BSON_TYPE_NULL:\n   case BSON_TYPE_UNDEFINED:\n      iter->next_off = o;\n      break;\n   default:\n      *unsupported = true;\n   /* FALL THROUGH */\n   case BSON_TYPE_EOD:\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   /*\n    * Check to see if any of the field locations would overflow the\n    * current BSON buffer. If so, set the error location to the offset\n    * of where the field starts.\n    */\n   if (iter->next_off >= len) {\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   iter->err_off = 0;\n\n   return true;\n\nmark_invalid:\n   iter->raw = NULL;\n   iter->len = 0;\n   iter->next_off = 0;\n\n   return false;\n}", "func_src_after": "_bson_iter_next_internal (bson_iter_t *iter,    /* INOUT */\n                          uint32_t next_keylen, /* IN */\n                          const char **key,     /* OUT */\n                          uint32_t *bson_type,  /* OUT */\n                          bool *unsupported)    /* OUT */\n{\n   const uint8_t *data;\n   uint32_t o;\n   unsigned int len;\n\n   BSON_ASSERT (iter);\n\n   *unsupported = false;\n\n   if (!iter->raw) {\n      *key = NULL;\n      *bson_type = BSON_TYPE_EOD;\n      return false;\n   }\n\n   data = iter->raw;\n   len = iter->len;\n\n   iter->off = iter->next_off;\n   iter->type = iter->off;\n   iter->key = iter->off + 1;\n   iter->d1 = 0;\n   iter->d2 = 0;\n   iter->d3 = 0;\n   iter->d4 = 0;\n\n   if (next_keylen == 0) {\n      /* iterate from start to end of NULL-terminated key string */\n      for (o = iter->key; o < len; o++) {\n         if (!data[o]) {\n            iter->d1 = ++o;\n            goto fill_data_fields;\n         }\n      }\n   } else {\n      o = iter->key + next_keylen + 1;\n      iter->d1 = o;\n      goto fill_data_fields;\n   }\n\n   goto mark_invalid;\n\nfill_data_fields:\n\n   *key = bson_iter_key_unsafe (iter);\n   *bson_type = ITER_TYPE (iter);\n\n   switch (*bson_type) {\n   case BSON_TYPE_DATE_TIME:\n   case BSON_TYPE_DOUBLE:\n   case BSON_TYPE_INT64:\n   case BSON_TYPE_TIMESTAMP:\n      iter->next_off = o + 8;\n      break;\n   case BSON_TYPE_CODE:\n   case BSON_TYPE_SYMBOL:\n   case BSON_TYPE_UTF8: {\n      uint32_t l;\n\n      if ((o + 4) >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l > (len - (o + 4))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 4 + l;\n\n      /*\n       * Make sure the string length includes the NUL byte.\n       */\n      if (BSON_UNLIKELY ((l == 0) || (iter->next_off >= len))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      /*\n       * Make sure the last byte is a NUL byte.\n       */\n      if (BSON_UNLIKELY ((iter->raw + iter->d2)[l - 1] != '\\0')) {\n         iter->err_off = o + 4 + l - 1;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_BINARY: {\n      bson_subtype_t subtype;\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 5;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l >= (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      subtype = *(iter->raw + iter->d2);\n\n      if (subtype == BSON_SUBTYPE_BINARY_DEPRECATED) {\n         int32_t binary_len;\n\n         if (l < 4) {\n            iter->err_off = o;\n            goto mark_invalid;\n         }\n\n         /* subtype 2 has a redundant length header in the data */\n         memcpy (&binary_len, (iter->raw + iter->d3), sizeof (binary_len));\n         binary_len = BSON_UINT32_FROM_LE (binary_len);\n         if (binary_len + 4 != l) {\n            iter->err_off = iter->d3;\n            goto mark_invalid;\n         }\n      }\n\n      iter->next_off = o + 5 + l;\n   } break;\n   case BSON_TYPE_ARRAY:\n   case BSON_TYPE_DOCUMENT: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l > len) || (l > (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n   } break;\n   case BSON_TYPE_OID:\n      iter->next_off = o + 12;\n      break;\n   case BSON_TYPE_BOOL: {\n      char val;\n\n      if (iter->d1 >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&val, iter->raw + iter->d1, 1);\n      if (val != 0x00 && val != 0x01) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_REGEX: {\n      bool eor = false;\n      bool eoo = false;\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            iter->d2 = ++o;\n            eor = true;\n            break;\n         }\n      }\n\n      if (!eor) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            eoo = true;\n            break;\n         }\n      }\n\n      if (!eoo) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_DBPOINTER: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      /* Check valid string length. l counts '\\0' but not 4 bytes for itself. */\n      if (l == 0 || l > (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if (*(iter->raw + o + l + 3)) {\n         /* not null terminated */\n         iter->err_off = o + l + 3;\n         goto mark_invalid;\n      }\n\n      iter->d3 = o + 4 + l;\n      iter->next_off = o + 4 + l + 12;\n   } break;\n   case BSON_TYPE_CODEWSCOPE: {\n      uint32_t l;\n      uint32_t doclen;\n\n      if ((len < 19) || (o >= (len - 14))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 8;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l < 14) || (l >= (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n\n      if (iter->next_off >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d2, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l == 0 || l >= (len - o - 4 - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if ((o + 4 + 4 + l + 4) >= iter->next_off) {\n         iter->err_off = o + 4;\n         goto mark_invalid;\n      }\n\n      iter->d4 = o + 4 + 4 + l;\n      memcpy (&doclen, iter->raw + iter->d4, sizeof (doclen));\n      doclen = BSON_UINT32_FROM_LE (doclen);\n\n      if ((o + 4 + 4 + l + doclen) != iter->next_off) {\n         iter->err_off = o + 4 + 4 + l;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_INT32:\n      iter->next_off = o + 4;\n      break;\n   case BSON_TYPE_DECIMAL128:\n      iter->next_off = o + 16;\n      break;\n   case BSON_TYPE_MAXKEY:\n   case BSON_TYPE_MINKEY:\n   case BSON_TYPE_NULL:\n   case BSON_TYPE_UNDEFINED:\n      iter->next_off = o;\n      break;\n   default:\n      *unsupported = true;\n   /* FALL THROUGH */\n   case BSON_TYPE_EOD:\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   /*\n    * Check to see if any of the field locations would overflow the\n    * current BSON buffer. If so, set the error location to the offset\n    * of where the field starts.\n    */\n   if (iter->next_off >= len) {\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   iter->err_off = 0;\n\n   return true;\n\nmark_invalid:\n   iter->raw = NULL;\n   iter->len = 0;\n   iter->next_off = 0;\n\n   return false;\n}", "commit_link": "github.com/mongodb/mongo-c-driver/commit/0d9a4d98bfdf4acd2c0138d4aaeb4e2e0934bd84", "file_name": "src/libbson/src/bson/bson-iter.c", "vul_type": "cwe-125", "description": "Write a C function to iterate over BSON document fields, updating the iterator state and handling various BSON types."}
{"func_name": "create_token", "func_src_before": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 15, "char_start": 585, "char_end": 618, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 15, "char_start": 585, "char_end": 622, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 607, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously insert a new session token into a database for a given user ID and handle exceptions by raising an HTTP forbidden error."}
{"func_name": "index", "func_src_before": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "func_src_after": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "line_changes": {"deleted": [{"line_no": 22, "char_start": 929, "char_end": 1069, "line": "      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n"}], "added": [{"line_no": 22, "char_start": 929, "char_end": 1030, "line": "      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n"}, {"line_no": 23, "char_start": 1030, "char_end": 1089, "line": "      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 963, "char_end": 996, "chars": "params[:q].split(\" \").map{|text| "}, {"char_start": 1040, "char_end": 1068, "chars": "'%#{text}%'\" }.join(\" OR \"))"}], "added": [{"char_start": 1007, "char_end": 1088, "chars": "?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\""}]}, "commit_link": "github.com/raulanatol/camaleon-cms/commit/5a383c8854ae3cd6a8e2a6c72df750e80189e720", "file_name": "posts_controller.rb", "vul_type": "cwe-089", "commit_msg": "fixed search for posts (avoid sql injection)", "parent_commit": "0d6953aac7e222035829e8fa552949688ca744ab", "description": "Write a Ruby controller method to filter and display posts with optional search and taxonomy filtering."}
{"func_name": "tag_to_tag_num", "func_src_before": "    def tag_to_tag_num(self, tag):\n        ''' Returns tag_num given tag. '''\n\n        q = \"SELECT rowid FROM tags WHERE tag = '\" + tag + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]", "func_src_after": "    def tag_to_tag_num(self, tag):\n        ''' Returns tag_num given tag. '''\n\n        q = \"SELECT rowid FROM tags WHERE tag = ?\"\n        self.query(q, tag)\n        return self.c.fetchone()[0]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves the numerical ID of a tag from a database."}
{"func_name": "deleteKey", "func_src_before": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\n\tif re.search('[^a-zA-Z0-9]', token_data['key']):\n\t\traise FoxlockError(BAD_REQUEST, 'Invalid key requested')\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "func_src_after": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateKeyName(token_data['key'])\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "commit_link": "github.com/Mimickal/FoxLock/commit/7c665e556987f4e2c1a75e143a1e80ae066ad833", "file_name": "impl.py", "vul_type": "cwe-022", "description": "Write a Python function named `deleteKey` that removes a client's key file and handles the case where the key does not exist."}
{"func_name": "hi3660_stub_clk_probe", "func_src_before": "static int hi3660_stub_clk_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *res;\n\tunsigned int i;\n\tint ret;\n\n\t/* Use mailbox client without blocking */\n\tstub_clk_chan.cl.dev = dev;\n\tstub_clk_chan.cl.tx_done = NULL;\n\tstub_clk_chan.cl.tx_block = false;\n\tstub_clk_chan.cl.knows_txdone = false;\n\n\t/* Allocate mailbox channel */\n\tstub_clk_chan.mbox = mbox_request_channel(&stub_clk_chan.cl, 0);\n\tif (IS_ERR(stub_clk_chan.mbox))\n\t\treturn PTR_ERR(stub_clk_chan.mbox);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tfreq_reg = devm_ioremap(dev, res->start, resource_size(res));\n\tif (!freq_reg)\n\t\treturn -ENOMEM;\n\n\tfreq_reg += HI3660_STUB_CLOCK_DATA;\n\n\tfor (i = 0; i < HI3660_CLK_STUB_NUM; i++) {\n\t\tret = devm_clk_hw_register(&pdev->dev, &hi3660_stub_clks[i].hw);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn devm_of_clk_add_hw_provider(&pdev->dev, hi3660_stub_clk_hw_get,\n\t\t\t\t\t   hi3660_stub_clks);\n}", "func_src_after": "static int hi3660_stub_clk_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *res;\n\tunsigned int i;\n\tint ret;\n\n\t/* Use mailbox client without blocking */\n\tstub_clk_chan.cl.dev = dev;\n\tstub_clk_chan.cl.tx_done = NULL;\n\tstub_clk_chan.cl.tx_block = false;\n\tstub_clk_chan.cl.knows_txdone = false;\n\n\t/* Allocate mailbox channel */\n\tstub_clk_chan.mbox = mbox_request_channel(&stub_clk_chan.cl, 0);\n\tif (IS_ERR(stub_clk_chan.mbox))\n\t\treturn PTR_ERR(stub_clk_chan.mbox);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res)\n\t\treturn -EINVAL;\n\tfreq_reg = devm_ioremap(dev, res->start, resource_size(res));\n\tif (!freq_reg)\n\t\treturn -ENOMEM;\n\n\tfreq_reg += HI3660_STUB_CLOCK_DATA;\n\n\tfor (i = 0; i < HI3660_CLK_STUB_NUM; i++) {\n\t\tret = devm_clk_hw_register(&pdev->dev, &hi3660_stub_clks[i].hw);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn devm_of_clk_add_hw_provider(&pdev->dev, hi3660_stub_clk_hw_get,\n\t\t\t\t\t   hi3660_stub_clks);\n}", "commit_link": "github.com/torvalds/linux/commit/9903e41ae1f5d50c93f268ca3304d4d7c64b9311", "file_name": "drivers/clk/hisilicon/clk-hi3660-stub.c", "vul_type": "cwe-476", "description": "Write a C function for initializing a clock in the hi3660 platform driver."}
{"func_name": "changeValue", "func_src_before": "\tchangeValue:function(val){ \n\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n\t\tthis.editValue(val)\n\t},", "func_src_after": "\tchangeValue:function(val){ \n\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n\t\tthis.editValue(val)\n\t},", "line_changes": {"deleted": [{"line_no": 2, "char_start": 29, "char_end": 107, "line": "\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n"}], "added": [{"line_no": 2, "char_start": 29, "char_end": 110, "line": "\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n"}, {"line_no": 3, "char_start": 110, "char_end": 161, "line": "\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n"}]}, "char_changes": {"deleted": [{"char_start": 48, "char_end": 54, "chars": "Static"}, {"char_start": 62, "char_end": 68, "chars": "html( "}, {"char_start": 85, "char_end": 90, "chars": "Field"}, {"char_start": 98, "char_end": 103, "chars": "val()"}], "added": [{"char_start": 31, "char_end": 63, "chars": "var txtValue = $('<div/>').text("}, {"char_start": 80, "char_end": 85, "chars": "Field"}, {"char_start": 93, "char_end": 112, "chars": "val()).html();\t\t\n\t\t"}, {"char_start": 129, "char_end": 135, "chars": "Static"}, {"char_start": 143, "char_end": 157, "chars": "html( txtValue"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function named `changeValue` that updates the HTML content of an element and calls another function to edit the value."}
{"func_name": "dbd_st_fetch", "func_src_before": "dbd_st_fetch(SV *sth, imp_sth_t* imp_sth)\n{\n  dTHX;\n  int num_fields, ChopBlanks, i, rc;\n  unsigned long *lengths;\n  AV *av;\n  int av_length, av_readonly;\n  MYSQL_ROW cols;\n  D_imp_dbh_from_sth;\n  MYSQL* svsock= imp_dbh->pmysql;\n  imp_sth_fbh_t *fbh;\n  D_imp_xxh(sth);\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  MYSQL_BIND *buffer;\n#endif\n  MYSQL_FIELD *fields;\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t-> dbd_st_fetch\\n\");\n\n#if MYSQL_ASYNC\n  if(imp_dbh->async_query_in_flight) {\n      if(mysql_db_async_result(sth, &imp_sth->result) <= 0) {\n        return Nullav;\n      }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (!DBIc_ACTIVE(imp_sth) )\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"no statement executing\\n\",NULL);\n      return Nullav;\n    }\n\n    if (imp_sth->fetch_done)\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"fetch() but fetch already done\",NULL);\n      return Nullav;\n    }\n\n    if (!imp_sth->done_desc)\n    {\n      if (!dbd_describe(sth, imp_sth))\n      {\n        do_error(sth, JW_ERR_SEQUENCE, \"Error while describe result set.\",\n                 NULL);\n        return Nullav;\n      }\n    }\n  }\n#endif\n\n  ChopBlanks = DBIc_is(imp_sth, DBIcf_ChopBlanks);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                  \"\\t\\tdbd_st_fetch for %p, chopblanks %d\\n\",\n                  sth, ChopBlanks);\n\n  if (!imp_sth->result)\n  {\n    do_error(sth, JW_ERR_SEQUENCE, \"fetch() without execute()\" ,NULL);\n    return Nullav;\n  }\n\n  /* fix from 2.9008 */\n  imp_dbh->pmysql->net.last_errno = 0;\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch calling mysql_fetch\\n\");\n\n    if ((rc= mysql_stmt_fetch(imp_sth->stmt)))\n    {\n      if (rc == 1)\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_stmt_sqlstate(imp_sth->stmt));\n\n#if MYSQL_VERSION_ID >= MYSQL_VERSION_5_0 \n      if (rc == MYSQL_DATA_TRUNCATED) {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch data truncated\\n\");\n        goto process;\n      }\n#endif\n\n      if (rc == MYSQL_NO_DATA)\n      {\n        /* Update row_num to affected_rows value */\n        imp_sth->row_num= mysql_stmt_affected_rows(imp_sth->stmt);\n        imp_sth->fetch_done=1;\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch no data\\n\");\n      }\n\n      dbd_st_finish(sth, imp_sth);\n\n      return Nullav;\n    }\n\nprocess:\n    imp_sth->currow++;\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n    num_fields=mysql_stmt_field_count(imp_sth->stmt);\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tdbd_st_fetch called mysql_fetch, rc %d num_fields %d\\n\",\n                    rc, num_fields);\n\n    for (\n         buffer= imp_sth->buffer,\n         fbh= imp_sth->fbh,\n         i= 0;\n         i < num_fields;\n         i++,\n         fbh++,\n         buffer++\n        )\n    {\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n      STRLEN len;\n\n      /* This is wrong, null is not being set correctly\n       * This is not the way to determine length (this would break blobs!)\n       */\n      if (fbh->is_null)\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n      else\n      {\n        /* In case of BLOB/TEXT fields we allocate only 8192 bytes\n           in dbd_describe() for data. Here we know real size of field\n           so we should increase buffer size and refetch column value\n        */\n        if (fbh->length > buffer->buffer_length || fbh->error)\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n              \"\\t\\tRefetch BLOB/TEXT column: %d, length: %lu, error: %d\\n\",\n              i, fbh->length, fbh->error);\n\n          Renew(fbh->data, fbh->length, char);\n          buffer->buffer_length= fbh->length;\n          buffer->buffer= (char *) fbh->data;\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tbefore buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n\n          /*TODO: Use offset instead of 0 to fetch only remain part of data*/\n          if (mysql_stmt_fetch_column(imp_sth->stmt, buffer , i, 0))\n            do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                     mysql_stmt_error(imp_sth->stmt),\n                     mysql_stmt_sqlstate(imp_sth->stmt));\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tafter buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n        }\n\n        /* This does look a lot like Georg's PHP driver doesn't it?  --Brian */\n        /* Credit due to Georg - mysqli_api.c  ;) --PMG */\n        switch (buffer->buffer_type) {\n        case MYSQL_TYPE_DOUBLE:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch double data %f\\n\", fbh->ddata);\n          sv_setnv(sv, fbh->ddata);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch int data %\"IVdf\", unsigned? %d\\n\",\n                          fbh->ldata, buffer->is_unsigned);\n          if (buffer->is_unsigned)\n            sv_setuv(sv, fbh->ldata);\n          else\n            sv_setiv(sv, fbh->ldata);\n\n          break;\n\n        case MYSQL_TYPE_BIT:\n          sv_setpvn(sv, fbh->data, fbh->length);\n\n          break;\n\n        default:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tERROR IN st_fetch_string\");\n          len= fbh->length;\n\t  /* ChopBlanks server-side prepared statement */\n          if (ChopBlanks)\n          {\n            /* \n              see bottom of:\n              http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html\n            */\n            if (fbh->charsetnr != 63)\n              while (len && fbh->data[len-1] == ' ') { --len; }\n          }\n\t  /* END OF ChopBlanks */\n\n          sv_setpvn(sv, fbh->data, len);\n\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n#if MYSQL_VERSION_ID >= FIELD_CHARSETNR_VERSION \n  /* SHOW COLLATION WHERE Id = 63; -- 63 == charset binary, collation binary */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fbh->charsetnr != 63)\n#else\n\tif ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && !(fbh->flags & BINARY_FLAG))\n#endif\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n\n        }\n\n      }\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n\n    return av;\n  }\n  else\n  {\n#endif\n\n    imp_sth->currow++;\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    {\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch result set details\\n\");\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\timp_sth->result=%p\\n\", imp_sth->result);\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_fields=%u\\n\",\n                    mysql_num_fields(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_rows=%llu\\n\",\n                    mysql_num_rows(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_affected_rows=%llu\\n\",\n                    mysql_affected_rows(imp_dbh->pmysql));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch for %p, currow= %d\\n\",\n                    sth,imp_sth->currow);\n    }\n\n    if (!(cols= mysql_fetch_row(imp_sth->result)))\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      {\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch, no more rows to fetch\");\n      }\n      if (mysql_errno(imp_dbh->pmysql))\n        do_error(sth, mysql_errno(imp_dbh->pmysql),\n                 mysql_error(imp_dbh->pmysql),\n                 mysql_sqlstate(imp_dbh->pmysql));\n\n\n#if MYSQL_VERSION_ID >= MULTIPLE_RESULT_SET_VERSION\n      if (!mysql_more_results(svsock))\n#endif\n        dbd_st_finish(sth, imp_sth);\n      return Nullav;\n    }\n\n    num_fields= mysql_num_fields(imp_sth->result);\n    fields= mysql_fetch_fields(imp_sth->result);\n    lengths= mysql_fetch_lengths(imp_sth->result);\n\n    if ((av= DBIc_FIELDS_AV(imp_sth)) != Nullav)\n    {\n      av_length= av_len(av)+1;\n\n      if (av_length != num_fields)              /* Resize array if necessary */\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, size of results array(%d) != num_fields(%d)\\n\",\n                                   av_length, num_fields);\n\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, result fields(%d)\\n\",\n                                   DBIc_NUM_FIELDS(imp_sth));\n\n        av_readonly = SvREADONLY(av);\n\n        if (av_readonly)\n          SvREADONLY_off( av );              /* DBI sets this readonly */\n\n        while (av_length < num_fields)\n        {\n          av_store(av, av_length++, newSV(0));\n        }\n\n        while (av_length > num_fields)\n        {\n          SvREFCNT_dec(av_pop(av));\n          av_length--;\n        }\n        if (av_readonly)\n          SvREADONLY_on(av);\n      }\n    }\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n\n    for (i= 0;  i < num_fields; ++i)\n    {\n      char *col= cols[i];\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n\n      if (col)\n      {\n        STRLEN len= lengths[i];\n        if (ChopBlanks)\n        {\n          while (len && col[len-1] == ' ')\n          {\t--len; }\n        }\n\n        /* Set string value returned from mysql server */\n        sv_setpvn(sv, col, len);\n\n        switch (mysql_to_perl_type(fields[i].type)) {\n        case MYSQL_TYPE_DOUBLE:\n          /* Coerce to dobule and set scalar as NV */\n          (void) SvNV(sv);\n          SvNOK_only(sv);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          /* Coerce to integer and set scalar as UV resp. IV */\n          if (fields[i].flags & UNSIGNED_FLAG)\n          {\n            (void) SvUV(sv);\n            SvIOK_only_UV(sv);\n          }\n          else\n          {\n            (void) SvIV(sv);\n            SvIOK_only(sv);\n          }\n          break;\n\n#if MYSQL_VERSION_ID > NEW_DATATYPE_VERSION\n        case MYSQL_TYPE_BIT:\n          /* Let it as binary string */\n          break;\n#endif\n\n        default:\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n  /* see bottom of: http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fields[i].charsetnr != 63)\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n        }\n      }\n      else\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n    return av;\n\n#if MYSQL_VERSION_ID  >= SERVER_PREPARE_VERSION\n  }\n#endif\n\n}", "func_src_after": "dbd_st_fetch(SV *sth, imp_sth_t* imp_sth)\n{\n  dTHX;\n  int num_fields, ChopBlanks, i, rc;\n  unsigned long *lengths;\n  AV *av;\n  int av_length, av_readonly;\n  MYSQL_ROW cols;\n  D_imp_dbh_from_sth;\n  MYSQL* svsock= imp_dbh->pmysql;\n  imp_sth_fbh_t *fbh;\n  D_imp_xxh(sth);\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  MYSQL_BIND *buffer;\n#endif\n  MYSQL_FIELD *fields;\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t-> dbd_st_fetch\\n\");\n\n#if MYSQL_ASYNC\n  if(imp_dbh->async_query_in_flight) {\n      if(mysql_db_async_result(sth, &imp_sth->result) <= 0) {\n        return Nullav;\n      }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (!DBIc_ACTIVE(imp_sth) )\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"no statement executing\\n\",NULL);\n      return Nullav;\n    }\n\n    if (imp_sth->fetch_done)\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"fetch() but fetch already done\",NULL);\n      return Nullav;\n    }\n\n    if (!imp_sth->done_desc)\n    {\n      if (!dbd_describe(sth, imp_sth))\n      {\n        do_error(sth, JW_ERR_SEQUENCE, \"Error while describe result set.\",\n                 NULL);\n        return Nullav;\n      }\n    }\n  }\n#endif\n\n  ChopBlanks = DBIc_is(imp_sth, DBIcf_ChopBlanks);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                  \"\\t\\tdbd_st_fetch for %p, chopblanks %d\\n\",\n                  sth, ChopBlanks);\n\n  if (!imp_sth->result)\n  {\n    do_error(sth, JW_ERR_SEQUENCE, \"fetch() without execute()\" ,NULL);\n    return Nullav;\n  }\n\n  /* fix from 2.9008 */\n  imp_dbh->pmysql->net.last_errno = 0;\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch calling mysql_fetch\\n\");\n\n    if ((rc= mysql_stmt_fetch(imp_sth->stmt)))\n    {\n      if (rc == 1)\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_stmt_sqlstate(imp_sth->stmt));\n\n#if MYSQL_VERSION_ID >= MYSQL_VERSION_5_0 \n      if (rc == MYSQL_DATA_TRUNCATED) {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch data truncated\\n\");\n        goto process;\n      }\n#endif\n\n      if (rc == MYSQL_NO_DATA)\n      {\n        /* Update row_num to affected_rows value */\n        imp_sth->row_num= mysql_stmt_affected_rows(imp_sth->stmt);\n        imp_sth->fetch_done=1;\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch no data\\n\");\n      }\n\n      dbd_st_finish(sth, imp_sth);\n\n      return Nullav;\n    }\n\nprocess:\n    imp_sth->currow++;\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n    num_fields=mysql_stmt_field_count(imp_sth->stmt);\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tdbd_st_fetch called mysql_fetch, rc %d num_fields %d\\n\",\n                    rc, num_fields);\n\n    for (\n         buffer= imp_sth->buffer,\n         fbh= imp_sth->fbh,\n         i= 0;\n         i < num_fields;\n         i++,\n         fbh++,\n         buffer++\n        )\n    {\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n      STRLEN len;\n\n      /* This is wrong, null is not being set correctly\n       * This is not the way to determine length (this would break blobs!)\n       */\n      if (fbh->is_null)\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n      else\n      {\n        /* In case of BLOB/TEXT fields we allocate only 8192 bytes\n           in dbd_describe() for data. Here we know real size of field\n           so we should increase buffer size and refetch column value\n        */\n        if (fbh->length > buffer->buffer_length || fbh->error)\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n              \"\\t\\tRefetch BLOB/TEXT column: %d, length: %lu, error: %d\\n\",\n              i, fbh->length, fbh->error);\n\n          Renew(fbh->data, fbh->length, char);\n          buffer->buffer_length= fbh->length;\n          buffer->buffer= (char *) fbh->data;\n          imp_sth->stmt->bind[i].buffer_length = fbh->length;\n          imp_sth->stmt->bind[i].buffer = (char *)fbh->data;\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tbefore buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n\n          /*TODO: Use offset instead of 0 to fetch only remain part of data*/\n          if (mysql_stmt_fetch_column(imp_sth->stmt, buffer , i, 0))\n            do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                     mysql_stmt_error(imp_sth->stmt),\n                     mysql_stmt_sqlstate(imp_sth->stmt));\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tafter buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n        }\n\n        /* This does look a lot like Georg's PHP driver doesn't it?  --Brian */\n        /* Credit due to Georg - mysqli_api.c  ;) --PMG */\n        switch (buffer->buffer_type) {\n        case MYSQL_TYPE_DOUBLE:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch double data %f\\n\", fbh->ddata);\n          sv_setnv(sv, fbh->ddata);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch int data %\"IVdf\", unsigned? %d\\n\",\n                          fbh->ldata, buffer->is_unsigned);\n          if (buffer->is_unsigned)\n            sv_setuv(sv, fbh->ldata);\n          else\n            sv_setiv(sv, fbh->ldata);\n\n          break;\n\n        case MYSQL_TYPE_BIT:\n          sv_setpvn(sv, fbh->data, fbh->length);\n\n          break;\n\n        default:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tERROR IN st_fetch_string\");\n          len= fbh->length;\n\t  /* ChopBlanks server-side prepared statement */\n          if (ChopBlanks)\n          {\n            /* \n              see bottom of:\n              http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html\n            */\n            if (fbh->charsetnr != 63)\n              while (len && fbh->data[len-1] == ' ') { --len; }\n          }\n\t  /* END OF ChopBlanks */\n\n          sv_setpvn(sv, fbh->data, len);\n\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n#if MYSQL_VERSION_ID >= FIELD_CHARSETNR_VERSION \n  /* SHOW COLLATION WHERE Id = 63; -- 63 == charset binary, collation binary */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fbh->charsetnr != 63)\n#else\n\tif ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && !(fbh->flags & BINARY_FLAG))\n#endif\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n\n        }\n\n      }\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n\n    return av;\n  }\n  else\n  {\n#endif\n\n    imp_sth->currow++;\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    {\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch result set details\\n\");\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\timp_sth->result=%p\\n\", imp_sth->result);\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_fields=%u\\n\",\n                    mysql_num_fields(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_rows=%llu\\n\",\n                    mysql_num_rows(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_affected_rows=%llu\\n\",\n                    mysql_affected_rows(imp_dbh->pmysql));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch for %p, currow= %d\\n\",\n                    sth,imp_sth->currow);\n    }\n\n    if (!(cols= mysql_fetch_row(imp_sth->result)))\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      {\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch, no more rows to fetch\");\n      }\n      if (mysql_errno(imp_dbh->pmysql))\n        do_error(sth, mysql_errno(imp_dbh->pmysql),\n                 mysql_error(imp_dbh->pmysql),\n                 mysql_sqlstate(imp_dbh->pmysql));\n\n\n#if MYSQL_VERSION_ID >= MULTIPLE_RESULT_SET_VERSION\n      if (!mysql_more_results(svsock))\n#endif\n        dbd_st_finish(sth, imp_sth);\n      return Nullav;\n    }\n\n    num_fields= mysql_num_fields(imp_sth->result);\n    fields= mysql_fetch_fields(imp_sth->result);\n    lengths= mysql_fetch_lengths(imp_sth->result);\n\n    if ((av= DBIc_FIELDS_AV(imp_sth)) != Nullav)\n    {\n      av_length= av_len(av)+1;\n\n      if (av_length != num_fields)              /* Resize array if necessary */\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, size of results array(%d) != num_fields(%d)\\n\",\n                                   av_length, num_fields);\n\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, result fields(%d)\\n\",\n                                   DBIc_NUM_FIELDS(imp_sth));\n\n        av_readonly = SvREADONLY(av);\n\n        if (av_readonly)\n          SvREADONLY_off( av );              /* DBI sets this readonly */\n\n        while (av_length < num_fields)\n        {\n          av_store(av, av_length++, newSV(0));\n        }\n\n        while (av_length > num_fields)\n        {\n          SvREFCNT_dec(av_pop(av));\n          av_length--;\n        }\n        if (av_readonly)\n          SvREADONLY_on(av);\n      }\n    }\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n\n    for (i= 0;  i < num_fields; ++i)\n    {\n      char *col= cols[i];\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n\n      if (col)\n      {\n        STRLEN len= lengths[i];\n        if (ChopBlanks)\n        {\n          while (len && col[len-1] == ' ')\n          {\t--len; }\n        }\n\n        /* Set string value returned from mysql server */\n        sv_setpvn(sv, col, len);\n\n        switch (mysql_to_perl_type(fields[i].type)) {\n        case MYSQL_TYPE_DOUBLE:\n          /* Coerce to dobule and set scalar as NV */\n          (void) SvNV(sv);\n          SvNOK_only(sv);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          /* Coerce to integer and set scalar as UV resp. IV */\n          if (fields[i].flags & UNSIGNED_FLAG)\n          {\n            (void) SvUV(sv);\n            SvIOK_only_UV(sv);\n          }\n          else\n          {\n            (void) SvIV(sv);\n            SvIOK_only(sv);\n          }\n          break;\n\n#if MYSQL_VERSION_ID > NEW_DATATYPE_VERSION\n        case MYSQL_TYPE_BIT:\n          /* Let it as binary string */\n          break;\n#endif\n\n        default:\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n  /* see bottom of: http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fields[i].charsetnr != 63)\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n        }\n      }\n      else\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n    return av;\n\n#if MYSQL_VERSION_ID  >= SERVER_PREPARE_VERSION\n  }\n#endif\n\n}", "commit_link": "github.com/perl5-dbi/DBD-mysql/commit/3619c170461a3107a258d1fd2d00ed4832adb1b1", "file_name": "dbdimp.c", "vul_type": "cwe-416", "description": "Write a Perl function to fetch a row of data from a MySQL database using the DBI module."}
{"func_name": "hfs_cat_traverse", "func_src_before": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "func_src_after": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "commit_link": "github.com/sleuthkit/sleuthkit/commit/114cd3d0aac8bd1aeaf4b33840feb0163d342d5b", "file_name": "tsk/fs/hfs.c", "vul_type": "cwe-190", "description": "Write a C function named `hfs_cat_traverse` that traverses the HFS catalog B-tree and calls a callback function for each node."}
{"func_name": "wiki_handle_rest_call", "func_src_before": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t      file_write(page, wikitext);\t      \n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "func_src_after": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t  if (page_name_is_good(page))\n\t    {\n\t      file_write(page, wikitext);\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "commit_link": "github.com/yarolig/didiwiki/commit/5e5c796617e1712905dc5462b94bd5e6c08d15ea", "file_name": "src/wiki.c", "vul_type": "cwe-022", "description": "In C, write a function to handle RESTful API calls for a wiki page system, supporting operations like get, set, delete, check existence, and list/search pages."}
{"func_name": "FromkLinuxSockAddr", "func_src_before": "bool FromkLinuxSockAddr(const struct klinux_sockaddr *input,\n                        socklen_t input_len, struct sockaddr *output,\n                        socklen_t *output_len,\n                        void (*abort_handler)(const char *)) {\n  if (!input || !output || !output_len || input_len == 0) {\n    output = nullptr;\n    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n    sockaddr_un_out.sun_family = AF_UNIX;\n    InitializeToZeroArray(sockaddr_un_out.sun_path);\n    ReinterpretCopyArray(\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n    sockaddr_in_out.sin_family = AF_INET;\n    sockaddr_in_out.sin_port = klinux_sockaddr_in_in->klinux_sin_port;\n    InitializeToZeroSingle(&sockaddr_in_out.sin_addr);\n    ReinterpretCopySingle(&sockaddr_in_out.sin_addr,\n                          &klinux_sockaddr_in_in->klinux_sin_addr);\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;\n    sockaddr_in6_out.sin6_family = AF_INET6;\n    sockaddr_in6_out.sin6_port = klinux_sockaddr_in6_in->klinux_sin6_port;\n    sockaddr_in6_out.sin6_flowinfo =\n        klinux_sockaddr_in6_in->klinux_sin6_flowinfo;\n    sockaddr_in6_out.sin6_scope_id =\n        klinux_sockaddr_in6_in->klinux_sin6_scope_id;\n    InitializeToZeroSingle(&sockaddr_in6_out.sin6_addr);\n    ReinterpretCopySingle(&sockaddr_in6_out.sin6_addr,\n                          &klinux_sockaddr_in6_in->klinux_sin6_addr);\n    CopySockaddr(&sockaddr_in6_out, sizeof(sockaddr_in6_out), output,\n                 output_len);\n  } else if (klinux_family == kLinux_AF_UNSPEC) {\n    output = nullptr;\n    *output_len = 0;\n  } else {\n    if (abort_handler != nullptr) {\n      std::string message = absl::StrCat(\n          \"Type conversion error - Unsupported AF family: \", klinux_family);\n      abort_handler(message.c_str());\n    } else {\n      abort();\n    }\n  }\n  return true;\n}", "func_src_after": "bool FromkLinuxSockAddr(const struct klinux_sockaddr *input,\n                        socklen_t input_len, struct sockaddr *output,\n                        socklen_t *output_len,\n                        void (*abort_handler)(const char *)) {\n  if (!input || !output || !output_len || input_len == 0) {\n    output = nullptr;\n    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    if (input_len < sizeof(struct klinux_sockaddr_un)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n    sockaddr_un_out.sun_family = AF_UNIX;\n    InitializeToZeroArray(sockaddr_un_out.sun_path);\n    ReinterpretCopyArray(\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    if (input_len < sizeof(struct klinux_sockaddr_in)) {\n      return false;\n    }\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n    sockaddr_in_out.sin_family = AF_INET;\n    sockaddr_in_out.sin_port = klinux_sockaddr_in_in->klinux_sin_port;\n    InitializeToZeroSingle(&sockaddr_in_out.sin_addr);\n    ReinterpretCopySingle(&sockaddr_in_out.sin_addr,\n                          &klinux_sockaddr_in_in->klinux_sin_addr);\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    if (input_len < sizeof(struct klinux_sockaddr_in6)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;\n    sockaddr_in6_out.sin6_family = AF_INET6;\n    sockaddr_in6_out.sin6_port = klinux_sockaddr_in6_in->klinux_sin6_port;\n    sockaddr_in6_out.sin6_flowinfo =\n        klinux_sockaddr_in6_in->klinux_sin6_flowinfo;\n    sockaddr_in6_out.sin6_scope_id =\n        klinux_sockaddr_in6_in->klinux_sin6_scope_id;\n    InitializeToZeroSingle(&sockaddr_in6_out.sin6_addr);\n    ReinterpretCopySingle(&sockaddr_in6_out.sin6_addr,\n                          &klinux_sockaddr_in6_in->klinux_sin6_addr);\n    CopySockaddr(&sockaddr_in6_out, sizeof(sockaddr_in6_out), output,\n                 output_len);\n  } else if (klinux_family == kLinux_AF_UNSPEC) {\n    output = nullptr;\n    *output_len = 0;\n  } else {\n    if (abort_handler != nullptr) {\n      std::string message = absl::StrCat(\n          \"Type conversion error - Unsupported AF family: \", klinux_family);\n      abort_handler(message.c_str());\n    } else {\n      abort();\n    }\n  }\n  return true;\n}", "commit_link": "github.com/google/asylo/commit/bda9772e7872b0d2b9bee32930cf7a4983837b39", "file_name": "asylo/platform/system_call/type_conversions/manual_types_functions.cc", "vul_type": "cwe-787", "description": "Write a C++ function to convert a Linux-specific socket address structure to a generic socket address structure, handling different address families and providing an abort callback for errors."}
{"func_name": "_keyify", "func_src_before": "def _keyify(key):\n    return _key_pattern.sub(' ', key.lower())", "func_src_after": "def _keyify(key):\n    key = escape(key.lower(), quote=True)\n    return _key_pattern.sub(' ', key)", "commit_link": "github.com/lepture/mistune/commit/5f06d724bc05580e7f203db2d4a4905fc1127f98", "file_name": "mistune.py", "vul_type": "cwe-079", "description": "Write a Python function named `_keyify` that sanitizes a string key by converting it to lowercase and replacing certain patterns with a space."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/dscho/hg/commit/bfe415fca85c8dcfcdb91347c4fffb4cf43e302e", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "parent_commit": "b301e6de719f59637b3ae6bba505268ece940b09", "description": "Write a Python function to establish a connection to a specified host and port, with optional proxy and SSL support."}
{"func_name": "render_page_edit", "func_src_before": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "func_src_after": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to display the latest content of a wiki page for editing, using a SQL query to retrieve the data."}
{"func_name": "login", "func_src_before": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                               password=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Not activated\" % username)\n    except NoResultFound:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username or Password wrong\" % username)\n    return None", "func_src_after": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n                if passwords_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n                log.info(\"Login failed for user '%s'. \"\n                         \"Reason: Not activated\" % username)\n        else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username not known\" % username)\n    return None", "line_changes": {"deleted": [{"line_no": 10, "char_start": 279, "char_end": 306, "line": "    md5_pw = hashlib.md5()\n"}, {"line_no": 11, "char_start": 306, "char_end": 340, "line": "    md5_pw.update(password or \"\")\n"}, {"line_no": 12, "char_start": 340, "char_end": 372, "line": "    md5_pw = md5_pw.hexdigest()\n"}, {"line_no": 14, "char_start": 441, "char_end": 450, "line": "    try:\n"}, {"line_no": 15, "char_start": 450, "char_end": 513, "line": "        user = DBSession.query(User).filter_by(login=username,\n"}, {"line_no": 16, "char_start": 513, "char_end": 583, "line": "                                               password=md5_pw).one()\n"}, {"line_no": 17, "char_start": 583, "char_end": 610, "line": "        if user.activated:\n"}, {"line_no": 18, "char_start": 610, "char_end": 670, "line": "            log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 19, "char_start": 670, "char_end": 694, "line": "            return user\n"}, {"line_no": 20, "char_start": 694, "char_end": 742, "line": "        log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 21, "char_start": 742, "char_end": 795, "line": "                 \"Reason: Not activated\" % username)\n"}, {"line_no": 22, "char_start": 795, "char_end": 821, "line": "    except NoResultFound:\n"}, {"line_no": 24, "char_start": 869, "char_end": 935, "line": "                 \"Reason: Username or Password wrong\" % username)\n"}], "added": [{"line_no": 11, "char_start": 348, "char_end": 379, "line": "    user = load_user(username)\n"}, {"line_no": 12, "char_start": 379, "char_end": 392, "line": "    if user:\n"}, {"line_no": 13, "char_start": 392, "char_end": 445, "line": "        if verify_password(password, user.password):\n"}, {"line_no": 14, "char_start": 445, "char_end": 476, "line": "            if user.activated:\n"}, {"line_no": 15, "char_start": 476, "char_end": 540, "line": "                log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 16, "char_start": 540, "char_end": 598, "line": "                if passwords_needs_update(user.password):\n"}, {"line_no": 17, "char_start": 598, "char_end": 675, "line": "                    log.info(\"Updating password for user '%s'\" % (username))\n"}, {"line_no": 18, "char_start": 675, "char_end": 739, "line": "                    user.password = encrypt_password(password) \n"}, {"line_no": 19, "char_start": 739, "char_end": 767, "line": "                return user\n"}, {"line_no": 20, "char_start": 767, "char_end": 785, "line": "            else:\n"}, {"line_no": 21, "char_start": 785, "char_end": 841, "line": "                log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 22, "char_start": 841, "char_end": 902, "line": "                         \"Reason: Not activated\" % username)\n"}, {"line_no": 23, "char_start": 902, "char_end": 916, "line": "        else:\n"}, {"line_no": 24, "char_start": 916, "char_end": 968, "line": "            log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 25, "char_start": 968, "char_end": 1026, "line": "                     \"Reason: Wrong password\" % username)\n"}, {"line_no": 26, "char_start": 1026, "char_end": 1036, "line": "    else:\n"}, {"line_no": 28, "char_start": 1084, "char_end": 1142, "line": "                 \"Reason: Username not known\" % username)\n"}]}, "char_changes": {"deleted": [{"char_start": 283, "char_end": 545, "chars": "md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                "}, {"char_start": 568, "char_end": 694, "chars": "=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n"}, {"char_start": 799, "char_end": 819, "chars": "except NoResultFound"}, {"char_start": 904, "char_end": 921, "chars": "or Password wrong"}], "added": [{"char_start": 283, "char_end": 540, "chars": "log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n"}, {"char_start": 555, "char_end": 559, "chars": " if "}, {"char_start": 567, "char_end": 793, "chars": "s_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n        "}, {"char_start": 858, "char_end": 863, "chars": "     "}, {"char_start": 863, "char_end": 866, "chars": "   "}, {"char_start": 906, "char_end": 1034, "chars": "    else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else"}, {"char_start": 1119, "char_end": 1128, "chars": "not known"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/8e92641fee542f6e7004e827136dea3ce5e99eb2", "file_name": "security.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new\nencryption methods using passlib. Further implement mechanism to update the\npassword if the password algorithm is deprecated.", "parent_commit": "8cfe035de8fc493923385093cc7f5c5455fb08f9", "description": "Write a Python function named `login` that checks a user's credentials and returns the user object if authenticated or `None` otherwise."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "func_src_after": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 34, "char_end": 172, "line": "\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}], "added": [{"line_no": 2, "char_start": 34, "char_end": 196, "line": "\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}]}, "char_changes": {"deleted": [{"char_start": 73, "char_end": 75, "chars": "{`"}, {"char_start": 97, "char_end": 99, "chars": "`}"}], "added": [{"char_start": 73, "char_end": 74, "chars": "\""}, {"char_start": 96, "char_end": 97, "chars": "\""}, {"char_start": 113, "char_end": 139, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/MyHomeworkSpace/client/commit/ec5d9dd6c12b12bb89de7ed96fadc37e45710396", "file_name": "CalendarEventPopover.jsx", "vul_type": "cwe-200", "commit_msg": "add noopener noreferrer to event actions", "parent_commit": "f248047d17897e99f2ae3a9a7b7bbb7a6f885e8f", "description": "Generate a React component in JavaScript that maps over an array of action objects to create a list of anchor elements with icons and names."}
{"func_name": "get_requested_day", "func_src_before": "    def get_requested_day(self, date):\n\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, SUM(Power) AS Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN %s AND %s \n            GROUP BY TimeStamp;\n        '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (day_start, day_end)):\n            data['data'].append({ 'time': row[0], 'power': row[1] })\n\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT SUM(EToday) as EToday\n                FROM Inverters;\n                '''\n        else:\n            query = '''\n                SELECT SUM(DayYield) AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN %s AND %s\n                GROUP BY TimeStamp\n                ''' % (day_start, day_end)\n        self.c.execute(query)\n        row = self.c.fetchone()\n        if row and row[0]: data['total'] = row[0]\n        else: data['total'] = 0\n\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data):  data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        #print(json.dumps(data, indent=4))\n        return data", "func_src_after": "    def get_requested_day(self, date):\n\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, SUM(Power) AS Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN ? AND ?\n            GROUP BY TimeStamp;\n        '''\n\n        data['data'] = list()\n        for row in self.c.execute(query, (day_start, day_end)):\n            data['data'].append({ 'time': row[0], 'power': row[1] })\n\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT SUM(EToday) as EToday\n                FROM Inverters;\n                '''\n            self.c.execute(query)\n        else:\n            query = '''\n                SELECT SUM(DayYield) AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN ? AND ?\n                GROUP BY TimeStamp;\n                '''\n            self.c.execute(query, (day_start, day_end))\n\n        row = self.c.fetchone()\n        if row and row[0]: data['total'] = row[0]\n        else: data['total'] = 0\n\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data):  data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        #print(json.dumps(data, indent=4))\n        return data", "commit_link": "github.com/philipptrenz/sunportal/commit/7eef493a168ed4e6731ff800713bfb8aee99a506", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves and summarizes power data for a given day, including checking for previous and next day data availability."}
{"func_name": "PHP_FUNCTION", "func_src_before": "PHPAPI PHP_FUNCTION(fread)\n{\n\tzval *arg1;\n\tlong len;\n\tphp_stream *stream;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rl\", &arg1, &len) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tPHP_STREAM_TO_ZVAL(stream, &arg1);\n\n\tif (len <= 0) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be greater than 0\");\n\t\tRETURN_FALSE;\n\t}\n\n\tZ_STRVAL_P(return_value) = emalloc(len + 1);\n\tZ_STRLEN_P(return_value) = php_stream_read(stream, Z_STRVAL_P(return_value), len);\n\n\t/* needed because recv/read/gzread doesnt put a null at the end*/\n\tZ_STRVAL_P(return_value)[Z_STRLEN_P(return_value)] = 0;\n\tZ_TYPE_P(return_value) = IS_STRING;\n}", "func_src_after": "PHPAPI PHP_FUNCTION(fread)\n{\n\tzval *arg1;\n\tlong len;\n\tphp_stream *stream;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rl\", &arg1, &len) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tPHP_STREAM_TO_ZVAL(stream, &arg1);\n\n\tif (len <= 0) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be greater than 0\");\n\t\tRETURN_FALSE;\n\t}\n\n\tif (len > INT_MAX) {\n\t\t/* string length is int in 5.x so we can not read more than int */\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be no more than %d\", INT_MAX);\n\t\tRETURN_FALSE;\n\t}\n\n\tZ_STRVAL_P(return_value) = emalloc(len + 1);\n\tZ_STRLEN_P(return_value) = php_stream_read(stream, Z_STRVAL_P(return_value), len);\n\n\t/* needed because recv/read/gzread doesnt put a null at the end*/\n\tZ_STRVAL_P(return_value)[Z_STRLEN_P(return_value)] = 0;\n\tZ_TYPE_P(return_value) = IS_STRING;\n}", "commit_link": "github.com/php/php-src/commit/abd159cce48f3e34f08e4751c568e09677d5ec9c", "file_name": "ext/standard/file.c", "vul_type": "cwe-190", "description": "Write a PHP function to read a specified number of bytes from a stream resource."}
{"func_name": "(anonymous)", "func_src_before": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', url);\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "func_src_after": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', eHtml(url));\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "line_changes": {"deleted": [{"line_no": 7, "char_start": 280, "char_end": 317, "line": "            elBtn.attr('href', url);\n"}], "added": [{"line_no": 7, "char_start": 280, "char_end": 324, "line": "            elBtn.attr('href', eHtml(url));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 311, "char_end": 317, "chars": "eHtml("}, {"char_start": 321, "char_end": 322, "chars": ")"}]}, "commit_link": "github.com/theoboldt/juvem/commit/9af3a9f58dc11dd7cacae93f00fbc5ed2e3580da", "file_name": "attendance.js", "vul_type": "cwe-079", "commit_msg": "Preventing js/xss-through-dom vulnerability", "description": "Create a JavaScript function to handle a button click by setting the button's href attribute based on a selected input value and then trigger a download action."}
{"func_name": "NeXTDecode", "func_src_before": "NeXTDecode(TIFF* tif, uint8* buf, tmsize_t occ, uint16 s)\n{\n\tstatic const char module[] = \"NeXTDecode\";\n\tunsigned char *bp, *op;\n\ttmsize_t cc;\n\tuint8* row;\n\ttmsize_t scanline, n;\n\n\t(void) s;\n\t/*\n\t * Each scanline is assumed to start off as all\n\t * white (we assume a PhotometricInterpretation\n\t * of ``min-is-black'').\n\t */\n\tfor (op = (unsigned char*) buf, cc = occ; cc-- > 0;)\n\t\t*op++ = 0xff;\n\n\tbp = (unsigned char *)tif->tif_rawcp;\n\tcc = tif->tif_rawcc;\n\tscanline = tif->tif_scanlinesize;\n\tif (occ % scanline)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata, module, \"Fractional scanlines cannot be read\");\n\t\treturn (0);\n\t}\n\tfor (row = buf; cc > 0 && occ > 0; occ -= scanline, row += scanline) {\n\t\tn = *bp++, cc--;\n\t\tswitch (n) {\n\t\tcase LITERALROW:\n\t\t\t/*\n\t\t\t * The entire scanline is given as literal values.\n\t\t\t */\n\t\t\tif (cc < scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row, bp, scanline);\n\t\t\tbp += scanline;\n\t\t\tcc -= scanline;\n\t\t\tbreak;\n\t\tcase LITERALSPAN: {\n\t\t\ttmsize_t off;\n\t\t\t/*\n\t\t\t * The scanline has a literal span that begins at some\n\t\t\t * offset.\n\t\t\t */\n\t\t\tif( cc < 4 )\n\t\t\t\tgoto bad;\n\t\t\toff = (bp[0] * 256) + bp[1];\n\t\t\tn = (bp[2] * 256) + bp[3];\n\t\t\tif (cc < 4+n || off+n > scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row+off, bp+4, n);\n\t\t\tbp += 4+n;\n\t\t\tcc -= 4+n;\n\t\t\tbreak;\n\t\t}\n\t\tdefault: {\n\t\t\tuint32 npixels = 0, grey;\n\t\t\tuint32 imagewidth = tif->tif_dir.td_imagewidth;\n            if( isTiled(tif) )\n                imagewidth = tif->tif_dir.td_tilewidth;\n\n\t\t\t/*\n\t\t\t * The scanline is composed of a sequence of constant\n\t\t\t * color ``runs''.  We shift into ``run mode'' and\n\t\t\t * interpret bytes as codes of the form\n\t\t\t * <color><npixels> until we've filled the scanline.\n\t\t\t */\n\t\t\top = row;\n\t\t\tfor (;;) {\n\t\t\t\tgrey = (uint32)((n>>6) & 0x3);\n\t\t\t\tn &= 0x3f;\n\t\t\t\t/*\n\t\t\t\t * Ensure the run does not exceed the scanline\n\t\t\t\t * bounds, potentially resulting in a security\n\t\t\t\t * issue.\n\t\t\t\t */\n\t\t\t\twhile (n-- > 0 && npixels < imagewidth)\n\t\t\t\t\tSETPIXEL(op, grey);\n\t\t\t\tif (npixels >= imagewidth)\n\t\t\t\t\tbreak;\n\t\t\t\tif (cc == 0)\n\t\t\t\t\tgoto bad;\n\t\t\t\tn = *bp++, cc--;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t}\n\t}\n\ttif->tif_rawcp = (uint8*) bp;\n\ttif->tif_rawcc = cc;\n\treturn (1);\nbad:\n\tTIFFErrorExt(tif->tif_clientdata, module, \"Not enough data for scanline %ld\",\n\t    (long) tif->tif_row);\n\treturn (0);\n}", "func_src_after": "NeXTDecode(TIFF* tif, uint8* buf, tmsize_t occ, uint16 s)\n{\n\tstatic const char module[] = \"NeXTDecode\";\n\tunsigned char *bp, *op;\n\ttmsize_t cc;\n\tuint8* row;\n\ttmsize_t scanline, n;\n\n\t(void) s;\n\t/*\n\t * Each scanline is assumed to start off as all\n\t * white (we assume a PhotometricInterpretation\n\t * of ``min-is-black'').\n\t */\n\tfor (op = (unsigned char*) buf, cc = occ; cc-- > 0;)\n\t\t*op++ = 0xff;\n\n\tbp = (unsigned char *)tif->tif_rawcp;\n\tcc = tif->tif_rawcc;\n\tscanline = tif->tif_scanlinesize;\n\tif (occ % scanline)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata, module, \"Fractional scanlines cannot be read\");\n\t\treturn (0);\n\t}\n\tfor (row = buf; cc > 0 && occ > 0; occ -= scanline, row += scanline) {\n\t\tn = *bp++, cc--;\n\t\tswitch (n) {\n\t\tcase LITERALROW:\n\t\t\t/*\n\t\t\t * The entire scanline is given as literal values.\n\t\t\t */\n\t\t\tif (cc < scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row, bp, scanline);\n\t\t\tbp += scanline;\n\t\t\tcc -= scanline;\n\t\t\tbreak;\n\t\tcase LITERALSPAN: {\n\t\t\ttmsize_t off;\n\t\t\t/*\n\t\t\t * The scanline has a literal span that begins at some\n\t\t\t * offset.\n\t\t\t */\n\t\t\tif( cc < 4 )\n\t\t\t\tgoto bad;\n\t\t\toff = (bp[0] * 256) + bp[1];\n\t\t\tn = (bp[2] * 256) + bp[3];\n\t\t\tif (cc < 4+n || off+n > scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row+off, bp+4, n);\n\t\t\tbp += 4+n;\n\t\t\tcc -= 4+n;\n\t\t\tbreak;\n\t\t}\n\t\tdefault: {\n\t\t\tuint32 npixels = 0, grey;\n\t\t\tuint32 imagewidth = tif->tif_dir.td_imagewidth;\n            if( isTiled(tif) )\n                imagewidth = tif->tif_dir.td_tilewidth;\n            tmsize_t op_offset = 0;\n\n\t\t\t/*\n\t\t\t * The scanline is composed of a sequence of constant\n\t\t\t * color ``runs''.  We shift into ``run mode'' and\n\t\t\t * interpret bytes as codes of the form\n\t\t\t * <color><npixels> until we've filled the scanline.\n\t\t\t */\n\t\t\top = row;\n\t\t\tfor (;;) {\n\t\t\t\tgrey = (uint32)((n>>6) & 0x3);\n\t\t\t\tn &= 0x3f;\n\t\t\t\t/*\n\t\t\t\t * Ensure the run does not exceed the scanline\n\t\t\t\t * bounds, potentially resulting in a security\n\t\t\t\t * issue.\n\t\t\t\t */\n\t\t\t\twhile (n-- > 0 && npixels < imagewidth && op_offset < scanline)\n\t\t\t\t\tSETPIXEL(op, grey);\n\t\t\t\tif (npixels >= imagewidth)\n\t\t\t\t\tbreak;\n                if (op_offset >= scanline ) {\n                    TIFFErrorExt(tif->tif_clientdata, module, \"Invalid data for scanline %ld\",\n                        (long) tif->tif_row);\n                    return (0);\n                }\n\t\t\t\tif (cc == 0)\n\t\t\t\t\tgoto bad;\n\t\t\t\tn = *bp++, cc--;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t}\n\t}\n\ttif->tif_rawcp = (uint8*) bp;\n\ttif->tif_rawcc = cc;\n\treturn (1);\nbad:\n\tTIFFErrorExt(tif->tif_clientdata, module, \"Not enough data for scanline %ld\",\n\t    (long) tif->tif_row);\n\treturn (0);\n}", "commit_link": "github.com/vadz/libtiff/commit/b18012dae552f85dcc5c57d3bf4e997a15b1cc1c", "file_name": "libtiff/tif_next.c", "vul_type": "cwe-787", "description": "Write a C function named `NeXTDecode` that decodes a scanline from a TIFF image."}
{"func_name": "(anonymous)", "func_src_before": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n\t\t\t\t\t +'</tr>');\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "func_src_after": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 113, "line": "\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n"}, {"line_no": 3, "char_start": 113, "char_end": 212, "line": "\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n"}, {"line_no": 4, "char_start": 212, "char_end": 229, "line": "\t\t\t\t\t +'</tr>');\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 122, "line": "\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n"}, {"line_no": 3, "char_start": 122, "char_end": 187, "line": "\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n"}, {"line_no": 4, "char_start": 187, "char_end": 263, "line": "\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 81, "chars": " data-dir=\"'+dir+'\" "}, {"char_start": 94, "char_end": 98, "chars": "=\"'+"}, {"char_start": 107, "char_end": 126, "chars": "+'\">'\n\t\t\t\t\t\t+'<td c"}, {"char_start": 130, "char_end": 132, "chars": "=\""}, {"char_start": 140, "char_end": 144, "chars": "\">'+"}, {"char_start": 153, "char_end": 154, "chars": "+"}, {"char_start": 156, "char_end": 157, "chars": "/"}, {"char_start": 159, "char_end": 165, "chars": "><td c"}, {"char_start": 169, "char_end": 171, "chars": "=\""}, {"char_start": 175, "char_end": 179, "chars": "\">'+"}, {"char_start": 203, "char_end": 226, "chars": "+'</td>'\n\t\t\t\t\t +'</tr>'"}], "added": [{"char_start": 61, "char_end": 94, "chars": "/>').attr('data-dir', dir).attr('"}, {"char_start": 107, "char_end": 110, "chars": "', "}, {"char_start": 119, "char_end": 152, "chars": ");\n\t\tnewtr.append($('<td/>').addC"}, {"char_start": 156, "char_end": 158, "chars": "('"}, {"char_start": 166, "char_end": 174, "chars": "').text("}, {"char_start": 183, "char_end": 204, "chars": "));\n\t\tnewtr.append($("}, {"char_start": 208, "char_end": 217, "chars": "/>').addC"}, {"char_start": 221, "char_end": 223, "chars": "('"}, {"char_start": 227, "char_end": 235, "chars": "').text("}, {"char_start": 259, "char_end": 260, "chars": ")"}]}, "commit_link": "github.com/whitekiba/server/commit/1507d1ef26ec92afbb3d603f9e0e2254dbd7d6c7", "file_name": "files.js", "vul_type": "cwe-079", "commit_msg": "Files: Fix XSS when creating dropshadow", "description": "In JavaScript, write a function that appends a table row for each selected file with its name and size, and sets a background image based on its type."}
{"func_name": "fpm_log_write", "func_src_before": "int fpm_log_write(char *log_format) /* {{{ */\n{\n\tchar *s, *b;\n\tchar buffer[FPM_LOG_BUFFER+1];\n\tint token, test;\n\tsize_t len, len2;\n\tstruct fpm_scoreboard_proc_s proc, *proc_p;\n\tstruct fpm_scoreboard_s *scoreboard;\n\tchar tmp[129];\n\tchar format[129];\n\ttime_t now_epoch;\n#ifdef HAVE_TIMES\n\tclock_t tms_total;\n#endif\n\n\tif (!log_format && (!fpm_log_format || fpm_log_fd == -1)) {\n\t\treturn -1;\n\t}\n\n\tif (!log_format) {\n\t\tlog_format = fpm_log_format;\n\t\ttest = 0;\n\t} else {\n\t\ttest = 1;\n\t}\n\n\tnow_epoch = time(NULL);\n\n\tif (!test) {\n\t\tscoreboard = fpm_scoreboard_get();\n\t\tif (!scoreboard) {\n\t\t\tzlog(ZLOG_WARNING, \"unable to get scoreboard while preparing the access log\");\n\t\t\treturn -1;\n\t\t}\n\t\tproc_p = fpm_scoreboard_proc_acquire(NULL, -1, 0);\n\t\tif (!proc_p) {\n\t\t\tzlog(ZLOG_WARNING, \"[pool %s] Unable to acquire shm slot while preparing the access log\", scoreboard->pool);\n\t\t\treturn -1;\n\t\t}\n\t\tproc = *proc_p;\n\t\tfpm_scoreboard_proc_release(proc_p);\n\t}\n\n\ttoken = 0;\n\n\tmemset(buffer, '\\0', sizeof(buffer));\n\tb = buffer;\n\tlen = 0;\n\n\n\ts = log_format;\n\n\twhile (*s != '\\0') {\n\t\t/* Test is we have place for 1 more char. */\n\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!token && *s == '%') {\n\t\t\ttoken = 1;\n\t\t\tmemset(format, '\\0', sizeof(format)); /* reset format */\n\t\t\ts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (token) {\n\t\t\ttoken = 0;\n\t\t\tlen2 = 0;\n\t\t\tswitch (*s) {\n\n\t\t\t\tcase '%': /* '%' */\n\t\t\t\t\t*b = '%';\n\t\t\t\t\tlen2 = 1;\n\t\t\t\t\tbreak;\n\n#ifdef HAVE_TIMES\n\t\t\t\tcase 'C': /* %CPU */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"total\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cutime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"user\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_cutime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"system\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'total', 'user' or 'system' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.2f\", tms_total / fpm_scoreboard_get_tick() / (proc.cpu_duration.tv_sec + proc.cpu_duration.tv_usec / 1000000.) * 100.);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n#endif\n\n\t\t\t\tcase 'd': /* duration \u00b5s */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"seconds\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec + proc.duration.tv_usec / 1000000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* miliseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"miliseconds\") || !strcasecmp(format, \"mili\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec * 1000. + proc.duration.tv_usec / 1000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* microseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"microseconds\") || !strcasecmp(format, \"micro\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.duration.tv_sec * 1000000UL + proc.duration.tv_usec);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'seconds', 'mili', 'miliseconds', 'micro' or 'microseconds' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'e': /* fastcgi env  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the environment variable must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tchar *env = fcgi_getenv((fcgi_request*) SG(server_context), format, strlen(format));\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", env ? env : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'f': /* script */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\",  *proc.script_filename ? proc.script_filename : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'l': /* content length */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.content_length);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'm': /* method */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.request_method ? proc.request_method : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'M': /* memory */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"bytes\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.memory);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* kilobytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"kilobytes\") || !strcasecmp(format, \"kilo\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* megabytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"megabytes\") || !strcasecmp(format, \"mega\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024 / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'bytes', 'kilo', 'kilobytes', 'mega' or 'megabytes' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'n': /* pool name */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", scoreboard->pool[0] ? scoreboard->pool : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'o': /* header output  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the header must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tsapi_header_struct *h;\n\t\t\t\t\t\tzend_llist_position pos;\n\t\t\t\t\t\tsapi_headers_struct *sapi_headers = &SG(sapi_headers);\n\t\t\t\t\t\tsize_t format_len = strlen(format);\n\n\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_first_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\twhile (h) {\n\t\t\t\t\t\t\tchar *header;\n\t\t\t\t\t\t\tif (!h->header_len) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (!strstr(h->header, format)) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t/* test if enought char after the header name + ': ' */\n\t\t\t\t\t\t\tif (h->header_len <= format_len + 2) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (h->header[format_len] != ':' || h->header[format_len + 1] != ' ') {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\theader = h->header + format_len + 2;\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", header && *header ? header : \"-\");\n\n\t\t\t\t\t\t\t/* found, done */\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!len2) {\n\t\t\t\t\t\t\tlen2 = 1;\n\t\t\t\t\t\t\t*b = '-';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'p': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getpid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'P': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getppid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'q': /* query_string */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.query_string);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'Q': /* '?' */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.query_string  ? \"?\" : \"\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'r': /* request URI */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.request_uri);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'R': /* remote IP address */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tconst char *tmp = fcgi_get_last_client_ip();\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp ? tmp : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 's': /* status */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%d\", SG(sapi_headers).http_response_code);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'T':\n\t\t\t\tcase 't': /* time */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\ttime_t *t;\n\t\t\t\t\t\tif (*s == 't') {\n\t\t\t\t\t\t\tt = &proc.accepted_epoch;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt = &now_epoch;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, \"%d/%b/%Y:%H:%M:%S %z\", localtime(t));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, format, localtime(t));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp);\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'u': /* remote user */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.auth_user);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase '{': /* complex var */\n\t\t\t\t\ttoken = 1;\n\t\t\t\t\t{\n\t\t\t\t\t\tchar *start;\n\t\t\t\t\t\tsize_t l;\n\n\t\t\t\t\t\tstart = ++s;\n\n\t\t\t\t\t\twhile (*s != '\\0') {\n\t\t\t\t\t\t\tif (*s == '}') {\n\t\t\t\t\t\t\t\tl = s - start;\n\n\t\t\t\t\t\t\t\tif (l >= sizeof(format) - 1) {\n\t\t\t\t\t\t\t\t\tl = sizeof(format) - 1;\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tmemcpy(format, start, l);\n\t\t\t\t\t\t\t\tformat[l] = '\\0';\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (s[1] == '\\0') {\n\t\t\t\t\t\t\tzlog(ZLOG_WARNING, \"missing closing embrace in the access.format\");\n\t\t\t\t\t\t\treturn -1;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tzlog(ZLOG_WARNING, \"Invalid token in the access.format (%%%c)\", *s);\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (*s != '}' && format[0] != '\\0') {\n\t\t\t\tzlog(ZLOG_WARNING, \"embrace is not allowed for modifier %%%c\", *s);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ts++;\n\t\t\tif (!test) {\n\t\t\t\tb += len2;\n\t\t\t\tlen += len2;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test) {\n\t\t\t// push the normal char to the output buffer\n\t\t\t*b = *s;\n\t\t\tb++;\n\t\t\tlen++;\n\t\t}\n\t\ts++;\n\t}\n\n\tif (!test && strlen(buffer) > 0) {\n\t\tbuffer[len] = '\\n';\n\t\twrite(fpm_log_fd, buffer, len + 1);\n\t}\n\n\treturn 0;\n}", "func_src_after": "int fpm_log_write(char *log_format) /* {{{ */\n{\n\tchar *s, *b;\n\tchar buffer[FPM_LOG_BUFFER+1];\n\tint token, test;\n\tsize_t len, len2;\n\tstruct fpm_scoreboard_proc_s proc, *proc_p;\n\tstruct fpm_scoreboard_s *scoreboard;\n\tchar tmp[129];\n\tchar format[129];\n\ttime_t now_epoch;\n#ifdef HAVE_TIMES\n\tclock_t tms_total;\n#endif\n\n\tif (!log_format && (!fpm_log_format || fpm_log_fd == -1)) {\n\t\treturn -1;\n\t}\n\n\tif (!log_format) {\n\t\tlog_format = fpm_log_format;\n\t\ttest = 0;\n\t} else {\n\t\ttest = 1;\n\t}\n\n\tnow_epoch = time(NULL);\n\n\tif (!test) {\n\t\tscoreboard = fpm_scoreboard_get();\n\t\tif (!scoreboard) {\n\t\t\tzlog(ZLOG_WARNING, \"unable to get scoreboard while preparing the access log\");\n\t\t\treturn -1;\n\t\t}\n\t\tproc_p = fpm_scoreboard_proc_acquire(NULL, -1, 0);\n\t\tif (!proc_p) {\n\t\t\tzlog(ZLOG_WARNING, \"[pool %s] Unable to acquire shm slot while preparing the access log\", scoreboard->pool);\n\t\t\treturn -1;\n\t\t}\n\t\tproc = *proc_p;\n\t\tfpm_scoreboard_proc_release(proc_p);\n\t}\n\n\ttoken = 0;\n\n\tmemset(buffer, '\\0', sizeof(buffer));\n\tb = buffer;\n\tlen = 0;\n\n\n\ts = log_format;\n\n\twhile (*s != '\\0') {\n\t\t/* Test is we have place for 1 more char. */\n\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!token && *s == '%') {\n\t\t\ttoken = 1;\n\t\t\tmemset(format, '\\0', sizeof(format)); /* reset format */\n\t\t\ts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (token) {\n\t\t\ttoken = 0;\n\t\t\tlen2 = 0;\n\t\t\tswitch (*s) {\n\n\t\t\t\tcase '%': /* '%' */\n\t\t\t\t\t*b = '%';\n\t\t\t\t\tlen2 = 1;\n\t\t\t\t\tbreak;\n\n#ifdef HAVE_TIMES\n\t\t\t\tcase 'C': /* %CPU */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"total\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cutime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"user\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_cutime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"system\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'total', 'user' or 'system' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.2f\", tms_total / fpm_scoreboard_get_tick() / (proc.cpu_duration.tv_sec + proc.cpu_duration.tv_usec / 1000000.) * 100.);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n#endif\n\n\t\t\t\tcase 'd': /* duration \u00b5s */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"seconds\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec + proc.duration.tv_usec / 1000000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* miliseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"miliseconds\") || !strcasecmp(format, \"mili\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec * 1000. + proc.duration.tv_usec / 1000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* microseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"microseconds\") || !strcasecmp(format, \"micro\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.duration.tv_sec * 1000000UL + proc.duration.tv_usec);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'seconds', 'mili', 'miliseconds', 'micro' or 'microseconds' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'e': /* fastcgi env  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the environment variable must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tchar *env = fcgi_getenv((fcgi_request*) SG(server_context), format, strlen(format));\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", env ? env : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'f': /* script */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\",  *proc.script_filename ? proc.script_filename : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'l': /* content length */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.content_length);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'm': /* method */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.request_method ? proc.request_method : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'M': /* memory */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"bytes\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.memory);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* kilobytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"kilobytes\") || !strcasecmp(format, \"kilo\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* megabytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"megabytes\") || !strcasecmp(format, \"mega\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024 / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'bytes', 'kilo', 'kilobytes', 'mega' or 'megabytes' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'n': /* pool name */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", scoreboard->pool[0] ? scoreboard->pool : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'o': /* header output  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the header must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tsapi_header_struct *h;\n\t\t\t\t\t\tzend_llist_position pos;\n\t\t\t\t\t\tsapi_headers_struct *sapi_headers = &SG(sapi_headers);\n\t\t\t\t\t\tsize_t format_len = strlen(format);\n\n\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_first_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\twhile (h) {\n\t\t\t\t\t\t\tchar *header;\n\t\t\t\t\t\t\tif (!h->header_len) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (!strstr(h->header, format)) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t/* test if enought char after the header name + ': ' */\n\t\t\t\t\t\t\tif (h->header_len <= format_len + 2) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (h->header[format_len] != ':' || h->header[format_len + 1] != ' ') {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\theader = h->header + format_len + 2;\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", header && *header ? header : \"-\");\n\n\t\t\t\t\t\t\t/* found, done */\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!len2) {\n\t\t\t\t\t\t\tlen2 = 1;\n\t\t\t\t\t\t\t*b = '-';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'p': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getpid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'P': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getppid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'q': /* query_string */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.query_string);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'Q': /* '?' */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.query_string  ? \"?\" : \"\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'r': /* request URI */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.request_uri);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'R': /* remote IP address */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tconst char *tmp = fcgi_get_last_client_ip();\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp ? tmp : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 's': /* status */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%d\", SG(sapi_headers).http_response_code);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'T':\n\t\t\t\tcase 't': /* time */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\ttime_t *t;\n\t\t\t\t\t\tif (*s == 't') {\n\t\t\t\t\t\t\tt = &proc.accepted_epoch;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt = &now_epoch;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, \"%d/%b/%Y:%H:%M:%S %z\", localtime(t));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, format, localtime(t));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp);\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'u': /* remote user */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.auth_user);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase '{': /* complex var */\n\t\t\t\t\ttoken = 1;\n\t\t\t\t\t{\n\t\t\t\t\t\tchar *start;\n\t\t\t\t\t\tsize_t l;\n\n\t\t\t\t\t\tstart = ++s;\n\n\t\t\t\t\t\twhile (*s != '\\0') {\n\t\t\t\t\t\t\tif (*s == '}') {\n\t\t\t\t\t\t\t\tl = s - start;\n\n\t\t\t\t\t\t\t\tif (l >= sizeof(format) - 1) {\n\t\t\t\t\t\t\t\t\tl = sizeof(format) - 1;\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tmemcpy(format, start, l);\n\t\t\t\t\t\t\t\tformat[l] = '\\0';\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (s[1] == '\\0') {\n\t\t\t\t\t\t\tzlog(ZLOG_WARNING, \"missing closing embrace in the access.format\");\n\t\t\t\t\t\t\treturn -1;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tzlog(ZLOG_WARNING, \"Invalid token in the access.format (%%%c)\", *s);\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (*s != '}' && format[0] != '\\0') {\n\t\t\t\tzlog(ZLOG_WARNING, \"embrace is not allowed for modifier %%%c\", *s);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ts++;\n\t\t\tif (!test) {\n\t\t\t\tb += len2;\n\t\t\t\tlen += len2;\n\t\t\t}\n\t\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test) {\n\t\t\t// push the normal char to the output buffer\n\t\t\t*b = *s;\n\t\t\tb++;\n\t\t\tlen++;\n\t\t}\n\t\ts++;\n\t}\n\n\tif (!test && strlen(buffer) > 0) {\n\t\tbuffer[len] = '\\n';\n\t\twrite(fpm_log_fd, buffer, len + 1);\n\t}\n\n\treturn 0;\n}", "commit_link": "github.com/php/php-src/commit/2721a0148649e07ed74468f097a28899741eb58f", "file_name": "sapi/fpm/fpm/fpm_log.c", "vul_type": "cwe-125", "description": "Write a C function named `fpm_log_write` that processes a log format string and writes the formatted log entry to a file."}
{"func_name": "check_max_open_files", "func_src_before": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = len([\n        key for key in os.listdir(accepted_keys_dir) if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ])\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "func_src_after": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 610, "char_end": 637, "line": "    accepted_count = len([\n"}, {"line_no": 16, "char_start": 637, "char_end": 693, "line": "        key for key in os.listdir(accepted_keys_dir) if\n"}, {"line_no": 17, "char_start": 693, "char_end": 754, "line": "        os.path.isfile(os.path.join(accepted_keys_dir, key))\n"}, {"line_no": 18, "char_start": 754, "char_end": 761, "line": "    ])\n"}], "added": [{"line_no": 15, "char_start": 610, "char_end": 677, "line": "    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n"}]}, "char_changes": {"deleted": [{"char_start": 631, "char_end": 648, "chars": "len([\n        key"}, {"char_start": 653, "char_end": 656, "chars": "key"}, {"char_start": 689, "char_end": 759, "chars": " if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ]"}], "added": [{"char_start": 631, "char_end": 636, "chars": "sum(1"}, {"char_start": 641, "char_end": 642, "chars": "_"}]}, "commit_link": "github.com/saltstack/salt/commit/887017fa0a5b691b40dbc1831cd4472e88157733", "file_name": "verify.py", "vul_type": "cwe-022", "commit_msg": "Do not waste CPU cycles on stat(2) on each _auth\n\nCurrently servers that handle many thousands of minions spend measurable time\ndoing only stat(2) calls.\n\nIn strace it looks like::\n\n     0.000093 stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0\n     0.000236 open(\"/etc/salt/pki/master/minions\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 154\n     0.011412 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000102 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     ...many thousands lines...\n     0.000064 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000062 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=796, ...}) = 0\n     0.000485 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000065 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000184 stat(\"/etc/salt/pki/master/minions_rejected/hostXXXX.linkedin.com\", 0x7fff28209f40) = -1 ENOENT (No such file or directory)\n     0.000071 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000074 open(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", O_RDONLY) = 154\n\nHappens that on each _auth() call salt is counting files in pki/master/minions\nand checks whenever those are regular files by applying os.path.isfile()\nfunction to each which considerably slows things down.\n\nThis patch removes isfile() call that effectively makes check_max_open_files()\ncheck less precise (but since it's already subject to external race conditions\nlike creation/removal of files by another process it should be OK) in exchange\nfor making it much faster.\n\nPS. Note that calling salt.utils.verify.check_max_open_files() on each _auth is\nnot really efficient, it probably should be done periodically in a background\nthread.\n\nSponsored by: LinkedIn\nSigned-off-by: Alexey Ivanov <SaveTheRbtz@GMail.com>", "parent_commit": "26edc398f706b4266fd9cfdf886d15ab0e32049b", "description": "Write a Python function to evaluate and log system resource limits against the number of accepted keys for a service."}
{"func_name": "handle_client_startup", "func_src_before": "static bool handle_client_startup(PgSocket *client, PktHdr *pkt)\n{\n\tconst char *passwd;\n\tconst uint8_t *key;\n\tbool ok;\n\n\tSBuf *sbuf = &client->sbuf;\n\n\t/* don't tolerate partial packets */\n\tif (incomplete_pkt(pkt)) {\n\t\tdisconnect_client(client, true, \"client sent partial pkt in startup phase\");\n\t\treturn false;\n\t}\n\n\tif (client->wait_for_welcome) {\n\t\tif  (finish_client_login(client)) {\n\t\t\t/* the packet was already parsed */\n\t\t\tsbuf_prepare_skip(sbuf, pkt->len);\n\t\t\treturn true;\n\t\t} else\n\t\t\treturn false;\n\t}\n\n\tswitch (pkt->type) {\n\tcase PKT_SSLREQ:\n\t\tslog_noise(client, \"C: req SSL\");\n\t\tslog_noise(client, \"P: nak\");\n\n\t\t/* reject SSL attempt */\n\t\tif (!sbuf_answer(&client->sbuf, \"N\", 1)) {\n\t\t\tdisconnect_client(client, false, \"failed to nak SSL\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_STARTUP_V2:\n\t\tdisconnect_client(client, true, \"Old V2 protocol not supported\");\n\t\treturn false;\n\tcase PKT_STARTUP:\n\t\tif (client->pool) {\n\t\t\tdisconnect_client(client, true, \"client re-sent startup pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!decide_startup_pool(client, pkt))\n\t\t\treturn false;\n\n\t\tif (client->pool->db->admin) {\n\t\t\tif (!admin_pre_login(client))\n\t\t\t\treturn false;\n\t\t}\n\n\t\tif (cf_auth_type <= AUTH_TRUST || client->own_user) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (!send_client_authreq(client)) {\n\t\t\t\tdisconnect_client(client, false, \"failed to send auth req\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 'p':\t\t/* PasswordMessage */\n\t\t/* haven't requested it */\n\t\tif (cf_auth_type <= AUTH_TRUST) {\n\t\t\tdisconnect_client(client, true, \"unrequested passwd pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tok = mbuf_get_string(&pkt->data, &passwd);\n\t\tif (ok && check_client_passwd(client, passwd)) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tdisconnect_client(client, true, \"Auth failed\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_CANCEL:\n\t\tif (mbuf_avail_for_read(&pkt->data) == BACKENDKEY_LEN\n\t\t    && mbuf_get_bytes(&pkt->data, BACKENDKEY_LEN, &key))\n\t\t{\n\t\t\tmemcpy(client->cancel_key, key, BACKENDKEY_LEN);\n\t\t\taccept_cancel_request(client);\n\t\t} else\n\t\t\tdisconnect_client(client, false, \"bad cancel request\");\n\t\treturn false;\n\tdefault:\n\t\tdisconnect_client(client, false, \"bad packet\");\n\t\treturn false;\n\t}\n\tsbuf_prepare_skip(sbuf, pkt->len);\n\tclient->request_time = get_cached_time();\n\treturn true;\n}", "func_src_after": "static bool handle_client_startup(PgSocket *client, PktHdr *pkt)\n{\n\tconst char *passwd;\n\tconst uint8_t *key;\n\tbool ok;\n\n\tSBuf *sbuf = &client->sbuf;\n\n\t/* don't tolerate partial packets */\n\tif (incomplete_pkt(pkt)) {\n\t\tdisconnect_client(client, true, \"client sent partial pkt in startup phase\");\n\t\treturn false;\n\t}\n\n\tif (client->wait_for_welcome) {\n\t\tif  (finish_client_login(client)) {\n\t\t\t/* the packet was already parsed */\n\t\t\tsbuf_prepare_skip(sbuf, pkt->len);\n\t\t\treturn true;\n\t\t} else\n\t\t\treturn false;\n\t}\n\n\tswitch (pkt->type) {\n\tcase PKT_SSLREQ:\n\t\tslog_noise(client, \"C: req SSL\");\n\t\tslog_noise(client, \"P: nak\");\n\n\t\t/* reject SSL attempt */\n\t\tif (!sbuf_answer(&client->sbuf, \"N\", 1)) {\n\t\t\tdisconnect_client(client, false, \"failed to nak SSL\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_STARTUP_V2:\n\t\tdisconnect_client(client, true, \"Old V2 protocol not supported\");\n\t\treturn false;\n\tcase PKT_STARTUP:\n\t\tif (client->pool) {\n\t\t\tdisconnect_client(client, true, \"client re-sent startup pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!decide_startup_pool(client, pkt))\n\t\t\treturn false;\n\n\t\tif (client->pool->db->admin) {\n\t\t\tif (!admin_pre_login(client))\n\t\t\t\treturn false;\n\t\t}\n\n\t\tif (cf_auth_type <= AUTH_TRUST || client->own_user) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (!send_client_authreq(client)) {\n\t\t\t\tdisconnect_client(client, false, \"failed to send auth req\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 'p':\t\t/* PasswordMessage */\n\t\t/* too early */\n\t\tif (!client->auth_user) {\n\t\t\tdisconnect_client(client, true, \"client password pkt before startup packet\");\n\t\t\treturn false;\n\t\t}\n\n\t\t/* haven't requested it */\n\t\tif (cf_auth_type <= AUTH_TRUST) {\n\t\t\tdisconnect_client(client, true, \"unrequested passwd pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tok = mbuf_get_string(&pkt->data, &passwd);\n\t\tif (ok && check_client_passwd(client, passwd)) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tdisconnect_client(client, true, \"Auth failed\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_CANCEL:\n\t\tif (mbuf_avail_for_read(&pkt->data) == BACKENDKEY_LEN\n\t\t    && mbuf_get_bytes(&pkt->data, BACKENDKEY_LEN, &key))\n\t\t{\n\t\t\tmemcpy(client->cancel_key, key, BACKENDKEY_LEN);\n\t\t\taccept_cancel_request(client);\n\t\t} else\n\t\t\tdisconnect_client(client, false, \"bad cancel request\");\n\t\treturn false;\n\tdefault:\n\t\tdisconnect_client(client, false, \"bad packet\");\n\t\treturn false;\n\t}\n\tsbuf_prepare_skip(sbuf, pkt->len);\n\tclient->request_time = get_cached_time();\n\treturn true;\n}", "commit_link": "github.com/pgbouncer/pgbouncer/commit/74d6e5f7de5ec736f71204b7b422af7380c19ac5", "file_name": "src/client.c", "vul_type": "cwe-476", "description": "Write a C function `handle_client_startup` to process different types of startup packets for a PostgreSQL client connection."}
{"func_name": "(anonymous)", "func_src_before": "(function() {\n  'use strict';\n\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    var iframes = [];\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n        return;\n      }\n\n      iframes[data.id].height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframes.push(iframe);\n\n      var id = iframes.length - 1;\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "func_src_after": "(function() {\n  'use strict';\n\n  /**\n   * @param {() => void} loaded\n   */\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    /** @type {Map<number, HTMLIFrameElement>} */\n    var iframes = new Map();\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source) {\n        return;\n      }\n\n      iframe.height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      // select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }\n\n      iframes.set(id, iframe);\n\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "line_changes": {"deleted": [{"line_no": 13, "char_start": 262, "char_end": 284, "line": "    var iframes = [];\n"}, {"line_no": 18, "char_start": 370, "char_end": 507, "line": "      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n"}, {"line_no": 22, "char_start": 532, "char_end": 577, "line": "      iframes[data.id].height = data.height;\n"}, {"line_no": 26, "char_start": 677, "char_end": 713, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 27, "char_start": 713, "char_end": 753, "line": "      iframe.style.overflow = 'hidden';\n"}, {"line_no": 29, "char_start": 754, "char_end": 782, "line": "      iframes.push(iframe);\n"}, {"line_no": 31, "char_start": 783, "char_end": 818, "line": "      var id = iframes.length - 1;\n"}], "added": [{"line_no": 4, "char_start": 31, "char_end": 37, "line": "  /**\n"}, {"line_no": 5, "char_start": 37, "char_end": 69, "line": "   * @param {() => void} loaded\n"}, {"line_no": 6, "char_start": 69, "char_end": 75, "line": "   */\n"}, {"line_no": 16, "char_start": 306, "char_end": 356, "line": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"line_no": 17, "char_start": 356, "char_end": 385, "line": "    var iframes = new Map();\n"}, {"line_no": 22, "char_start": 471, "char_end": 563, "line": "      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n"}, {"line_no": 23, "char_start": 563, "char_end": 579, "line": "        return;\n"}, {"line_no": 24, "char_start": 579, "char_end": 587, "line": "      }\n"}, {"line_no": 25, "char_start": 587, "char_end": 588, "line": "\n"}, {"line_no": 26, "char_start": 588, "char_end": 629, "line": "      var iframe = iframes.get(data.id);\n"}, {"line_no": 27, "char_start": 629, "char_end": 630, "line": "\n"}, {"line_no": 28, "char_start": 630, "char_end": 694, "line": "      if ('source' in e && iframe.contentWindow !== e.source) {\n"}, {"line_no": 32, "char_start": 719, "char_end": 754, "line": "      iframe.height = data.height;\n"}, {"line_no": 37, "char_start": 896, "char_end": 960, "line": "      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n"}, {"line_no": 38, "char_start": 960, "char_end": 1004, "line": "      while (id === 0 || iframes.has(id)) {\n"}, {"line_no": 39, "char_start": 1004, "char_end": 1054, "line": "        id = crypto.getRandomValues(idBuffer)[0];\n"}, {"line_no": 40, "char_start": 1054, "char_end": 1075, "line": "        failCount++;\n"}, {"line_no": 41, "char_start": 1075, "char_end": 1106, "line": "        if (failCount > 100) {\n"}, {"line_no": 43, "char_start": 1211, "char_end": 1247, "line": "          id = -(iframes.size + 1);\n"}, {"line_no": 44, "char_start": 1247, "char_end": 1264, "line": "          break;\n"}, {"line_no": 45, "char_start": 1264, "char_end": 1274, "line": "        }\n"}, {"line_no": 46, "char_start": 1274, "char_end": 1282, "line": "      }\n"}, {"line_no": 48, "char_start": 1283, "char_end": 1314, "line": "      iframes.set(id, iframe);\n"}, {"line_no": 50, "char_start": 1315, "char_end": 1351, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 51, "char_start": 1351, "char_end": 1391, "line": "      iframe.style.overflow = 'hidden';\n"}]}, "char_changes": {"deleted": [{"char_start": 280, "char_end": 282, "chars": "[]"}, {"char_start": 417, "char_end": 418, "chars": "["}, {"char_start": 425, "char_end": 503, "chars": "] || window.location.origin !== e.origin || data.id.toString() === '__proto__'"}, {"char_start": 544, "char_end": 554, "chars": "s[data.id]"}, {"char_start": 683, "char_end": 752, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';"}, {"char_start": 768, "char_end": 773, "chars": "push("}, {"char_start": 789, "char_end": 816, "chars": "var id = iframes.length - 1"}], "added": [{"char_start": 31, "char_end": 75, "chars": "  /**\n   * @param {() => void} loaded\n   */\n"}, {"char_start": 306, "char_end": 356, "chars": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"char_start": 374, "char_end": 383, "chars": "new Map()"}, {"char_start": 481, "char_end": 509, "chars": "typeof data !== 'object' || "}, {"char_start": 546, "char_end": 551, "chars": ".has("}, {"char_start": 558, "char_end": 690, "chars": ")) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source"}, {"char_start": 860, "char_end": 1281, "chars": "// select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }"}, {"char_start": 1297, "char_end": 1305, "chars": "set(id, "}, {"char_start": 1321, "char_end": 1389, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden'"}]}, "commit_link": "github.com/yukimochi/mastodon/commit/6e736f2452d2e6fdd4da6d8f6f2f44da9d83fa4f", "file_name": "embed.js", "vul_type": "cwe-915", "commit_msg": "fix: embed.js doesn't expands iframes height (#18301)\n\nalso including some refactoring:\r\n- add `// @ts-check`\r\n- use Map to completely avoid prototype pollution\r\n- assign random id to each iframe for reduce chance to brute-force attack, and leak of iframe counts\r\n- check iframe.contentWindow and MessageEvent.source to validate message is coming from correct iframe (it works on latest Chrome/Firefox/Safari but I'm not sure this is allowed by spec)\r\n\r\nfollow-up of #17420\r\nfix #18299", "parent_commit": "a01580f09f33c275fcc0ffe616b5b5b403f46cae", "description": "Create a JavaScript function that initializes iframes for embedding content and adjusts their height based on postMessage events."}
{"func_name": "compare_and_update", "func_src_before": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name='{user.first_name}', \"\n                 f\"nickname='{user.nickname}', \"\n                 f\"last_name='{user.last_name}' \"\n                 f\"WHERE chat_id={user.chat_id}\")\n\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "func_src_after": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name=%s, \"\n                 f\"nickname=%s, \"\n                 f\"last_name=%s \"\n                 f\"WHERE chat_id=%s\")\n\n        parameters = (user.first_name, user.nickname, user.last_name,\n                      user.chat_id)\n\n        try:\n            db.add(query, parameters)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "In Python, write a static method to compare a user's information from a Telegram bot with the incoming message data and update the user's info in the database if it has changed."}
{"func_name": "vips_foreign_load_gif_scan_image", "func_src_before": "vips_foreign_load_gif_scan_image( VipsForeignLoadGif *gif ) \n{\n\tVipsObjectClass *class = VIPS_OBJECT_GET_CLASS( gif );\n\tGifFileType *file = gif->file;\n\tColorMapObject *map = file->Image.ColorMap ?\n\t\tfile->Image.ColorMap : file->SColorMap;\n\n\tGifByteType *extension;\n\n\tif( DGifGetImageDesc( gif->file ) == GIF_ERROR ) {\n\t\tvips_foreign_load_gif_error( gif ); \n\t\treturn( -1 );\n\t}\n\n\t/* Check that the frame looks sane. Perhaps giflib checks\n\t * this for us.\n\t */\n\tif( file->Image.Left < 0 ||\n\t\tfile->Image.Width < 1 ||\n\t\tfile->Image.Width > 10000 ||\n\t\tfile->Image.Left + file->Image.Width > file->SWidth ||\n\t\tfile->Image.Top < 0 ||\n\t\tfile->Image.Height < 1 ||\n\t\tfile->Image.Height > 10000 ||\n\t\tfile->Image.Top + file->Image.Height > file->SHeight ) {\n\t\tvips_error( class->nickname, \"%s\", _( \"bad frame size\" ) ); \n\t\treturn( -1 ); \n\t}\n\n\t/* Test for a non-greyscale colourmap for this frame.\n\t */\n\tif( !gif->has_colour &&\n\t\tmap ) {\n\t\tint i;\n\n\t\tfor( i = 0; i < map->ColorCount; i++ ) \n\t\t\tif( map->Colors[i].Red != map->Colors[i].Green ||\n\t\t\t\tmap->Colors[i].Green != map->Colors[i].Blue ) {\n\t\t\t\tgif->has_colour = TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\t/* Step over compressed image data.\n\t */\n\tdo {\n\t\tif( vips_foreign_load_gif_code_next( gif, &extension ) ) \n\t\t\treturn( -1 );\n\t} while( extension != NULL );\n\n\treturn( 0 );\n}", "func_src_after": "vips_foreign_load_gif_scan_image( VipsForeignLoadGif *gif ) \n{\n\tVipsObjectClass *class = VIPS_OBJECT_GET_CLASS( gif );\n\tGifFileType *file = gif->file;\n\n\tColorMapObject *map;\n\tGifByteType *extension;\n\n\tif( DGifGetImageDesc( gif->file ) == GIF_ERROR ) {\n\t\tvips_foreign_load_gif_error( gif ); \n\t\treturn( -1 );\n\t}\n\n\t/* Check that the frame looks sane. Perhaps giflib checks\n\t * this for us.\n\t */\n\tif( file->Image.Left < 0 ||\n\t\tfile->Image.Width < 1 ||\n\t\tfile->Image.Width > 10000 ||\n\t\tfile->Image.Left + file->Image.Width > file->SWidth ||\n\t\tfile->Image.Top < 0 ||\n\t\tfile->Image.Height < 1 ||\n\t\tfile->Image.Height > 10000 ||\n\t\tfile->Image.Top + file->Image.Height > file->SHeight ) {\n\t\tvips_error( class->nickname, \"%s\", _( \"bad frame size\" ) ); \n\t\treturn( -1 ); \n\t}\n\n\t/* Test for a non-greyscale colourmap for this frame.\n\t */\n\tmap = file->Image.ColorMap ? file->Image.ColorMap : file->SColorMap;\n\tif( !gif->has_colour &&\n\t\tmap ) {\n\t\tint i;\n\n\t\tfor( i = 0; i < map->ColorCount; i++ ) \n\t\t\tif( map->Colors[i].Red != map->Colors[i].Green ||\n\t\t\t\tmap->Colors[i].Green != map->Colors[i].Blue ) {\n\t\t\t\tgif->has_colour = TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\t/* Step over compressed image data.\n\t */\n\tdo {\n\t\tif( vips_foreign_load_gif_code_next( gif, &extension ) ) \n\t\t\treturn( -1 );\n\t} while( extension != NULL );\n\n\treturn( 0 );\n}", "commit_link": "github.com/libvips/libvips/commit/ce684dd008532ea0bf9d4a1d89bacb35f4a83f4d", "file_name": "libvips/foreign/gifload.c", "vul_type": "cwe-416", "description": "Write a C function to scan and validate a GIF image frame for the Vips image processing library."}
{"func_name": "ReadPSDImage", "func_src_before": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  if ((image->depth == 1) && (image->storage_class != PseudoClass))\n    ThrowReaderException(CorruptImageError, \"ImproperImageHeader\");\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/198fffab4daf8aea88badd9c629350e5b26ec32f", "file_name": "coders/psd.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a PSD image file."}
{"func_name": "markTokenUsedExternal", "func_src_before": "def markTokenUsedExternal(token, optStr=\"\"):\n    conn, c = connectDB()\n    req = \"UPDATE {} SET \\\"options_selected\\\"='{}' WHERE token='{}'\".format(CFG(\"tokens_table_name\"), \\\n                    optStr, token)\n    c.execute(req)\n    closeDB(conn)", "func_src_after": "def markTokenUsedExternal(token, optStr=\"\"):\n    conn, c = connectDB()\n    req = \"UPDATE {} SET \\\"options_selected\\\"=? WHERE token=?\".format(CFG(\"tokens_table_name\"))\n    c.execute(req, (optStr, token,))\n    closeDB(conn)", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function to update a database record's options_selected field for a given token."}
{"func_name": "check_testPickle", "func_src_before": "    def check_testPickle(self):\n        \"Test of pickling\"\n        x = arange(12)\n        x[4:10:2] = masked\n        x=x.reshape(4,3)\n        f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb')\n        y = pickle.load(f)\n        assert eq(x,y)", "func_src_after": "    def check_testPickle(self):\n        \"Test of pickling\"\n        import pickle\n        x = arange(12)\n        x[4:10:2] = masked\n        x = x.reshape(4,3)\n        s = pickle.dumps(x)\n        y = pickle.loads(s)\n        assert eq(x,y)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 109, "char_end": 134, "line": "        x=x.reshape(4,3)\n"}, {"line_no": 6, "char_start": 134, "char_end": 169, "line": "        f = open('test9.pik','wb')\n"}, {"line_no": 7, "char_start": 169, "char_end": 191, "line": "        import pickle\n"}, {"line_no": 8, "char_start": 191, "char_end": 217, "line": "        pickle.dump(x, f)\n"}, {"line_no": 9, "char_start": 217, "char_end": 235, "line": "        f.close()\n"}, {"line_no": 10, "char_start": 235, "char_end": 271, "line": "        f = open('test9.pik', 'rb')\n"}, {"line_no": 11, "char_start": 271, "char_end": 298, "line": "        y = pickle.load(f)\n"}, {"line_no": 12, "char_start": 298, "char_end": 320, "line": "        assert eq(x,y) \n"}], "added": [{"line_no": 3, "char_start": 59, "char_end": 81, "line": "        import pickle\n"}]}, "char_changes": {"deleted": [{"char_start": 118, "char_end": 119, "chars": "="}, {"char_start": 142, "char_end": 269, "chars": "f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb'"}, {"char_start": 294, "char_end": 296, "chars": "(f"}], "added": [{"char_start": 59, "char_end": 81, "chars": "        import pickle\n"}, {"char_start": 140, "char_end": 143, "chars": " = "}, {"char_start": 166, "char_end": 167, "chars": "s"}, {"char_start": 170, "char_end": 184, "chars": "pickle.dumps(x"}, {"char_start": 209, "char_end": 212, "chars": "s(s"}]}, "commit_link": "github.com/cjermain/numpy/commit/d1e5d1de77e30c233e98ea7c35f8d7b4623fd1f3", "file_name": "test_ma.py", "vul_type": "cwe-502", "commit_msg": "Use pickle.loads/dumps for test_ma to avoid littering the filesystem with test9.pik files.", "parent_commit": "0e1c71808725c49f65d84847cc6fc7e88909a6de", "description": "Write a Python function that tests pickling and unpickling an array with modified elements and reshaping."}
{"func_name": "nav_path", "func_src_before": "def nav_path(request):\n  \"\"\"Return current path as list of items with \"name\" and \"href\" members\n\n  The href members are view_directory links for directories and view_log\n  links for files, but are set to None when the link would point to\n  the current view\"\"\"\n\n  if not request.repos:\n    return []\n\n  is_dir = request.pathtype == vclib.DIR\n\n  # add root item\n  items = []\n  root_item = _item(name=request.server.escape(request.repos.name), href=None)\n  if request.path_parts or request.view_func is not view_directory:\n    root_item.href = request.get_url(view_func=view_directory,\n                                     where='', pathtype=vclib.DIR,\n                                     params={}, escape=1)\n  items.append(root_item)\n\n  # add path part items\n  path_parts = []\n  for part in request.path_parts:\n    path_parts.append(part)\n    is_last = len(path_parts) == len(request.path_parts)\n\n    item = _item(name=part, href=None)\n\n    if not is_last or (is_dir and request.view_func is not view_directory):\n      item.href = request.get_url(view_func=view_directory,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.DIR,\n                                  params={}, escape=1)\n    elif not is_dir and request.view_func is not view_log:\n      item.href = request.get_url(view_func=view_log,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.FILE,\n                                  params={}, escape=1)\n    items.append(item)\n\n  return items", "func_src_after": "def nav_path(request):\n  \"\"\"Return current path as list of items with \"name\" and \"href\" members\n\n  The href members are view_directory links for directories and view_log\n  links for files, but are set to None when the link would point to\n  the current view\"\"\"\n\n  if not request.repos:\n    return []\n\n  is_dir = request.pathtype == vclib.DIR\n\n  # add root item\n  items = []\n  root_item = _item(name=request.server.escape(request.repos.name), href=None)\n  if request.path_parts or request.view_func is not view_directory:\n    root_item.href = request.get_url(view_func=view_directory,\n                                     where='', pathtype=vclib.DIR,\n                                     params={}, escape=1)\n  items.append(root_item)\n\n  # add path part items\n  path_parts = []\n  for part in request.path_parts:\n    path_parts.append(part)\n    is_last = len(path_parts) == len(request.path_parts)\n\n    item = _item(name=request.server.escape(part), href=None)\n\n    if not is_last or (is_dir and request.view_func is not view_directory):\n      item.href = request.get_url(view_func=view_directory,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.DIR,\n                                  params={}, escape=1)\n    elif not is_dir and request.view_func is not view_log:\n      item.href = request.get_url(view_func=view_log,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.FILE,\n                                  params={}, escape=1)\n    items.append(item)\n\n  return items", "commit_link": "github.com/viewvc/viewvc/commit/9dcfc7daa4c940992920d3b2fbd317da20e44aad", "file_name": "lib/viewvc.py", "vul_type": "cwe-079", "description": "Write a Python function that generates a breadcrumb navigation path from a request object."}
{"func_name": "zmi_page_request", "func_src_before": "    def zmi_page_request(self, *args, **kwargs):\r\n      request = self.REQUEST\r\n      RESPONSE = request.RESPONSE\r\n      SESSION = request.SESSION\r\n      self._zmi_page_request()\r\n      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())\r\n      RESPONSE.setHeader('Cache-Control', 'no-cache')\r\n      RESPONSE.setHeader('Pragma', 'no-cache')\r\n      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])\r\n      if not request.get( 'preview'):\r\n        request.set( 'preview','preview')\r\n      langs = self.getLanguages(request)\r\n      if request.get('lang') not in langs:\r\n        request.set('lang',langs[0])\r\n      if request.get('manage_lang') not in self.getLocale().get_manage_langs():\r\n        request.set('manage_lang',self.get_manage_lang())\r\n      if not request.get('manage_tabs_message'):\r\n        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))\r\n      # manage_system\r\n      if request.form.has_key('zmi-manage-system'):\r\n        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))\r\n      # avoid declarative urls\r\n      physical_path = self.getPhysicalPath()\r\n      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')\r\n      path = path_to_handle[:-1]\r\n      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:\r\n        for i in range(len(path)):\r\n          if path[:-(i+1)] != physical_path[:-(i+1)]:\r\n            path[:-(i+1)] = physical_path[:-(i+1)]\r\n        new_path = path+[path_to_handle[-1]]\r\n        if path_to_handle != new_path:\r\n          request.RESPONSE.redirect('/'.join(new_path))", "func_src_after": "    def zmi_page_request(self, *args, **kwargs):\r\n      request = self.REQUEST\r\n      RESPONSE = request.RESPONSE\r\n      SESSION = request.SESSION\r\n      self._zmi_page_request()\r\n      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())\r\n      RESPONSE.setHeader('Cache-Control', 'no-cache')\r\n      RESPONSE.setHeader('Pragma', 'no-cache')\r\n      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])\r\n      if not request.get( 'preview'):\r\n        request.set( 'preview','preview')\r\n      langs = self.getLanguages(request)\r\n      if request.get('lang') not in langs:\r\n        request.set('lang',langs[0])\r\n      if request.get('manage_lang') not in self.getLocale().get_manage_langs():\r\n        request.set('manage_lang',self.get_manage_lang())\r\n      if not request.get('manage_tabs_message'):\r\n        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))\r\n      # manage_system\r\n      if request.form.has_key('zmi-manage-system'):\r\n        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))\r\n      # avoid declarative urls\r\n      physical_path = self.getPhysicalPath()\r\n      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')\r\n      path = path_to_handle[:-1]\r\n      if self.getDocumentElement().id in path and len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:\r\n        for i in range(len(path)):\r\n          if path[:-(i+1)] != physical_path[:-(i+1)]:\r\n            path[:-(i+1)] = physical_path[:-(i+1)]\r\n        new_path = path+[path_to_handle[-1]]\r\n        if path_to_handle != new_path:\r\n          request.RESPONSE.redirect('/'.join(new_path))", "commit_link": "github.com/zms-publishing/zms4/commit/3f28620d475220dfdb06f79787158ac50727c61a", "file_name": "ZMSItem.py", "vul_type": "cwe-022", "description": "Write a Python function named `zmi_page_request` that modifies HTTP response headers, manages session variables, and redirects to a normalized URL if necessary."}
{"func_name": "uas_switch_interface", "func_src_before": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tint alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (alt < 0)\n\t\treturn alt;\n\n\treturn usb_set_interface(udev,\n\t\t\tintf->altsetting[0].desc.bInterfaceNumber, alt);\n}", "func_src_after": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tstruct usb_host_interface *alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (!alt)\n\t\treturn -ENODEV;\n\n\treturn usb_set_interface(udev, alt->desc.bInterfaceNumber,\n\t\t\talt->desc.bAlternateSetting);\n}", "commit_link": "github.com/torvalds/linux/commit/786de92b3cb26012d3d0f00ee37adf14527f35c4", "file_name": "drivers/usb/storage/uas.c", "vul_type": "cwe-125", "description": "Write a C function named `uas_switch_interface` that switches the USB interface to an alternate setting for a given USB device and interface."}
{"func_name": "SecurityConfig::configure", "func_src_before": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().disable();\n        //.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "func_src_after": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "line_changes": {"deleted": [{"line_no": 9, "char_start": 250, "char_end": 281, "line": "            .csrf().disable();\n"}], "added": [{"line_no": 9, "char_start": 250, "char_end": 338, "line": "            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n"}]}, "char_changes": {"deleted": [{"char_start": 262, "char_end": 291, "chars": ".csrf().disable();\n        //"}], "added": []}, "commit_link": "github.com/skarsaune/hawtio/commit/95fb0e855e122e618dac7e13523976b1ce4b7726", "file_name": "SecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "chore(examples): enable CSRF protection for springboot-security example", "parent_commit": "bab089fe3b9d8ebf9438288d0a89b6cf51c127d5", "description": "Write a Java method to configure HTTP security, enabling form login, basic authentication, and optionally disabling CSRF protection."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=q.question, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)", "func_src_after": "    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            label = escape(q.question)  # django-bootstrap3 calls mark_safe\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=label, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)", "commit_link": "github.com/pretix/pretix/commit/affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "file_name": "src/pretix/base/forms/questions.py", "vul_type": "cwe-079", "description": "Write a Python class initializer that processes event-related form fields with dynamic question types and initial values."}
{"func_name": "changedline", "func_src_before": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes in the way */\n}", "func_src_after": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  if (p->lineinfo == NULL)  /* no debug information? */\n    return 0;\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes between positions */\n}", "commit_link": "github.com/lua/lua/commit/ae5b5ba529753c7a653901ffc29b5ea24c3fdf3a", "file_name": "ldebug.c", "vul_type": "cwe-476", "description": "Write a C function named `changedline` that checks if the execution line has changed between two program counters within a Lua function prototype."}
{"func_name": "__rds_rdma_map", "func_src_before": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}", "func_src_after": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0 || !rs->rs_transport) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/f3069c6d33f6ae63a1668737bc78aaaa51bff7ca", "file_name": "net/rds/rdma.c", "vul_type": "cwe-476", "description": "Write a C function named `__rds_rdma_map` that maps a user buffer for RDMA operations and returns a cookie for the mapped memory region."}
{"func_name": "get_last_active_users", "func_src_before": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "func_src_after": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT %s')\n\n        parameters = limit,\n\n        try:\n            cursor = db.execute_query(query, parameters)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch a specified number of the most recent active bot users from a database."}
{"func_name": "kmod_module_new_from_name", "func_src_before": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize((char *)name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "func_src_after": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize(name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 244, "char_end": 299, "line": "\tmodname_normalize((char *)name, name_norm, &namelen);\n"}], "added": [{"line_no": 12, "char_start": 244, "char_end": 291, "line": "\tmodname_normalize(name, name_norm, &namelen);\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 271, "chars": "(char *)"}], "added": []}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to create or reference a module by name in a given context."}
{"func_name": "move", "func_src_before": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        tmp = tempfile.mktemp(suffix='.beets',\n                              prefix=py3_path(b'.' + os.path.basename(dest)),\n                              dir=py3_path(os.path.dirname(dest)))\n        tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)\n            os.replace(tmp, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "func_src_after": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        # Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n                                          prefix=b'.' + os.path.basename(dest),\n                                          dir=os.path.dirname(dest),\n                                          delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:\n            os.replace(tmp.name, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "line_changes": {"deleted": [{"line_no": 25, "char_start": 973, "char_end": 1020, "line": "        tmp = tempfile.mktemp(suffix='.beets',\n"}, {"line_no": 26, "char_start": 1020, "char_end": 1098, "line": "                              prefix=py3_path(b'.' + os.path.basename(dest)),\n"}, {"line_no": 27, "char_start": 1098, "char_end": 1165, "line": "                              dir=py3_path(os.path.dirname(dest)))\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1192, "line": "        tmp = syspath(tmp)\n"}, {"line_no": 30, "char_start": 1205, "char_end": 1244, "line": "            shutil.copyfile(path, tmp)\n"}, {"line_no": 31, "char_start": 1244, "char_end": 1278, "line": "            os.replace(tmp, dest)\n"}], "added": [{"line_no": 26, "char_start": 1025, "char_end": 1085, "line": "        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n"}, {"line_no": 27, "char_start": 1085, "char_end": 1165, "line": "                                          prefix=b'.' + os.path.basename(dest),\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1234, "line": "                                          dir=os.path.dirname(dest),\n"}, {"line_no": 29, "char_start": 1234, "char_end": 1290, "line": "                                          delete=False)\n"}, {"line_no": 31, "char_start": 1303, "char_end": 1343, "line": "            with open(path, 'rb') as f:\n"}, {"line_no": 32, "char_start": 1343, "char_end": 1386, "line": "                shutil.copyfileobj(f, tmp)\n"}, {"line_no": 33, "char_start": 1386, "char_end": 1403, "line": "        finally:\n"}, {"line_no": 34, "char_start": 1403, "char_end": 1427, "line": "            tmp.close()\n"}, {"line_no": 35, "char_start": 1427, "char_end": 1428, "line": "\n"}, {"line_no": 37, "char_start": 1471, "char_end": 1484, "line": "        try:\n"}, {"line_no": 38, "char_start": 1484, "char_end": 1523, "line": "            os.replace(tmp.name, dest)\n"}]}, "char_changes": {"deleted": [{"char_start": 981, "char_end": 1002, "chars": "tmp = tempfile.mktemp"}, {"char_start": 1050, "char_end": 1066, "chars": "prefix=py3_path("}, {"char_start": 1095, "char_end": 1096, "chars": ")"}, {"char_start": 1132, "char_end": 1141, "chars": "py3_path("}, {"char_start": 1162, "char_end": 1164, "chars": "))"}, {"char_start": 1173, "char_end": 1243, "chars": "tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)"}], "added": [{"char_start": 981, "char_end": 1066, "chars": "# Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile"}, {"char_start": 1074, "char_end": 1075, "chars": "b"}, {"char_start": 1115, "char_end": 1134, "chars": "            prefix="}, {"char_start": 1165, "char_end": 1177, "chars": "            "}, {"char_start": 1232, "char_end": 1233, "chars": ","}, {"char_start": 1242, "char_end": 1483, "chars": "                                  delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:"}, {"char_start": 1510, "char_end": 1515, "chars": ".name"}]}, "commit_link": "github.com/beetbox/beets/commit/4bb695bcdbada9c8153442688e8494199f015f04", "file_name": "__init__.py", "vul_type": "cwe-377", "commit_msg": "Fix copying for atomic file moves\n\nFixes #4168. Also closes #4192, which it supersedes.\n\nThe original problem is that this implementation used bytestrings\nincorrectly to invoke `mktemp`. However, `mktemp` is deprecated, so this\nPR just avoids it altogether. Fortunately, the non-deprecated APIs in\n`tempfile` support all-bytes arguments.", "description": "Write a Python function named `move` that renames a file, with an option to replace the destination file if it already exists."}
{"func_name": "edit_workflow", "func_src_before": "@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "func_src_after": "@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "commit_link": "github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "file_name": "apps/oozie/src/oozie/views/editor2.py", "vul_type": "cwe-079", "description": "Write a Python function with a decorator to check user permissions, which retrieves and edits workflow data based on a request, handling credentials and rendering a response."}
{"func_name": "rds_cmsg_atomic", "func_src_before": "int rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,\n\t\t    struct cmsghdr *cmsg)\n{\n\tstruct page *page = NULL;\n\tstruct rds_atomic_args *args;\n\tint ret = 0;\n\n\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_atomic_args))\n\t || rm->atomic.op_active)\n\t\treturn -EINVAL;\n\n\targs = CMSG_DATA(cmsg);\n\n\t/* Nonmasked & masked cmsg ops converted to masked hw ops */\n\tswitch (cmsg->cmsg_type) {\n\tcase RDS_CMSG_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = 0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->m_fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = args->m_fadd.nocarry_mask;\n\t\tbreak;\n\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = ~0;\n\t\trm->atomic.op_m_cswp.swap_mask = ~0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->m_cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->m_cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = args->m_cswp.compare_mask;\n\t\trm->atomic.op_m_cswp.swap_mask = args->m_cswp.swap_mask;\n\t\tbreak;\n\tdefault:\n\t\tBUG(); /* should never happen */\n\t}\n\n\trm->atomic.op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\n\trm->atomic.op_silent = !!(args->flags & RDS_RDMA_SILENT);\n\trm->atomic.op_active = 1;\n\trm->atomic.op_recverr = rs->rs_recverr;\n\trm->atomic.op_sg = rds_message_alloc_sgs(rm, 1);\n\tif (!rm->atomic.op_sg) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* verify 8 byte-aligned */\n\tif (args->local_addr & 0x7) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tret = rds_pin_pages(args->local_addr, 1, &page, 1);\n\tif (ret != 1)\n\t\tgoto err;\n\tret = 0;\n\n\tsg_set_page(rm->atomic.op_sg, page, 8, offset_in_page(args->local_addr));\n\n\tif (rm->atomic.op_notify || rm->atomic.op_recverr) {\n\t\t/* We allocate an uninitialized notifier here, because\n\t\t * we don't want to do that in the completion handler. We\n\t\t * would have to use GFP_ATOMIC there, and don't want to deal\n\t\t * with failed allocations.\n\t\t */\n\t\trm->atomic.op_notifier = kmalloc(sizeof(*rm->atomic.op_notifier), GFP_KERNEL);\n\t\tif (!rm->atomic.op_notifier) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\trm->atomic.op_notifier->n_user_token = args->user_token;\n\t\trm->atomic.op_notifier->n_status = RDS_RDMA_SUCCESS;\n\t}\n\n\trm->atomic.op_rkey = rds_rdma_cookie_key(args->cookie);\n\trm->atomic.op_remote_addr = args->remote_addr + rds_rdma_cookie_offset(args->cookie);\n\n\treturn ret;\nerr:\n\tif (page)\n\t\tput_page(page);\n\tkfree(rm->atomic.op_notifier);\n\n\treturn ret;\n}", "func_src_after": "int rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,\n\t\t    struct cmsghdr *cmsg)\n{\n\tstruct page *page = NULL;\n\tstruct rds_atomic_args *args;\n\tint ret = 0;\n\n\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_atomic_args))\n\t || rm->atomic.op_active)\n\t\treturn -EINVAL;\n\n\targs = CMSG_DATA(cmsg);\n\n\t/* Nonmasked & masked cmsg ops converted to masked hw ops */\n\tswitch (cmsg->cmsg_type) {\n\tcase RDS_CMSG_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = 0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->m_fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = args->m_fadd.nocarry_mask;\n\t\tbreak;\n\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = ~0;\n\t\trm->atomic.op_m_cswp.swap_mask = ~0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->m_cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->m_cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = args->m_cswp.compare_mask;\n\t\trm->atomic.op_m_cswp.swap_mask = args->m_cswp.swap_mask;\n\t\tbreak;\n\tdefault:\n\t\tBUG(); /* should never happen */\n\t}\n\n\trm->atomic.op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\n\trm->atomic.op_silent = !!(args->flags & RDS_RDMA_SILENT);\n\trm->atomic.op_active = 1;\n\trm->atomic.op_recverr = rs->rs_recverr;\n\trm->atomic.op_sg = rds_message_alloc_sgs(rm, 1);\n\tif (!rm->atomic.op_sg) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* verify 8 byte-aligned */\n\tif (args->local_addr & 0x7) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tret = rds_pin_pages(args->local_addr, 1, &page, 1);\n\tif (ret != 1)\n\t\tgoto err;\n\tret = 0;\n\n\tsg_set_page(rm->atomic.op_sg, page, 8, offset_in_page(args->local_addr));\n\n\tif (rm->atomic.op_notify || rm->atomic.op_recverr) {\n\t\t/* We allocate an uninitialized notifier here, because\n\t\t * we don't want to do that in the completion handler. We\n\t\t * would have to use GFP_ATOMIC there, and don't want to deal\n\t\t * with failed allocations.\n\t\t */\n\t\trm->atomic.op_notifier = kmalloc(sizeof(*rm->atomic.op_notifier), GFP_KERNEL);\n\t\tif (!rm->atomic.op_notifier) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\trm->atomic.op_notifier->n_user_token = args->user_token;\n\t\trm->atomic.op_notifier->n_status = RDS_RDMA_SUCCESS;\n\t}\n\n\trm->atomic.op_rkey = rds_rdma_cookie_key(args->cookie);\n\trm->atomic.op_remote_addr = args->remote_addr + rds_rdma_cookie_offset(args->cookie);\n\n\treturn ret;\nerr:\n\tif (page)\n\t\tput_page(page);\n\trm->atomic.op_active = 0;\n\tkfree(rm->atomic.op_notifier);\n\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/7d11f77f84b27cef452cee332f4e469503084737", "file_name": "net/rds/rdma.c", "vul_type": "cwe-476", "description": "Write a C function named `rds_cmsg_atomic` that processes atomic operations in RDS messages."}
{"func_name": "IRC_PROTOCOL_CALLBACK", "func_src_before": "IRC_PROTOCOL_CALLBACK(352)\n{\n    char *pos_attr, *pos_hopcount, *pos_realname, *str_host;\n    int arg_start, length;\n    struct t_irc_channel *ptr_channel;\n    struct t_irc_nick *ptr_nick;\n\n    IRC_PROTOCOL_MIN_ARGS(5);\n\n    /* silently ignore malformed 352 message (missing infos) */\n    if (argc < 8)\n        return WEECHAT_RC_OK;\n\n    pos_attr = NULL;\n    pos_hopcount = NULL;\n    pos_realname = NULL;\n\n    if (argc > 8)\n    {\n        arg_start = (strcmp (argv[8], \"*\") == 0) ? 9 : 8;\n        if (argv[arg_start][0] == ':')\n        {\n            pos_attr = NULL;\n            pos_hopcount = (argc > arg_start) ? argv[arg_start] + 1 : NULL;\n            pos_realname = (argc > arg_start + 1) ? argv_eol[arg_start + 1] : NULL;\n        }\n        else\n        {\n            pos_attr = argv[arg_start];\n            pos_hopcount = (argc > arg_start + 1) ? argv[arg_start + 1] + 1 : NULL;\n            pos_realname = (argc > arg_start + 2) ? argv_eol[arg_start + 2] : NULL;\n        }\n    }\n\n    ptr_channel = irc_channel_search (server, argv[3]);\n    ptr_nick = (ptr_channel) ?\n        irc_nick_search (server, ptr_channel, argv[7]) : NULL;\n\n    /* update host in nick */\n    if (ptr_nick)\n    {\n        length = strlen (argv[4]) + 1 + strlen (argv[5]) + 1;\n        str_host = malloc (length);\n        if (str_host)\n        {\n            snprintf (str_host, length, \"%s@%s\", argv[4], argv[5]);\n            irc_nick_set_host (ptr_nick, str_host);\n            free (str_host);\n        }\n    }\n\n    /* update away flag in nick */\n    if (ptr_channel && ptr_nick && pos_attr)\n    {\n        irc_nick_set_away (server, ptr_channel, ptr_nick,\n                           (pos_attr[0] == 'G') ? 1 : 0);\n    }\n\n    /* update realname in nick */\n    if (ptr_channel && ptr_nick && pos_realname)\n    {\n        if (ptr_nick->realname)\n            free (ptr_nick->realname);\n        if (pos_realname &&\n            weechat_hashtable_has_key (server->cap_list, \"extended-join\"))\n        {\n            ptr_nick->realname = strdup (pos_realname);\n        }\n        else\n        {\n            ptr_nick->realname = NULL;\n        }\n    }\n\n    /* display output of who (manual who from user) */\n    if (!ptr_channel || (ptr_channel->checking_whox <= 0))\n    {\n        weechat_printf_date_tags (\n            irc_msgbuffer_get_target_buffer (\n                server, NULL, command, \"who\", NULL),\n            date,\n            irc_protocol_tags (command, \"irc_numeric\", NULL, NULL),\n            \"%s%s[%s%s%s] %s%s %s(%s%s@%s%s)%s %s%s%s%s(%s)\",\n            weechat_prefix (\"network\"),\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_CHANNEL,\n            argv[3],\n            IRC_COLOR_CHAT_DELIMITERS,\n            irc_nick_color_for_msg (server, 1, NULL, argv[7]),\n            argv[7],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_HOST,\n            argv[4],\n            argv[5],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_RESET,\n            (pos_attr) ? pos_attr : \"\",\n            (pos_attr) ? \" \" : \"\",\n            (pos_hopcount) ? pos_hopcount : \"\",\n            (pos_hopcount) ? \" \" : \"\",\n            (pos_realname) ? pos_realname : \"\");\n    }\n\n    return WEECHAT_RC_OK;\n}", "func_src_after": "IRC_PROTOCOL_CALLBACK(352)\n{\n    char *pos_attr, *pos_hopcount, *pos_realname, *str_host;\n    int arg_start, length;\n    struct t_irc_channel *ptr_channel;\n    struct t_irc_nick *ptr_nick;\n\n    IRC_PROTOCOL_MIN_ARGS(5);\n\n    /* silently ignore malformed 352 message (missing infos) */\n    if (argc < 8)\n        return WEECHAT_RC_OK;\n\n    pos_attr = NULL;\n    pos_hopcount = NULL;\n    pos_realname = NULL;\n\n    if (argc > 8)\n    {\n        arg_start = ((argc > 9) && (strcmp (argv[8], \"*\") == 0)) ? 9 : 8;\n        if (argv[arg_start][0] == ':')\n        {\n            pos_attr = NULL;\n            pos_hopcount = (argc > arg_start) ? argv[arg_start] + 1 : NULL;\n            pos_realname = (argc > arg_start + 1) ? argv_eol[arg_start + 1] : NULL;\n        }\n        else\n        {\n            pos_attr = argv[arg_start];\n            pos_hopcount = (argc > arg_start + 1) ? argv[arg_start + 1] + 1 : NULL;\n            pos_realname = (argc > arg_start + 2) ? argv_eol[arg_start + 2] : NULL;\n        }\n    }\n\n    ptr_channel = irc_channel_search (server, argv[3]);\n    ptr_nick = (ptr_channel) ?\n        irc_nick_search (server, ptr_channel, argv[7]) : NULL;\n\n    /* update host in nick */\n    if (ptr_nick)\n    {\n        length = strlen (argv[4]) + 1 + strlen (argv[5]) + 1;\n        str_host = malloc (length);\n        if (str_host)\n        {\n            snprintf (str_host, length, \"%s@%s\", argv[4], argv[5]);\n            irc_nick_set_host (ptr_nick, str_host);\n            free (str_host);\n        }\n    }\n\n    /* update away flag in nick */\n    if (ptr_channel && ptr_nick && pos_attr)\n    {\n        irc_nick_set_away (server, ptr_channel, ptr_nick,\n                           (pos_attr[0] == 'G') ? 1 : 0);\n    }\n\n    /* update realname in nick */\n    if (ptr_channel && ptr_nick && pos_realname)\n    {\n        if (ptr_nick->realname)\n            free (ptr_nick->realname);\n        if (pos_realname &&\n            weechat_hashtable_has_key (server->cap_list, \"extended-join\"))\n        {\n            ptr_nick->realname = strdup (pos_realname);\n        }\n        else\n        {\n            ptr_nick->realname = NULL;\n        }\n    }\n\n    /* display output of who (manual who from user) */\n    if (!ptr_channel || (ptr_channel->checking_whox <= 0))\n    {\n        weechat_printf_date_tags (\n            irc_msgbuffer_get_target_buffer (\n                server, NULL, command, \"who\", NULL),\n            date,\n            irc_protocol_tags (command, \"irc_numeric\", NULL, NULL),\n            \"%s%s[%s%s%s] %s%s %s(%s%s@%s%s)%s %s%s%s%s(%s)\",\n            weechat_prefix (\"network\"),\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_CHANNEL,\n            argv[3],\n            IRC_COLOR_CHAT_DELIMITERS,\n            irc_nick_color_for_msg (server, 1, NULL, argv[7]),\n            argv[7],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_HOST,\n            argv[4],\n            argv[5],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_RESET,\n            (pos_attr) ? pos_attr : \"\",\n            (pos_attr) ? \" \" : \"\",\n            (pos_hopcount) ? pos_hopcount : \"\",\n            (pos_hopcount) ? \" \" : \"\",\n            (pos_realname) ? pos_realname : \"\");\n    }\n\n    return WEECHAT_RC_OK;\n}", "commit_link": "github.com/weechat/weechat/commit/9904cb6d2eb40f679d8ff6557c22d53a3e3dc75a", "file_name": "src/plugins/irc/irc-protocol.c", "vul_type": "cwe-476", "description": "In C, write a function to handle IRC protocol 352 messages, parsing user details and updating channel and nick information."}
{"func_name": "_find_host_from_wwpn", "func_src_before": "    def _find_host_from_wwpn(self, connector):\n        for wwpn in connector['wwpns']:\n            ssh_cmd = 'svcinfo lsfabric -wwpn %s -delim !' % wwpn\n            out, err = self._run_ssh(ssh_cmd)\n\n            if not len(out.strip()):\n                # This WWPN is not in use\n                continue\n\n            host_lines = out.strip().split('\\n')\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('remote_wwpn' in header and\n                                    'name' in header,\n                                    '_find_host_from_wwpn',\n                                    ssh_cmd, out, err)\n            rmt_wwpn_idx = header.index('remote_wwpn')\n            name_idx = header.index('name')\n\n            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)\n\n            if wwpn in wwpns:\n                # All the wwpns will be the mapping for the same\n                # host from this WWPN-based query. Just pick\n                # the name from first line.\n                hostname = host_lines[0].split('!')[name_idx]\n                return hostname\n\n        # Didn't find a host\n        return None", "func_src_after": "    def _find_host_from_wwpn(self, connector):\n        for wwpn in connector['wwpns']:\n            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']\n            out, err = self._run_ssh(ssh_cmd)\n\n            if not len(out.strip()):\n                # This WWPN is not in use\n                continue\n\n            host_lines = out.strip().split('\\n')\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('remote_wwpn' in header and\n                                    'name' in header,\n                                    '_find_host_from_wwpn',\n                                    ssh_cmd, out, err)\n            rmt_wwpn_idx = header.index('remote_wwpn')\n            name_idx = header.index('name')\n\n            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)\n\n            if wwpn in wwpns:\n                # All the wwpns will be the mapping for the same\n                # host from this WWPN-based query. Just pick\n                # the name from first line.\n                hostname = host_lines[0].split('!')[name_idx]\n                return hostname\n\n        # Didn't find a host\n        return None", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "In Python, write a function to find a host's name using a WWPN (World Wide Port Name) by executing an SSH command and parsing the output."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "amqp_handle_input", "func_src_before": "int amqp_handle_input(amqp_connection_state_t state, amqp_bytes_t received_data,\n                      amqp_frame_t *decoded_frame) {\n  size_t bytes_consumed;\n  void *raw_frame;\n\n  /* Returning frame_type of zero indicates either insufficient input,\n     or a complete, ignored frame was read. */\n  decoded_frame->frame_type = 0;\n\n  if (received_data.len == 0) {\n    return AMQP_STATUS_OK;\n  }\n\n  if (state->state == CONNECTION_STATE_IDLE) {\n    state->state = CONNECTION_STATE_HEADER;\n  }\n\n  bytes_consumed = consume_data(state, &received_data);\n\n  /* do we have target_size data yet? if not, return with the\n     expectation that more will arrive */\n  if (state->inbound_offset < state->target_size) {\n    return (int)bytes_consumed;\n  }\n\n  raw_frame = state->inbound_buffer.bytes;\n\n  switch (state->state) {\n    case CONNECTION_STATE_INITIAL:\n      /* check for a protocol header from the server */\n      if (memcmp(raw_frame, \"AMQP\", 4) == 0) {\n        decoded_frame->frame_type = AMQP_PSEUDOFRAME_PROTOCOL_HEADER;\n        decoded_frame->channel = 0;\n\n        decoded_frame->payload.protocol_header.transport_high =\n            amqp_d8(amqp_offset(raw_frame, 4));\n        decoded_frame->payload.protocol_header.transport_low =\n            amqp_d8(amqp_offset(raw_frame, 5));\n        decoded_frame->payload.protocol_header.protocol_version_major =\n            amqp_d8(amqp_offset(raw_frame, 6));\n        decoded_frame->payload.protocol_header.protocol_version_minor =\n            amqp_d8(amqp_offset(raw_frame, 7));\n\n        return_to_idle(state);\n        return (int)bytes_consumed;\n      }\n\n    /* it's not a protocol header; fall through to process it as a\n       regular frame header */\n\n    case CONNECTION_STATE_HEADER: {\n      amqp_channel_t channel;\n      amqp_pool_t *channel_pool;\n      /* frame length is 3 bytes in */\n      channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      state->target_size =\n          amqp_d32(amqp_offset(raw_frame, 3)) + HEADER_SIZE + FOOTER_SIZE;\n\n      if ((size_t)state->frame_max < state->target_size) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      channel_pool = amqp_get_or_create_channel_pool(state, channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      amqp_pool_alloc_bytes(channel_pool, state->target_size,\n                            &state->inbound_buffer);\n      if (NULL == state->inbound_buffer.bytes) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n      memcpy(state->inbound_buffer.bytes, state->header_buffer, HEADER_SIZE);\n      raw_frame = state->inbound_buffer.bytes;\n\n      state->state = CONNECTION_STATE_BODY;\n\n      bytes_consumed += consume_data(state, &received_data);\n\n      /* do we have target_size data yet? if not, return with the\n         expectation that more will arrive */\n      if (state->inbound_offset < state->target_size) {\n        return (int)bytes_consumed;\n      }\n    }\n    /* fall through to process body */\n\n    case CONNECTION_STATE_BODY: {\n      amqp_bytes_t encoded;\n      int res;\n      amqp_pool_t *channel_pool;\n\n      /* Check frame end marker (footer) */\n      if (amqp_d8(amqp_offset(raw_frame, state->target_size - 1)) !=\n          AMQP_FRAME_END) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      decoded_frame->frame_type = amqp_d8(amqp_offset(raw_frame, 0));\n      decoded_frame->channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      channel_pool =\n          amqp_get_or_create_channel_pool(state, decoded_frame->channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      switch (decoded_frame->frame_type) {\n        case AMQP_FRAME_METHOD:\n          decoded_frame->payload.method.id =\n              amqp_d32(amqp_offset(raw_frame, HEADER_SIZE));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 4);\n          encoded.len = state->target_size - HEADER_SIZE - 4 - FOOTER_SIZE;\n\n          res = amqp_decode_method(decoded_frame->payload.method.id,\n                                   channel_pool, encoded,\n                                   &decoded_frame->payload.method.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_HEADER:\n          decoded_frame->payload.properties.class_id =\n              amqp_d16(amqp_offset(raw_frame, HEADER_SIZE));\n          /* unused 2-byte weight field goes here */\n          decoded_frame->payload.properties.body_size =\n              amqp_d64(amqp_offset(raw_frame, HEADER_SIZE + 4));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 12);\n          encoded.len = state->target_size - HEADER_SIZE - 12 - FOOTER_SIZE;\n          decoded_frame->payload.properties.raw = encoded;\n\n          res = amqp_decode_properties(\n              decoded_frame->payload.properties.class_id, channel_pool, encoded,\n              &decoded_frame->payload.properties.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_BODY:\n          decoded_frame->payload.body_fragment.len =\n              state->target_size - HEADER_SIZE - FOOTER_SIZE;\n          decoded_frame->payload.body_fragment.bytes =\n              amqp_offset(raw_frame, HEADER_SIZE);\n          break;\n\n        case AMQP_FRAME_HEARTBEAT:\n          break;\n\n        default:\n          /* Ignore the frame */\n          decoded_frame->frame_type = 0;\n          break;\n      }\n\n      return_to_idle(state);\n      return (int)bytes_consumed;\n    }\n\n    default:\n      amqp_abort(\"Internal error: invalid amqp_connection_state_t->state %d\",\n                 state->state);\n  }\n}", "func_src_after": "int amqp_handle_input(amqp_connection_state_t state, amqp_bytes_t received_data,\n                      amqp_frame_t *decoded_frame) {\n  size_t bytes_consumed;\n  void *raw_frame;\n\n  /* Returning frame_type of zero indicates either insufficient input,\n     or a complete, ignored frame was read. */\n  decoded_frame->frame_type = 0;\n\n  if (received_data.len == 0) {\n    return AMQP_STATUS_OK;\n  }\n\n  if (state->state == CONNECTION_STATE_IDLE) {\n    state->state = CONNECTION_STATE_HEADER;\n  }\n\n  bytes_consumed = consume_data(state, &received_data);\n\n  /* do we have target_size data yet? if not, return with the\n     expectation that more will arrive */\n  if (state->inbound_offset < state->target_size) {\n    return (int)bytes_consumed;\n  }\n\n  raw_frame = state->inbound_buffer.bytes;\n\n  switch (state->state) {\n    case CONNECTION_STATE_INITIAL:\n      /* check for a protocol header from the server */\n      if (memcmp(raw_frame, \"AMQP\", 4) == 0) {\n        decoded_frame->frame_type = AMQP_PSEUDOFRAME_PROTOCOL_HEADER;\n        decoded_frame->channel = 0;\n\n        decoded_frame->payload.protocol_header.transport_high =\n            amqp_d8(amqp_offset(raw_frame, 4));\n        decoded_frame->payload.protocol_header.transport_low =\n            amqp_d8(amqp_offset(raw_frame, 5));\n        decoded_frame->payload.protocol_header.protocol_version_major =\n            amqp_d8(amqp_offset(raw_frame, 6));\n        decoded_frame->payload.protocol_header.protocol_version_minor =\n            amqp_d8(amqp_offset(raw_frame, 7));\n\n        return_to_idle(state);\n        return (int)bytes_consumed;\n      }\n\n    /* it's not a protocol header; fall through to process it as a\n       regular frame header */\n\n    case CONNECTION_STATE_HEADER: {\n      amqp_channel_t channel;\n      amqp_pool_t *channel_pool;\n      uint32_t frame_size;\n\n      channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      /* frame length is 3 bytes in */\n      frame_size = amqp_d32(amqp_offset(raw_frame, 3));\n      /* To prevent the target_size calculation below from overflowing, check\n       * that the stated frame_size is smaller than a signed 32-bit. Given\n       * the library only allows configuring frame_max as an int32_t, and\n       * frame_size is uint32_t, the math below is safe from overflow. */\n      if (frame_size >= INT32_MAX) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      state->target_size = frame_size + HEADER_SIZE + FOOTER_SIZE;\n      if ((size_t)state->frame_max < state->target_size) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      channel_pool = amqp_get_or_create_channel_pool(state, channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      amqp_pool_alloc_bytes(channel_pool, state->target_size,\n                            &state->inbound_buffer);\n      if (NULL == state->inbound_buffer.bytes) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n      memcpy(state->inbound_buffer.bytes, state->header_buffer, HEADER_SIZE);\n      raw_frame = state->inbound_buffer.bytes;\n\n      state->state = CONNECTION_STATE_BODY;\n\n      bytes_consumed += consume_data(state, &received_data);\n\n      /* do we have target_size data yet? if not, return with the\n         expectation that more will arrive */\n      if (state->inbound_offset < state->target_size) {\n        return (int)bytes_consumed;\n      }\n    }\n    /* fall through to process body */\n\n    case CONNECTION_STATE_BODY: {\n      amqp_bytes_t encoded;\n      int res;\n      amqp_pool_t *channel_pool;\n\n      /* Check frame end marker (footer) */\n      if (amqp_d8(amqp_offset(raw_frame, state->target_size - 1)) !=\n          AMQP_FRAME_END) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      decoded_frame->frame_type = amqp_d8(amqp_offset(raw_frame, 0));\n      decoded_frame->channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      channel_pool =\n          amqp_get_or_create_channel_pool(state, decoded_frame->channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      switch (decoded_frame->frame_type) {\n        case AMQP_FRAME_METHOD:\n          decoded_frame->payload.method.id =\n              amqp_d32(amqp_offset(raw_frame, HEADER_SIZE));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 4);\n          encoded.len = state->target_size - HEADER_SIZE - 4 - FOOTER_SIZE;\n\n          res = amqp_decode_method(decoded_frame->payload.method.id,\n                                   channel_pool, encoded,\n                                   &decoded_frame->payload.method.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_HEADER:\n          decoded_frame->payload.properties.class_id =\n              amqp_d16(amqp_offset(raw_frame, HEADER_SIZE));\n          /* unused 2-byte weight field goes here */\n          decoded_frame->payload.properties.body_size =\n              amqp_d64(amqp_offset(raw_frame, HEADER_SIZE + 4));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 12);\n          encoded.len = state->target_size - HEADER_SIZE - 12 - FOOTER_SIZE;\n          decoded_frame->payload.properties.raw = encoded;\n\n          res = amqp_decode_properties(\n              decoded_frame->payload.properties.class_id, channel_pool, encoded,\n              &decoded_frame->payload.properties.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_BODY:\n          decoded_frame->payload.body_fragment.len =\n              state->target_size - HEADER_SIZE - FOOTER_SIZE;\n          decoded_frame->payload.body_fragment.bytes =\n              amqp_offset(raw_frame, HEADER_SIZE);\n          break;\n\n        case AMQP_FRAME_HEARTBEAT:\n          break;\n\n        default:\n          /* Ignore the frame */\n          decoded_frame->frame_type = 0;\n          break;\n      }\n\n      return_to_idle(state);\n      return (int)bytes_consumed;\n    }\n\n    default:\n      amqp_abort(\"Internal error: invalid amqp_connection_state_t->state %d\",\n                 state->state);\n  }\n}", "commit_link": "github.com/alanxz/rabbitmq-c/commit/fc85be7123050b91b054e45b91c78d3241a5047a", "file_name": "librabbitmq/amqp_connection.c", "vul_type": "cwe-190", "description": "Write a C function named `amqp_handle_input` that processes incoming AMQP data and decodes it into a frame structure."}
{"func_name": "CompileAndRun", "func_src_before": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "func_src_after": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 14, "char_start": 513, "char_end": 589, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n"}], "added": [{"line_no": 14, "char_start": 513, "char_end": 585, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 582, "char_end": 586, "chars": "name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "loader.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that compiles and runs a program from an input reader, handling errors and metrics."}
{"func_name": "make_tarfile", "func_src_before": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    unzipped_filename = tempfile.mktemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.remove(unzipped_filename)", "func_src_after": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    descriptor, unzipped_filename = tempfile.mkstemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.close(descriptor)\n        os.remove(unzipped_filename)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 394, "char_end": 436, "line": "    unzipped_filename = tempfile.mktemp()\n"}], "added": [{"line_no": 12, "char_start": 394, "char_end": 449, "line": "    descriptor, unzipped_filename = tempfile.mkstemp()\n"}, {"line_no": 23, "char_start": 1018, "char_end": 1047, "line": "        os.close(descriptor)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 397, "char_end": 409, "chars": " descriptor,"}, {"char_start": 441, "char_end": 442, "chars": "s"}, {"char_start": 1018, "char_end": 1047, "chars": "        os.close(descriptor)\n"}]}, "commit_link": "github.com/wandb/client/commit/7c1306c1851da5628ac96e0f89728d16852611dd", "file_name": "util.py", "vul_type": "cwe-377", "commit_msg": "Port of #3357: Fix/insecure tempfile (#3360)\n\n* Replaced insecure tempfile.mktemp() with tempfile.mkstemp()\r\nCo-authored-by: whokilleddb <whokilleddb@gmail.com>", "description": "Write a Python function to create a gzipped tar file from a directory, optionally applying a custom filter to the files."}
{"func_name": "saveNewOption", "func_src_before": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val=newValF.val();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "func_src_after": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val = $('<div/>').text(newValF.val()).html();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "line_changes": {"deleted": [{"line_no": 3, "char_start": 70, "char_end": 95, "line": "\t\tvar val=newValF.val();\n"}], "added": [{"line_no": 3, "char_start": 70, "char_end": 122, "line": "\t\tvar val = $('<div/>').text(newValF.val()).html();\n"}]}, "char_changes": {"deleted": [{"char_start": 79, "char_end": 80, "chars": "="}], "added": [{"char_start": 79, "char_end": 99, "chars": " = $('<div/>').text("}, {"char_start": 112, "char_end": 120, "chars": ").html()"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function to add a new option to a list, using a template, without sanitizing the input."}
{"func_name": "ResolveStateAndPredicate", "func_src_before": "ResolveStateAndPredicate(ExprDef *expr, enum xkb_match_operation *pred_rtrn,\n                         xkb_mod_mask_t *mods_rtrn, CompatInfo *info)\n{\n    if (expr == NULL) {\n        *pred_rtrn = MATCH_ANY_OR_NONE;\n        *mods_rtrn = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    *pred_rtrn = MATCH_EXACTLY;\n    if (expr->expr.op == EXPR_ACTION_DECL) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->action.name);\n        if (!LookupString(symInterpretMatchMaskNames, pred_txt, pred_rtrn)) {\n            log_err(info->ctx,\n                    \"Illegal modifier predicate \\\"%s\\\"; Ignored\\n\", pred_txt);\n            return false;\n        }\n        expr = expr->action.args;\n    }\n    else if (expr->expr.op == EXPR_IDENT) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->ident.ident);\n        if (pred_txt && istreq(pred_txt, \"any\")) {\n            *pred_rtrn = MATCH_ANY;\n            *mods_rtrn = MOD_REAL_MASK_ALL;\n            return true;\n        }\n    }\n\n    return ExprResolveModMask(info->ctx, expr, MOD_REAL, &info->mods,\n                              mods_rtrn);\n}", "func_src_after": "ResolveStateAndPredicate(ExprDef *expr, enum xkb_match_operation *pred_rtrn,\n                         xkb_mod_mask_t *mods_rtrn, CompatInfo *info)\n{\n    if (expr == NULL) {\n        *pred_rtrn = MATCH_ANY_OR_NONE;\n        *mods_rtrn = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    *pred_rtrn = MATCH_EXACTLY;\n    if (expr->expr.op == EXPR_ACTION_DECL) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->action.name);\n        if (!LookupString(symInterpretMatchMaskNames, pred_txt, pred_rtrn) ||\n            !expr->action.args) {\n            log_err(info->ctx,\n                    \"Illegal modifier predicate \\\"%s\\\"; Ignored\\n\", pred_txt);\n            return false;\n        }\n        expr = expr->action.args;\n    }\n    else if (expr->expr.op == EXPR_IDENT) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->ident.ident);\n        if (pred_txt && istreq(pred_txt, \"any\")) {\n            *pred_rtrn = MATCH_ANY;\n            *mods_rtrn = MOD_REAL_MASK_ALL;\n            return true;\n        }\n    }\n\n    return ExprResolveModMask(info->ctx, expr, MOD_REAL, &info->mods,\n                              mods_rtrn);\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/96df3106d49438e442510c59acad306e94f3db4d", "file_name": "src/xkbcomp/compat.c", "vul_type": "cwe-476", "description": "In C, write a function `ResolveStateAndPredicate` that processes an expression to determine a matching operation and modifier mask for keyboard input compatibility."}
{"func_name": "add_rpmmd_repo", "func_src_before": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      gz = open(primary_xml)\n      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n      repo.add_rpmmd(fd, nil, 0)\n      pool.createwhatprovides\n    ensure\n      gz&.close\n    end", "func_src_after": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      File.open(primary_xml) do |gz|\n        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n        repo.add_rpmmd(fd, nil, 0)\n        pool.createwhatprovides\n      end\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 75, "char_end": 104, "line": "      gz = open(primary_xml)\n"}, {"line_no": 4, "char_start": 104, "char_end": 154, "line": "      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 154, "char_end": 187, "line": "      repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 187, "char_end": 217, "line": "      pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 217, "char_end": 228, "line": "    ensure\n"}, {"line_no": 8, "char_start": 228, "char_end": 244, "line": "      gz&.close\n"}], "added": [{"line_no": 3, "char_start": 75, "char_end": 112, "line": "      File.open(primary_xml) do |gz|\n"}, {"line_no": 4, "char_start": 112, "char_end": 164, "line": "        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 164, "char_end": 199, "line": "        repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 199, "char_end": 231, "line": "        pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 231, "char_end": 241, "line": "      end\n"}]}, "char_changes": {"deleted": [{"char_start": 81, "char_end": 86, "chars": "gz = "}, {"char_start": 103, "char_end": 104, "chars": "\n"}, {"char_start": 221, "char_end": 243, "chars": "ensure\n      gz&.close"}], "added": [{"char_start": 81, "char_end": 86, "chars": "File."}, {"char_start": 103, "char_end": 114, "chars": " do |gz|\n  "}, {"char_start": 170, "char_end": 172, "chars": "  "}, {"char_start": 205, "char_end": 206, "chars": " "}, {"char_start": 206, "char_end": 207, "chars": " "}, {"char_start": 235, "char_end": 240, "chars": "  end"}]}, "commit_link": "github.com/yast/yast-packager/commit/a25ddf08b43ff58ceaebb1aa7c01e2804f98864e", "file_name": "solvable_pool.rb", "vul_type": "cwe-078", "commit_msg": "use secure File.open instead of Kernel.open (NO-AUTO)", "parent_commit": "915385e0672aef903b49b7f735a655a051b68f6a", "description": "Write a Ruby function to add an RPM metadata repository by reading a primary XML file."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion > 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "func_src_after": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion >= 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "line_changes": {"deleted": [{"line_no": 105, "char_start": 4704, "char_end": 4746, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 185, "char_start": 8062, "char_end": 8104, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 241, "char_start": 10904, "char_end": 10946, "line": "        if self._hexversion > 0x10100000:\n"}], "added": [{"line_no": 105, "char_start": 4704, "char_end": 4747, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 185, "char_start": 8063, "char_end": 8106, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 241, "char_start": 10906, "char_end": 10949, "line": "        if self._hexversion >= 0x10100000:\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4733, "char_end": 4734, "chars": "="}, {"char_start": 8092, "char_end": 8093, "chars": "="}, {"char_start": 10935, "char_end": 10936, "chars": "="}]}, "commit_link": "github.com/bmng-dev/PyBitmessage/commit/59b5ac3a61fcfb1fd55812198ffe208243c3c160", "file_name": "openssl.py", "vul_type": "cwe-327", "commit_msg": "OpenSSL 1.1.0 compatibility fixes\n\n- function check missed 1.1.0 release\n- TLS didn't work with anonymous ciphers", "description": "Create a Python class that wraps cryptographic functions from a dynamic library using ctypes."}
{"func_name": "get_list_context", "func_src_before": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "func_src_after": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "commit_link": "github.com/omirajkar/bench_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "file_name": "frappe/website/doctype/blog_post/blog_post.py", "vul_type": "cwe-079", "description": "Write a Python function in Frappe to customize the context of a blog list page based on filters like category, blogger, or search text."}
{"func_name": "rfbHandleAuthResult", "func_src_before": "rfbHandleAuthResult(rfbClient* client)\n{\n    uint32_t authResult=0, reasonLen=0;\n    char *reason=NULL;\n\n    if (!ReadFromRFBServer(client, (char *)&authResult, 4)) return FALSE;\n\n    authResult = rfbClientSwap32IfLE(authResult);\n\n    switch (authResult) {\n    case rfbVncAuthOK:\n      rfbClientLog(\"VNC authentication succeeded\\n\");\n      return TRUE;\n      break;\n    case rfbVncAuthFailed:\n      if (client->major==3 && client->minor>7)\n      {\n        /* we have an error following */\n        if (!ReadFromRFBServer(client, (char *)&reasonLen, 4)) return FALSE;\n        reasonLen = rfbClientSwap32IfLE(reasonLen);\n        reason = malloc((uint64_t)reasonLen+1);\n        if (!ReadFromRFBServer(client, reason, reasonLen)) { free(reason); return FALSE; }\n        reason[reasonLen]=0;\n        rfbClientLog(\"VNC connection failed: %s\\n\",reason);\n        free(reason);\n        return FALSE;\n      }\n      rfbClientLog(\"VNC authentication failed\\n\");\n      return FALSE;\n    case rfbVncAuthTooMany:\n      rfbClientLog(\"VNC authentication failed - too many tries\\n\");\n      return FALSE;\n    }\n\n    rfbClientLog(\"Unknown VNC authentication result: %d\\n\",\n                 (int)authResult);\n    return FALSE;\n}", "func_src_after": "rfbHandleAuthResult(rfbClient* client)\n{\n    uint32_t authResult=0;\n\n    if (!ReadFromRFBServer(client, (char *)&authResult, 4)) return FALSE;\n\n    authResult = rfbClientSwap32IfLE(authResult);\n\n    switch (authResult) {\n    case rfbVncAuthOK:\n      rfbClientLog(\"VNC authentication succeeded\\n\");\n      return TRUE;\n      break;\n    case rfbVncAuthFailed:\n      if (client->major==3 && client->minor>7)\n      {\n        /* we have an error following */\n        ReadReason(client);\n        return FALSE;\n      }\n      rfbClientLog(\"VNC authentication failed\\n\");\n      return FALSE;\n    case rfbVncAuthTooMany:\n      rfbClientLog(\"VNC authentication failed - too many tries\\n\");\n      return FALSE;\n    }\n\n    rfbClientLog(\"Unknown VNC authentication result: %d\\n\",\n                 (int)authResult);\n    return FALSE;\n}", "commit_link": "github.com/LibVNC/libvncserver/commit/e34bcbb759ca5bef85809967a268fdf214c1ad2c", "file_name": "libvncclient/rfbproto.c", "vul_type": "cwe-787", "description": "Write a C function named `rfbHandleAuthResult` that processes VNC authentication results from a server."}
{"func_name": "archive_directory", "func_src_before": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(top_dir, subdir, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in file_list:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n                return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "func_src_after": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(full_backup_path, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in files:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n            return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "line_changes": {"deleted": [{"line_no": 20, "char_start": 1003, "char_end": 1052, "line": "        fpath = os.path.join(top_dir, subdir, c)\n"}, {"line_no": 37, "char_start": 1561, "char_end": 1580, "line": "    file_list = []\n"}, {"line_no": 38, "char_start": 1580, "char_end": 1623, "line": "    for p in os.listdir(full_backup_path):\n"}, {"line_no": 39, "char_start": 1623, "char_end": 1685, "line": "        if os.path.isfile(os.path.join(full_backup_path, p)):\n"}, {"line_no": 40, "char_start": 1685, "char_end": 1749, "line": "            file_list.append(os.path.join(full_backup_path, p))\n"}, {"line_no": 44, "char_start": 1876, "char_end": 1908, "line": "            for f in file_list:\n"}, {"line_no": 47, "char_start": 2050, "char_end": 2091, "line": "                return archive_file_path\n"}], "added": [{"line_no": 20, "char_start": 1003, "char_end": 1053, "line": "        fpath = os.path.join(full_backup_path, c)\n"}, {"line_no": 40, "char_start": 1689, "char_end": 1717, "line": "            for f in files:\n"}, {"line_no": 43, "char_start": 1859, "char_end": 1896, "line": "            return archive_file_path\n"}]}, "char_changes": {"deleted": [{"char_start": 1032, "char_end": 1047, "chars": "top_dir, subdir"}, {"char_start": 1560, "char_end": 1748, "chars": "\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))"}, {"char_start": 1901, "char_end": 1906, "chars": "_list"}, {"char_start": 2050, "char_end": 2054, "chars": "    "}], "added": [{"char_start": 1032, "char_end": 1048, "chars": "full_backup_path"}, {"char_start": 1714, "char_end": 1715, "chars": "s"}]}, "commit_link": "github.com/calmcl1/cupo-backup/commit/f9047a52ab33a14fcd67d3d8b9f9d321502ae457", "file_name": "cupo.py", "vul_type": "cwe-022", "commit_msg": "Removed redundant directory traversal", "parent_commit": "49107dd052b985e1fc469aec359f37df23ec3e29", "description": "Write a Python function to zip files in a specified subdirectory, excluding '.ini' files, and save the archive to a temporary directory."}
{"func_name": "article", "func_src_before": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n  end", "func_src_after": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 55, "char_end": 138, "line": "    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n"}], "added": [{"line_no": 3, "char_start": 55, "char_end": 137, "line": "    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 108, "chars": "\""}, {"char_start": 121, "char_end": 123, "chars": "#{"}, {"char_start": 135, "char_end": 136, "chars": "\""}], "added": [{"char_start": 107, "char_end": 109, "chars": "{:"}, {"char_start": 121, "char_end": 122, "chars": ">"}]}, "commit_link": "github.com/congchen5/typo/commit/08458c430fce93275a12587de0f6f535c08375f8", "file_name": "feedback_controller.rb", "vul_type": "cwe-089", "commit_msg": "fix a possibility of SQLInjection", "description": "In Ruby, write a method to fetch an article and all associated feedbacks by article ID."}
{"func_name": "parse", "func_src_before": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "func_src_after": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.safe_load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "line_changes": {"deleted": [{"line_no": 10, "char_start": 298, "char_end": 334, "line": "                return yaml.load(f)\n"}], "added": [{"line_no": 10, "char_start": 298, "char_end": 339, "line": "                return yaml.safe_load(f)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 326, "char_end": 331, "chars": "safe_"}]}, "commit_link": "github.com/silas/rock/commit/0852b4f55891e5dfe7cc6af3881942484daf9132", "file_name": "config.py", "vul_type": "cwe-502", "commit_msg": "Use yaml.safe_load instead of yaml.load", "parent_commit": "93a26daa34d92236cfcfe4e7ccf0d4814687c009", "description": "Create a Python function that loads a YAML file, with options to enforce file existence and successful parsing."}
{"func_name": "TestCreateBasket_InvalidName", "func_src_before": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "func_src_after": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 447, "char_end": 574, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 12, "char_start": 447, "char_end": 570, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 487, "char_end": 499, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 487, "char_end": 495, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function to validate that creating a basket with an invalid name results in a bad request and no database entry."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2190, "char_end": 2234, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2362, "char_end": 2437, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2405, "char_end": 2436, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/6914c36ce984cccebed7d89c4791e80511fdf47e", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function using pyOpenSSL to initiate an SSL/TLS handshake with optional client certificate authentication."}
{"func_name": "is_case_sensitive", "func_src_before": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = os.tempnam(self.base_folder, '.caseTest_')\n            os.mkdir(path)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "func_src_after": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "line_changes": {"deleted": [{"line_no": 4, "char_start": 149, "char_end": 211, "line": "            path = os.tempnam(self.base_folder, '.caseTest_')\n"}, {"line_no": 5, "char_start": 211, "char_end": 238, "line": "            os.mkdir(path)\n"}], "added": [{"line_no": 4, "char_start": 149, "char_end": 228, "line": "            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n"}]}, "char_changes": {"deleted": [{"char_start": 168, "char_end": 171, "chars": "os."}, {"char_start": 175, "char_end": 236, "chars": "nam(self.base_folder, '.caseTest_')\n            os.mkdir(path"}], "added": [{"char_start": 172, "char_end": 226, "chars": "file.mkdtemp(prefix='.caseTest_', dir=self.base_folder"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/8349d04615fc0f02b4b7588f04a24382431e2d3a", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-170: Fix encoding issue when testing for case sensitive OS (and don't use unsafe tempnam)", "description": "Write a Python function to determine if a filesystem is case-sensitive."}
{"func_name": "WavpackVerifySingleBlock", "func_src_before": "int WavpackVerifySingleBlock (unsigned char *buffer, int verify_checksum)\n{\n    WavpackHeader *wphdr = (WavpackHeader *) buffer;\n    uint32_t checksum_passed = 0, bcount, meta_bc;\n    unsigned char *dp, meta_id, c1, c2;\n\n    if (strncmp (wphdr->ckID, \"wvpk\", 4) || wphdr->ckSize + 8 < sizeof (WavpackHeader))\n        return FALSE;\n\n    bcount = wphdr->ckSize - sizeof (WavpackHeader) + 8;\n    dp = (unsigned char *)(wphdr + 1);\n\n    while (bcount >= 2) {\n        meta_id = *dp++;\n        c1 = *dp++;\n\n        meta_bc = c1 << 1;\n        bcount -= 2;\n\n        if (meta_id & ID_LARGE) {\n            if (bcount < 2)\n                return FALSE;\n\n            c1 = *dp++;\n            c2 = *dp++;\n            meta_bc += ((uint32_t) c1 << 9) + ((uint32_t) c2 << 17);\n            bcount -= 2;\n        }\n\n        if (bcount < meta_bc)\n            return FALSE;\n\n        if (verify_checksum && (meta_id & ID_UNIQUE) == ID_BLOCK_CHECKSUM) {\n#ifdef BITSTREAM_SHORTS\n            uint16_t *csptr = (uint16_t*) buffer;\n#else\n            unsigned char *csptr = buffer;\n#endif\n            int wcount = (int)(dp - 2 - buffer) >> 1;\n            uint32_t csum = (uint32_t) -1;\n\n            if ((meta_id & ID_ODD_SIZE) || meta_bc < 2 || meta_bc > 4)\n                return FALSE;\n\n#ifdef BITSTREAM_SHORTS\n            while (wcount--)\n                csum = (csum * 3) + *csptr++;\n#else\n            WavpackNativeToLittleEndian ((WavpackHeader *) buffer, WavpackHeaderFormat);\n\n            while (wcount--) {\n                csum = (csum * 3) + csptr [0] + (csptr [1] << 8);\n                csptr += 2;\n            }\n\n            WavpackLittleEndianToNative ((WavpackHeader *) buffer, WavpackHeaderFormat);\n#endif\n\n            if (meta_bc == 4) {\n                if (*dp++ != (csum & 0xff) || *dp++ != ((csum >> 8) & 0xff) || *dp++ != ((csum >> 16) & 0xff) || *dp++ != ((csum >> 24) & 0xff))\n                    return FALSE;\n            }\n            else {\n                csum ^= csum >> 16;\n\n                if (*dp++ != (csum & 0xff) || *dp++ != ((csum >> 8) & 0xff))\n                    return FALSE;\n            }\n\n            checksum_passed++;\n        }\n\n        bcount -= meta_bc;\n        dp += meta_bc;\n    }\n\n    return (bcount == 0) && (!verify_checksum || !(wphdr->flags & HAS_CHECKSUM) || checksum_passed);\n}", "func_src_after": "int WavpackVerifySingleBlock (unsigned char *buffer, int verify_checksum)\n{\n    WavpackHeader *wphdr = (WavpackHeader *) buffer;\n    uint32_t checksum_passed = 0, bcount, meta_bc;\n    unsigned char *dp, meta_id, c1, c2;\n\n    if (strncmp (wphdr->ckID, \"wvpk\", 4) || wphdr->ckSize + 8 < sizeof (WavpackHeader))\n        return FALSE;\n\n    bcount = wphdr->ckSize - sizeof (WavpackHeader) + 8;\n    dp = (unsigned char *)(wphdr + 1);\n\n    while (bcount >= 2) {\n        meta_id = *dp++;\n        c1 = *dp++;\n\n        meta_bc = c1 << 1;\n        bcount -= 2;\n\n        if (meta_id & ID_LARGE) {\n            if (bcount < 2)\n                return FALSE;\n\n            c1 = *dp++;\n            c2 = *dp++;\n            meta_bc += ((uint32_t) c1 << 9) + ((uint32_t) c2 << 17);\n            bcount -= 2;\n        }\n\n        if (bcount < meta_bc)\n            return FALSE;\n\n        if (verify_checksum && (meta_id & ID_UNIQUE) == ID_BLOCK_CHECKSUM) {\n#ifdef BITSTREAM_SHORTS\n            uint16_t *csptr = (uint16_t*) buffer;\n#else\n            unsigned char *csptr = buffer;\n#endif\n            int wcount = (int)(dp - 2 - buffer) >> 1;\n            uint32_t csum = (uint32_t) -1;\n\n            if ((meta_id & ID_ODD_SIZE) || meta_bc < 2 || meta_bc > 4)\n                return FALSE;\n\n#ifdef BITSTREAM_SHORTS\n            while (wcount--)\n                csum = (csum * 3) + *csptr++;\n#else\n            WavpackNativeToLittleEndian ((WavpackHeader *) buffer, WavpackHeaderFormat);\n\n            while (wcount--) {\n                csum = (csum * 3) + csptr [0] + (csptr [1] << 8);\n                csptr += 2;\n            }\n\n            WavpackLittleEndianToNative ((WavpackHeader *) buffer, WavpackHeaderFormat);\n#endif\n\n            if (meta_bc == 4) {\n                if (*dp != (csum & 0xff) || dp[1] != ((csum >> 8) & 0xff) || dp[2] != ((csum >> 16) & 0xff) || dp[3] != ((csum >> 24) & 0xff))\n                    return FALSE;\n            }\n            else {\n                csum ^= csum >> 16;\n\n                if (*dp != (csum & 0xff) || dp[1] != ((csum >> 8) & 0xff))\n                    return FALSE;\n            }\n\n            checksum_passed++;\n        }\n\n        bcount -= meta_bc;\n        dp += meta_bc;\n    }\n\n    return (bcount == 0) && (!verify_checksum || !(wphdr->flags & HAS_CHECKSUM) || checksum_passed);\n}", "commit_link": "github.com/dbry/WavPack/commit/bba5389dc598a92bdf2b297c3ea34620b6679b5b", "file_name": "src/open_utils.c", "vul_type": "cwe-125", "description": "In C, write a function to verify the integrity of a single Wavpack audio block, optionally checking its checksum."}
{"func_name": "PHP_FUNCTION", "func_src_before": "PHP_FUNCTION(imagegammacorrect)\n{\n\tzval *IM;\n\tgdImagePtr im;\n\tint i;\n\tdouble input, output;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rdd\", &IM, &input, &output) == FAILURE) {\n\t\treturn;\n\t}\n\n\tZEND_FETCH_RESOURCE(im, gdImagePtr, &IM, -1, \"Image\", le_gd);\n\n\tif (gdImageTrueColor(im))\t{\n\t\tint x, y, c;\n\n\t\tfor (y = 0; y < gdImageSY(im); y++)\t{\n\t\t\tfor (x = 0; x < gdImageSX(im); x++)\t{\n\t\t\t\tc = gdImageGetPixel(im, x, y);\n\t\t\t\tgdImageSetPixel(im, x, y,\n\t\t\t\t\tgdTrueColorAlpha(\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetRed(c)   / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetGreen(c) / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetBlue(c)  / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\tgdTrueColorGetAlpha(c)\n\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tRETURN_TRUE;\n\t}\n\n\tfor (i = 0; i < gdImageColorsTotal(im); i++) {\n\t\tim->red[i]   = (int)((pow((pow((im->red[i]   / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->green[i] = (int)((pow((pow((im->green[i] / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->blue[i]  = (int)((pow((pow((im->blue[i]  / 255.0), input)), 1.0 / output) * 255) + .5);\n\t}\n\n\tRETURN_TRUE;\n}", "func_src_after": "PHP_FUNCTION(imagegammacorrect)\n{\n\tzval *IM;\n\tgdImagePtr im;\n\tint i;\n\tdouble input, output;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rdd\", &IM, &input, &output) == FAILURE) {\n\t\treturn;\n\t}\n\n\tif ( input <= 0.0 || output <= 0.0 ) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Gamma values should be positive\");\n\t\tRETURN_FALSE;\n\t}\n\n\tZEND_FETCH_RESOURCE(im, gdImagePtr, &IM, -1, \"Image\", le_gd);\n\n\tif (gdImageTrueColor(im))\t{\n\t\tint x, y, c;\n\n\t\tfor (y = 0; y < gdImageSY(im); y++)\t{\n\t\t\tfor (x = 0; x < gdImageSX(im); x++)\t{\n\t\t\t\tc = gdImageGetPixel(im, x, y);\n\t\t\t\tgdImageSetPixel(im, x, y,\n\t\t\t\t\tgdTrueColorAlpha(\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetRed(c)   / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetGreen(c) / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetBlue(c)  / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\tgdTrueColorGetAlpha(c)\n\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tRETURN_TRUE;\n\t}\n\n\tfor (i = 0; i < gdImageColorsTotal(im); i++) {\n\t\tim->red[i]   = (int)((pow((pow((im->red[i]   / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->green[i] = (int)((pow((pow((im->green[i] / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->blue[i]  = (int)((pow((pow((im->blue[i]  / 255.0), input)), 1.0 / output) * 255) + .5);\n\t}\n\n\tRETURN_TRUE;\n}", "commit_link": "github.com/php/php-src/commit/1bd103df00f49cf4d4ade2cfe3f456ac058a4eae", "file_name": "ext/gd/gd.c", "vul_type": "cwe-787", "description": "Write a PHP function to apply gamma correction to an image resource with given input and output gamma values."}
{"func_name": "encryptPassword", "func_src_before": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n    }", "func_src_after": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return bcrypt.hashSync(password, 10);\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 76, "char_end": 160, "line": "        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n"}], "added": [{"line_no": 3, "char_start": 76, "char_end": 122, "line": "        return bcrypt.hashSync(password, 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 96, "char_end": 157, "chars": "o.createHmac('sha1', this.salt).update(password).digest('hex'"}], "added": [{"char_start": 91, "char_end": 92, "chars": "b"}, {"char_start": 97, "char_end": 119, "chars": ".hashSync(password, 10"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `encryptPassword` that takes a password string and returns its hashed version."}
{"func_name": "generate_fZ", "func_src_before": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "func_src_after": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                tmpfZ = pickle.load(f)\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "line_changes": {"deleted": [{"line_no": 25, "char_start": 1257, "char_end": 1296, "line": "                print(pickle.load(f))\r\n"}, {"line_no": 27, "char_start": 1336, "char_end": 1358, "line": "                try:\r\n"}, {"line_no": 28, "char_start": 1358, "char_end": 1389, "line": "                    f.close()\r\n"}, {"line_no": 29, "char_start": 1389, "char_end": 1414, "line": "                except:\r\n"}, {"line_no": 30, "char_start": 1414, "char_end": 1440, "line": "                    pass\r\n"}, {"line_no": 48, "char_start": 2302, "char_end": 2362, "line": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1273, "char_end": 1438, "chars": "print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass"}, {"char_start": 2302, "char_end": 2362, "chars": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": [{"char_start": 1273, "char_end": 1295, "chars": "tmpfZ = pickle.load(f)"}]}, "commit_link": "github.com/dsavransky/EXOSIMS/commit/2df12d23c54a140161c24e92b3c03aaf522c61ec", "file_name": "ZodiacalLight.py", "vul_type": "cwe-502", "commit_msg": "fixed pickle load errors", "parent_commit": "c4660a0de665797559fd4d048b4d971661366f50", "description": "In Python, write a function to calculate and cache surface brightness values for stars, loading from cache if available."}
{"func_name": "search", "func_src_before": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "func_src_after": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 96, "char_end": 203, "line": "    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n"}], "added": [{"line_no": 3, "char_start": 96, "char_end": 195, "line": "    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n"}]}, "char_changes": {"deleted": [{"char_start": 140, "char_end": 200, "chars": "'%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'"}], "added": [{"char_start": 140, "char_end": 192, "chars": "? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%"}]}, "commit_link": "github.com/ryupitbros4/itswitter/commit/8847c333ae9d3e632e4d31b92e984be76e57354a", "file_name": "restaurants_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent SQL injection.", "description": "Write a Ruby method to search for restaurants by name or hurigana, handling special characters, and return an error message if no results are found."}
{"func_name": "pathRoleWrite", "func_src_before": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 1024 and it can also be 2048 and 4096.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "func_src_after": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 2048, but it can also be 1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "line_changes": {"deleted": [{"line_no": 99, "char_start": 3202, "char_end": 3279, "line": "\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n"}, {"line_no": 100, "char_start": 3279, "char_end": 3342, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 101, "char_start": 3342, "char_end": 3346, "line": "\t\t}\n"}, {"line_no": 102, "char_start": 3346, "char_end": 3347, "line": "\n"}], "added": [{"line_no": 103, "char_start": 3320, "char_end": 3419, "line": "\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n"}, {"line_no": 104, "char_start": 3419, "char_end": 3482, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 105, "char_start": 3482, "char_end": 3486, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 3126, "char_end": 3134, "chars": "1024 and"}, {"char_start": 3150, "char_end": 3163, "chars": "2048 and 4096"}, {"char_start": 3215, "char_end": 3216, "chars": "!"}, {"char_start": 3220, "char_end": 3222, "chars": "&&"}, {"char_start": 3272, "char_end": 3276, "chars": "4096"}, {"char_start": 3347, "char_end": 3446, "chars": "\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n"}], "added": [{"char_start": 3126, "char_end": 3135, "chars": "2048, but"}, {"char_start": 3151, "char_end": 3239, "chars": "1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys"}, {"char_start": 3291, "char_end": 3292, "chars": "="}, {"char_start": 3296, "char_end": 3324, "chars": "{\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif"}, {"char_start": 3374, "char_end": 3416, "chars": "3072 && keyBits != 4096 && keyBits != 8192"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "path_roles.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to handle creating or updating SSH role configurations based on provided fields."}
{"func_name": "mpeg4_decode_studio_block", "func_src_before": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "func_src_after": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/d227ed5d598340e719eff7156b1aa0a4469e9a6a", "file_name": "libavcodec/mpeg4videodec.c", "vul_type": "cwe-125", "description": "Write a C function named `mpeg4_decode_studio_block` that decodes a single block of MPEG4 studio profile video."}
{"func_name": "_inject_admin_password_into_fs", "func_src_before": "def _inject_admin_password_into_fs(admin_passwd, fs, execute=None):\n    \"\"\"Set the root password to admin_passwd\n\n    admin_password is a root password\n    fs is the path to the base of the filesystem into which to inject\n    the key.\n\n    This method modifies the instance filesystem directly,\n    and does not require a guest agent running in the instance.\n\n    \"\"\"\n    # The approach used here is to copy the password and shadow\n    # files from the instance filesystem to local files, make any\n    # necessary changes, and then copy them back.\n\n    admin_user = 'root'\n\n    fd, tmp_passwd = tempfile.mkstemp()\n    os.close(fd)\n    fd, tmp_shadow = tempfile.mkstemp()\n    os.close(fd)\n\n    utils.execute('cp', os.path.join(fs, 'etc', 'passwd'), tmp_passwd,\n                  run_as_root=True)\n    utils.execute('cp', os.path.join(fs, 'etc', 'shadow'), tmp_shadow,\n                  run_as_root=True)\n    _set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)\n    utils.execute('cp', tmp_passwd, os.path.join(fs, 'etc', 'passwd'),\n                  run_as_root=True)\n    os.unlink(tmp_passwd)\n    utils.execute('cp', tmp_shadow, os.path.join(fs, 'etc', 'shadow'),\n                  run_as_root=True)\n    os.unlink(tmp_shadow)", "func_src_after": "def _inject_admin_password_into_fs(admin_passwd, fs, execute=None):\n    \"\"\"Set the root password to admin_passwd\n\n    admin_password is a root password\n    fs is the path to the base of the filesystem into which to inject\n    the key.\n\n    This method modifies the instance filesystem directly,\n    and does not require a guest agent running in the instance.\n\n    \"\"\"\n    # The approach used here is to copy the password and shadow\n    # files from the instance filesystem to local files, make any\n    # necessary changes, and then copy them back.\n\n    admin_user = 'root'\n\n    fd, tmp_passwd = tempfile.mkstemp()\n    os.close(fd)\n    fd, tmp_shadow = tempfile.mkstemp()\n    os.close(fd)\n\n    passwd_path = _join_and_check_path_within_fs(fs, 'etc', 'passwd')\n    shadow_path = _join_and_check_path_within_fs(fs, 'etc', 'shadow')\n\n    utils.execute('cp', passwd_path, tmp_passwd, run_as_root=True)\n    utils.execute('cp', shadow_path, tmp_shadow, run_as_root=True)\n    _set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)\n    utils.execute('cp', tmp_passwd, passwd_path, run_as_root=True)\n    os.unlink(tmp_passwd)\n    utils.execute('cp', tmp_shadow, shadow_path, run_as_root=True)\n    os.unlink(tmp_shadow)", "commit_link": "github.com/openstack/nova/commit/2427d4a99bed35baefd8f17ba422cb7aae8dcca7", "file_name": "nova/virt/disk/api.py", "vul_type": "cwe-022", "description": "Provide a Python function to set the root password on a filesystem without using a guest agent."}
{"func_name": "fetch", "func_src_before": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  os.system('shasum -c %s.sha256' % base)\n  os.system('shasum -c %s.asc.sha256' % base)\n  os.system('gpg --verify %s.asc %s' % (base, base))\n  os.system('keybase verify %s.asc' % base)", "func_src_after": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  subprocess.check_call(['shasum', '-c', base + '.sha256'])\n  subprocess.check_call(['shasum', '-c', base + '.asc.sha256'])\n  subprocess.check_call(['gpg', '--verify', base + '.asc', base])\n  subprocess.check_call(['keybase', 'verify', base + '.asc'])", "commit_link": "github.com/rillian/rust-build/commit/b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb", "file_name": "repack_rust.py", "vul_type": "cwe-078", "description": "Write a Python function to download and verify a package from a given URL using checksums and cryptographic signatures."}
{"func_name": "disk_seqf_stop", "func_src_before": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t}\n}", "func_src_after": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t\tseqf->private = NULL;\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/77da160530dd1dc94f6ae15a981f24e5f0021e84", "file_name": "block/genhd.c", "vul_type": "cwe-416", "description": "Write a C function named `disk_seqf_stop` that cleans up an iterator for a sequence file, ensuring memory is freed and the iterator is reset if necessary."}
{"func_name": "process_packet_tail", "func_src_before": "void process_packet_tail(struct msg_digest *md)\n{\n\tstruct state *st = md->st;\n\tenum state_kind from_state = md->v1_from_state;\n\tconst struct state_v1_microcode *smc = md->smc;\n\tbool new_iv_set = md->new_iv_set;\n\tbool self_delete = FALSE;\n\n\tif (md->hdr.isa_flags & ISAKMP_FLAGS_v1_ENCRYPTION) {\n\t\tendpoint_buf b;\n\t\tdbg(\"received encrypted packet from %s\", str_endpoint(&md->sender, &b));\n\n\t\tif (st == NULL) {\n\t\t\tlibreswan_log(\n\t\t\t\t\"discarding encrypted message for an unknown ISAKMP SA\");\n\t\t\treturn;\n\t\t}\n\t\tif (st->st_skeyid_e_nss == NULL) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\"discarding encrypted message because we haven't yet negotiated keying material\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* Mark as encrypted */\n\t\tmd->encrypted = TRUE;\n\n\t\t/* do the specified decryption\n\t\t *\n\t\t * IV is from st->st_iv or (if new_iv_set) st->st_new_iv.\n\t\t * The new IV is placed in st->st_new_iv\n\t\t *\n\t\t * See RFC 2409 \"IKE\" Appendix B\n\t\t *\n\t\t * XXX The IV should only be updated really if the packet\n\t\t * is successfully processed.\n\t\t * We should keep this value, check for a success return\n\t\t * value from the parsing routines and then replace.\n\t\t *\n\t\t * Each post phase 1 exchange generates IVs from\n\t\t * the last phase 1 block, not the last block sent.\n\t\t */\n\t\tconst struct encrypt_desc *e = st->st_oakley.ta_encrypt;\n\n\t\tif (pbs_left(&md->message_pbs) % e->enc_blocksize != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS, \"malformed message: not a multiple of encryption blocksize\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* XXX Detect weak keys */\n\n\t\t/* grab a copy of raw packet (for duplicate packet detection) */\n\t\tmd->raw_packet = clone_bytes_as_chunk(md->packet_pbs.start,\n\t\t\t\t\t\t      pbs_room(&md->packet_pbs),\n\t\t\t\t\t\t      \"raw packet\");\n\n\t\t/* Decrypt everything after header */\n\t\tif (!new_iv_set) {\n\t\t\tif (st->st_v1_iv.len == 0) {\n\t\t\t\tinit_phase2_iv(st, &md->hdr.isa_msgid);\n\t\t\t} else {\n\t\t\t\t/* use old IV */\n\t\t\t\trestore_new_iv(st, st->st_v1_iv);\n\t\t\t}\n\t\t}\n\n\t\tpassert(st->st_v1_new_iv.len >= e->enc_blocksize);\n\t\tst->st_v1_new_iv.len = e->enc_blocksize;   /* truncate */\n\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_log(\"decrypting %u bytes using algorithm %s\",\n\t\t\t\t(unsigned) pbs_left(&md->message_pbs),\n\t\t\t\tst->st_oakley.ta_encrypt->common.fqn);\n\t\t\tDBG_dump_hunk(\"IV before:\", st->st_v1_new_iv);\n\t\t}\n\t\te->encrypt_ops->do_crypt(e, md->message_pbs.cur,\n\t\t\t\t\t pbs_left(&md->message_pbs),\n\t\t\t\t\t st->st_enc_key_nss,\n\t\t\t\t\t st->st_v1_new_iv.ptr, FALSE);\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_dump_hunk(\"IV after:\", st->st_v1_new_iv);\n\t\t\tDBG_log(\"decrypted payload (starts at offset %td):\",\n\t\t\t\tmd->message_pbs.cur - md->message_pbs.roof);\n\t\t\tDBG_dump(NULL, md->message_pbs.start,\n\t\t\t\t md->message_pbs.roof - md->message_pbs.start);\n\t\t}\n\t} else {\n\t\t/* packet was not encryped -- should it have been? */\n\n\t\tif (smc->flags & SMF_INPUT_ENCRYPTED) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"packet rejected: should have been encrypted\");\n\t\t\tSEND_NOTIFICATION(INVALID_FLAGS);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Digest the message.\n\t * Padding must be removed to make hashing work.\n\t * Padding comes from encryption (so this code must be after decryption).\n\t * Padding rules are described before the definition of\n\t * struct isakmp_hdr in packet.h.\n\t */\n\t{\n\t\tenum next_payload_types_ikev1 np = md->hdr.isa_np;\n\t\tlset_t needed = smc->req_payloads;\n\t\tconst char *excuse =\n\t\t\tLIN(SMF_PSK_AUTH | SMF_FIRST_ENCRYPTED_INPUT,\n\t\t\t    smc->flags) ?\n\t\t\t\"probable authentication failure (mismatch of preshared secrets?): \"\n\t\t\t:\n\t\t\t\"\";\n\n\t\twhile (np != ISAKMP_NEXT_NONE) {\n\t\t\tstruct_desc *sd = v1_payload_desc(np);\n\n\t\t\tif (md->digest_roof >= elemsof(md->digest)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"more than %zu payloads in message; ignored\",\n\t\t\t\t       elemsof(md->digest));\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tstruct payload_digest *const pd = md->digest + md->digest_roof;\n\n\t\t\t/*\n\t\t\t * only do this in main mode. In aggressive mode, there\n\t\t\t * is no negotiation of NAT-T method. Get it right.\n\t\t\t */\n\t\t\tif (st != NULL && st->st_connection != NULL &&\n\t\t\t    (st->st_connection->policy & POLICY_AGGRESSIVE) == LEMPTY)\n\t\t\t{\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_NATD_RFC:\n\t\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t\tif ((st->hidden_variables.st_nat_traversal & NAT_T_WITH_RFC_VALUES) == LEMPTY) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * don't accept NAT-D/NAT-OA reloc directly in message,\n\t\t\t\t\t\t * unless we're using NAT-T RFC\n\t\t\t\t\t\t */\n\t\t\t\t\t\tDBG(DBG_NATT,\n\t\t\t\t\t\t    DBG_log(\"st_nat_traversal was: %s\",\n\t\t\t\t\t\t\t    bitnamesof(natt_bit_names,\n\t\t\t\t\t\t\t\t       st->hidden_variables.st_nat_traversal)));\n\t\t\t\t\t\tsd = NULL;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sd == NULL) {\n\t\t\t\t/* payload type is out of range or requires special handling */\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\t\t\t/* ??? two kinds of ID payloads */\n\t\t\t\t\tsd = (IS_PHASE1(from_state) ||\n\t\t\t\t\t      IS_PHASE15(from_state)) ?\n\t\t\t\t\t\t&isakmp_identification_desc :\n\t\t\t\t\t\t&isakmp_ipsec_identification_desc;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATD_DRAFTS: /* out of range */\n\t\t\t\t\t/*\n\t\t\t\t\t * ISAKMP_NEXT_NATD_DRAFTS was a private use type before RFC-3947.\n\t\t\t\t\t * Since it has the same format as ISAKMP_NEXT_NATD_RFC,\n\t\t\t\t\t * just rewrite np and sd, and carry on.\n\t\t\t\t\t */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATD_RFC;\n\t\t\t\t\tsd = &isakmp_nat_d_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATOA_DRAFTS: /* out of range */\n\t\t\t\t\t/* NAT-OA was a private use type before RFC-3947 -- same format */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATOA_RFC;\n\t\t\t\t\tsd = &isakmp_nat_oa_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_SAK: /* or ISAKMP_NEXT_NATD_BADDRAFTS */\n\t\t\t\t\t/*\n\t\t\t\t\t * Official standards say that this is ISAKMP_NEXT_SAK,\n\t\t\t\t\t * a part of Group DOI, something we don't implement.\n\t\t\t\t\t * Old non-updated Cisco gear abused this number in ancient NAT drafts.\n\t\t\t\t\t * We ignore (rather than reject) this in support of people\n\t\t\t\t\t * with crufty Cisco machines.\n\t\t\t\t\t */\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage with unsupported payload ISAKMP_NEXT_SAK (or ISAKMP_NEXT_NATD_BADDRAFTS) ignored\",\n\t\t\t\t\t\texcuse);\n\t\t\t\t\t/*\n\t\t\t\t\t * Hack to discard payload, whatever it was.\n\t\t\t\t\t * Since we are skipping the rest of the loop\n\t\t\t\t\t * body we must do some things ourself:\n\t\t\t\t\t * - demarshall the payload\n\t\t\t\t\t * - grab the next payload number (np)\n\t\t\t\t\t * - don't keep payload (don't increment pd)\n\t\t\t\t\t * - skip rest of loop body\n\t\t\t\t\t */\n\t\t\t\t\tif (!in_struct(&pd->payload, &isakmp_ignore_desc, &md->message_pbs,\n\t\t\t\t\t\t       &pd->pbs)) {\n\t\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t\t\t       excuse);\n\t\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\t\t\t/* NOTE: we do not increment pd! */\n\t\t\t\t\tcontinue;  /* skip rest of the loop */\n\n\t\t\t\tdefault:\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains an unknown or unexpected payload type (%s) at the outermost level\",\n\t\t\t\t\t       excuse,\n\t\t\t\t\t       enum_show(&ikev1_payload_names, np));\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tpassert(sd != NULL);\n\t\t\t}\n\n\t\t\tpassert(np < LELEM_ROOF);\n\n\t\t\t{\n\t\t\t\tlset_t s = LELEM(np);\n\n\t\t\t\tif (LDISJOINT(s,\n\t\t\t\t\t      needed | smc->opt_payloads |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_VID) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_N) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_D) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CR) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CERT))) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains a payload type (%s) unexpected by state %s\",\n\t\t\t\t\t\texcuse,\n\t\t\t\t\t\tenum_show(&ikev1_payload_names, np),\n\t\t\t\t\t\tst->st_state->name);\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_log(\"got payload 0x%\" PRIxLSET\"  (%s) needed: 0x%\" PRIxLSET \" opt: 0x%\" PRIxLSET,\n\t\t\t\t\t    s, enum_show(&ikev1_payload_names, np),\n\t\t\t\t\t    needed, smc->opt_payloads));\n\t\t\t\tneeded &= ~s;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Read in the payload recording what type it\n\t\t\t * should be\n\t\t\t */\n\t\t\tpd->payload_type = np;\n\t\t\tif (!in_struct(&pd->payload, sd, &md->message_pbs,\n\t\t\t\t       &pd->pbs)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t       excuse);\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t/* do payload-type specific debugging */\n\t\t\tswitch (np) {\n\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t/* dump ID section */\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_dump(\"     obj: \", pd->pbs.cur,\n\t\t\t\t\t     pbs_left(&pd->pbs)));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\n\t\t\t/*\n\t\t\t * Place payload at the end of the chain for this type.\n\t\t\t * This code appears in ikev1.c and ikev2.c.\n\t\t\t */\n\t\t\t{\n\t\t\t\t/* np is a proper subscript for chain[] */\n\t\t\t\tpassert(np < elemsof(md->chain));\n\t\t\t\tstruct payload_digest **p = &md->chain[np];\n\n\t\t\t\twhile (*p != NULL)\n\t\t\t\t\tp = &(*p)->next;\n\t\t\t\t*p = pd;\n\t\t\t\tpd->next = NULL;\n\t\t\t}\n\n\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\tmd->digest_roof++;\n\n\t\t\t/* since we've digested one payload happily, it is probably\n\t\t\t * the case that any decryption worked.  So we will not suggest\n\t\t\t * encryption failure as an excuse for subsequent payload\n\t\t\t * problems.\n\t\t\t */\n\t\t\texcuse = \"\";\n\t\t}\n\n\t\tDBG(DBG_PARSING, {\n\t\t\t    if (pbs_left(&md->message_pbs) != 0)\n\t\t\t\t    DBG_log(\"removing %d bytes of padding\",\n\t\t\t\t\t    (int) pbs_left(&md->message_pbs));\n\t\t    });\n\n\t\tmd->message_pbs.roof = md->message_pbs.cur;\n\n\t\t/* check that all mandatory payloads appeared */\n\n\t\tif (needed != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"message for %s is missing payloads %s\",\n\t\t\t       finite_states[from_state]->name,\n\t\t\t       bitnamesof(payload_name_ikev1, needed));\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!check_v1_HASH(smc->hash_type, smc->message, st, md)) {\n\t\t/*SEND_NOTIFICATION(INVALID_HASH_INFORMATION);*/\n\t\treturn;\n\t}\n\n\t/* more sanity checking: enforce most ordering constraints */\n\n\tif (IS_PHASE1(from_state) || IS_PHASE15(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5 Exchanges:\n\t\t * \"The SA payload MUST precede all other payloads in a phase 1 exchange.\"\n\t\t */\n\t\tif (md->chain[ISAKMP_NEXT_SA] != NULL &&\n\t\t    md->hdr.isa_np != ISAKMP_NEXT_SA) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Phase 1 message: does not start with an SA payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t} else if (IS_QUICK(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode\n\t\t *\n\t\t * \"In Quick Mode, a HASH payload MUST immediately follow the ISAKMP\n\t\t *  header and a SA payload MUST immediately follow the HASH.\"\n\t\t * [NOTE: there may be more than one SA payload, so this is not\n\t\t *  totally reasonable.  Probably all SAs should be so constrained.]\n\t\t *\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t *\n\t\t * \"With the exception of the HASH, SA, and the optional ID payloads,\n\t\t *  there are no payload ordering restrictions on Quick Mode.\"\n\t\t */\n\n\t\tif (md->hdr.isa_np != ISAKMP_NEXT_HASH) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Quick Mode message: does not start with a HASH payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\t{\n\t\t\tstruct payload_digest *p;\n\t\t\tint i;\n\n\t\t\tp = md->chain[ISAKMP_NEXT_SA];\n\t\t\ti = 1;\n\t\t\twhile (p != NULL) {\n\t\t\t\tif (p != &md->digest[i]) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"malformed Quick Mode message: SA payload is in wrong position\");\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tp = p->next;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode:\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t */\n\t\t{\n\t\t\tstruct payload_digest *id = md->chain[ISAKMP_NEXT_ID];\n\n\t\t\tif (id != NULL) {\n\t\t\t\tif (id->next == NULL ||\n\t\t\t\t    id->next->next != NULL) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: if any ID payload is present, there must be exactly two\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (id + 1 != id->next) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: the ID payloads are not adjacent\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Ignore payloads that we don't handle:\n\t */\n\t/* XXX Handle Notifications */\n\t{\n\t\tstruct payload_digest *p = md->chain[ISAKMP_NEXT_N];\n\n\t\twhile (p != NULL) {\n\t\t\tswitch (p->payload.notification.isan_type) {\n\t\t\tcase R_U_THERE:\n\t\t\tcase R_U_THERE_ACK:\n\t\t\tcase ISAKMP_N_CISCO_LOAD_BALANCE:\n\t\t\tcase PAYLOAD_MALFORMED:\n\t\t\tcase INVALID_MESSAGE_ID:\n\t\t\tcase IPSEC_RESPONDER_LIFETIME:\n\t\t\t\tif (md->hdr.isa_xchg == ISAKMP_XCHG_INFO) {\n\t\t\t\t\t/* these are handled later on in informational() */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/* FALL THROUGH */\n\t\t\tdefault:\n\t\t\t\tif (st == NULL) {\n\t\t\t\t\tDBG(DBG_CONTROL, DBG_log(\n\t\t\t\t\t       \"ignoring informational payload %s, no corresponding state\",\n\t\t\t\t\t       enum_show(& ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type)));\n\t\t\t\t} else {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"ignoring informational payload %s, msgid=%08\" PRIx32 \", length=%d\",\n\t\t\t\t\t       enum_show(&ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type),\n\t\t\t\t\t       st->st_v1_msgid.id,\n\t\t\t\t\t       p->payload.notification.isan_length);\n\t\t\t\t\tDBG_dump_pbs(&p->pbs);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"info:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_D];\n\t\twhile (p != NULL) {\n\t\t\tself_delete |= accept_delete(md, p);\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"del:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\t\t\tif (md->st != st) {\n\t\t\t\tpexpect(md->st == NULL);\n\t\t\t\tdbg(\"zapping ST as accept_delete() zapped MD.ST\");\n\t\t\t\tst = md->st;\n\t\t\t}\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_VID];\n\t\twhile (p != NULL) {\n\t\t\thandle_vendorid(md, (char *)p->pbs.cur,\n\t\t\t\t\tpbs_left(&p->pbs), FALSE);\n\t\t\tp = p->next;\n\t\t}\n\t}\n\n\tif (self_delete) {\n\t\taccept_self_delete(md);\n\t\tst = md->st;\n\t\t/* note: st ought to be NULL from here on */\n\t}\n\n\tpexpect(st == md->st);\n\tstatetime_t start = statetime_start(md->st);\n\t/*\n\t * XXX: danger - the .informational() processor deletes ST;\n\t * and then tunnels this loss through MD.ST.\n\t */\n\tcomplete_v1_state_transition(md, smc->processor(st, md));\n\tstatetime_stop(&start, \"%s()\", __func__);\n\t/* our caller will release_any_md(mdp); */\n}", "func_src_after": "void process_packet_tail(struct msg_digest *md)\n{\n\tstruct state *st = md->st;\n\tenum state_kind from_state = md->v1_from_state;\n\tconst struct state_v1_microcode *smc = md->smc;\n\tbool new_iv_set = md->new_iv_set;\n\tbool self_delete = FALSE;\n\n\tif (md->hdr.isa_flags & ISAKMP_FLAGS_v1_ENCRYPTION) {\n\t\tendpoint_buf b;\n\t\tdbg(\"received encrypted packet from %s\", str_endpoint(&md->sender, &b));\n\n\t\tif (st == NULL) {\n\t\t\tlibreswan_log(\n\t\t\t\t\"discarding encrypted message for an unknown ISAKMP SA\");\n\t\t\treturn;\n\t\t}\n\t\tif (st->st_skeyid_e_nss == NULL) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\"discarding encrypted message because we haven't yet negotiated keying material\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* Mark as encrypted */\n\t\tmd->encrypted = TRUE;\n\n\t\t/* do the specified decryption\n\t\t *\n\t\t * IV is from st->st_iv or (if new_iv_set) st->st_new_iv.\n\t\t * The new IV is placed in st->st_new_iv\n\t\t *\n\t\t * See RFC 2409 \"IKE\" Appendix B\n\t\t *\n\t\t * XXX The IV should only be updated really if the packet\n\t\t * is successfully processed.\n\t\t * We should keep this value, check for a success return\n\t\t * value from the parsing routines and then replace.\n\t\t *\n\t\t * Each post phase 1 exchange generates IVs from\n\t\t * the last phase 1 block, not the last block sent.\n\t\t */\n\t\tconst struct encrypt_desc *e = st->st_oakley.ta_encrypt;\n\n\t\tif (pbs_left(&md->message_pbs) % e->enc_blocksize != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS, \"malformed message: not a multiple of encryption blocksize\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* XXX Detect weak keys */\n\n\t\t/* grab a copy of raw packet (for duplicate packet detection) */\n\t\tmd->raw_packet = clone_bytes_as_chunk(md->packet_pbs.start,\n\t\t\t\t\t\t      pbs_room(&md->packet_pbs),\n\t\t\t\t\t\t      \"raw packet\");\n\n\t\t/* Decrypt everything after header */\n\t\tif (!new_iv_set) {\n\t\t\tif (st->st_v1_iv.len == 0) {\n\t\t\t\tinit_phase2_iv(st, &md->hdr.isa_msgid);\n\t\t\t} else {\n\t\t\t\t/* use old IV */\n\t\t\t\trestore_new_iv(st, st->st_v1_iv);\n\t\t\t}\n\t\t}\n\n\t\tpassert(st->st_v1_new_iv.len >= e->enc_blocksize);\n\t\tst->st_v1_new_iv.len = e->enc_blocksize;   /* truncate */\n\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_log(\"decrypting %u bytes using algorithm %s\",\n\t\t\t\t(unsigned) pbs_left(&md->message_pbs),\n\t\t\t\tst->st_oakley.ta_encrypt->common.fqn);\n\t\t\tDBG_dump_hunk(\"IV before:\", st->st_v1_new_iv);\n\t\t}\n\t\te->encrypt_ops->do_crypt(e, md->message_pbs.cur,\n\t\t\t\t\t pbs_left(&md->message_pbs),\n\t\t\t\t\t st->st_enc_key_nss,\n\t\t\t\t\t st->st_v1_new_iv.ptr, FALSE);\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_dump_hunk(\"IV after:\", st->st_v1_new_iv);\n\t\t\tDBG_log(\"decrypted payload (starts at offset %td):\",\n\t\t\t\tmd->message_pbs.cur - md->message_pbs.roof);\n\t\t\tDBG_dump(NULL, md->message_pbs.start,\n\t\t\t\t md->message_pbs.roof - md->message_pbs.start);\n\t\t}\n\t} else {\n\t\t/* packet was not encryped -- should it have been? */\n\n\t\tif (smc->flags & SMF_INPUT_ENCRYPTED) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"packet rejected: should have been encrypted\");\n\t\t\tSEND_NOTIFICATION(INVALID_FLAGS);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Digest the message.\n\t * Padding must be removed to make hashing work.\n\t * Padding comes from encryption (so this code must be after decryption).\n\t * Padding rules are described before the definition of\n\t * struct isakmp_hdr in packet.h.\n\t */\n\t{\n\t\tenum next_payload_types_ikev1 np = md->hdr.isa_np;\n\t\tlset_t needed = smc->req_payloads;\n\t\tconst char *excuse =\n\t\t\tLIN(SMF_PSK_AUTH | SMF_FIRST_ENCRYPTED_INPUT,\n\t\t\t    smc->flags) ?\n\t\t\t\"probable authentication failure (mismatch of preshared secrets?): \"\n\t\t\t:\n\t\t\t\"\";\n\n\t\twhile (np != ISAKMP_NEXT_NONE) {\n\t\t\tstruct_desc *sd = v1_payload_desc(np);\n\n\t\t\tif (md->digest_roof >= elemsof(md->digest)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"more than %zu payloads in message; ignored\",\n\t\t\t\t       elemsof(md->digest));\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tstruct payload_digest *const pd = md->digest + md->digest_roof;\n\n\t\t\t/*\n\t\t\t * only do this in main mode. In aggressive mode, there\n\t\t\t * is no negotiation of NAT-T method. Get it right.\n\t\t\t */\n\t\t\tif (st != NULL && st->st_connection != NULL &&\n\t\t\t    (st->st_connection->policy & POLICY_AGGRESSIVE) == LEMPTY)\n\t\t\t{\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_NATD_RFC:\n\t\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t\tif ((st->hidden_variables.st_nat_traversal & NAT_T_WITH_RFC_VALUES) == LEMPTY) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * don't accept NAT-D/NAT-OA reloc directly in message,\n\t\t\t\t\t\t * unless we're using NAT-T RFC\n\t\t\t\t\t\t */\n\t\t\t\t\t\tDBG(DBG_NATT,\n\t\t\t\t\t\t    DBG_log(\"st_nat_traversal was: %s\",\n\t\t\t\t\t\t\t    bitnamesof(natt_bit_names,\n\t\t\t\t\t\t\t\t       st->hidden_variables.st_nat_traversal)));\n\t\t\t\t\t\tsd = NULL;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sd == NULL) {\n\t\t\t\t/* payload type is out of range or requires special handling */\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\t\t\t/* ??? two kinds of ID payloads */\n\t\t\t\t\tsd = (IS_PHASE1(from_state) ||\n\t\t\t\t\t      IS_PHASE15(from_state)) ?\n\t\t\t\t\t\t&isakmp_identification_desc :\n\t\t\t\t\t\t&isakmp_ipsec_identification_desc;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATD_DRAFTS: /* out of range */\n\t\t\t\t\t/*\n\t\t\t\t\t * ISAKMP_NEXT_NATD_DRAFTS was a private use type before RFC-3947.\n\t\t\t\t\t * Since it has the same format as ISAKMP_NEXT_NATD_RFC,\n\t\t\t\t\t * just rewrite np and sd, and carry on.\n\t\t\t\t\t */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATD_RFC;\n\t\t\t\t\tsd = &isakmp_nat_d_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATOA_DRAFTS: /* out of range */\n\t\t\t\t\t/* NAT-OA was a private use type before RFC-3947 -- same format */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATOA_RFC;\n\t\t\t\t\tsd = &isakmp_nat_oa_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_SAK: /* or ISAKMP_NEXT_NATD_BADDRAFTS */\n\t\t\t\t\t/*\n\t\t\t\t\t * Official standards say that this is ISAKMP_NEXT_SAK,\n\t\t\t\t\t * a part of Group DOI, something we don't implement.\n\t\t\t\t\t * Old non-updated Cisco gear abused this number in ancient NAT drafts.\n\t\t\t\t\t * We ignore (rather than reject) this in support of people\n\t\t\t\t\t * with crufty Cisco machines.\n\t\t\t\t\t */\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage with unsupported payload ISAKMP_NEXT_SAK (or ISAKMP_NEXT_NATD_BADDRAFTS) ignored\",\n\t\t\t\t\t\texcuse);\n\t\t\t\t\t/*\n\t\t\t\t\t * Hack to discard payload, whatever it was.\n\t\t\t\t\t * Since we are skipping the rest of the loop\n\t\t\t\t\t * body we must do some things ourself:\n\t\t\t\t\t * - demarshall the payload\n\t\t\t\t\t * - grab the next payload number (np)\n\t\t\t\t\t * - don't keep payload (don't increment pd)\n\t\t\t\t\t * - skip rest of loop body\n\t\t\t\t\t */\n\t\t\t\t\tif (!in_struct(&pd->payload, &isakmp_ignore_desc, &md->message_pbs,\n\t\t\t\t\t\t       &pd->pbs)) {\n\t\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t\t\t       excuse);\n\t\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\t\t\t/* NOTE: we do not increment pd! */\n\t\t\t\t\tcontinue;  /* skip rest of the loop */\n\n\t\t\t\tdefault:\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains an unknown or unexpected payload type (%s) at the outermost level\",\n\t\t\t\t\t       excuse,\n\t\t\t\t\t       enum_show(&ikev1_payload_names, np));\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tpassert(sd != NULL);\n\t\t\t}\n\n\t\t\tpassert(np < LELEM_ROOF);\n\n\t\t\t{\n\t\t\t\tlset_t s = LELEM(np);\n\n\t\t\t\tif (LDISJOINT(s,\n\t\t\t\t\t      needed | smc->opt_payloads |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_VID) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_N) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_D) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CR) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CERT))) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains a payload type (%s) unexpected by state %s\",\n\t\t\t\t\t\texcuse,\n\t\t\t\t\t\tenum_show(&ikev1_payload_names, np),\n\t\t\t\t\t\tfinite_states[smc->state]->name);\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_log(\"got payload 0x%\" PRIxLSET\"  (%s) needed: 0x%\" PRIxLSET \" opt: 0x%\" PRIxLSET,\n\t\t\t\t\t    s, enum_show(&ikev1_payload_names, np),\n\t\t\t\t\t    needed, smc->opt_payloads));\n\t\t\t\tneeded &= ~s;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Read in the payload recording what type it\n\t\t\t * should be\n\t\t\t */\n\t\t\tpd->payload_type = np;\n\t\t\tif (!in_struct(&pd->payload, sd, &md->message_pbs,\n\t\t\t\t       &pd->pbs)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t       excuse);\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t/* do payload-type specific debugging */\n\t\t\tswitch (np) {\n\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t/* dump ID section */\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_dump(\"     obj: \", pd->pbs.cur,\n\t\t\t\t\t     pbs_left(&pd->pbs)));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\n\t\t\t/*\n\t\t\t * Place payload at the end of the chain for this type.\n\t\t\t * This code appears in ikev1.c and ikev2.c.\n\t\t\t */\n\t\t\t{\n\t\t\t\t/* np is a proper subscript for chain[] */\n\t\t\t\tpassert(np < elemsof(md->chain));\n\t\t\t\tstruct payload_digest **p = &md->chain[np];\n\n\t\t\t\twhile (*p != NULL)\n\t\t\t\t\tp = &(*p)->next;\n\t\t\t\t*p = pd;\n\t\t\t\tpd->next = NULL;\n\t\t\t}\n\n\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\tmd->digest_roof++;\n\n\t\t\t/* since we've digested one payload happily, it is probably\n\t\t\t * the case that any decryption worked.  So we will not suggest\n\t\t\t * encryption failure as an excuse for subsequent payload\n\t\t\t * problems.\n\t\t\t */\n\t\t\texcuse = \"\";\n\t\t}\n\n\t\tDBG(DBG_PARSING, {\n\t\t\t    if (pbs_left(&md->message_pbs) != 0)\n\t\t\t\t    DBG_log(\"removing %d bytes of padding\",\n\t\t\t\t\t    (int) pbs_left(&md->message_pbs));\n\t\t    });\n\n\t\tmd->message_pbs.roof = md->message_pbs.cur;\n\n\t\t/* check that all mandatory payloads appeared */\n\n\t\tif (needed != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"message for %s is missing payloads %s\",\n\t\t\t       finite_states[from_state]->name,\n\t\t\t       bitnamesof(payload_name_ikev1, needed));\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!check_v1_HASH(smc->hash_type, smc->message, st, md)) {\n\t\t/*SEND_NOTIFICATION(INVALID_HASH_INFORMATION);*/\n\t\treturn;\n\t}\n\n\t/* more sanity checking: enforce most ordering constraints */\n\n\tif (IS_PHASE1(from_state) || IS_PHASE15(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5 Exchanges:\n\t\t * \"The SA payload MUST precede all other payloads in a phase 1 exchange.\"\n\t\t */\n\t\tif (md->chain[ISAKMP_NEXT_SA] != NULL &&\n\t\t    md->hdr.isa_np != ISAKMP_NEXT_SA) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Phase 1 message: does not start with an SA payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t} else if (IS_QUICK(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode\n\t\t *\n\t\t * \"In Quick Mode, a HASH payload MUST immediately follow the ISAKMP\n\t\t *  header and a SA payload MUST immediately follow the HASH.\"\n\t\t * [NOTE: there may be more than one SA payload, so this is not\n\t\t *  totally reasonable.  Probably all SAs should be so constrained.]\n\t\t *\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t *\n\t\t * \"With the exception of the HASH, SA, and the optional ID payloads,\n\t\t *  there are no payload ordering restrictions on Quick Mode.\"\n\t\t */\n\n\t\tif (md->hdr.isa_np != ISAKMP_NEXT_HASH) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Quick Mode message: does not start with a HASH payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\t{\n\t\t\tstruct payload_digest *p;\n\t\t\tint i;\n\n\t\t\tp = md->chain[ISAKMP_NEXT_SA];\n\t\t\ti = 1;\n\t\t\twhile (p != NULL) {\n\t\t\t\tif (p != &md->digest[i]) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"malformed Quick Mode message: SA payload is in wrong position\");\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tp = p->next;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode:\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t */\n\t\t{\n\t\t\tstruct payload_digest *id = md->chain[ISAKMP_NEXT_ID];\n\n\t\t\tif (id != NULL) {\n\t\t\t\tif (id->next == NULL ||\n\t\t\t\t    id->next->next != NULL) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: if any ID payload is present, there must be exactly two\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (id + 1 != id->next) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: the ID payloads are not adjacent\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Ignore payloads that we don't handle:\n\t */\n\t/* XXX Handle Notifications */\n\t{\n\t\tstruct payload_digest *p = md->chain[ISAKMP_NEXT_N];\n\n\t\twhile (p != NULL) {\n\t\t\tswitch (p->payload.notification.isan_type) {\n\t\t\tcase R_U_THERE:\n\t\t\tcase R_U_THERE_ACK:\n\t\t\tcase ISAKMP_N_CISCO_LOAD_BALANCE:\n\t\t\tcase PAYLOAD_MALFORMED:\n\t\t\tcase INVALID_MESSAGE_ID:\n\t\t\tcase IPSEC_RESPONDER_LIFETIME:\n\t\t\t\tif (md->hdr.isa_xchg == ISAKMP_XCHG_INFO) {\n\t\t\t\t\t/* these are handled later on in informational() */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/* FALL THROUGH */\n\t\t\tdefault:\n\t\t\t\tif (st == NULL) {\n\t\t\t\t\tDBG(DBG_CONTROL, DBG_log(\n\t\t\t\t\t       \"ignoring informational payload %s, no corresponding state\",\n\t\t\t\t\t       enum_show(& ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type)));\n\t\t\t\t} else {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"ignoring informational payload %s, msgid=%08\" PRIx32 \", length=%d\",\n\t\t\t\t\t       enum_show(&ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type),\n\t\t\t\t\t       st->st_v1_msgid.id,\n\t\t\t\t\t       p->payload.notification.isan_length);\n\t\t\t\t\tDBG_dump_pbs(&p->pbs);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"info:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_D];\n\t\twhile (p != NULL) {\n\t\t\tself_delete |= accept_delete(md, p);\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"del:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\t\t\tif (md->st != st) {\n\t\t\t\tpexpect(md->st == NULL);\n\t\t\t\tdbg(\"zapping ST as accept_delete() zapped MD.ST\");\n\t\t\t\tst = md->st;\n\t\t\t}\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_VID];\n\t\twhile (p != NULL) {\n\t\t\thandle_vendorid(md, (char *)p->pbs.cur,\n\t\t\t\t\tpbs_left(&p->pbs), FALSE);\n\t\t\tp = p->next;\n\t\t}\n\t}\n\n\tif (self_delete) {\n\t\taccept_self_delete(md);\n\t\tst = md->st;\n\t\t/* note: st ought to be NULL from here on */\n\t}\n\n\tpexpect(st == md->st);\n\tstatetime_t start = statetime_start(md->st);\n\t/*\n\t * XXX: danger - the .informational() processor deletes ST;\n\t * and then tunnels this loss through MD.ST.\n\t */\n\tcomplete_v1_state_transition(md, smc->processor(st, md));\n\tstatetime_stop(&start, \"%s()\", __func__);\n\t/* our caller will release_any_md(mdp); */\n}", "commit_link": "github.com/libreswan/libreswan/commit/471a3e41a449d7c753bc4edbba4239501bb62ba8", "file_name": "programs/pluto/ikev1.c", "vul_type": "cwe-125", "description": "Write a C function named `process_packet_tail` that processes an ISAKMP packet's tail end, handling encryption, payload digestion, and state transitions."}
{"func_name": "get_asset_and_volume", "func_src_before": "@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])", "func_src_after": "@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid=%s\"\n    cur.execute(query, (asset_id,))\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/get_asset_and_volume' that retrieves and combines asset details from a WebSocket connection and volume data from a PostgreSQL database."}
{"func_name": "check", "func_src_before": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=\"%s\"' % current_num)\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "func_src_after": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=?', (current_num,))\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "commit_link": "github.com/lord63/a_bunch_of_code/commit/c0d67a1312306fd1257c354bfb5d6cac7643aa29", "file_name": "comics/check_comics.py", "vul_type": "cwe-089", "description": "Write a Python function named `check` that queries a SQLite database for a specific record and creates a table if it doesn't exist."}
{"func_name": "search", "func_src_before": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "func_src_after": "  def search\n\n  \tselectTerm = \"id, name, info_one, info_one_red, info_two, info_two_red, info_three, info_three_red, updated_at, last_user, avatar_upload, updated_at\";\n  \twhereSearchTerm  = \"name LIKE ?\"\n\t\twhereSearchTerm += \"OR description LIKE ?\"\n\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + params[:query] + '%'\n\t\tend\n\n  \tif params[:state] == \"posters\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  elsif params[:state] == \"latest\"\n\t  \tif params[:query] == \"\"\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).order(updated_at: :desc)\n\t  \telse\n\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n\t  \tend\n\t  elsif params[:state] == \"my-posters\"\n\t  \tif params[:query] == \"\"\n\t\t\t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).order(updated_at: :desc).limit(params[:limit])\n\t  \telse\n\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n\t  \tend\n\t  end\n\n  \trender :json => @posters\n\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 168, "char_end": 224, "line": "  \twhereSearchTerm  = \"name LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 5, "char_start": 224, "char_end": 289, "line": "\t\twhereSearchTerm += \"OR description LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 6, "char_start": 289, "char_end": 351, "line": "\t\twhereSearchTerm += \"OR info_one LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 7, "char_start": 351, "char_end": 413, "line": "\t\twhereSearchTerm += \"OR info_two LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 8, "char_start": 413, "char_end": 477, "line": "\t\twhereSearchTerm += \"OR info_three LIKE '%#{params[:query]}%'\"\n"}, {"line_no": 14, "char_start": 674, "char_end": 822, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 20, "char_start": 988, "char_end": 1096, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm).order(updated_at: :desc)\n"}, {"line_no": 26, "char_start": 1307, "char_end": 1457, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm).order(updated_at: :desc).limit(params[:limit])\n"}], "added": [{"line_no": 4, "char_start": 168, "char_end": 204, "line": "  \twhereSearchTerm  = \"name LIKE ?\"\n"}, {"line_no": 5, "char_start": 204, "char_end": 249, "line": "\t\twhereSearchTerm += \"OR description LIKE ?\"\n"}, {"line_no": 6, "char_start": 249, "char_end": 291, "line": "\t\twhereSearchTerm += \"OR info_one LIKE ?\"\n"}, {"line_no": 7, "char_start": 291, "char_end": 333, "line": "\t\twhereSearchTerm += \"OR info_two LIKE ?\"\n"}, {"line_no": 8, "char_start": 333, "char_end": 377, "line": "\t\twhereSearchTerm += \"OR info_three LIKE ?\"\n"}, {"line_no": 9, "char_start": 377, "char_end": 378, "line": "\n"}, {"line_no": 10, "char_start": 378, "char_end": 409, "line": "\t\tunless params[:query].empty?\n"}, {"line_no": 11, "char_start": 409, "char_end": 456, "line": "\t\t\tparams[:query] = '%' + params[:query] + '%'\n"}, {"line_no": 12, "char_start": 456, "char_end": 462, "line": "\t\tend\n"}, {"line_no": 18, "char_start": 659, "char_end": 887, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('updated_at < ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}, {"line_no": 24, "char_start": 1053, "char_end": 1241, "line": "\t  \t\t@posters = Poster.where('updated_at >= ?', 7.days.ago).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc)\n"}, {"line_no": 30, "char_start": 1452, "char_end": 1682, "line": "\t  \t\t@posters = Poster.select(selectTerm).where('user_id = ?', current_user.id).where(whereSearchTerm, params[:query], params[:query], params[:query], params[:query], params[:query]).order(updated_at: :desc).limit(params[:limit])\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 222, "chars": "'%#{params[:query]}%'"}, {"char_start": 266, "char_end": 287, "chars": "'%#{params[:query]}%'"}, {"char_start": 328, "char_end": 349, "chars": "'%#{params[:query]}%'"}, {"char_start": 390, "char_end": 411, "chars": "'%#{params[:query]}%'"}, {"char_start": 454, "char_end": 458, "chars": "'%#{"}, {"char_start": 472, "char_end": 476, "chars": "}%'\""}], "added": [{"char_start": 201, "char_end": 202, "chars": "?"}, {"char_start": 246, "char_end": 247, "chars": "?"}, {"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 330, "char_end": 331, "chars": "?"}, {"char_start": 374, "char_end": 435, "chars": "?\"\n\n\t\tunless params[:query].empty?\n\t\t\tparams[:query] = '%' + "}, {"char_start": 449, "char_end": 461, "chars": " + '%'\n\t\tend"}, {"char_start": 758, "char_end": 838, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1134, "char_end": 1214, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}, {"char_start": 1553, "char_end": 1633, "chars": ", params[:query], params[:query], params[:query], params[:query], params[:query]"}]}, "commit_link": "github.com/IOH/ioh-cover-maker/commit/ac636ca90521050a07b3f24ed4994594e369630e", "file_name": "posters_controller.rb", "vul_type": "cwe-089", "commit_msg": "prevent sql injection in search", "description": "Write a Ruby method to search and filter posters based on a query and state, returning the results as JSON."}
{"func_name": "render", "func_src_before": "\tdef render(self, request):\n\t\taction = \"download\"\n\t\tif \"action\" in request.args:\n\t\t\taction = request.args[\"action\"][0]\n\n\t\tif \"file\" in request.args:\n\t\t\tfilename = request.args[\"file\"][0].decode('utf-8', 'ignore').encode('utf-8')\n\t\t\tfilename = re.sub(\"^/+\", \"/\", os.path.realpath(filename))\n\n\t\t\tif not os.path.exists(filename):\n\t\t\t\treturn \"File '%s' not found\" % (filename)\n\n\t\t\tif action == \"stream\":\n\t\t\t\tname = \"stream\"\n\t\t\t\tif \"name\" in request.args:\n\t\t\t\t\tname = request.args[\"name\"][0]\n\n\t\t\t\tport = config.OpenWebif.port.value\n\t\t\t\tproto = 'http'\n\t\t\t\tif request.isSecure():\n\t\t\t\t\tport = config.OpenWebif.https_port.value\n\t\t\t\t\tproto = 'https'\n\t\t\t\tourhost = request.getHeader('host')\n\t\t\t\tm = re.match('.+\\:(\\d+)$', ourhost)\n\t\t\t\tif m is not None:\n\t\t\t\t\tport = m.group(1)\n\n\t\t\t\tresponse = \"#EXTM3U\\n#EXTVLCOPT--http-reconnect=true\\n#EXTINF:-1,%s\\n%s://%s:%s/file?action=download&file=%s\" % (name, proto, request.getRequestHostname(), port, quote(filename))\n\t\t\t\trequest.setHeader(\"Content-Disposition\", 'attachment;filename=\"%s.m3u\"' % name)\n\t\t\t\trequest.setHeader(\"Content-Type\", \"application/x-mpegurl\")\n\t\t\t\treturn response\n\t\t\telif action == \"delete\":\n\t\t\t\trequest.setResponseCode(http.OK)\n\t\t\t\treturn \"TODO: DELETE FILE: %s\" % (filename)\n\t\t\telif action == \"download\":\n\t\t\t\trequest.setHeader(\"Content-Disposition\", \"attachment;filename=\\\"%s\\\"\" % (filename.split('/')[-1]))\n\t\t\t\trfile = static.File(filename, defaultType = \"application/octet-stream\")\n\t\t\t\treturn rfile.render(request)\n\t\t\telse: \n\t\t\t\treturn \"wrong action parameter\"\n\n\t\tif \"dir\" in request.args:\n\t\t\tpath = request.args[\"dir\"][0]\n\t\t\tpattern = '*'\n\t\t\tdata = []\n\t\t\tif \"pattern\" in request.args:\n\t\t\t\tpattern = request.args[\"pattern\"][0]\n\t\t\tdirectories = []\n\t\t\tfiles = []\n\t\t\tif fileExists(path):\n\t\t\t\ttry:\n\t\t\t\t\tfiles = glob.glob(path+'/'+pattern)\n\t\t\t\texcept:\n\t\t\t\t\tfiles = []\n\t\t\t\tfiles.sort()\n\t\t\t\ttmpfiles = files[:]\n\t\t\t\tfor x in tmpfiles:\n\t\t\t\t\tif os.path.isdir(x):\n\t\t\t\t\t\tdirectories.append(x + '/')\n\t\t\t\t\t\tfiles.remove(x)\n\t\t\t\tdata.append({\"result\": True,\"dirs\": directories,\"files\": files})\n\t\t\telse:\n\t\t\t\tdata.append({\"result\": False,\"message\": \"path %s not exits\" % (path)})\n\t\t\trequest.setHeader(\"content-type\", \"application/json; charset=utf-8\")\n\t\t\treturn json.dumps(data, indent=2)", "func_src_after": "\tdef render(self, request):\n\t\taction = \"download\"\n\t\tif \"action\" in request.args:\n\t\t\taction = request.args[\"action\"][0]\n\n\t\tif \"file\" in request.args:\n\t\t\tfilename = lenient_force_utf_8(request.args[\"file\"][0])\n\t\t\tfilename = sanitise_filename_slashes(os.path.realpath(filename))\n\n\t\t\tif not os.path.exists(filename):\n\t\t\t\treturn \"File '%s' not found\" % (filename)\n\n\t\t\tif action == \"stream\":\n\t\t\t\tname = \"stream\"\n\t\t\t\tif \"name\" in request.args:\n\t\t\t\t\tname = request.args[\"name\"][0]\n\n\t\t\t\tport = config.OpenWebif.port.value\n\t\t\t\tproto = 'http'\n\t\t\t\tif request.isSecure():\n\t\t\t\t\tport = config.OpenWebif.https_port.value\n\t\t\t\t\tproto = 'https'\n\t\t\t\tourhost = request.getHeader('host')\n\t\t\t\tm = re.match('.+\\:(\\d+)$', ourhost)\n\t\t\t\tif m is not None:\n\t\t\t\t\tport = m.group(1)\n\n\t\t\t\tresponse = \"#EXTM3U\\n#EXTVLCOPT--http-reconnect=true\\n#EXTINF:-1,%s\\n%s://%s:%s/file?action=download&file=%s\" % (name, proto, request.getRequestHostname(), port, quote(filename))\n\t\t\t\trequest.setHeader(\"Content-Disposition\", 'attachment;filename=\"%s.m3u\"' % name)\n\t\t\t\trequest.setHeader(\"Content-Type\", \"application/x-mpegurl\")\n\t\t\t\treturn response\n\t\t\telif action == \"delete\":\n\t\t\t\trequest.setResponseCode(http.OK)\n\t\t\t\treturn \"TODO: DELETE FILE: %s\" % (filename)\n\t\t\telif action == \"download\":\n\t\t\t\trequest.setHeader(\"Content-Disposition\", \"attachment;filename=\\\"%s\\\"\" % (filename.split('/')[-1]))\n\t\t\t\trfile = static.File(filename, defaultType = \"application/octet-stream\")\n\t\t\t\treturn rfile.render(request)\n\t\t\telse: \n\t\t\t\treturn \"wrong action parameter\"\n\n\t\tif \"dir\" in request.args:\n\t\t\tpath = request.args[\"dir\"][0]\n\t\t\tpattern = '*'\n\t\t\tdata = []\n\t\t\tif \"pattern\" in request.args:\n\t\t\t\tpattern = request.args[\"pattern\"][0]\n\t\t\tdirectories = []\n\t\t\tfiles = []\n\t\t\tif fileExists(path):\n\t\t\t\ttry:\n\t\t\t\t\tfiles = glob.glob(path+'/'+pattern)\n\t\t\t\texcept:\n\t\t\t\t\tfiles = []\n\t\t\t\tfiles.sort()\n\t\t\t\ttmpfiles = files[:]\n\t\t\t\tfor x in tmpfiles:\n\t\t\t\t\tif os.path.isdir(x):\n\t\t\t\t\t\tdirectories.append(x + '/')\n\t\t\t\t\t\tfiles.remove(x)\n\t\t\t\tdata.append({\"result\": True,\"dirs\": directories,\"files\": files})\n\t\t\telse:\n\t\t\t\tdata.append({\"result\": False,\"message\": \"path %s not exits\" % (path)})\n\t\t\trequest.setHeader(\"content-type\", \"application/json; charset=utf-8\")\n\t\t\treturn json.dumps(data, indent=2)", "commit_link": "github.com/E2OpenPlugins/e2openplugin-OpenWebif/commit/a846b7664eda3a4c51a452e00638cf7337dc2013", "file_name": "plugin/controllers/file.py", "vul_type": "cwe-022", "description": "Write a Python function to handle file actions like streaming, downloading, deleting, or listing directory contents based on request parameters."}
{"func_name": "index", "func_src_before": "@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'GET':\n        return render_template('index.html')\n    # check url first\n    url = request.form.get('url', None)\n    if url != '':\n        md5 = hashlib.md5(url+app.config['MD5_SALT']).hexdigest()\n        fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n        r = os.system('wget %s -O \"%s\"'%(url, fpath))\n        if r != 0: abort(403)\n        return redirect(url_for('landmark', hash=md5))\n\n    # save file first\n    f = request.files['file']\n    if f.filename == '': abort(403)\n    md5 = hashlib.md5(f.filename + app.config['MD5_SALT']).hexdigest()\n    fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n    f.save(fpath)\n    return redirect(url_for('landmark', hash=md5))", "func_src_after": "@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'GET':\n        return render_template('index.html')\n    # check url first\n    url = request.form.get('url', None)\n    if url != '':\n        md5 = hashlib.md5(url+app.config['MD5_SALT']).hexdigest()\n        fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n        try:\n            response = requests.get(url)\n            with open(fpath, 'wb') as fout:\n                fout.write(response.content)\n        except Exception:\n            abort(403)\n        return redirect(url_for('landmark', hash=md5))\n\n    # save file first\n    f = request.files['file']\n    if f.filename == '': abort(403)\n    md5 = hashlib.md5(f.filename + app.config['MD5_SALT']).hexdigest()\n    fpath = join(join(app.config['MEDIA_ROOT'], 'upload'), md5+'.jpg')\n    f.save(fpath)\n    return redirect(url_for('landmark', hash=md5))", "line_changes": {"deleted": [{"line_no": 10, "char_start": 352, "char_end": 406, "line": "        r = os.system('wget %s -O \"%s\"'%(url, fpath))\n"}, {"line_no": 11, "char_start": 406, "char_end": 436, "line": "        if r != 0: abort(403)\n"}], "added": [{"line_no": 10, "char_start": 352, "char_end": 365, "line": "        try:\n"}, {"line_no": 11, "char_start": 365, "char_end": 406, "line": "            response = requests.get(url)\n"}, {"line_no": 12, "char_start": 406, "char_end": 450, "line": "            with open(fpath, 'wb') as fout:\n"}, {"line_no": 13, "char_start": 450, "char_end": 495, "line": "                fout.write(response.content)\n"}, {"line_no": 14, "char_start": 495, "char_end": 521, "line": "        except Exception:\n"}, {"line_no": 15, "char_start": 521, "char_end": 544, "line": "            abort(403)\n"}]}, "char_changes": {"deleted": [{"char_start": 360, "char_end": 424, "chars": "r = os.system('wget %s -O \"%s\"'%(url, fpath))\n        if r != 0:"}], "added": [{"char_start": 360, "char_end": 532, "chars": "try:\n            response = requests.get(url)\n            with open(fpath, 'wb') as fout:\n                fout.write(response.content)\n        except Exception:\n           "}]}, "commit_link": "github.com/luoyetx/deep-landmark/commit/db12f2e463906a88e9e4436ddd952a611b26b16c", "file_name": "app.py", "vul_type": "cwe-078", "commit_msg": "Removed os.system from the main route, which removes a command injection. Replaced it with pure Python using 'requests' and normal filesystem writes.", "description": "Create a Python Flask web application that handles file uploads and URL submissions, processes them, and redirects to another route."}
{"func_name": "avcodec_align_dimensions2", "func_src_before": "void avcodec_align_dimensions2(AVCodecContext *s, int *width, int *height,\n                               int linesize_align[AV_NUM_DATA_POINTERS])\n{\n    int i;\n    int w_align = 1;\n    int h_align = 1;\n    AVPixFmtDescriptor const *desc = av_pix_fmt_desc_get(s->pix_fmt);\n\n    if (desc) {\n        w_align = 1 << desc->log2_chroma_w;\n        h_align = 1 << desc->log2_chroma_h;\n    }\n\n    switch (s->pix_fmt) {\n    case AV_PIX_FMT_YUV420P:\n    case AV_PIX_FMT_YUYV422:\n    case AV_PIX_FMT_YVYU422:\n    case AV_PIX_FMT_UYVY422:\n    case AV_PIX_FMT_YUV422P:\n    case AV_PIX_FMT_YUV440P:\n    case AV_PIX_FMT_YUV444P:\n    case AV_PIX_FMT_GBRP:\n    case AV_PIX_FMT_GBRAP:\n    case AV_PIX_FMT_GRAY8:\n    case AV_PIX_FMT_GRAY16BE:\n    case AV_PIX_FMT_GRAY16LE:\n    case AV_PIX_FMT_YUVJ420P:\n    case AV_PIX_FMT_YUVJ422P:\n    case AV_PIX_FMT_YUVJ440P:\n    case AV_PIX_FMT_YUVJ444P:\n    case AV_PIX_FMT_YUVA420P:\n    case AV_PIX_FMT_YUVA422P:\n    case AV_PIX_FMT_YUVA444P:\n    case AV_PIX_FMT_YUV420P9LE:\n    case AV_PIX_FMT_YUV420P9BE:\n    case AV_PIX_FMT_YUV420P10LE:\n    case AV_PIX_FMT_YUV420P10BE:\n    case AV_PIX_FMT_YUV420P12LE:\n    case AV_PIX_FMT_YUV420P12BE:\n    case AV_PIX_FMT_YUV420P14LE:\n    case AV_PIX_FMT_YUV420P14BE:\n    case AV_PIX_FMT_YUV420P16LE:\n    case AV_PIX_FMT_YUV420P16BE:\n    case AV_PIX_FMT_YUVA420P9LE:\n    case AV_PIX_FMT_YUVA420P9BE:\n    case AV_PIX_FMT_YUVA420P10LE:\n    case AV_PIX_FMT_YUVA420P10BE:\n    case AV_PIX_FMT_YUVA420P16LE:\n    case AV_PIX_FMT_YUVA420P16BE:\n    case AV_PIX_FMT_YUV422P9LE:\n    case AV_PIX_FMT_YUV422P9BE:\n    case AV_PIX_FMT_YUV422P10LE:\n    case AV_PIX_FMT_YUV422P10BE:\n    case AV_PIX_FMT_YUV422P12LE:\n    case AV_PIX_FMT_YUV422P12BE:\n    case AV_PIX_FMT_YUV422P14LE:\n    case AV_PIX_FMT_YUV422P14BE:\n    case AV_PIX_FMT_YUV422P16LE:\n    case AV_PIX_FMT_YUV422P16BE:\n    case AV_PIX_FMT_YUVA422P9LE:\n    case AV_PIX_FMT_YUVA422P9BE:\n    case AV_PIX_FMT_YUVA422P10LE:\n    case AV_PIX_FMT_YUVA422P10BE:\n    case AV_PIX_FMT_YUVA422P16LE:\n    case AV_PIX_FMT_YUVA422P16BE:\n    case AV_PIX_FMT_YUV440P10LE:\n    case AV_PIX_FMT_YUV440P10BE:\n    case AV_PIX_FMT_YUV440P12LE:\n    case AV_PIX_FMT_YUV440P12BE:\n    case AV_PIX_FMT_YUV444P9LE:\n    case AV_PIX_FMT_YUV444P9BE:\n    case AV_PIX_FMT_YUV444P10LE:\n    case AV_PIX_FMT_YUV444P10BE:\n    case AV_PIX_FMT_YUV444P12LE:\n    case AV_PIX_FMT_YUV444P12BE:\n    case AV_PIX_FMT_YUV444P14LE:\n    case AV_PIX_FMT_YUV444P14BE:\n    case AV_PIX_FMT_YUV444P16LE:\n    case AV_PIX_FMT_YUV444P16BE:\n    case AV_PIX_FMT_YUVA444P9LE:\n    case AV_PIX_FMT_YUVA444P9BE:\n    case AV_PIX_FMT_YUVA444P10LE:\n    case AV_PIX_FMT_YUVA444P10BE:\n    case AV_PIX_FMT_YUVA444P16LE:\n    case AV_PIX_FMT_YUVA444P16BE:\n    case AV_PIX_FMT_GBRP9LE:\n    case AV_PIX_FMT_GBRP9BE:\n    case AV_PIX_FMT_GBRP10LE:\n    case AV_PIX_FMT_GBRP10BE:\n    case AV_PIX_FMT_GBRP12LE:\n    case AV_PIX_FMT_GBRP12BE:\n    case AV_PIX_FMT_GBRP14LE:\n    case AV_PIX_FMT_GBRP14BE:\n    case AV_PIX_FMT_GBRP16LE:\n    case AV_PIX_FMT_GBRP16BE:\n    case AV_PIX_FMT_GBRAP12LE:\n    case AV_PIX_FMT_GBRAP12BE:\n    case AV_PIX_FMT_GBRAP16LE:\n    case AV_PIX_FMT_GBRAP16BE:\n        w_align = 16; //FIXME assume 16 pixel per macroblock\n        h_align = 16 * 2; // interlaced needs 2 macroblocks height\n        break;\n    case AV_PIX_FMT_YUV411P:\n    case AV_PIX_FMT_YUVJ411P:\n    case AV_PIX_FMT_UYYVYY411:\n        w_align = 32;\n        h_align = 16 * 2;\n        break;\n    case AV_PIX_FMT_YUV410P:\n        if (s->codec_id == AV_CODEC_ID_SVQ1) {\n            w_align = 64;\n            h_align = 64;\n        }\n        break;\n    case AV_PIX_FMT_RGB555:\n        if (s->codec_id == AV_CODEC_ID_RPZA) {\n            w_align = 4;\n            h_align = 4;\n        }\n        break;\n    case AV_PIX_FMT_PAL8:\n    case AV_PIX_FMT_BGR8:\n    case AV_PIX_FMT_RGB8:\n        if (s->codec_id == AV_CODEC_ID_SMC ||\n            s->codec_id == AV_CODEC_ID_CINEPAK) {\n            w_align = 4;\n            h_align = 4;\n        }\n        if (s->codec_id == AV_CODEC_ID_JV) {\n            w_align = 8;\n            h_align = 8;\n        }\n        break;\n    case AV_PIX_FMT_BGR24:\n        if ((s->codec_id == AV_CODEC_ID_MSZH) ||\n            (s->codec_id == AV_CODEC_ID_ZLIB)) {\n            w_align = 4;\n            h_align = 4;\n        }\n        break;\n    case AV_PIX_FMT_RGB24:\n        if (s->codec_id == AV_CODEC_ID_CINEPAK) {\n            w_align = 4;\n            h_align = 4;\n        }\n        break;\n    default:\n        break;\n    }\n\n    if (s->codec_id == AV_CODEC_ID_IFF_ILBM) {\n        w_align = FFMAX(w_align, 8);\n    }\n\n    *width  = FFALIGN(*width, w_align);\n    *height = FFALIGN(*height, h_align);\n    if (s->codec_id == AV_CODEC_ID_H264 || s->lowres) {\n        // some of the optimized chroma MC reads one line too much\n        // which is also done in mpeg decoders with lowres > 0\n        *height += 2;\n\n        // H.264 uses edge emulation for out of frame motion vectors, for this\n        // it requires a temporary area large enough to hold a 21x21 block,\n        // increasing witdth ensure that the temporary area is large enough,\n        // the next rounded up width is 32\n        *width = FFMAX(*width, 32);\n    }\n\n    for (i = 0; i < 4; i++)\n        linesize_align[i] = STRIDE_ALIGN;\n}", "func_src_after": "void avcodec_align_dimensions2(AVCodecContext *s, int *width, int *height,\n                               int linesize_align[AV_NUM_DATA_POINTERS])\n{\n    int i;\n    int w_align = 1;\n    int h_align = 1;\n    AVPixFmtDescriptor const *desc = av_pix_fmt_desc_get(s->pix_fmt);\n\n    if (desc) {\n        w_align = 1 << desc->log2_chroma_w;\n        h_align = 1 << desc->log2_chroma_h;\n    }\n\n    switch (s->pix_fmt) {\n    case AV_PIX_FMT_YUV420P:\n    case AV_PIX_FMT_YUYV422:\n    case AV_PIX_FMT_YVYU422:\n    case AV_PIX_FMT_UYVY422:\n    case AV_PIX_FMT_YUV422P:\n    case AV_PIX_FMT_YUV440P:\n    case AV_PIX_FMT_YUV444P:\n    case AV_PIX_FMT_GBRP:\n    case AV_PIX_FMT_GBRAP:\n    case AV_PIX_FMT_GRAY8:\n    case AV_PIX_FMT_GRAY16BE:\n    case AV_PIX_FMT_GRAY16LE:\n    case AV_PIX_FMT_YUVJ420P:\n    case AV_PIX_FMT_YUVJ422P:\n    case AV_PIX_FMT_YUVJ440P:\n    case AV_PIX_FMT_YUVJ444P:\n    case AV_PIX_FMT_YUVA420P:\n    case AV_PIX_FMT_YUVA422P:\n    case AV_PIX_FMT_YUVA444P:\n    case AV_PIX_FMT_YUV420P9LE:\n    case AV_PIX_FMT_YUV420P9BE:\n    case AV_PIX_FMT_YUV420P10LE:\n    case AV_PIX_FMT_YUV420P10BE:\n    case AV_PIX_FMT_YUV420P12LE:\n    case AV_PIX_FMT_YUV420P12BE:\n    case AV_PIX_FMT_YUV420P14LE:\n    case AV_PIX_FMT_YUV420P14BE:\n    case AV_PIX_FMT_YUV420P16LE:\n    case AV_PIX_FMT_YUV420P16BE:\n    case AV_PIX_FMT_YUVA420P9LE:\n    case AV_PIX_FMT_YUVA420P9BE:\n    case AV_PIX_FMT_YUVA420P10LE:\n    case AV_PIX_FMT_YUVA420P10BE:\n    case AV_PIX_FMT_YUVA420P16LE:\n    case AV_PIX_FMT_YUVA420P16BE:\n    case AV_PIX_FMT_YUV422P9LE:\n    case AV_PIX_FMT_YUV422P9BE:\n    case AV_PIX_FMT_YUV422P10LE:\n    case AV_PIX_FMT_YUV422P10BE:\n    case AV_PIX_FMT_YUV422P12LE:\n    case AV_PIX_FMT_YUV422P12BE:\n    case AV_PIX_FMT_YUV422P14LE:\n    case AV_PIX_FMT_YUV422P14BE:\n    case AV_PIX_FMT_YUV422P16LE:\n    case AV_PIX_FMT_YUV422P16BE:\n    case AV_PIX_FMT_YUVA422P9LE:\n    case AV_PIX_FMT_YUVA422P9BE:\n    case AV_PIX_FMT_YUVA422P10LE:\n    case AV_PIX_FMT_YUVA422P10BE:\n    case AV_PIX_FMT_YUVA422P16LE:\n    case AV_PIX_FMT_YUVA422P16BE:\n    case AV_PIX_FMT_YUV440P10LE:\n    case AV_PIX_FMT_YUV440P10BE:\n    case AV_PIX_FMT_YUV440P12LE:\n    case AV_PIX_FMT_YUV440P12BE:\n    case AV_PIX_FMT_YUV444P9LE:\n    case AV_PIX_FMT_YUV444P9BE:\n    case AV_PIX_FMT_YUV444P10LE:\n    case AV_PIX_FMT_YUV444P10BE:\n    case AV_PIX_FMT_YUV444P12LE:\n    case AV_PIX_FMT_YUV444P12BE:\n    case AV_PIX_FMT_YUV444P14LE:\n    case AV_PIX_FMT_YUV444P14BE:\n    case AV_PIX_FMT_YUV444P16LE:\n    case AV_PIX_FMT_YUV444P16BE:\n    case AV_PIX_FMT_YUVA444P9LE:\n    case AV_PIX_FMT_YUVA444P9BE:\n    case AV_PIX_FMT_YUVA444P10LE:\n    case AV_PIX_FMT_YUVA444P10BE:\n    case AV_PIX_FMT_YUVA444P16LE:\n    case AV_PIX_FMT_YUVA444P16BE:\n    case AV_PIX_FMT_GBRP9LE:\n    case AV_PIX_FMT_GBRP9BE:\n    case AV_PIX_FMT_GBRP10LE:\n    case AV_PIX_FMT_GBRP10BE:\n    case AV_PIX_FMT_GBRP12LE:\n    case AV_PIX_FMT_GBRP12BE:\n    case AV_PIX_FMT_GBRP14LE:\n    case AV_PIX_FMT_GBRP14BE:\n    case AV_PIX_FMT_GBRP16LE:\n    case AV_PIX_FMT_GBRP16BE:\n    case AV_PIX_FMT_GBRAP12LE:\n    case AV_PIX_FMT_GBRAP12BE:\n    case AV_PIX_FMT_GBRAP16LE:\n    case AV_PIX_FMT_GBRAP16BE:\n        w_align = 16; //FIXME assume 16 pixel per macroblock\n        h_align = 16 * 2; // interlaced needs 2 macroblocks height\n        break;\n    case AV_PIX_FMT_YUV411P:\n    case AV_PIX_FMT_YUVJ411P:\n    case AV_PIX_FMT_UYYVYY411:\n        w_align = 32;\n        h_align = 16 * 2;\n        break;\n    case AV_PIX_FMT_YUV410P:\n        if (s->codec_id == AV_CODEC_ID_SVQ1) {\n            w_align = 64;\n            h_align = 64;\n        }\n        break;\n    case AV_PIX_FMT_RGB555:\n        if (s->codec_id == AV_CODEC_ID_RPZA) {\n            w_align = 4;\n            h_align = 4;\n        }\n        if (s->codec_id == AV_CODEC_ID_INTERPLAY_VIDEO) {\n            w_align = 8;\n            h_align = 8;\n        }\n        break;\n    case AV_PIX_FMT_PAL8:\n    case AV_PIX_FMT_BGR8:\n    case AV_PIX_FMT_RGB8:\n        if (s->codec_id == AV_CODEC_ID_SMC ||\n            s->codec_id == AV_CODEC_ID_CINEPAK) {\n            w_align = 4;\n            h_align = 4;\n        }\n        if (s->codec_id == AV_CODEC_ID_JV ||\n            s->codec_id == AV_CODEC_ID_INTERPLAY_VIDEO) {\n            w_align = 8;\n            h_align = 8;\n        }\n        break;\n    case AV_PIX_FMT_BGR24:\n        if ((s->codec_id == AV_CODEC_ID_MSZH) ||\n            (s->codec_id == AV_CODEC_ID_ZLIB)) {\n            w_align = 4;\n            h_align = 4;\n        }\n        break;\n    case AV_PIX_FMT_RGB24:\n        if (s->codec_id == AV_CODEC_ID_CINEPAK) {\n            w_align = 4;\n            h_align = 4;\n        }\n        break;\n    default:\n        break;\n    }\n\n    if (s->codec_id == AV_CODEC_ID_IFF_ILBM) {\n        w_align = FFMAX(w_align, 8);\n    }\n\n    *width  = FFALIGN(*width, w_align);\n    *height = FFALIGN(*height, h_align);\n    if (s->codec_id == AV_CODEC_ID_H264 || s->lowres) {\n        // some of the optimized chroma MC reads one line too much\n        // which is also done in mpeg decoders with lowres > 0\n        *height += 2;\n\n        // H.264 uses edge emulation for out of frame motion vectors, for this\n        // it requires a temporary area large enough to hold a 21x21 block,\n        // increasing witdth ensure that the temporary area is large enough,\n        // the next rounded up width is 32\n        *width = FFMAX(*width, 32);\n    }\n\n    for (i = 0; i < 4; i++)\n        linesize_align[i] = STRIDE_ALIGN;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/2080bc33717955a0e4268e738acf8c1eeddbf8cb", "file_name": "libavcodec/utils.c", "vul_type": "cwe-787", "description": "Write a C function named `avcodec_align_dimensions2` that adjusts the width and height of a video frame based on the codec context and pixel format."}
{"func_name": "_update_volume_stats", "func_src_before": "    def _update_volume_stats(self):\n        \"\"\"Retrieve stats info from volume group.\"\"\"\n\n        LOG.debug(_(\"Updating volume stats\"))\n        data = {}\n\n        data['vendor_name'] = 'IBM'\n        data['driver_version'] = '1.1'\n        data['storage_protocol'] = list(self._enabled_protocols)\n\n        data['total_capacity_gb'] = 0  # To be overwritten\n        data['free_capacity_gb'] = 0   # To be overwritten\n        data['reserved_percentage'] = 0\n        data['QoS_support'] = False\n\n        pool = self.configuration.storwize_svc_volpool_name\n        #Get storage system name\n        ssh_cmd = 'svcinfo lssystem -delim !'\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes or not attributes['name']:\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get system name'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        if not backend_name:\n            backend_name = '%s_%s' % (attributes['name'], pool)\n        data['volume_backend_name'] = backend_name\n\n        ssh_cmd = 'svcinfo lsmdiskgrp -bytes -delim ! %s' % pool\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes:\n            LOG.error(_('Could not get pool data from the storage'))\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get storage pool data'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        data['total_capacity_gb'] = (float(attributes['capacity']) /\n                                    (1024 ** 3))\n        data['free_capacity_gb'] = (float(attributes['free_capacity']) /\n                                    (1024 ** 3))\n        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']\n        data['compression_support'] = self._compression_enabled\n\n        self._stats = data", "func_src_after": "    def _update_volume_stats(self):\n        \"\"\"Retrieve stats info from volume group.\"\"\"\n\n        LOG.debug(_(\"Updating volume stats\"))\n        data = {}\n\n        data['vendor_name'] = 'IBM'\n        data['driver_version'] = '1.1'\n        data['storage_protocol'] = list(self._enabled_protocols)\n\n        data['total_capacity_gb'] = 0  # To be overwritten\n        data['free_capacity_gb'] = 0   # To be overwritten\n        data['reserved_percentage'] = 0\n        data['QoS_support'] = False\n\n        pool = self.configuration.storwize_svc_volpool_name\n        #Get storage system name\n        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes or not attributes['name']:\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get system name'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        if not backend_name:\n            backend_name = '%s_%s' % (attributes['name'], pool)\n        data['volume_backend_name'] = backend_name\n\n        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes:\n            LOG.error(_('Could not get pool data from the storage'))\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get storage pool data'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        data['total_capacity_gb'] = (float(attributes['capacity']) /\n                                    (1024 ** 3))\n        data['free_capacity_gb'] = (float(attributes['free_capacity']) /\n                                    (1024 ** 3))\n        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']\n        data['compression_support'] = self._compression_enabled\n\n        self._stats = data", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to update storage volume statistics with IBM vendor details and capacity information."}
{"func_name": "Utility::UnZip", "func_src_before": "bool Utility::UnZip(const QString &zippath, const QString &destpath)\n{\n    int res = 0;\n    QDir dir(destpath);\n    if (!cp437) {\n        cp437 = new QCodePage437Codec();\n    }\n#ifdef Q_OS_WIN32\n    zlib_filefunc64_def ffunc;\n    fill_win32_filefunc64W(&ffunc);\n    unzFile zfile = unzOpen2_64(Utility::QStringToStdWString(QDir::toNativeSeparators(zippath)).c_str(), &ffunc);\n#else\n    unzFile zfile = unzOpen64(QDir::toNativeSeparators(zippath).toUtf8().constData());\n#endif\n\n    if ((zfile == NULL) || (!IsFileReadable(zippath)) || (!dir.exists())) {\n        return false;\n    }\n\n    res = unzGoToFirstFile(zfile);\n\n    if (res == UNZ_OK) {\n        do {\n            // Get the name of the file in the archive.\n            char file_name[MAX_PATH] = {0};\n            unz_file_info64 file_info;\n            unzGetCurrentFileInfo64(zfile, &file_info, file_name, MAX_PATH, NULL, 0, NULL, 0);\n            QString qfile_name;\n            QString cp437_file_name;\n            qfile_name = QString::fromUtf8(file_name);\n            if (!(file_info.flag & (1<<11))) {\n                // General purpose bit 11 says the filename is utf-8 encoded. If not set then\n                // IBM 437 encoding might be used.\n                cp437_file_name = cp437->toUnicode(file_name);\n            }\n\n            // If there is no file name then we can't do anything with it.\n            if (!qfile_name.isEmpty()) {\n                // We use the dir object to create the path in the temporary directory.\n                // Unfortunately, we need a dir ojbect to do this as it's not a static function.\n                // Full file path in the temporary directory.\n                QString file_path = destpath + \"/\" + qfile_name;\n                QFileInfo qfile_info(file_path);\n\n                // Is this entry a directory?\n                if (file_info.uncompressed_size == 0 && qfile_name.endsWith('/')) {\n                    dir.mkpath(qfile_name);\n                    continue;\n                } else {\n                    dir.mkpath(qfile_info.path());\n                }\n\n                // Open the file entry in the archive for reading.\n                if (unzOpenCurrentFile(zfile) != UNZ_OK) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Open the file on disk to write the entry in the archive to.\n                QFile entry(file_path);\n\n                if (!entry.open(QIODevice::WriteOnly | QIODevice::Truncate)) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Buffered reading and writing.\n                char buff[BUFF_SIZE] = {0};\n                int read = 0;\n\n                while ((read = unzReadCurrentFile(zfile, buff, BUFF_SIZE)) > 0) {\n                    entry.write(buff, read);\n                }\n\n                entry.close();\n\n                // Read errors are marked by a negative read amount.\n                if (read < 0) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // The file was read but the CRC did not match.\n                // We don't check the read file size vs the uncompressed file size\n                // because if they're different there should be a CRC error.\n                if (unzCloseCurrentFile(zfile) == UNZ_CRCERROR) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                if (!cp437_file_name.isEmpty() && cp437_file_name != qfile_name) {\n                    QString cp437_file_path = destpath + \"/\" + cp437_file_name;\n                    QFile::copy(file_path, cp437_file_path);\n                }\n            }\n        } while ((res = unzGoToNextFile(zfile)) == UNZ_OK);\n    }\n\n    if (res != UNZ_END_OF_LIST_OF_FILE) {\n        unzClose(zfile);\n        return false;\n    }\n\n    unzClose(zfile);\n    return true;\n}", "func_src_after": "bool Utility::UnZip(const QString &zippath, const QString &destpath)\n{\n    int res = 0;\n    QDir dir(destpath);\n    if (!cp437) {\n        cp437 = new QCodePage437Codec();\n    }\n#ifdef Q_OS_WIN32\n    zlib_filefunc64_def ffunc;\n    fill_win32_filefunc64W(&ffunc);\n    unzFile zfile = unzOpen2_64(Utility::QStringToStdWString(QDir::toNativeSeparators(zippath)).c_str(), &ffunc);\n#else\n    unzFile zfile = unzOpen64(QDir::toNativeSeparators(zippath).toUtf8().constData());\n#endif\n\n    if ((zfile == NULL) || (!IsFileReadable(zippath)) || (!dir.exists())) {\n        return false;\n    }\n\n    res = unzGoToFirstFile(zfile);\n\n    if (res == UNZ_OK) {\n        do {\n            // Get the name of the file in the archive.\n            char file_name[MAX_PATH] = {0};\n            unz_file_info64 file_info;\n            unzGetCurrentFileInfo64(zfile, &file_info, file_name, MAX_PATH, NULL, 0, NULL, 0);\n            QString qfile_name;\n            QString cp437_file_name;\n            qfile_name = QString::fromUtf8(file_name);\n            if (!(file_info.flag & (1<<11))) {\n                // General purpose bit 11 says the filename is utf-8 encoded. If not set then\n                // IBM 437 encoding might be used.\n                cp437_file_name = cp437->toUnicode(file_name);\n            }\n\n            // If there is no file name then we can't do anything with it.\n            if (!qfile_name.isEmpty()) {\n\n\t        // for security reasons against maliciously crafted zip archives\n\t        // we need the file path to always be inside the target folder \n\t        // and not outside, so we will remove all illegal backslashes\n\t        // and all relative upward paths segments \"/../\" from the zip's local \n\t        // file name/path before prepending the target folder to create \n\t        // the final path\n\n\t        QString original_path = qfile_name;\n\t        bool evil_or_corrupt_epub = false;\n\n\t        if (qfile_name.contains(\"\\\\\")) evil_or_corrupt_epub = true; \n\t        qfile_name = \"/\" + qfile_name.replace(\"\\\\\",\"\");\n\n\t        if (qfile_name.contains(\"/../\")) evil_or_corrupt_epub = true;\n\t        qfile_name = qfile_name.replace(\"/../\",\"/\");\n\n\t        while(qfile_name.startsWith(\"/\")) { \n\t\t  qfile_name = qfile_name.remove(0,1);\n\t        }\n                \n\t        if (cp437_file_name.contains(\"\\\\\")) evil_or_corrupt_epub = true; \n\t        cp437_file_name = \"/\" + cp437_file_name.replace(\"\\\\\",\"\");\n\n\t        if (cp437_file_name.contains(\"/../\")) evil_or_corrupt_epub = true;\n\t        cp437_file_name = cp437_file_name.replace(\"/../\",\"/\");\n\n\t        while(cp437_file_name.startsWith(\"/\")) { \n\t\t  cp437_file_name = cp437_file_name.remove(0,1);\n\t        }\n\n\t        if (evil_or_corrupt_epub) {\n\t\t    unzCloseCurrentFile(zfile);\n\t\t    unzClose(zfile);\n\t\t    // throw (UNZIPLoadParseError(QString(QObject::tr(\"Possible evil or corrupt zip file name: %1\")).arg(original_path).toStdString()));\n                    return false;\n\t        }\n\n                // We use the dir object to create the path in the temporary directory.\n                // Unfortunately, we need a dir ojbect to do this as it's not a static function.\n                // Full file path in the temporary directory.\n                QString file_path = destpath + \"/\" + qfile_name;\n                QFileInfo qfile_info(file_path);\n\n                // Is this entry a directory?\n                if (file_info.uncompressed_size == 0 && qfile_name.endsWith('/')) {\n                    dir.mkpath(qfile_name);\n                    continue;\n                } else {\n                    dir.mkpath(qfile_info.path());\n                }\n\n                // Open the file entry in the archive for reading.\n                if (unzOpenCurrentFile(zfile) != UNZ_OK) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Open the file on disk to write the entry in the archive to.\n                QFile entry(file_path);\n\n                if (!entry.open(QIODevice::WriteOnly | QIODevice::Truncate)) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // Buffered reading and writing.\n                char buff[BUFF_SIZE] = {0};\n                int read = 0;\n\n                while ((read = unzReadCurrentFile(zfile, buff, BUFF_SIZE)) > 0) {\n                    entry.write(buff, read);\n                }\n\n                entry.close();\n\n                // Read errors are marked by a negative read amount.\n                if (read < 0) {\n                    unzCloseCurrentFile(zfile);\n                    unzClose(zfile);\n                    return false;\n                }\n\n                // The file was read but the CRC did not match.\n                // We don't check the read file size vs the uncompressed file size\n                // because if they're different there should be a CRC error.\n                if (unzCloseCurrentFile(zfile) == UNZ_CRCERROR) {\n                    unzClose(zfile);\n                    return false;\n                }\n\n                if (!cp437_file_name.isEmpty() && cp437_file_name != qfile_name) {\n                    QString cp437_file_path = destpath + \"/\" + cp437_file_name;\n                    QFile::copy(file_path, cp437_file_path);\n                }\n            }\n        } while ((res = unzGoToNextFile(zfile)) == UNZ_OK);\n    }\n\n    if (res != UNZ_END_OF_LIST_OF_FILE) {\n        unzClose(zfile);\n        return false;\n    }\n\n    unzClose(zfile);\n    return true;\n}", "commit_link": "github.com/Sigil-Ebook/Sigil/commit/0979ba8d10c96ebca330715bfd4494ea0e019a8f", "file_name": "src/Misc/Utility.cpp", "vul_type": "cwe-022", "description": "Write a C++ function to unzip a file to a specified directory using the QuaZIP library."}
{"func_name": "tcp_test", "func_src_before": "int tcp_test(const char* ip_str, const short port)\n{\n    int sock, i;\n    struct sockaddr_in s_in;\n    int packetsize = 1024;\n    unsigned char packet[packetsize];\n    struct timeval tv, tv2, tv3;\n    int caplen = 0;\n    int times[REQUESTS];\n    int min, avg, max, len;\n    struct net_hdr nh;\n\n    tv3.tv_sec=0;\n    tv3.tv_usec=1;\n\n    s_in.sin_family = PF_INET;\n    s_in.sin_port = htons(port);\n    if (!inet_aton(ip_str, &s_in.sin_addr))\n            return -1;\n\n    if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n            return -1;\n\n    /* avoid blocking on reading the socket */\n    if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n    {\n        perror( \"fcntl(O_NONBLOCK) failed\" );\n        return( 1 );\n    }\n\n    gettimeofday( &tv, NULL );\n\n    while (1)  //waiting for relayed packet\n    {\n        if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n        {\n            if(errno != EINPROGRESS && errno != EALREADY)\n            {\n                perror(\"connect\");\n                close(sock);\n\n                printf(\"Failed to connect\\n\");\n\n                return -1;\n            }\n        }\n        else\n        {\n            gettimeofday( &tv2, NULL );\n            break;\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 3000ms for a successful connect\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (3000*1000))\n        {\n            printf(\"Connection timed out\\n\");\n            close(sock);\n            return(-1);\n        }\n        usleep(10);\n    }\n\n    PCT; printf(\"TCP connection successful\\n\");\n\n    //trying to identify airserv-ng\n    memset(&nh, 0, sizeof(nh));\n//     command: GET_CHAN\n    nh.nh_type\t= 2;\n    nh.nh_len\t= htonl(0);\n\n    if (send(sock, &nh, sizeof(nh), 0) != sizeof(nh))\n    {\n        perror(\"send\");\n        return -1;\n    }\n\n    gettimeofday( &tv, NULL );\n    i=0;\n\n    while (1)  //waiting for GET_CHAN answer\n    {\n        caplen = read(sock, &nh, sizeof(nh));\n\n        if(caplen == -1)\n        {\n            if( errno != EAGAIN )\n            {\n                perror(\"read\");\n                return -1;\n            }\n        }\n\n        if( (unsigned)caplen == sizeof(nh))\n        {\n            len = ntohl(nh.nh_len);\n            if( nh.nh_type == 1 && i==0 )\n            {\n                i=1;\n                caplen = read(sock, packet, len);\n                if(caplen == len)\n                {\n                    i=2;\n                    break;\n                }\n                else\n                {\n                    i=0;\n                }\n            }\n            else\n            {\n                caplen = read(sock, packet, len);\n            }\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 1000ms for an answer\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n        {\n            break;\n        }\n        if(caplen == -1)\n            usleep(10);\n    }\n\n    if(i==2)\n    {\n        PCT; printf(\"airserv-ng found\\n\");\n    }\n    else\n    {\n        PCT; printf(\"airserv-ng NOT found\\n\");\n    }\n\n    close(sock);\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n                return -1;\n\n        /* avoid blocking on reading the socket */\n        if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n        {\n            perror( \"fcntl(O_NONBLOCK) failed\" );\n            return( 1 );\n        }\n\n        usleep(1000);\n\n        gettimeofday( &tv, NULL );\n\n        while (1)  //waiting for relayed packet\n        {\n            if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n            {\n                if(errno != EINPROGRESS && errno != EALREADY)\n                {\n                    perror(\"connect\");\n                    close(sock);\n\n                    printf(\"Failed to connect\\n\");\n\n                    return -1;\n                }\n            }\n            else\n            {\n                gettimeofday( &tv2, NULL );\n                break;\n            }\n\n            gettimeofday( &tv2, NULL );\n            //wait 1000ms for a successful connect\n            if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n            {\n                break;\n            }\n            //simple \"high-precision\" usleep\n            select(1, NULL, NULL, NULL, &tv3);\n        }\n        times[i] = ((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec));\n        printf( \"\\r%d/%d\\r\", i, REQUESTS);\n        fflush(stdout);\n        close(sock);\n    }\n\n    min = INT_MAX;\n    avg = 0;\n    max = 0;\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if(times[i] < min) min = times[i];\n        if(times[i] > max) max = times[i];\n        avg += times[i];\n    }\n    avg /= REQUESTS;\n\n    PCT; printf(\"ping %s:%d (min/avg/max): %.3fms/%.3fms/%.3fms\\n\", ip_str, port, min/1000.0, avg/1000.0, max/1000.0);\n\n    return 0;\n}", "func_src_after": "int tcp_test(const char* ip_str, const short port)\n{\n    int sock, i;\n    struct sockaddr_in s_in;\n    int packetsize = 1024;\n    unsigned char packet[packetsize];\n    struct timeval tv, tv2, tv3;\n    int caplen = 0;\n    int times[REQUESTS];\n    int min, avg, max, len;\n    struct net_hdr nh;\n\n    tv3.tv_sec=0;\n    tv3.tv_usec=1;\n\n    s_in.sin_family = PF_INET;\n    s_in.sin_port = htons(port);\n    if (!inet_aton(ip_str, &s_in.sin_addr))\n            return -1;\n\n    if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n            return -1;\n\n    /* avoid blocking on reading the socket */\n    if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n    {\n        perror( \"fcntl(O_NONBLOCK) failed\" );\n        return( 1 );\n    }\n\n    gettimeofday( &tv, NULL );\n\n    while (1)  //waiting for relayed packet\n    {\n        if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n        {\n            if(errno != EINPROGRESS && errno != EALREADY)\n            {\n                perror(\"connect\");\n                close(sock);\n\n                printf(\"Failed to connect\\n\");\n\n                return -1;\n            }\n        }\n        else\n        {\n            gettimeofday( &tv2, NULL );\n            break;\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 3000ms for a successful connect\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (3000*1000))\n        {\n            printf(\"Connection timed out\\n\");\n            close(sock);\n            return(-1);\n        }\n        usleep(10);\n    }\n\n    PCT; printf(\"TCP connection successful\\n\");\n\n    //trying to identify airserv-ng\n    memset(&nh, 0, sizeof(nh));\n//     command: GET_CHAN\n    nh.nh_type\t= 2;\n    nh.nh_len\t= htonl(0);\n\n    if (send(sock, &nh, sizeof(nh), 0) != sizeof(nh))\n    {\n        perror(\"send\");\n        return -1;\n    }\n\n    gettimeofday( &tv, NULL );\n    i=0;\n\n    while (1)  //waiting for GET_CHAN answer\n    {\n        caplen = read(sock, &nh, sizeof(nh));\n\n        if(caplen == -1)\n        {\n            if( errno != EAGAIN )\n            {\n                perror(\"read\");\n                return -1;\n            }\n        }\n\n        if( (unsigned)caplen == sizeof(nh))\n        {\n            len = ntohl(nh.nh_len);\n            if (len > 1024 || len < 0)\n                continue;\n            if( nh.nh_type == 1 && i==0 )\n            {\n                i=1;\n                caplen = read(sock, packet, len);\n                if(caplen == len)\n                {\n                    i=2;\n                    break;\n                }\n                else\n                {\n                    i=0;\n                }\n            }\n            else\n            {\n                caplen = read(sock, packet, len);\n            }\n        }\n\n        gettimeofday( &tv2, NULL );\n        //wait 1000ms for an answer\n        if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n        {\n            break;\n        }\n        if(caplen == -1)\n            usleep(10);\n    }\n\n    if(i==2)\n    {\n        PCT; printf(\"airserv-ng found\\n\");\n    }\n    else\n    {\n        PCT; printf(\"airserv-ng NOT found\\n\");\n    }\n\n    close(sock);\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if ((sock = socket(s_in.sin_family, SOCK_STREAM, IPPROTO_TCP)) == -1)\n                return -1;\n\n        /* avoid blocking on reading the socket */\n        if( fcntl( sock, F_SETFL, O_NONBLOCK ) < 0 )\n        {\n            perror( \"fcntl(O_NONBLOCK) failed\" );\n            return( 1 );\n        }\n\n        usleep(1000);\n\n        gettimeofday( &tv, NULL );\n\n        while (1)  //waiting for relayed packet\n        {\n            if (connect(sock, (struct sockaddr*) &s_in, sizeof(s_in)) == -1)\n            {\n                if(errno != EINPROGRESS && errno != EALREADY)\n                {\n                    perror(\"connect\");\n                    close(sock);\n\n                    printf(\"Failed to connect\\n\");\n\n                    return -1;\n                }\n            }\n            else\n            {\n                gettimeofday( &tv2, NULL );\n                break;\n            }\n\n            gettimeofday( &tv2, NULL );\n            //wait 1000ms for a successful connect\n            if (((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec)) > (1000*1000))\n            {\n                break;\n            }\n            //simple \"high-precision\" usleep\n            select(1, NULL, NULL, NULL, &tv3);\n        }\n        times[i] = ((tv2.tv_sec*1000000 - tv.tv_sec*1000000) + (tv2.tv_usec - tv.tv_usec));\n        printf( \"\\r%d/%d\\r\", i, REQUESTS);\n        fflush(stdout);\n        close(sock);\n    }\n\n    min = INT_MAX;\n    avg = 0;\n    max = 0;\n\n    for(i=0; i<REQUESTS; i++)\n    {\n        if(times[i] < min) min = times[i];\n        if(times[i] > max) max = times[i];\n        avg += times[i];\n    }\n    avg /= REQUESTS;\n\n    PCT; printf(\"ping %s:%d (min/avg/max): %.3fms/%.3fms/%.3fms\\n\", ip_str, port, min/1000.0, avg/1000.0, max/1000.0);\n\n    return 0;\n}", "commit_link": "github.com/aircrack-ng/aircrack-ng/commit/091b153f294b9b695b0b2831e65936438b550d7b", "file_name": "src/aireplay-ng.c", "vul_type": "cwe-787", "description": "Write a C function named `tcp_test` that attempts to establish a TCP connection to a specified IP and port, sends a specific command, and measures connection times."}
{"func_name": "main", "func_src_before": "def main(_):\n  # Generate some fake Iris data.\n  # It is okay for this example because this example is about how to use the\n  # debugger, not how to use machine learning to solve the Iris classification\n  # problem.\n  def training_input_fn():\n    return ({\n        \"features\": tf.random_normal([128, 4])\n    }, tf.random_uniform([128], minval=0, maxval=3, dtype=tf.int32))\n\n  def test_input_fn():\n    return ({\n        \"features\": tf.random_normal([32, 4])\n    }, tf.random_uniform([32], minval=0, maxval=3, dtype=tf.int32))\n\n  feature_columns = [tf.feature_column.numeric_column(\"features\", shape=(4,))]\n\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\n  model_dir = FLAGS.model_dir or tempfile.mkdtemp(prefix=\"debug_tflearn_iris_\")\n\n  classifier = tf.estimator.DNNClassifier(\n      feature_columns=feature_columns,\n      hidden_units=[10, 20, 10],\n      n_classes=3,\n      model_dir=model_dir)\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  hooks = []\n  if FLAGS.debug:\n    config_file_path = (\n        tempfile.mktemp(\".tfdbg_config\")\n        if FLAGS.use_random_config_path else None)\n    hooks.append(\n        tf_debug.LocalCLIDebugHook(\n            ui_type=FLAGS.ui_type,\n            dump_root=FLAGS.dump_root,\n            config_file_path=config_file_path))\n  elif FLAGS.tensorboard_debug_address:\n    hooks.append(tf_debug.TensorBoardDebugHook(FLAGS.tensorboard_debug_address))\n\n  # Train model, using tfdbg hook.\n  classifier.train(training_input_fn, steps=FLAGS.train_steps, hooks=hooks)\n\n  # Evaluate accuracy, using tfdbg hook.\n  accuracy_score = classifier.evaluate(\n      test_input_fn, steps=FLAGS.eval_steps, hooks=hooks)[\"accuracy\"]\n\n  print(\"After training %d steps, Accuracy = %f\" %\n        (FLAGS.train_steps, accuracy_score))\n\n  # Make predictions, using tfdbg hook.\n  predict_results = classifier.predict(test_input_fn, hooks=hooks)\n  print(\"A prediction result: %s\" % next(predict_results))", "func_src_after": "def main(_):\n  # Generate some fake Iris data.\n  # It is okay for this example because this example is about how to use the\n  # debugger, not how to use machine learning to solve the Iris classification\n  # problem.\n  def training_input_fn():\n    return ({\n        \"features\": tf.random_normal([128, 4])\n    }, tf.random_uniform([128], minval=0, maxval=3, dtype=tf.int32))\n\n  def test_input_fn():\n    return ({\n        \"features\": tf.random_normal([32, 4])\n    }, tf.random_uniform([32], minval=0, maxval=3, dtype=tf.int32))\n\n  feature_columns = [tf.feature_column.numeric_column(\"features\", shape=(4,))]\n\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\n  model_dir = FLAGS.model_dir or tempfile.mkdtemp(prefix=\"debug_tflearn_iris_\")\n\n  classifier = tf.estimator.DNNClassifier(\n      feature_columns=feature_columns,\n      hidden_units=[10, 20, 10],\n      n_classes=3,\n      model_dir=model_dir)\n\n  if FLAGS.debug and FLAGS.tensorboard_debug_address:\n    raise ValueError(\n        \"The --debug and --tensorboard_debug_address flags are mutually \"\n        \"exclusive.\")\n  hooks = []\n  if FLAGS.debug:\n    if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n    else:\n      config_file_path = None\n    hooks.append(\n        tf_debug.LocalCLIDebugHook(\n            ui_type=FLAGS.ui_type,\n            dump_root=FLAGS.dump_root,\n            config_file_path=config_file_path))\n  elif FLAGS.tensorboard_debug_address:\n    hooks.append(tf_debug.TensorBoardDebugHook(FLAGS.tensorboard_debug_address))\n\n  # Train model, using tfdbg hook.\n  classifier.train(training_input_fn, steps=FLAGS.train_steps, hooks=hooks)\n\n  # Evaluate accuracy, using tfdbg hook.\n  accuracy_score = classifier.evaluate(\n      test_input_fn, steps=FLAGS.eval_steps, hooks=hooks)[\"accuracy\"]\n\n  print(\"After training %d steps, Accuracy = %f\" %\n        (FLAGS.train_steps, accuracy_score))\n\n  # Make predictions, using tfdbg hook.\n  predict_results = classifier.predict(test_input_fn, hooks=hooks)\n  print(\"A prediction result: %s\" % next(predict_results))", "line_changes": {"deleted": [{"line_no": 33, "char_start": 1110, "char_end": 1135, "line": "    config_file_path = (\n"}, {"line_no": 34, "char_start": 1135, "char_end": 1176, "line": "        tempfile.mktemp(\".tfdbg_config\")\n"}, {"line_no": 35, "char_start": 1176, "char_end": 1227, "line": "        if FLAGS.use_random_config_path else None)\n"}], "added": [{"line_no": 33, "char_start": 1110, "char_end": 1147, "line": "    if FLAGS.use_random_config_path:\n"}, {"line_no": 35, "char_start": 1200, "char_end": 1262, "line": "      _, config_file_path = tempfile.mkstemp(\".tfdbg_config\")\n"}, {"line_no": 36, "char_start": 1262, "char_end": 1272, "line": "    else:\n"}, {"line_no": 37, "char_start": 1272, "char_end": 1302, "line": "      config_file_path = None\n"}]}, "char_changes": {"deleted": [{"char_start": 1132, "char_end": 1142, "chars": " (\n       "}, {"char_start": 1180, "char_end": 1204, "chars": "    if FLAGS.use_random_"}, {"char_start": 1216, "char_end": 1220, "chars": "else"}, {"char_start": 1225, "char_end": 1226, "chars": ")"}], "added": [{"char_start": 1114, "char_end": 1209, "chars": "if FLAGS.use_random_config_path:\n      # TODO(mihaimaruseac): Safe to ignore fd here?\n      _, "}, {"char_start": 1239, "char_end": 1240, "chars": "s"}, {"char_start": 1266, "char_end": 1278, "chars": "else:\n      "}, {"char_start": 1285, "char_end": 1290, "chars": "file_"}, {"char_start": 1295, "char_end": 1296, "chars": "="}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/62644d6d2af6e185361f770099bf5d5e6d2d39ff", "file_name": "debug_tflearn_iris.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420360028\nChange-Id: Icd8a7ba3e47c2ff63a26a2fe007737ef01c0cb1d", "description": "Write a Python script using TensorFlow to create, train, and evaluate a DNNClassifier for the Iris dataset with debugging hooks."}
{"func_name": "_get_active_nsp", "func_src_before": "    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]", "func_src_after": "    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_iscsi.py", "vul_type": "cwe-078", "description": "Write a Python function to retrieve the active network service provider (NSP) for a specified host by parsing CLI command output."}
{"func_name": "dd_save_text", "func_src_before": "void dd_save_text(struct dump_dir *dd, const char *name, const char *data)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, strlen(data), dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "func_src_after": "void dd_save_text(struct dump_dir *dd, const char *name, const char *data)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    if (!str_is_correct_filename(name))\n        error_msg_and_die(\"Cannot save text. '%s' is not a valid file name\", name);\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, strlen(data), dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "Write a C function `dd_save_text` to save text data to a file within a directory structure, handling errors for directory access and file naming."}
{"func_name": "jpc_pi_nextrpcl", "func_src_before": "static int jpc_pi_nextrpcl(register jpc_pi_t *pi)\n{\n\tint rlvlno;\n\tjpc_pirlvl_t *pirlvl;\n\tjpc_pchg_t *pchg;\n\tint prchind;\n\tint prcvind;\n\tint *prclyrno;\n\tint compno;\n\tjpc_picomp_t *picomp;\n\tint xstep;\n\tint ystep;\n\tuint_fast32_t r;\n\tuint_fast32_t rpx;\n\tuint_fast32_t rpy;\n\tuint_fast32_t trx0;\n\tuint_fast32_t try0;\n\n\tpchg = pi->pchg;\n\tif (!pi->prgvolfirst) {\n\t\tgoto skip;\n\t} else {\n\t\tpi->xstep = 0;\n\t\tpi->ystep = 0;\n\t\tfor (compno = 0, picomp = pi->picomps; compno < pi->numcomps;\n\t\t  ++compno, ++picomp) {\n\t\t\tfor (rlvlno = 0, pirlvl = picomp->pirlvls; rlvlno <\n\t\t\t  picomp->numrlvls; ++rlvlno, ++pirlvl) {\n\t\t\t\t// Check for the potential for overflow problems.\n\t\t\t\tif (pirlvl->prcwidthexpn + pi->picomp->numrlvls >\n\t\t\t\t  JAS_UINTFAST32_NUMBITS - 2 ||\n\t\t\t\t  pirlvl->prcheightexpn + pi->picomp->numrlvls >\n\t\t\t\t  JAS_UINTFAST32_NUMBITS - 2) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\txstep = picomp->hsamp * (JAS_CAST(uint_fast32_t, 1) <<\n\t\t\t\t  (pirlvl->prcwidthexpn + picomp->numrlvls - rlvlno - 1));\n\t\t\t\tystep = picomp->vsamp * (JAS_CAST(uint_fast32_t, 1) <<\n\t\t\t\t  (pirlvl->prcheightexpn + picomp->numrlvls - rlvlno - 1));\n\t\t\t\tpi->xstep = (!pi->xstep) ? xstep : JAS_MIN(pi->xstep, xstep);\n\t\t\t\tpi->ystep = (!pi->ystep) ? ystep : JAS_MIN(pi->ystep, ystep);\n\t\t\t}\n\t\t}\n\t\tpi->prgvolfirst = 0;\n\t}\n\n\tfor (pi->rlvlno = pchg->rlvlnostart; pi->rlvlno < pchg->rlvlnoend &&\n\t  pi->rlvlno < pi->maxrlvls; ++pi->rlvlno) {\n\t\tfor (pi->y = pi->ystart; pi->y < pi->yend; pi->y +=\n\t\t  pi->ystep - (pi->y % pi->ystep)) {\n\t\t\tfor (pi->x = pi->xstart; pi->x < pi->xend; pi->x +=\n\t\t\t  pi->xstep - (pi->x % pi->xstep)) {\n\t\t\t\tfor (pi->compno = pchg->compnostart,\n\t\t\t\t  pi->picomp = &pi->picomps[pi->compno];\n\t\t\t\t  pi->compno < JAS_CAST(int, pchg->compnoend) && pi->compno <\n\t\t\t\t  pi->numcomps; ++pi->compno, ++pi->picomp) {\n\t\t\t\t\tif (pi->rlvlno >= pi->picomp->numrlvls) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tpi->pirlvl = &pi->picomp->pirlvls[pi->rlvlno];\n\t\t\t\t\tif (pi->pirlvl->numprcs == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tr = pi->picomp->numrlvls - 1 - pi->rlvlno;\n\t\t\t\t\trpx = r + pi->pirlvl->prcwidthexpn;\n\t\t\t\t\trpy = r + pi->pirlvl->prcheightexpn;\n\t\t\t\t\ttrx0 = JPC_CEILDIV(pi->xstart, pi->picomp->hsamp << r);\n\t\t\t\t\ttry0 = JPC_CEILDIV(pi->ystart, pi->picomp->vsamp << r);\n\t\t\t\t\tif (((pi->x == pi->xstart &&\n\t\t\t\t\t  ((trx0 << r) % (JAS_CAST(uint_fast32_t, 1) << rpx)))\n\t\t\t\t\t  || !(pi->x % (JAS_CAST(uint_fast32_t, 1) << rpx))) &&\n\t\t\t\t\t  ((pi->y == pi->ystart &&\n\t\t\t\t\t  ((try0 << r) % (JAS_CAST(uint_fast32_t, 1) << rpy)))\n\t\t\t\t\t  || !(pi->y % (JAS_CAST(uint_fast32_t, 1) << rpy)))) {\n\t\t\t\t\t\tprchind = JPC_FLOORDIVPOW2(JPC_CEILDIV(pi->x,\n\t\t\t\t\t\t  pi->picomp->hsamp << r), pi->pirlvl->prcwidthexpn) -\n\t\t\t\t\t\t  JPC_FLOORDIVPOW2(trx0, pi->pirlvl->prcwidthexpn);\n\t\t\t\t\t\tprcvind = JPC_FLOORDIVPOW2(JPC_CEILDIV(pi->y,\n\t\t\t\t\t\t  pi->picomp->vsamp << r), pi->pirlvl->prcheightexpn) -\n\t\t\t\t\t\t  JPC_FLOORDIVPOW2(try0, pi->pirlvl->prcheightexpn);\n\t\t\t\t\t\tpi->prcno = prcvind * pi->pirlvl->numhprcs + prchind;\n\n\t\t\t\t\t\tassert(pi->prcno < pi->pirlvl->numprcs);\n\t\t\t\t\t\tfor (pi->lyrno = 0; pi->lyrno <\n\t\t\t\t\t\t  pi->numlyrs && pi->lyrno < JAS_CAST(int,\n\t\t\t\t\t\t  pchg->lyrnoend); ++pi->lyrno) {\n\t\t\t\t\t\t\tprclyrno = &pi->pirlvl->prclyrnos[pi->prcno];\n\t\t\t\t\t\t\tif (pi->lyrno >= *prclyrno) {\n\t\t\t\t\t\t\t\t++(*prclyrno);\n\t\t\t\t\t\t\t\treturn 0;\n\t\t\t\t\t\t\t}\nskip:\n\t\t\t\t\t\t\t;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn 1;\n}", "func_src_after": "static int jpc_pi_nextrpcl(register jpc_pi_t *pi)\n{\n\tint rlvlno;\n\tjpc_pirlvl_t *pirlvl;\n\tjpc_pchg_t *pchg;\n\tint prchind;\n\tint prcvind;\n\tint *prclyrno;\n\tint compno;\n\tjpc_picomp_t *picomp;\n\tint xstep;\n\tint ystep;\n\tuint_fast32_t r;\n\tuint_fast32_t rpx;\n\tuint_fast32_t rpy;\n\tuint_fast32_t trx0;\n\tuint_fast32_t try0;\n\n\tpchg = pi->pchg;\n\tif (!pi->prgvolfirst) {\n\t\tgoto skip;\n\t} else {\n\t\tpi->xstep = 0;\n\t\tpi->ystep = 0;\n\t\tfor (compno = 0, picomp = pi->picomps; compno < pi->numcomps;\n\t\t  ++compno, ++picomp) {\n\t\t\tfor (rlvlno = 0, pirlvl = picomp->pirlvls; rlvlno <\n\t\t\t  picomp->numrlvls; ++rlvlno, ++pirlvl) {\n\t\t\t\t// Check for the potential for overflow problems.\n\t\t\t\tif (pirlvl->prcwidthexpn + picomp->numrlvls >\n\t\t\t\t  JAS_UINTFAST32_NUMBITS - 2 ||\n\t\t\t\t  pirlvl->prcheightexpn + picomp->numrlvls >\n\t\t\t\t  JAS_UINTFAST32_NUMBITS - 2) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\txstep = picomp->hsamp * (JAS_CAST(uint_fast32_t, 1) <<\n\t\t\t\t  (pirlvl->prcwidthexpn + picomp->numrlvls - rlvlno - 1));\n\t\t\t\tystep = picomp->vsamp * (JAS_CAST(uint_fast32_t, 1) <<\n\t\t\t\t  (pirlvl->prcheightexpn + picomp->numrlvls - rlvlno - 1));\n\t\t\t\tpi->xstep = (!pi->xstep) ? xstep : JAS_MIN(pi->xstep, xstep);\n\t\t\t\tpi->ystep = (!pi->ystep) ? ystep : JAS_MIN(pi->ystep, ystep);\n\t\t\t}\n\t\t}\n\t\tpi->prgvolfirst = 0;\n\t}\n\n\tfor (pi->rlvlno = pchg->rlvlnostart; pi->rlvlno < pchg->rlvlnoend &&\n\t  pi->rlvlno < pi->maxrlvls; ++pi->rlvlno) {\n\t\tfor (pi->y = pi->ystart; pi->y < pi->yend; pi->y +=\n\t\t  pi->ystep - (pi->y % pi->ystep)) {\n\t\t\tfor (pi->x = pi->xstart; pi->x < pi->xend; pi->x +=\n\t\t\t  pi->xstep - (pi->x % pi->xstep)) {\n\t\t\t\tfor (pi->compno = pchg->compnostart,\n\t\t\t\t  pi->picomp = &pi->picomps[pi->compno];\n\t\t\t\t  pi->compno < JAS_CAST(int, pchg->compnoend) && pi->compno <\n\t\t\t\t  pi->numcomps; ++pi->compno, ++pi->picomp) {\n\t\t\t\t\tif (pi->rlvlno >= pi->picomp->numrlvls) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tpi->pirlvl = &pi->picomp->pirlvls[pi->rlvlno];\n\t\t\t\t\tif (pi->pirlvl->numprcs == 0) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tr = pi->picomp->numrlvls - 1 - pi->rlvlno;\n\t\t\t\t\trpx = r + pi->pirlvl->prcwidthexpn;\n\t\t\t\t\trpy = r + pi->pirlvl->prcheightexpn;\n\t\t\t\t\ttrx0 = JPC_CEILDIV(pi->xstart, pi->picomp->hsamp << r);\n\t\t\t\t\ttry0 = JPC_CEILDIV(pi->ystart, pi->picomp->vsamp << r);\n\t\t\t\t\tif (((pi->x == pi->xstart &&\n\t\t\t\t\t  ((trx0 << r) % (JAS_CAST(uint_fast32_t, 1) << rpx)))\n\t\t\t\t\t  || !(pi->x % (JAS_CAST(uint_fast32_t, 1) << rpx))) &&\n\t\t\t\t\t  ((pi->y == pi->ystart &&\n\t\t\t\t\t  ((try0 << r) % (JAS_CAST(uint_fast32_t, 1) << rpy)))\n\t\t\t\t\t  || !(pi->y % (JAS_CAST(uint_fast32_t, 1) << rpy)))) {\n\t\t\t\t\t\tprchind = JPC_FLOORDIVPOW2(JPC_CEILDIV(pi->x,\n\t\t\t\t\t\t  pi->picomp->hsamp << r), pi->pirlvl->prcwidthexpn) -\n\t\t\t\t\t\t  JPC_FLOORDIVPOW2(trx0, pi->pirlvl->prcwidthexpn);\n\t\t\t\t\t\tprcvind = JPC_FLOORDIVPOW2(JPC_CEILDIV(pi->y,\n\t\t\t\t\t\t  pi->picomp->vsamp << r), pi->pirlvl->prcheightexpn) -\n\t\t\t\t\t\t  JPC_FLOORDIVPOW2(try0, pi->pirlvl->prcheightexpn);\n\t\t\t\t\t\tpi->prcno = prcvind * pi->pirlvl->numhprcs + prchind;\n\n\t\t\t\t\t\tassert(pi->prcno < pi->pirlvl->numprcs);\n\t\t\t\t\t\tfor (pi->lyrno = 0; pi->lyrno <\n\t\t\t\t\t\t  pi->numlyrs && pi->lyrno < JAS_CAST(int,\n\t\t\t\t\t\t  pchg->lyrnoend); ++pi->lyrno) {\n\t\t\t\t\t\t\tprclyrno = &pi->pirlvl->prclyrnos[pi->prcno];\n\t\t\t\t\t\t\tif (pi->lyrno >= *prclyrno) {\n\t\t\t\t\t\t\t\t++(*prclyrno);\n\t\t\t\t\t\t\t\treturn 0;\n\t\t\t\t\t\t\t}\nskip:\n\t\t\t\t\t\t\t;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn 1;\n}", "commit_link": "github.com/mdadams/jasper/commit/f25486c3d4aa472fec79150f2c41ed4333395d3d", "file_name": "src/libjasper/jpc/jpc_t2cod.c", "vul_type": "cwe-125", "description": "Write a C function named `jpc_pi_nextrpcl` that processes image components and returns an integer status code."}
{"func_name": "xdp_umem_reg", "func_src_before": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint size_chk, err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\t * - making sure the memory area is consecutive\n\t\t * but for now, we simply say \"computer says no\".\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\n\t\t\tXDP_UMEM_USES_NEED_WAKEUP))\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && !is_power_of_2(chunk_size))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(addr)) {\n\t\t/* Memory area has to be page size aligned. For\n\t\t * simplicity, this might change.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif ((addr + size) < addr)\n\t\treturn -EINVAL;\n\n\tchunks = (unsigned int)div_u64(size, chunk_size);\n\tif (chunks == 0)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks) {\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsize_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;\n\tif (size_chk < 0)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);\n\tumem->size = size;\n\tumem->headroom = headroom;\n\tumem->chunk_size_nohr = chunk_size - headroom;\n\tumem->npgs = size / PAGE_SIZE;\n\tumem->pgs = NULL;\n\tumem->user = NULL;\n\tumem->flags = mr->flags;\n\tINIT_LIST_HEAD(&umem->xsk_list);\n\tspin_lock_init(&umem->xsk_list_lock);\n\n\trefcount_set(&umem->users, 1);\n\n\terr = xdp_umem_account_pages(umem);\n\tif (err)\n\t\treturn err;\n\n\terr = xdp_umem_pin_pages(umem);\n\tif (err)\n\t\tgoto out_account;\n\n\tumem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\n\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!umem->pages) {\n\t\terr = -ENOMEM;\n\t\tgoto out_pin;\n\t}\n\n\terr = xdp_umem_map_pages(umem);\n\tif (!err)\n\t\treturn 0;\n\n\tkvfree(umem->pages);\n\nout_pin:\n\txdp_umem_unpin_pages(umem);\nout_account:\n\txdp_umem_unaccount_pages(umem);\n\treturn err;\n}", "func_src_after": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\t * - making sure the memory area is consecutive\n\t\t * but for now, we simply say \"computer says no\".\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\n\t\t\tXDP_UMEM_USES_NEED_WAKEUP))\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && !is_power_of_2(chunk_size))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(addr)) {\n\t\t/* Memory area has to be page size aligned. For\n\t\t * simplicity, this might change.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif ((addr + size) < addr)\n\t\treturn -EINVAL;\n\n\tchunks = (unsigned int)div_u64(size, chunk_size);\n\tif (chunks == 0)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks) {\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (headroom >= chunk_size - XDP_PACKET_HEADROOM)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);\n\tumem->size = size;\n\tumem->headroom = headroom;\n\tumem->chunk_size_nohr = chunk_size - headroom;\n\tumem->npgs = size / PAGE_SIZE;\n\tumem->pgs = NULL;\n\tumem->user = NULL;\n\tumem->flags = mr->flags;\n\tINIT_LIST_HEAD(&umem->xsk_list);\n\tspin_lock_init(&umem->xsk_list_lock);\n\n\trefcount_set(&umem->users, 1);\n\n\terr = xdp_umem_account_pages(umem);\n\tif (err)\n\t\treturn err;\n\n\terr = xdp_umem_pin_pages(umem);\n\tif (err)\n\t\tgoto out_account;\n\n\tumem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\n\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!umem->pages) {\n\t\terr = -ENOMEM;\n\t\tgoto out_pin;\n\t}\n\n\terr = xdp_umem_map_pages(umem);\n\tif (!err)\n\t\treturn 0;\n\n\tkvfree(umem->pages);\n\nout_pin:\n\txdp_umem_unpin_pages(umem);\nout_account:\n\txdp_umem_unaccount_pages(umem);\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/99e3a236dd43d06c65af0a2ef9cb44306aef6e02", "file_name": "net/xdp/xdp_umem.c", "vul_type": "cwe-787", "description": "Write a C function named `xdp_umem_reg` that registers a user memory (`umem`) area for use with XDP sockets, validating the provided memory region parameters."}
{"func_name": "Curl_close", "func_src_before": "CURLcode Curl_close(struct Curl_easy *data)\n{\n  struct Curl_multi *m;\n\n  if(!data)\n    return CURLE_OK;\n\n  Curl_expire_clear(data); /* shut off timers */\n\n  m = data->multi;\n  if(m)\n    /* This handle is still part of a multi handle, take care of this first\n       and detach this handle from there. */\n    curl_multi_remove_handle(data->multi, data);\n\n  if(data->multi_easy)\n    /* when curl_easy_perform() is used, it creates its own multi handle to\n       use and this is the one */\n    curl_multi_cleanup(data->multi_easy);\n\n  /* Destroy the timeout list that is held in the easy handle. It is\n     /normally/ done by curl_multi_remove_handle() but this is \"just in\n     case\" */\n  Curl_llist_destroy(&data->state.timeoutlist, NULL);\n\n  data->magic = 0; /* force a clear AFTER the possibly enforced removal from\n                      the multi handle, since that function uses the magic\n                      field! */\n\n  if(data->state.rangestringalloc)\n    free(data->state.range);\n\n  /* freed here just in case DONE wasn't called */\n  Curl_free_request_state(data);\n\n  /* Close down all open SSL info and sessions */\n  Curl_ssl_close_all(data);\n  Curl_safefree(data->state.first_host);\n  Curl_safefree(data->state.scratch);\n  Curl_ssl_free_certinfo(data);\n\n  /* Cleanup possible redirect junk */\n  free(data->req.newurl);\n  data->req.newurl = NULL;\n\n  if(data->change.referer_alloc) {\n    Curl_safefree(data->change.referer);\n    data->change.referer_alloc = FALSE;\n  }\n  data->change.referer = NULL;\n\n  Curl_up_free(data);\n  Curl_safefree(data->state.buffer);\n  Curl_safefree(data->state.headerbuff);\n  Curl_safefree(data->state.ulbuf);\n  Curl_flush_cookies(data, 1);\n  Curl_digest_cleanup(data);\n  Curl_safefree(data->info.contenttype);\n  Curl_safefree(data->info.wouldredirect);\n\n  /* this destroys the channel and we cannot use it anymore after this */\n  Curl_resolver_cleanup(data->state.resolver);\n\n  Curl_http2_cleanup_dependencies(data);\n  Curl_convert_close(data);\n\n  /* No longer a dirty share, if it exists */\n  if(data->share) {\n    Curl_share_lock(data, CURL_LOCK_DATA_SHARE, CURL_LOCK_ACCESS_SINGLE);\n    data->share->dirty--;\n    Curl_share_unlock(data, CURL_LOCK_DATA_SHARE);\n  }\n\n  /* destruct wildcard structures if it is needed */\n  Curl_wildcard_dtor(&data->wildcard);\n  Curl_freeset(data);\n  free(data);\n  return CURLE_OK;\n}", "func_src_after": "CURLcode Curl_close(struct Curl_easy *data)\n{\n  struct Curl_multi *m;\n\n  if(!data)\n    return CURLE_OK;\n\n  Curl_expire_clear(data); /* shut off timers */\n\n  m = data->multi;\n  if(m)\n    /* This handle is still part of a multi handle, take care of this first\n       and detach this handle from there. */\n    curl_multi_remove_handle(data->multi, data);\n\n  if(data->multi_easy) {\n    /* when curl_easy_perform() is used, it creates its own multi handle to\n       use and this is the one */\n    curl_multi_cleanup(data->multi_easy);\n    data->multi_easy = NULL;\n  }\n\n  /* Destroy the timeout list that is held in the easy handle. It is\n     /normally/ done by curl_multi_remove_handle() but this is \"just in\n     case\" */\n  Curl_llist_destroy(&data->state.timeoutlist, NULL);\n\n  data->magic = 0; /* force a clear AFTER the possibly enforced removal from\n                      the multi handle, since that function uses the magic\n                      field! */\n\n  if(data->state.rangestringalloc)\n    free(data->state.range);\n\n  /* freed here just in case DONE wasn't called */\n  Curl_free_request_state(data);\n\n  /* Close down all open SSL info and sessions */\n  Curl_ssl_close_all(data);\n  Curl_safefree(data->state.first_host);\n  Curl_safefree(data->state.scratch);\n  Curl_ssl_free_certinfo(data);\n\n  /* Cleanup possible redirect junk */\n  free(data->req.newurl);\n  data->req.newurl = NULL;\n\n  if(data->change.referer_alloc) {\n    Curl_safefree(data->change.referer);\n    data->change.referer_alloc = FALSE;\n  }\n  data->change.referer = NULL;\n\n  Curl_up_free(data);\n  Curl_safefree(data->state.buffer);\n  Curl_safefree(data->state.headerbuff);\n  Curl_safefree(data->state.ulbuf);\n  Curl_flush_cookies(data, 1);\n  Curl_digest_cleanup(data);\n  Curl_safefree(data->info.contenttype);\n  Curl_safefree(data->info.wouldredirect);\n\n  /* this destroys the channel and we cannot use it anymore after this */\n  Curl_resolver_cleanup(data->state.resolver);\n\n  Curl_http2_cleanup_dependencies(data);\n  Curl_convert_close(data);\n\n  /* No longer a dirty share, if it exists */\n  if(data->share) {\n    Curl_share_lock(data, CURL_LOCK_DATA_SHARE, CURL_LOCK_ACCESS_SINGLE);\n    data->share->dirty--;\n    Curl_share_unlock(data, CURL_LOCK_DATA_SHARE);\n  }\n\n  /* destruct wildcard structures if it is needed */\n  Curl_wildcard_dtor(&data->wildcard);\n  Curl_freeset(data);\n  free(data);\n  return CURLE_OK;\n}", "commit_link": "github.com/curl/curl/commit/81d135d67155c5295b1033679c606165d4e28f3f", "file_name": "lib/url.c", "vul_type": "cwe-416", "description": "Write a function in C to clean up and close a libcurl easy handle."}
{"func_name": "security_fips_decrypt", "func_src_before": "BOOL security_fips_decrypt(BYTE* data, size_t length, rdpRdp* rdp)\n{\n\tsize_t olen;\n\n\tif (!winpr_Cipher_Update(rdp->fips_decrypt, data, length, data, &olen))\n\t\treturn FALSE;\n\n\treturn TRUE;\n}", "func_src_after": "BOOL security_fips_decrypt(BYTE* data, size_t length, rdpRdp* rdp)\n{\n\tsize_t olen;\n\n\tif (!rdp || !rdp->fips_decrypt)\n\t\treturn FALSE;\n\n\tif (!winpr_Cipher_Update(rdp->fips_decrypt, data, length, data, &olen))\n\t\treturn FALSE;\n\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/d6cd14059b257318f176c0ba3ee0a348826a9ef8", "file_name": "libfreerdp/core/security.c", "vul_type": "cwe-125", "description": "Write a C function named `security_fips_decrypt` that decrypts data using FIPS-compliant decryption, returning a boolean status."}
{"func_name": "ReadRLEImage", "func_src_before": "static Image *ReadRLEImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n#define SkipLinesOp  0x01\n#define SetColorOp  0x02\n#define SkipPixelsOp  0x03\n#define ByteDataOp  0x05\n#define RunDataOp  0x06\n#define EOFOp  0x07\n\n  char\n    magick[12];\n\n  Image\n    *image;\n\n  int\n    opcode,\n    operand,\n    status;\n\n  MagickStatusType\n    flags;\n\n  MagickSizeType\n    number_pixels;\n\n  MemoryInfo\n    *pixel_info;\n\n  Quantum\n    index;\n\n  register ssize_t\n    x;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    i;\n\n  register unsigned char\n    *p;\n\n  size_t\n    bits_per_pixel,\n    map_length,\n    number_colormaps,\n    number_planes,\n    one,\n    offset,\n    pixel_info_length;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    background_color[256],\n    *colormap,\n    pixel,\n    plane,\n    *pixels;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  /*\n    Determine if this a RLE file.\n  */\n  count=ReadBlob(image,2,(unsigned char *) magick);\n  if ((count != 2) || (memcmp(magick,\"\\122\\314\",2) != 0))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  do\n  {\n    /*\n      Read image header.\n    */\n    image->page.x=ReadBlobLSBShort(image);\n    image->page.y=ReadBlobLSBShort(image);\n    image->columns=ReadBlobLSBShort(image);\n    image->rows=ReadBlobLSBShort(image);\n    flags=(MagickStatusType) ReadBlobByte(image);\n    image->alpha_trait=flags & 0x04 ? BlendPixelTrait : UndefinedPixelTrait;\n    number_planes=(size_t) ReadBlobByte(image);\n    bits_per_pixel=(size_t) ReadBlobByte(image);\n    number_colormaps=(size_t) ReadBlobByte(image);\n    map_length=(unsigned char) ReadBlobByte(image);\n    if (map_length >= 64)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    one=1;\n    map_length=one << map_length;\n    if ((number_planes == 0) || (number_planes == 2) || (bits_per_pixel != 8) ||\n        (image->columns == 0))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if (flags & 0x02)\n      {\n        /*\n          No background color-- initialize to black.\n        */\n        for (i=0; i < (ssize_t) number_planes; i++)\n          background_color[i]=0;\n        (void) ReadBlobByte(image);\n      }\n    else\n      {\n        /*\n          Initialize background color.\n        */\n        p=background_color;\n        for (i=0; i < (ssize_t) number_planes; i++)\n          *p++=(unsigned char) ReadBlobByte(image);\n      }\n    if ((number_planes & 0x01) == 0)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    colormap=(unsigned char *) NULL;\n    if (number_colormaps != 0)\n      {\n        /*\n          Read image colormaps.\n        */\n        colormap=(unsigned char *) AcquireQuantumMemory(number_colormaps,\n          3*map_length*sizeof(*colormap));\n        if (colormap == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        p=colormap;\n        for (i=0; i < (ssize_t) number_colormaps; i++)\n          for (x=0; x < (ssize_t) map_length; x++)\n            *p++=(unsigned char) ScaleShortToQuantum(ReadBlobLSBShort(image));\n      }\n    if ((flags & 0x08) != 0)\n      {\n        char\n          *comment;\n\n        size_t\n          length;\n\n        /*\n          Read image comment.\n        */\n        length=ReadBlobLSBShort(image);\n        if (length != 0)\n          {\n            comment=(char *) AcquireQuantumMemory(length,sizeof(*comment));\n            if (comment == (char *) NULL)\n              ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n            count=ReadBlob(image,length-1,(unsigned char *) comment);\n            comment[length-1]='\\0';\n            (void) SetImageProperty(image,\"comment\",comment,exception);\n            comment=DestroyString(comment);\n            if ((length & 0x01) == 0)\n              (void) ReadBlobByte(image);\n          }\n      }\n    if ((image_info->ping != MagickFalse) && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows,exception);\n    if (status == MagickFalse)\n      return(DestroyImageList(image));\n    /*\n      Allocate RLE pixels.\n    */\n    if (image->alpha_trait != UndefinedPixelTrait)\n      number_planes++;\n    number_pixels=(MagickSizeType) image->columns*image->rows;\n    if ((number_pixels*number_planes) != (size_t) (number_pixels*number_planes))\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixel_info_length=image->columns*image->rows*MagickMax(number_planes,4);\n    pixel_info=AcquireVirtualMemory(pixel_info_length,sizeof(*pixels));\n    if (pixel_info == (MemoryInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n    if ((flags & 0x01) && !(flags & 0x02))\n      {\n        ssize_t\n          j;\n\n        /*\n          Set background color.\n        */\n        p=pixels;\n        for (i=0; i < (ssize_t) number_pixels; i++)\n        {\n          if (image->alpha_trait == UndefinedPixelTrait)\n            for (j=0; j < (ssize_t) number_planes; j++)\n              *p++=background_color[j];\n          else\n            {\n              for (j=0; j < (ssize_t) (number_planes-1); j++)\n                *p++=background_color[j];\n              *p++=0;  /* initialize matte channel */\n            }\n        }\n      }\n    /*\n      Read runlength-encoded image.\n    */\n    plane=0;\n    x=0;\n    y=0;\n    opcode=ReadBlobByte(image);\n    do\n    {\n      switch (opcode & 0x3f)\n      {\n        case SkipLinesOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          x=0;\n          y+=operand;\n          break;\n        }\n        case SetColorOp:\n        {\n          operand=ReadBlobByte(image);\n          plane=(unsigned char) operand;\n          if (plane == 255)\n            plane=(unsigned char) (number_planes-1);\n          x=0;\n          break;\n        }\n        case SkipPixelsOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          x+=operand;\n          break;\n        }\n        case ByteDataOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          offset=((image->rows-y-1)*image->columns*number_planes)+x*\n            number_planes+plane;\n          operand++;\n          if (offset+((size_t) operand*number_planes) > pixel_info_length)\n            {\n              if (number_colormaps != 0)\n                colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n              pixel_info=RelinquishVirtualMemory(pixel_info);\n              ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n            }\n          p=pixels+offset;\n          for (i=0; i < (ssize_t) operand; i++)\n          {\n            pixel=(unsigned char) ReadBlobByte(image);\n            if ((y < (ssize_t) image->rows) &&\n                ((x+i) < (ssize_t) image->columns))\n              *p=pixel;\n            p+=number_planes;\n          }\n          if (operand & 0x01)\n            (void) ReadBlobByte(image);\n          x+=operand;\n          break;\n        }\n        case RunDataOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          pixel=(unsigned char) ReadBlobByte(image);\n          (void) ReadBlobByte(image);\n          offset=((image->rows-y-1)*image->columns*number_planes)+x*\n            number_planes+plane;\n          operand++;\n          if (offset+((size_t) operand*number_planes) > pixel_info_length)\n            {\n              if (number_colormaps != 0)\n                colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n              pixel_info=RelinquishVirtualMemory(pixel_info);\n              ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n            }\n          p=pixels+offset;\n          for (i=0; i < (ssize_t) operand; i++)\n          {\n            if ((y < (ssize_t) image->rows) &&\n                ((x+i) < (ssize_t) image->columns))\n              *p=pixel;\n            p+=number_planes;\n          }\n          x+=operand;\n          break;\n        }\n        default:\n          break;\n      }\n      opcode=ReadBlobByte(image);\n    } while (((opcode & 0x3f) != EOFOp) && (opcode != EOF));\n    if (number_colormaps != 0)\n      {\n        MagickStatusType\n          mask;\n\n        /*\n          Apply colormap affineation to image.\n        */\n        mask=(MagickStatusType) (map_length-1);\n        p=pixels;\n        x=(ssize_t) number_planes;\n        if (number_colormaps == 1)\n          for (i=0; i < (ssize_t) number_pixels; i++)\n          {\n            if (IsValidColormapIndex(image,*p & mask,&index,exception) ==\n                MagickFalse)\n              break;\n            *p=colormap[(ssize_t) index];\n            p++;\n          }\n        else\n          if ((number_planes >= 3) && (number_colormaps >= 3))\n            for (i=0; i < (ssize_t) number_pixels; i++)\n              for (x=0; x < (ssize_t) number_planes; x++)\n              {\n                if (IsValidColormapIndex(image,(size_t) (x*map_length+\n                    (*p & mask)),&index,exception) == MagickFalse)\n                  break;\n                *p=colormap[(ssize_t) index];\n                p++;\n              }\n        if ((i < (ssize_t) number_pixels) || (x < (ssize_t) number_planes))\n          {\n            colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n            pixel_info=RelinquishVirtualMemory(pixel_info);\n            ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n          }\n      }\n    /*\n      Initialize image structure.\n    */\n    if (number_planes >= 3)\n      {\n        /*\n          Convert raster image to DirectClass pixel packets.\n        */\n        p=pixels;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (Quantum *) NULL)\n            break;\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n            SetPixelRed(image,ScaleCharToQuantum(*p++),q);\n            SetPixelGreen(image,ScaleCharToQuantum(*p++),q);\n            SetPixelBlue(image,ScaleCharToQuantum(*p++),q);\n            if (image->alpha_trait != UndefinedPixelTrait)\n              SetPixelAlpha(image,ScaleCharToQuantum(*p++),q);\n            q+=GetPixelChannels(image);\n          }\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      }\n    else\n      {\n        /*\n          Create colormap.\n        */\n        if (number_colormaps == 0)\n          map_length=256;\n        if (AcquireImageColormap(image,map_length,exception) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        p=colormap;\n        if (number_colormaps == 1)\n          for (i=0; i < (ssize_t) image->colors; i++)\n          {\n            /*\n              Pseudocolor.\n            */\n            image->colormap[i].red=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n            image->colormap[i].green=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n            image->colormap[i].blue=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n          }\n        else\n          if (number_colormaps > 1)\n            for (i=0; i < (ssize_t) image->colors; i++)\n            {\n              image->colormap[i].red=(MagickRealType)\n                ScaleCharToQuantum(*p);\n              image->colormap[i].green=(MagickRealType)\n                ScaleCharToQuantum(*(p+map_length));\n              image->colormap[i].blue=(MagickRealType)\n                ScaleCharToQuantum(*(p+map_length*2));\n              p++;\n            }\n        p=pixels;\n        if (image->alpha_trait == UndefinedPixelTrait)\n          {\n            /*\n              Convert raster image to PseudoClass pixel packets.\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                SetPixelIndex(image,*p++,q);\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            (void) SyncImage(image,exception);\n          }\n        else\n          {\n            /*\n              Image has a matte channel-- promote to DirectClass.\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelRed(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].red),q);\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelGreen(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].green),q);\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelBlue(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].blue),q);\n                SetPixelAlpha(image,ScaleCharToQuantum(*p++),q);\n                q+=GetPixelChannels(image);\n              }\n              if (x < (ssize_t) image->columns)\n                break;\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            image->colormap=(PixelInfo *) RelinquishMagickMemory(\n              image->colormap);\n            image->storage_class=DirectClass;\n            image->colors=0;\n          }\n      }\n    if (number_colormaps != 0)\n      colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n    pixel_info=RelinquishVirtualMemory(pixel_info);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    (void) ReadBlobByte(image);\n    count=ReadBlob(image,2,(unsigned char *) magick);\n    if ((count != 0) && (memcmp(magick,\"\\122\\314\",2) == 0))\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image,exception);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while ((count != 0) && (memcmp(magick,\"\\122\\314\",2) == 0));\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadRLEImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n#define SkipLinesOp  0x01\n#define SetColorOp  0x02\n#define SkipPixelsOp  0x03\n#define ByteDataOp  0x05\n#define RunDataOp  0x06\n#define EOFOp  0x07\n\n  char\n    magick[12];\n\n  Image\n    *image;\n\n  int\n    opcode,\n    operand,\n    status;\n\n  MagickStatusType\n    flags;\n\n  MagickSizeType\n    number_pixels;\n\n  MemoryInfo\n    *pixel_info;\n\n  Quantum\n    index;\n\n  register ssize_t\n    x;\n\n  register Quantum\n    *q;\n\n  register ssize_t\n    i;\n\n  register unsigned char\n    *p;\n\n  size_t\n    bits_per_pixel,\n    map_length,\n    number_colormaps,\n    number_planes,\n    number_planes_filled,\n    one,\n    offset,\n    pixel_info_length;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    background_color[256],\n    *colormap,\n    pixel,\n    plane,\n    *pixels;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  /*\n    Determine if this a RLE file.\n  */\n  count=ReadBlob(image,2,(unsigned char *) magick);\n  if ((count != 2) || (memcmp(magick,\"\\122\\314\",2) != 0))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  do\n  {\n    /*\n      Read image header.\n    */\n    image->page.x=ReadBlobLSBShort(image);\n    image->page.y=ReadBlobLSBShort(image);\n    image->columns=ReadBlobLSBShort(image);\n    image->rows=ReadBlobLSBShort(image);\n    flags=(MagickStatusType) ReadBlobByte(image);\n    image->alpha_trait=flags & 0x04 ? BlendPixelTrait : UndefinedPixelTrait;\n    number_planes=(size_t) ReadBlobByte(image);\n    bits_per_pixel=(size_t) ReadBlobByte(image);\n    number_colormaps=(size_t) ReadBlobByte(image);\n    map_length=(unsigned char) ReadBlobByte(image);\n    if (map_length >= 64)\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    one=1;\n    map_length=one << map_length;\n    if ((number_planes == 0) || (number_planes == 2) || (bits_per_pixel != 8) ||\n        (image->columns == 0))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if (flags & 0x02)\n      {\n        /*\n          No background color-- initialize to black.\n        */\n        for (i=0; i < (ssize_t) number_planes; i++)\n          background_color[i]=0;\n        (void) ReadBlobByte(image);\n      }\n    else\n      {\n        /*\n          Initialize background color.\n        */\n        p=background_color;\n        for (i=0; i < (ssize_t) number_planes; i++)\n          *p++=(unsigned char) ReadBlobByte(image);\n      }\n    if ((number_planes & 0x01) == 0)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    colormap=(unsigned char *) NULL;\n    if (number_colormaps != 0)\n      {\n        /*\n          Read image colormaps.\n        */\n        colormap=(unsigned char *) AcquireQuantumMemory(number_colormaps,\n          3*map_length*sizeof(*colormap));\n        if (colormap == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        p=colormap;\n        for (i=0; i < (ssize_t) number_colormaps; i++)\n          for (x=0; x < (ssize_t) map_length; x++)\n            *p++=(unsigned char) ScaleShortToQuantum(ReadBlobLSBShort(image));\n      }\n    if ((flags & 0x08) != 0)\n      {\n        char\n          *comment;\n\n        size_t\n          length;\n\n        /*\n          Read image comment.\n        */\n        length=ReadBlobLSBShort(image);\n        if (length != 0)\n          {\n            comment=(char *) AcquireQuantumMemory(length,sizeof(*comment));\n            if (comment == (char *) NULL)\n              ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n            count=ReadBlob(image,length-1,(unsigned char *) comment);\n            comment[length-1]='\\0';\n            (void) SetImageProperty(image,\"comment\",comment,exception);\n            comment=DestroyString(comment);\n            if ((length & 0x01) == 0)\n              (void) ReadBlobByte(image);\n          }\n      }\n    if ((image_info->ping != MagickFalse) && (image_info->number_scenes != 0))\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    status=SetImageExtent(image,image->columns,image->rows,exception);\n    if (status == MagickFalse)\n      return(DestroyImageList(image));\n    /*\n      Allocate RLE pixels.\n    */\n    if (image->alpha_trait != UndefinedPixelTrait)\n      number_planes++;\n    number_pixels=(MagickSizeType) image->columns*image->rows;\n    number_planes_filled=(number_planes % 2 == 0) ? number_planes :\n      number_planes+1;\n    if ((number_pixels*number_planes_filled) != (size_t) (number_pixels*\n         number_planes_filled))\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixel_info_length=image->columns*image->rows*number_planes_filled;\n    pixel_info=AcquireVirtualMemory(pixel_info_length,sizeof(*pixels));\n    if (pixel_info == (MemoryInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n    if ((flags & 0x01) && !(flags & 0x02))\n      {\n        ssize_t\n          j;\n\n        /*\n          Set background color.\n        */\n        p=pixels;\n        for (i=0; i < (ssize_t) number_pixels; i++)\n        {\n          if (image->alpha_trait == UndefinedPixelTrait)\n            for (j=0; j < (ssize_t) number_planes; j++)\n              *p++=background_color[j];\n          else\n            {\n              for (j=0; j < (ssize_t) (number_planes-1); j++)\n                *p++=background_color[j];\n              *p++=0;  /* initialize matte channel */\n            }\n        }\n      }\n    /*\n      Read runlength-encoded image.\n    */\n    plane=0;\n    x=0;\n    y=0;\n    opcode=ReadBlobByte(image);\n    do\n    {\n      switch (opcode & 0x3f)\n      {\n        case SkipLinesOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          x=0;\n          y+=operand;\n          break;\n        }\n        case SetColorOp:\n        {\n          operand=ReadBlobByte(image);\n          plane=(unsigned char) operand;\n          if (plane == 255)\n            plane=(unsigned char) (number_planes-1);\n          x=0;\n          break;\n        }\n        case SkipPixelsOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          x+=operand;\n          break;\n        }\n        case ByteDataOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          offset=((image->rows-y-1)*image->columns*number_planes)+x*\n            number_planes+plane;\n          operand++;\n          if (offset+((size_t) operand*number_planes) > pixel_info_length)\n            {\n              if (number_colormaps != 0)\n                colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n              pixel_info=RelinquishVirtualMemory(pixel_info);\n              ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n            }\n          p=pixels+offset;\n          for (i=0; i < (ssize_t) operand; i++)\n          {\n            pixel=(unsigned char) ReadBlobByte(image);\n            if ((y < (ssize_t) image->rows) &&\n                ((x+i) < (ssize_t) image->columns))\n              *p=pixel;\n            p+=number_planes;\n          }\n          if (operand & 0x01)\n            (void) ReadBlobByte(image);\n          x+=operand;\n          break;\n        }\n        case RunDataOp:\n        {\n          operand=ReadBlobByte(image);\n          if (opcode & 0x40)\n            operand=(int) ReadBlobLSBShort(image);\n          pixel=(unsigned char) ReadBlobByte(image);\n          (void) ReadBlobByte(image);\n          offset=((image->rows-y-1)*image->columns*number_planes)+x*\n            number_planes+plane;\n          operand++;\n          if (offset+((size_t) operand*number_planes) > pixel_info_length)\n            {\n              if (number_colormaps != 0)\n                colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n              pixel_info=RelinquishVirtualMemory(pixel_info);\n              ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n            }\n          p=pixels+offset;\n          for (i=0; i < (ssize_t) operand; i++)\n          {\n            if ((y < (ssize_t) image->rows) &&\n                ((x+i) < (ssize_t) image->columns))\n              *p=pixel;\n            p+=number_planes;\n          }\n          x+=operand;\n          break;\n        }\n        default:\n          break;\n      }\n      opcode=ReadBlobByte(image);\n    } while (((opcode & 0x3f) != EOFOp) && (opcode != EOF));\n    if (number_colormaps != 0)\n      {\n        MagickStatusType\n          mask;\n\n        /*\n          Apply colormap affineation to image.\n        */\n        mask=(MagickStatusType) (map_length-1);\n        p=pixels;\n        x=(ssize_t) number_planes;\n        if (number_colormaps == 1)\n          for (i=0; i < (ssize_t) number_pixels; i++)\n          {\n            if (IsValidColormapIndex(image,*p & mask,&index,exception) ==\n                MagickFalse)\n              break;\n            *p=colormap[(ssize_t) index];\n            p++;\n          }\n        else\n          if ((number_planes >= 3) && (number_colormaps >= 3))\n            for (i=0; i < (ssize_t) number_pixels; i++)\n              for (x=0; x < (ssize_t) number_planes; x++)\n              {\n                if (IsValidColormapIndex(image,(size_t) (x*map_length+\n                    (*p & mask)),&index,exception) == MagickFalse)\n                  break;\n                *p=colormap[(ssize_t) index];\n                p++;\n              }\n        if ((i < (ssize_t) number_pixels) || (x < (ssize_t) number_planes))\n          {\n            colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n            pixel_info=RelinquishVirtualMemory(pixel_info);\n            ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n          }\n      }\n    /*\n      Initialize image structure.\n    */\n    if (number_planes >= 3)\n      {\n        /*\n          Convert raster image to DirectClass pixel packets.\n        */\n        p=pixels;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n          if (q == (Quantum *) NULL)\n            break;\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n            SetPixelRed(image,ScaleCharToQuantum(*p++),q);\n            SetPixelGreen(image,ScaleCharToQuantum(*p++),q);\n            SetPixelBlue(image,ScaleCharToQuantum(*p++),q);\n            if (image->alpha_trait != UndefinedPixelTrait)\n              SetPixelAlpha(image,ScaleCharToQuantum(*p++),q);\n            q+=GetPixelChannels(image);\n          }\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n      }\n    else\n      {\n        /*\n          Create colormap.\n        */\n        if (number_colormaps == 0)\n          map_length=256;\n        if (AcquireImageColormap(image,map_length,exception) == MagickFalse)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n        p=colormap;\n        if (number_colormaps == 1)\n          for (i=0; i < (ssize_t) image->colors; i++)\n          {\n            /*\n              Pseudocolor.\n            */\n            image->colormap[i].red=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n            image->colormap[i].green=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n            image->colormap[i].blue=(MagickRealType)\n              ScaleCharToQuantum((unsigned char) i);\n          }\n        else\n          if (number_colormaps > 1)\n            for (i=0; i < (ssize_t) image->colors; i++)\n            {\n              image->colormap[i].red=(MagickRealType)\n                ScaleCharToQuantum(*p);\n              image->colormap[i].green=(MagickRealType)\n                ScaleCharToQuantum(*(p+map_length));\n              image->colormap[i].blue=(MagickRealType)\n                ScaleCharToQuantum(*(p+map_length*2));\n              p++;\n            }\n        p=pixels;\n        if (image->alpha_trait == UndefinedPixelTrait)\n          {\n            /*\n              Convert raster image to PseudoClass pixel packets.\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                SetPixelIndex(image,*p++,q);\n                q+=GetPixelChannels(image);\n              }\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            (void) SyncImage(image,exception);\n          }\n        else\n          {\n            /*\n              Image has a matte channel-- promote to DirectClass.\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n              if (q == (Quantum *) NULL)\n                break;\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelRed(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].red),q);\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelGreen(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].green),q);\n                if (IsValidColormapIndex(image,(ssize_t) *p++,&index,\n                    exception) == MagickFalse)\n                  break;\n                SetPixelBlue(image,ClampToQuantum(image->colormap[(ssize_t)\n                  index].blue),q);\n                SetPixelAlpha(image,ScaleCharToQuantum(*p++),q);\n                q+=GetPixelChannels(image);\n              }\n              if (x < (ssize_t) image->columns)\n                break;\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,LoadImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            image->colormap=(PixelInfo *) RelinquishMagickMemory(\n              image->colormap);\n            image->storage_class=DirectClass;\n            image->colors=0;\n          }\n      }\n    if (number_colormaps != 0)\n      colormap=(unsigned char *) RelinquishMagickMemory(colormap);\n    pixel_info=RelinquishVirtualMemory(pixel_info);\n    if (EOFBlob(image) != MagickFalse)\n      {\n        ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n          image->filename);\n        break;\n      }\n    /*\n      Proceed to next image.\n    */\n    if (image_info->number_scenes != 0)\n      if (image->scene >= (image_info->scene+image_info->number_scenes-1))\n        break;\n    (void) ReadBlobByte(image);\n    count=ReadBlob(image,2,(unsigned char *) magick);\n    if ((count != 0) && (memcmp(magick,\"\\122\\314\",2) == 0))\n      {\n        /*\n          Allocate next image structure.\n        */\n        AcquireNextImage(image_info,image,exception);\n        if (GetNextImageInList(image) == (Image *) NULL)\n          {\n            image=DestroyImageList(image);\n            return((Image *) NULL);\n          }\n        image=SyncNextImageInList(image);\n        status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n          GetBlobSize(image));\n        if (status == MagickFalse)\n          break;\n      }\n  } while ((count != 0) && (memcmp(magick,\"\\122\\314\",2) == 0));\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/2ad6d33493750a28a5a655d319a8e0b16c392de1", "file_name": "coders/rle.c", "vul_type": "cwe-125", "description": "Write a C function to read and process RLE (run-length encoded) image data."}
{"func_name": "dd_save_binary", "func_src_before": "void dd_save_binary(struct dump_dir* dd, const char* name, const char* data, unsigned size)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, size, dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "func_src_after": "void dd_save_binary(struct dump_dir* dd, const char* name, const char* data, unsigned size)\n{\n    if (!dd->locked)\n        error_msg_and_die(\"dump_dir is not opened\"); /* bug */\n\n    if (!str_is_correct_filename(name))\n        error_msg_and_die(\"Cannot save binary. '%s' is not a valid file name\", name);\n\n    char *full_path = concat_path_file(dd->dd_dirname, name);\n    save_binary_file(full_path, data, size, dd->dd_uid, dd->dd_gid, dd->mode);\n    free(full_path);\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "Write a C function named `dd_save_binary` that saves binary data to a file within a directory structure, ensuring the directory is open and the filename is valid."}
{"func_name": "closeGame", "func_src_before": "def closeGame(ID):\n\tdb.execute(\"UPDATE games set Running = 'No' WHERE ID = %i\" % ID)\n\tdatabase.commit()", "func_src_after": "def closeGame(ID):\n\tdb.execute(\"UPDATE games set Running = 'No' WHERE ID = ?\", ID)\n\tdatabase.commit()", "commit_link": "github.com/iScrE4m/XLeague/commit/59cab6e5fd8bd5e47f2418a7c71cb1d4e3cad0d2", "file_name": "plugins/database.py", "vul_type": "cwe-089", "description": "Write a Python function to update the 'Running' status of a game to 'No' in a database using the game's ID."}
{"func_name": "ndpi_reset_packet_line_info", "func_src_before": "static void ndpi_reset_packet_line_info(struct ndpi_packet_struct *packet) {\n  packet->parsed_lines = 0, packet->empty_line_position_set = 0, packet->host_line.ptr = NULL,\n    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "func_src_after": "static void ndpi_reset_packet_line_info(struct ndpi_packet_struct *packet) {\n  packet->parsed_lines = 0, packet->empty_line_position_set = 0, packet->host_line.ptr = NULL,\n    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->content_disposition_line.ptr = NULL,\n    packet->content_disposition_line.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "commit_link": "github.com/ntop/nDPI/commit/6a9f5e4f7c3fd5ddab3e6727b071904d76773952", "file_name": "src/lib/ndpi_main.c", "vul_type": "cwe-416", "description": "In C, write a function to reset all fields of a given `ndpi_packet_struct` to their initial state."}
{"func_name": "handle_method_call", "func_src_before": "static void handle_method_call(GDBusConnection *connection,\n                        const gchar *caller,\n                        const gchar *object_path,\n                        const gchar *interface_name,\n                        const gchar *method_name,\n                        GVariant    *parameters,\n                        GDBusMethodInvocation *invocation,\n                        gpointer    user_data)\n{\n    reset_timeout();\n\n    uid_t caller_uid;\n    GVariant *response;\n\n    caller_uid = get_caller_uid(connection, invocation, caller);\n\n    log_notice(\"caller_uid:%ld method:'%s'\", (long)caller_uid, method_name);\n\n    if (caller_uid == (uid_t) -1)\n        return;\n\n    if (g_strcmp0(method_name, \"NewProblem\") == 0)\n    {\n        char *error = NULL;\n        char *problem_id = handle_new_problem(g_variant_get_child_value(parameters, 0), caller_uid, &error);\n        if (!problem_id)\n        {\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            free(error);\n            return;\n        }\n        /* else */\n        response = g_variant_new(\"(s)\", problem_id);\n        g_dbus_method_invocation_return_value(invocation, response);\n        free(problem_id);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetProblems\") == 0)\n    {\n        GList *dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        //I was told that g_dbus_method frees the response\n        //g_variant_unref(response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetAllProblems\") == 0)\n    {\n        /*\n        - so, we have UID,\n        - if it's 0, then we don't have to check anything and just return all directories\n        - if uid != 0 then we want to ask for authorization\n        */\n        if (caller_uid != 0)\n        {\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n                caller_uid = 0;\n        }\n\n        GList * dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetForeignProblems\") == 0)\n    {\n        GList * dirs = get_problem_dirs_not_accessible_by_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"ChownProblemDir\") == 0)\n    {\n        const gchar *problem_dir;\n        g_variant_get(parameters, \"(&s)\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int ddstat = fdump_dir_stat_for_uid(dir_fd, caller_uid);\n        if (ddstat < 0)\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"requested directory does not exist '%s'\", problem_dir);\n            }\n            else\n            {\n                perror_msg(\"can't get stat of '%s'\", problem_dir);\n            }\n\n            return_InvalidProblemDir_error(invocation, problem_dir);\n\n            close(dir_fd);\n            return;\n        }\n\n        if (ddstat & DD_STAT_OWNED_BY_UID)\n        {   //caller seems to be in group with access to this dir, so no action needed\n            log_notice(\"caller has access to the requested directory %s\", problem_dir);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n            close(dir_fd);\n            return;\n        }\n\n        if ((ddstat & DD_STAT_ACCESSIBLE_BY_UID) == 0 &&\n                polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n        {\n            log_notice(\"not authorized\");\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.AuthFailure\",\n                                              _(\"Not Authorized\"));\n            close(dir_fd);\n            return;\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int chown_res = dd_chown(dd, caller_uid);\n        if (chown_res != 0)\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.ChownError\",\n                                              _(\"Chowning directory failed. Check system logs for more details.\"));\n        else\n            g_dbus_method_invocation_return_value(invocation, NULL);\n\n        dd_close(dd);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetInfo\") == 0)\n    {\n        /* Parameter tuple is (sas) */\n\n\t/* Get 1st param - problem dir name */\n        const gchar *problem_dir;\n        g_variant_get_child(parameters, 0, \"&s\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"Requested directory does not exist '%s'\", problem_dir);\n                return_InvalidProblemDir_error(invocation, problem_dir);\n                close(dir_fd);\n                return;\n            }\n\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n            {\n                log_notice(\"not authorized\");\n                g_dbus_method_invocation_return_dbus_error(invocation,\n                                                  \"org.freedesktop.problems.AuthFailure\",\n                                                  _(\"Not Authorized\"));\n                close(dir_fd);\n                return;\n            }\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n\t/* Get 2nd param - vector of element names */\n        GVariant *array = g_variant_get_child_value(parameters, 1);\n        GList *elements = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        GVariantBuilder *builder = NULL;\n        for (GList *l = elements; l; l = l->next)\n        {\n            const char *element_name = (const char*)l->data;\n            char *value = dd_load_text_ext(dd, element_name, 0\n                                                | DD_LOAD_TEXT_RETURN_NULL_ON_FAILURE\n                                                | DD_FAIL_QUIETLY_ENOENT\n                                                | DD_FAIL_QUIETLY_EACCES);\n            log_notice(\"element '%s' %s\", element_name, value ? \"fetched\" : \"not found\");\n            if (value)\n            {\n                if (!builder)\n                    builder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n                /* g_variant_builder_add makes a copy. No need to xstrdup here */\n                g_variant_builder_add(builder, \"{ss}\", element_name, value);\n                free(value);\n            }\n        }\n        list_free_with_free(elements);\n        dd_close(dd);\n        /* It is OK to call g_variant_new(\"(a{ss})\", NULL) because */\n        /* G_VARIANT_TYPE_TUPLE allows NULL value */\n        GVariant *response = g_variant_new(\"(a{ss})\", builder);\n\n        if (builder)\n            g_variant_builder_unref(builder);\n\n        log_info(\"GetInfo: returning value for '%s'\", problem_dir);\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"SetElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n        const char *value;\n\n        g_variant_get(parameters, \"(&s&s&s)\", &problem_id, &element, &value);\n\n        if (element == NULL || element[0] == '\\0' || strlen(element) > 64)\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        /* Is it good idea to make it static? Is it possible to change the max size while a single run? */\n        const double max_dir_size = g_settings_nMaxCrashReportsSize * (1024 * 1024);\n        const long item_size = dd_get_item_size(dd, element);\n        if (item_size < 0)\n        {\n            log_notice(\"Can't get size of '%s/%s'\", problem_id, element);\n            char *error = xasprintf(_(\"Can't get size of '%s'\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            return;\n        }\n\n        const double requested_size = (double)strlen(value) - item_size;\n        /* Don't want to check the size limit in case of reducing of size */\n        if (requested_size > 0\n            && requested_size > (max_dir_size - get_dirsize(g_settings_dump_location)))\n        {\n            log_notice(\"No problem space left in '%s' (requested Bytes %f)\", problem_id, requested_size);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      _(\"No problem space left\"));\n        }\n        else\n        {\n            dd_save_text(dd, element, value);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n        }\n\n        dd_close(dd);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n\n        g_variant_get(parameters, \"(&s&s)\", &problem_id, &element);\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        const int res = dd_delete_item(dd, element);\n        dd_close(dd);\n\n        if (res != 0)\n        {\n            log_notice(\"Can't delete the element '%s' from the problem directory '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"Can't delete the element '%s' from the problem directory '%s'\"), element, problem_id);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                          \"org.freedesktop.problems.Failure\",\n                                          error);\n            free(error);\n            return;\n        }\n\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteProblem\") == 0)\n    {\n        /* Dbus parameters are always tuples.\n         * In this case, it's (as) - a tuple of one element (array of strings).\n         * Need to fetch the array:\n         */\n        GVariant *array = g_variant_get_child_value(parameters, 0);\n        GList *problem_dirs = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n            log_notice(\"dir_name:'%s'\", dir_name);\n            if (!allowed_problem_dir(dir_name))\n            {\n                return_InvalidProblemDir_error(invocation, dir_name);\n                goto ret;\n            }\n        }\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n\n            int dir_fd = dd_openfd(dir_name);\n            if (dir_fd < 0)\n            {\n                perror_msg(\"can't open problem directory '%s'\", dir_name);\n                return_InvalidProblemDir_error(invocation, dir_name);\n                return;\n            }\n\n            if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n            {\n                if (errno == ENOTDIR)\n                {\n                    log_notice(\"Requested directory does not exist '%s'\", dir_name);\n                    close(dir_fd);\n                    continue;\n                }\n\n                if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n                { // if user didn't provide correct credentials, just move to the next dir\n                    close(dir_fd);\n                    continue;\n                }\n            }\n\n            struct dump_dir *dd = dd_fdopendir(dir_fd, dir_name, /*flags:*/ 0);\n            if (dd)\n            {\n                if (dd_delete(dd) != 0)\n                {\n                    error_msg(\"Failed to delete problem directory '%s'\", dir_name);\n                    dd_close(dd);\n                }\n            }\n        }\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n ret:\n        list_free_with_free(problem_dirs);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"FindProblemByElementInTimeRange\") == 0)\n    {\n        const gchar *element;\n        const gchar *value;\n        glong timestamp_from;\n        glong timestamp_to;\n        gboolean all;\n\n        g_variant_get_child(parameters, 0, \"&s\", &element);\n        g_variant_get_child(parameters, 1, \"&s\", &value);\n        g_variant_get_child(parameters, 2, \"x\", &timestamp_from);\n        g_variant_get_child(parameters, 3, \"x\", &timestamp_to);\n        g_variant_get_child(parameters, 4, \"b\", &all);\n\n        if (all && polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n            caller_uid = 0;\n\n        GList *dirs = get_problem_dirs_for_element_in_time(caller_uid, element, value, timestamp_from,\n                                                        timestamp_to);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"Quit\") == 0)\n    {\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        g_main_loop_quit(loop);\n        return;\n    }\n}", "func_src_after": "static void handle_method_call(GDBusConnection *connection,\n                        const gchar *caller,\n                        const gchar *object_path,\n                        const gchar *interface_name,\n                        const gchar *method_name,\n                        GVariant    *parameters,\n                        GDBusMethodInvocation *invocation,\n                        gpointer    user_data)\n{\n    reset_timeout();\n\n    uid_t caller_uid;\n    GVariant *response;\n\n    caller_uid = get_caller_uid(connection, invocation, caller);\n\n    log_notice(\"caller_uid:%ld method:'%s'\", (long)caller_uid, method_name);\n\n    if (caller_uid == (uid_t) -1)\n        return;\n\n    if (g_strcmp0(method_name, \"NewProblem\") == 0)\n    {\n        char *error = NULL;\n        char *problem_id = handle_new_problem(g_variant_get_child_value(parameters, 0), caller_uid, &error);\n        if (!problem_id)\n        {\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            free(error);\n            return;\n        }\n        /* else */\n        response = g_variant_new(\"(s)\", problem_id);\n        g_dbus_method_invocation_return_value(invocation, response);\n        free(problem_id);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetProblems\") == 0)\n    {\n        GList *dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        //I was told that g_dbus_method frees the response\n        //g_variant_unref(response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetAllProblems\") == 0)\n    {\n        /*\n        - so, we have UID,\n        - if it's 0, then we don't have to check anything and just return all directories\n        - if uid != 0 then we want to ask for authorization\n        */\n        if (caller_uid != 0)\n        {\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n                caller_uid = 0;\n        }\n\n        GList * dirs = get_problem_dirs_for_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetForeignProblems\") == 0)\n    {\n        GList * dirs = get_problem_dirs_not_accessible_by_uid(caller_uid, g_settings_dump_location);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"ChownProblemDir\") == 0)\n    {\n        const gchar *problem_dir;\n        g_variant_get(parameters, \"(&s)\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int ddstat = fdump_dir_stat_for_uid(dir_fd, caller_uid);\n        if (ddstat < 0)\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"requested directory does not exist '%s'\", problem_dir);\n            }\n            else\n            {\n                perror_msg(\"can't get stat of '%s'\", problem_dir);\n            }\n\n            return_InvalidProblemDir_error(invocation, problem_dir);\n\n            close(dir_fd);\n            return;\n        }\n\n        if (ddstat & DD_STAT_OWNED_BY_UID)\n        {   //caller seems to be in group with access to this dir, so no action needed\n            log_notice(\"caller has access to the requested directory %s\", problem_dir);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n            close(dir_fd);\n            return;\n        }\n\n        if ((ddstat & DD_STAT_ACCESSIBLE_BY_UID) == 0 &&\n                polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n        {\n            log_notice(\"not authorized\");\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.AuthFailure\",\n                                              _(\"Not Authorized\"));\n            close(dir_fd);\n            return;\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int chown_res = dd_chown(dd, caller_uid);\n        if (chown_res != 0)\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.ChownError\",\n                                              _(\"Chowning directory failed. Check system logs for more details.\"));\n        else\n            g_dbus_method_invocation_return_value(invocation, NULL);\n\n        dd_close(dd);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"GetInfo\") == 0)\n    {\n        /* Parameter tuple is (sas) */\n\n\t/* Get 1st param - problem dir name */\n        const gchar *problem_dir;\n        g_variant_get_child(parameters, 0, \"&s\", &problem_dir);\n        log_notice(\"problem_dir:'%s'\", problem_dir);\n\n        if (!allowed_problem_dir(problem_dir))\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        int dir_fd = dd_openfd(problem_dir);\n        if (dir_fd < 0)\n        {\n            perror_msg(\"can't open problem directory '%s'\", problem_dir);\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n        if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n        {\n            if (errno == ENOTDIR)\n            {\n                log_notice(\"Requested directory does not exist '%s'\", problem_dir);\n                return_InvalidProblemDir_error(invocation, problem_dir);\n                close(dir_fd);\n                return;\n            }\n\n            if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n            {\n                log_notice(\"not authorized\");\n                g_dbus_method_invocation_return_dbus_error(invocation,\n                                                  \"org.freedesktop.problems.AuthFailure\",\n                                                  _(\"Not Authorized\"));\n                close(dir_fd);\n                return;\n            }\n        }\n\n        struct dump_dir *dd = dd_fdopendir(dir_fd, problem_dir, DD_OPEN_READONLY | DD_FAIL_QUIETLY_EACCES);\n        if (!dd)\n        {\n            return_InvalidProblemDir_error(invocation, problem_dir);\n            return;\n        }\n\n\t/* Get 2nd param - vector of element names */\n        GVariant *array = g_variant_get_child_value(parameters, 1);\n        GList *elements = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        GVariantBuilder *builder = NULL;\n        for (GList *l = elements; l; l = l->next)\n        {\n            const char *element_name = (const char*)l->data;\n            char *value = dd_load_text_ext(dd, element_name, 0\n                                                | DD_LOAD_TEXT_RETURN_NULL_ON_FAILURE\n                                                | DD_FAIL_QUIETLY_ENOENT\n                                                | DD_FAIL_QUIETLY_EACCES);\n            log_notice(\"element '%s' %s\", element_name, value ? \"fetched\" : \"not found\");\n            if (value)\n            {\n                if (!builder)\n                    builder = g_variant_builder_new(G_VARIANT_TYPE_ARRAY);\n\n                /* g_variant_builder_add makes a copy. No need to xstrdup here */\n                g_variant_builder_add(builder, \"{ss}\", element_name, value);\n                free(value);\n            }\n        }\n        list_free_with_free(elements);\n        dd_close(dd);\n        /* It is OK to call g_variant_new(\"(a{ss})\", NULL) because */\n        /* G_VARIANT_TYPE_TUPLE allows NULL value */\n        GVariant *response = g_variant_new(\"(a{ss})\", builder);\n\n        if (builder)\n            g_variant_builder_unref(builder);\n\n        log_info(\"GetInfo: returning value for '%s'\", problem_dir);\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"SetElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n        const char *value;\n\n        g_variant_get(parameters, \"(&s&s&s)\", &problem_id, &element, &value);\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        /* Is it good idea to make it static? Is it possible to change the max size while a single run? */\n        const double max_dir_size = g_settings_nMaxCrashReportsSize * (1024 * 1024);\n        const long item_size = dd_get_item_size(dd, element);\n        if (item_size < 0)\n        {\n            log_notice(\"Can't get size of '%s/%s'\", problem_id, element);\n            char *error = xasprintf(_(\"Can't get size of '%s'\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      error);\n            return;\n        }\n\n        const double requested_size = (double)strlen(value) - item_size;\n        /* Don't want to check the size limit in case of reducing of size */\n        if (requested_size > 0\n            && requested_size > (max_dir_size - get_dirsize(g_settings_dump_location)))\n        {\n            log_notice(\"No problem space left in '%s' (requested Bytes %f)\", problem_id, requested_size);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                                      \"org.freedesktop.problems.Failure\",\n                                                      _(\"No problem space left\"));\n        }\n        else\n        {\n            dd_save_text(dd, element, value);\n            g_dbus_method_invocation_return_value(invocation, NULL);\n        }\n\n        dd_close(dd);\n\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteElement\") == 0)\n    {\n        const char *problem_id;\n        const char *element;\n\n        g_variant_get(parameters, \"(&s&s)\", &problem_id, &element);\n\n        if (!str_is_correct_filename(element))\n        {\n            log_notice(\"'%s' is not a valid element name of '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"'%s' is not a valid element name\"), element);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                              \"org.freedesktop.problems.InvalidElement\",\n                                              error);\n\n            free(error);\n            return;\n        }\n\n        struct dump_dir *dd = open_directory_for_modification_of_element(\n                                    invocation, caller_uid, problem_id, element);\n        if (!dd)\n            /* Already logged from open_directory_for_modification_of_element() */\n            return;\n\n        const int res = dd_delete_item(dd, element);\n        dd_close(dd);\n\n        if (res != 0)\n        {\n            log_notice(\"Can't delete the element '%s' from the problem directory '%s'\", element, problem_id);\n            char *error = xasprintf(_(\"Can't delete the element '%s' from the problem directory '%s'\"), element, problem_id);\n            g_dbus_method_invocation_return_dbus_error(invocation,\n                                          \"org.freedesktop.problems.Failure\",\n                                          error);\n            free(error);\n            return;\n        }\n\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"DeleteProblem\") == 0)\n    {\n        /* Dbus parameters are always tuples.\n         * In this case, it's (as) - a tuple of one element (array of strings).\n         * Need to fetch the array:\n         */\n        GVariant *array = g_variant_get_child_value(parameters, 0);\n        GList *problem_dirs = string_list_from_variant(array);\n        g_variant_unref(array);\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n            log_notice(\"dir_name:'%s'\", dir_name);\n            if (!allowed_problem_dir(dir_name))\n            {\n                return_InvalidProblemDir_error(invocation, dir_name);\n                goto ret;\n            }\n        }\n\n        for (GList *l = problem_dirs; l; l = l->next)\n        {\n            const char *dir_name = (const char*)l->data;\n\n            int dir_fd = dd_openfd(dir_name);\n            if (dir_fd < 0)\n            {\n                perror_msg(\"can't open problem directory '%s'\", dir_name);\n                return_InvalidProblemDir_error(invocation, dir_name);\n                return;\n            }\n\n            if (!fdump_dir_accessible_by_uid(dir_fd, caller_uid))\n            {\n                if (errno == ENOTDIR)\n                {\n                    log_notice(\"Requested directory does not exist '%s'\", dir_name);\n                    close(dir_fd);\n                    continue;\n                }\n\n                if (polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") != PolkitYes)\n                { // if user didn't provide correct credentials, just move to the next dir\n                    close(dir_fd);\n                    continue;\n                }\n            }\n\n            struct dump_dir *dd = dd_fdopendir(dir_fd, dir_name, /*flags:*/ 0);\n            if (dd)\n            {\n                if (dd_delete(dd) != 0)\n                {\n                    error_msg(\"Failed to delete problem directory '%s'\", dir_name);\n                    dd_close(dd);\n                }\n            }\n        }\n\n        g_dbus_method_invocation_return_value(invocation, NULL);\n ret:\n        list_free_with_free(problem_dirs);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"FindProblemByElementInTimeRange\") == 0)\n    {\n        const gchar *element;\n        const gchar *value;\n        glong timestamp_from;\n        glong timestamp_to;\n        gboolean all;\n\n        g_variant_get_child(parameters, 0, \"&s\", &element);\n        g_variant_get_child(parameters, 1, \"&s\", &value);\n        g_variant_get_child(parameters, 2, \"x\", &timestamp_from);\n        g_variant_get_child(parameters, 3, \"x\", &timestamp_to);\n        g_variant_get_child(parameters, 4, \"b\", &all);\n\n        if (all && polkit_check_authorization_dname(caller, \"org.freedesktop.problems.getall\") == PolkitYes)\n            caller_uid = 0;\n\n        GList *dirs = get_problem_dirs_for_element_in_time(caller_uid, element, value, timestamp_from,\n                                                        timestamp_to);\n        response = variant_from_string_list(dirs);\n        list_free_with_free(dirs);\n\n        g_dbus_method_invocation_return_value(invocation, response);\n        return;\n    }\n\n    if (g_strcmp0(method_name, \"Quit\") == 0)\n    {\n        g_dbus_method_invocation_return_value(invocation, NULL);\n        g_main_loop_quit(loop);\n        return;\n    }\n}", "commit_link": "github.com/abrt/abrt/commit/f3c2a6af3455b2882e28570e8a04f1c2d4500d5b", "file_name": "src/dbus/abrt-dbus.c", "vul_type": "cwe-022", "description": "Write a C function to handle various method calls for problem management over D-Bus."}
{"func_name": "autodetect_recv_bandwidth_measure_results", "func_src_before": "static BOOL autodetect_recv_bandwidth_measure_results(rdpRdp* rdp, wStream* s,\n                                                      AUTODETECT_RSP_PDU* autodetectRspPdu)\n{\n\tBOOL success = TRUE;\n\n\tif (autodetectRspPdu->headerLength != 0x0E)\n\t\treturn FALSE;\n\n\tWLog_VRB(AUTODETECT_TAG, \"received Bandwidth Measure Results PDU\");\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureTimeDelta); /* timeDelta (4 bytes) */\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureByteCount); /* byteCount (4 bytes) */\n\n\tif (rdp->autodetect->bandwidthMeasureTimeDelta > 0)\n\t\trdp->autodetect->netCharBandwidth = rdp->autodetect->bandwidthMeasureByteCount * 8 /\n\t\t                                    rdp->autodetect->bandwidthMeasureTimeDelta;\n\telse\n\t\trdp->autodetect->netCharBandwidth = 0;\n\n\tIFCALLRET(rdp->autodetect->BandwidthMeasureResults, success, rdp->context,\n\t          autodetectRspPdu->sequenceNumber);\n\treturn success;\n}", "func_src_after": "static BOOL autodetect_recv_bandwidth_measure_results(rdpRdp* rdp, wStream* s,\n                                                      AUTODETECT_RSP_PDU* autodetectRspPdu)\n{\n\tBOOL success = TRUE;\n\n\tif (autodetectRspPdu->headerLength != 0x0E)\n\t\treturn FALSE;\n\n\tWLog_VRB(AUTODETECT_TAG, \"received Bandwidth Measure Results PDU\");\n\tif (Stream_GetRemainingLength(s) < 8)\n\t\treturn -1;\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureTimeDelta); /* timeDelta (4 bytes) */\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureByteCount); /* byteCount (4 bytes) */\n\n\tif (rdp->autodetect->bandwidthMeasureTimeDelta > 0)\n\t\trdp->autodetect->netCharBandwidth = rdp->autodetect->bandwidthMeasureByteCount * 8 /\n\t\t                                    rdp->autodetect->bandwidthMeasureTimeDelta;\n\telse\n\t\trdp->autodetect->netCharBandwidth = 0;\n\n\tIFCALLRET(rdp->autodetect->BandwidthMeasureResults, success, rdp->context,\n\t          autodetectRspPdu->sequenceNumber);\n\treturn success;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/f5e73cc7c9cd973b516a618da877c87b80950b65", "file_name": "libfreerdp/core/autodetect.c", "vul_type": "cwe-125", "description": "Write a C function to process bandwidth measurement results in an RDP session."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "auto_complete_for_user_name", "func_src_before": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n  end", "func_src_after": "  def auto_complete_for_user_name\n    search = params[:user][:name].to_s\n    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 73, "char_end": 189, "line": "    @users = User.find_by_sql(\"select * from users where LOWER(name) LIKE '%\" + search + \"%'\") unless search.blank?\n"}], "added": [{"line_no": 3, "char_start": 73, "char_end": 155, "line": "    @users = User.where(\"LOWER(name) LIKE ?\", \"%#{search}%\") unless search.blank?\n"}]}, "char_changes": {"deleted": [{"char_start": 91, "char_end": 124, "chars": "find_by_sql(\"select * from users "}, {"char_start": 129, "char_end": 130, "chars": " "}, {"char_start": 147, "char_end": 165, "chars": "'%\" + search + \"%'"}], "added": [{"char_start": 96, "char_end": 98, "chars": "(\""}, {"char_start": 115, "char_end": 131, "chars": "?\", \"%#{search}%"}]}, "commit_link": "github.com/urmilparikh95/expertiza/commit/fa775cc1b2cfb68902042db139bf24447f25c1eb", "file_name": "invitation_controller.rb", "vul_type": "cwe-089", "commit_msg": "Handle possible SQL injections.", "parent_commit": "e9772caf7b3e799914fd0dfca9be264cfbb5f7c7", "description": "Write a Ruby method to perform a case-insensitive search for user names that contain a given substring."}
{"func_name": "mark_context_stack", "func_src_before": "mark_context_stack(mrb_state *mrb, struct mrb_context *c)\n{\n  size_t i;\n  size_t e;\n\n  if (c->stack == NULL) return;\n  e = c->stack - c->stbase;\n  if (c->ci) e += c->ci->nregs;\n  if (c->stbase + e > c->stend) e = c->stend - c->stbase;\n  for (i=0; i<e; i++) {\n    mrb_value v = c->stbase[i];\n\n    if (!mrb_immediate_p(v)) {\n      if (mrb_basic_ptr(v)->tt == MRB_TT_FREE) {\n        c->stbase[i] = mrb_nil_value();\n      }\n      else {\n        mrb_gc_mark(mrb, mrb_basic_ptr(v));\n      }\n    }\n  }\n}", "func_src_after": "mark_context_stack(mrb_state *mrb, struct mrb_context *c)\n{\n  size_t i;\n  size_t e;\n  mrb_value nil;\n\n  if (c->stack == NULL) return;\n  e = c->stack - c->stbase;\n  if (c->ci) e += c->ci->nregs;\n  if (c->stbase + e > c->stend) e = c->stend - c->stbase;\n  for (i=0; i<e; i++) {\n    mrb_value v = c->stbase[i];\n\n    if (!mrb_immediate_p(v)) {\n      mrb_gc_mark(mrb, mrb_basic_ptr(v));\n    }\n  }\n  e = c->stend - c->stbase;\n  nil = mrb_nil_value();\n  for (; i<e; i++) {\n    c->stbase[i] = nil;\n  }\n}", "commit_link": "github.com/mruby/mruby/commit/5c114c91d4ff31859fcd84cf8bf349b737b90d99", "file_name": "src/gc.c", "vul_type": "cwe-416", "description": "Write a function in C for the MRuby engine that marks the stack context for garbage collection."}
{"func_name": "apiCallbacksStreams", "func_src_before": "func apiCallbacksStreams(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, challenge)\n\t\tfmt.Println(\"Responding to streams\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response streamsResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif len(response.Data) > 0 {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t} else {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t}\n}", "func_src_after": "func apiCallbacksStreams(w http.ResponseWriter, r *http.Request) {\n\tchallenge := r.URL.Query().Get(\"hub.challenge\")\n\tif challenge != \"\" {\n\t\tfmt.Fprint(w, html.EscapeString(challenge))\n\t\tfmt.Println(\"Responding to streams\")\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\tfmt.Printf(\"Streams response xd \\n\")\n\n\tbody, err := ioutil.ReadAll(r.Body)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tvar response streamsResponse\n\n\terr = json.Unmarshal(body, &response)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tif len(response.Data) > 0 {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t\tfmt.Printf(\"Online!\\n\")\n\t} else {\n\t\tfmt.Printf(\"%#v\\n\", response.Data)\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t\tfmt.Printf(\"Offline!\\n\")\n\t}\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 138, "char_end": 165, "line": "\t\tfmt.Fprint(w, challenge)\n"}], "added": [{"line_no": 4, "char_start": 138, "char_end": 184, "line": "\t\tfmt.Fprint(w, html.EscapeString(challenge))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 154, "char_end": 172, "chars": "html.EscapeString("}, {"char_start": 182, "char_end": 183, "chars": ")"}]}, "commit_link": "github.com/pajlada/pajbot2/commit/69ef922a07fc648760f030604e0aea86a573ea83", "file_name": "hook.go", "vul_type": "cwe-079", "commit_msg": "Fix cross-site scripting vulnerabilities (#447)", "parent_commit": "d8321a06903ec460cf6343c51ae50d12a3cb45e9", "description": "Write a Go function to handle webhooks for stream status updates, responding to a challenge parameter and printing multiple lines of \"Online!\" or \"Offline!\" based on the received data."}
{"func_name": "encode_frame", "func_src_before": "static int encode_frame(AVCodecContext *avctx, AVPacket *pkt,\n                               const AVFrame *pict, int *got_packet)\n{\n    Msvideo1EncContext * const c = avctx->priv_data;\n    const AVFrame *p = pict;\n    const uint16_t *src;\n    uint8_t *prevptr;\n    uint8_t *dst, *buf;\n    int keyframe = 0;\n    int no_skips = 1;\n    int i, j, k, x, y, ret;\n    int skips = 0;\n    int quality = 24;\n\n    if ((ret = ff_alloc_packet(avctx, pkt, avctx->width*avctx->height*9 + AV_INPUT_BUFFER_MIN_SIZE)) < 0)\n        return ret;\n    dst= buf= pkt->data;\n\n    if(!c->prev)\n        c->prev = av_malloc(avctx->width * 3 * (avctx->height + 3));\n    prevptr = c->prev + avctx->width * 3 * (FFALIGN(avctx->height, 4) - 1);\n    src = (const uint16_t*)(p->data[0] + p->linesize[0]*(FFALIGN(avctx->height, 4) - 1));\n    if(c->keyint >= avctx->keyint_min)\n        keyframe = 1;\n\n\n    for(y = 0; y < avctx->height; y += 4){\n        for(x = 0; x < avctx->width; x += 4){\n            int bestmode = MODE_SKIP;\n            int bestscore = INT_MAX;\n            int flags = 0;\n            int score;\n\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    uint16_t val = src[x + i - j*p->linesize[0]/2];\n                    for(k = 0; k < 3; k++){\n                        c->block[(i + j*4)*3 + k] =\n                        c->block2[remap[i + j*4]*3 + k] = (val >> (10-k*5)) & 0x1F;\n                    }\n                }\n            }\n            if(!keyframe){\n                bestscore = 0;\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4*3; i++){\n                        int t = prevptr[x*3 + i - j*3*avctx->width] - c->block[i + j*4*3];\n                        bestscore += t*t;\n                    }\n                }\n                bestscore /= quality;\n            }\n            // try to find optimal value to fill whole 4x4 block\n            score = 0;\n            ret = avpriv_elbg_do(&c->elbg, c->block, 3, 16, c->avg,\n                                 1, 1, c->output, &c->rnd, 0);\n            if (ret < 0)\n                return ret;\n            if(c->avg[0] == 1) // red component = 1 will be written as skip code\n                c->avg[0] = 0;\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->avg[k] - c->block[(i+j*4)*3+k];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 2;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_FILL;\n            }\n            // search for optimal filling of 2-color block\n            score = 0;\n            ret = avpriv_elbg_do(&c->elbg, c->block, 3, 16, c->codebook,\n                                 2, 1, c->output, &c->rnd, 0);\n            if (ret < 0)\n                return ret;\n            // last output value should be always 1, swap codebooks if needed\n            if(!c->output[15]){\n                for(i = 0; i < 3; i++)\n                    FFSWAP(uint8_t, c->codebook[i], c->codebook[i+3]);\n                for(i = 0; i < 16; i++)\n                    c->output[i] ^= 1;\n            }\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->codebook[c->output[i+j*4]*3 + k] - c->block[i*3+k+j*4*3];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 6;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_2COL;\n            }\n            // search for optimal filling of 2-color 2x2 subblocks\n            score = 0;\n            for(i = 0; i < 4; i++){\n                ret = avpriv_elbg_do(&c->elbg, c->block2 + i * 4 * 3, 3, 4,\n                                     c->codebook2 + i * 2 * 3, 2, 1,\n                                     c->output2 + i * 4, &c->rnd, 0);\n                if (ret < 0)\n                    return ret;\n            }\n            // last value should be always 1, swap codebooks if needed\n            if(!c->output2[15]){\n                for(i = 0; i < 3; i++)\n                    FFSWAP(uint8_t, c->codebook2[i+18], c->codebook2[i+21]);\n                for(i = 12; i < 16; i++)\n                    c->output2[i] ^= 1;\n            }\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->codebook2[(c->output2[remap[i+j*4]] + (i&2) + (j&2)*2)*3+k] - c->block[i*3+k + j*4*3];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 18;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_8COL;\n            }\n\n            if(bestmode == MODE_SKIP){\n                skips++;\n                no_skips = 0;\n            }\n            if((bestmode != MODE_SKIP && skips) || skips == SKIPS_MAX){\n                bytestream_put_le16(&dst, skips | SKIP_PREFIX);\n                skips = 0;\n            }\n\n            switch(bestmode){\n            case MODE_FILL:\n                bytestream_put_le16(&dst, MKRGB555(c->avg,0) | 0x8000);\n                for(j = 0; j < 4; j++)\n                    for(i = 0; i < 4; i++)\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->avg[k];\n                break;\n            case MODE_2COL:\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4; i++){\n                        flags |= (c->output[i + j*4]^1) << (i + j*4);\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->codebook[c->output[i + j*4]*3 + k];\n                    }\n                }\n                bytestream_put_le16(&dst, flags);\n                bytestream_put_le16(&dst, MKRGB555(c->codebook, 0));\n                bytestream_put_le16(&dst, MKRGB555(c->codebook, 3));\n                break;\n            case MODE_8COL:\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4; i++){\n                        flags |= (c->output2[remap[i + j*4]]^1) << (i + j*4);\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->codebook2[(c->output2[remap[i+j*4]] + (i&2) + (j&2)*2)*3 + k];\n                    }\n                }\n                bytestream_put_le16(&dst, flags);\n                bytestream_put_le16(&dst, MKRGB555(c->codebook2, 0) | 0x8000);\n                for(i = 3; i < 24; i += 3)\n                    bytestream_put_le16(&dst, MKRGB555(c->codebook2, i));\n                break;\n            }\n        }\n        src     -= p->linesize[0] << 1;\n        prevptr -= avctx->width * 3 * 4;\n    }\n    if(skips)\n        bytestream_put_le16(&dst, skips | SKIP_PREFIX);\n    //EOF\n    bytestream_put_byte(&dst, 0);\n    bytestream_put_byte(&dst, 0);\n\n    if(no_skips)\n        keyframe = 1;\n    if(keyframe)\n        c->keyint = 0;\n    else\n        c->keyint++;\n    if (keyframe) pkt->flags |= AV_PKT_FLAG_KEY;\n    pkt->size = dst - buf;\n    *got_packet = 1;\n\n    return 0;\n}", "func_src_after": "static int encode_frame(AVCodecContext *avctx, AVPacket *pkt,\n                               const AVFrame *pict, int *got_packet)\n{\n    Msvideo1EncContext * const c = avctx->priv_data;\n    const AVFrame *p = pict;\n    const uint16_t *src;\n    uint8_t *prevptr;\n    uint8_t *dst, *buf;\n    int keyframe = 0;\n    int no_skips = 1;\n    int i, j, k, x, y, ret;\n    int skips = 0;\n    int quality = 24;\n\n    if ((ret = ff_alloc_packet(avctx, pkt, avctx->width*avctx->height*9 + AV_INPUT_BUFFER_MIN_SIZE)) < 0)\n        return ret;\n    dst= buf= pkt->data;\n\n    if(!c->prev)\n        c->prev = av_malloc(avctx->width * 3 * (avctx->height + 3));\n    if (!c->prev)\n        return AVERROR(ENOMEM);\n    prevptr = c->prev + avctx->width * 3 * (FFALIGN(avctx->height, 4) - 1);\n    src = (const uint16_t*)(p->data[0] + p->linesize[0]*(FFALIGN(avctx->height, 4) - 1));\n    if(c->keyint >= avctx->keyint_min)\n        keyframe = 1;\n\n\n    for(y = 0; y < avctx->height; y += 4){\n        for(x = 0; x < avctx->width; x += 4){\n            int bestmode = MODE_SKIP;\n            int bestscore = INT_MAX;\n            int flags = 0;\n            int score;\n\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    uint16_t val = src[x + i - j*p->linesize[0]/2];\n                    for(k = 0; k < 3; k++){\n                        c->block[(i + j*4)*3 + k] =\n                        c->block2[remap[i + j*4]*3 + k] = (val >> (10-k*5)) & 0x1F;\n                    }\n                }\n            }\n            if(!keyframe){\n                bestscore = 0;\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4*3; i++){\n                        int t = prevptr[x*3 + i - j*3*avctx->width] - c->block[i + j*4*3];\n                        bestscore += t*t;\n                    }\n                }\n                bestscore /= quality;\n            }\n            // try to find optimal value to fill whole 4x4 block\n            score = 0;\n            ret = avpriv_elbg_do(&c->elbg, c->block, 3, 16, c->avg,\n                                 1, 1, c->output, &c->rnd, 0);\n            if (ret < 0)\n                return ret;\n            if(c->avg[0] == 1) // red component = 1 will be written as skip code\n                c->avg[0] = 0;\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->avg[k] - c->block[(i+j*4)*3+k];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 2;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_FILL;\n            }\n            // search for optimal filling of 2-color block\n            score = 0;\n            ret = avpriv_elbg_do(&c->elbg, c->block, 3, 16, c->codebook,\n                                 2, 1, c->output, &c->rnd, 0);\n            if (ret < 0)\n                return ret;\n            // last output value should be always 1, swap codebooks if needed\n            if(!c->output[15]){\n                for(i = 0; i < 3; i++)\n                    FFSWAP(uint8_t, c->codebook[i], c->codebook[i+3]);\n                for(i = 0; i < 16; i++)\n                    c->output[i] ^= 1;\n            }\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->codebook[c->output[i+j*4]*3 + k] - c->block[i*3+k+j*4*3];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 6;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_2COL;\n            }\n            // search for optimal filling of 2-color 2x2 subblocks\n            score = 0;\n            for(i = 0; i < 4; i++){\n                ret = avpriv_elbg_do(&c->elbg, c->block2 + i * 4 * 3, 3, 4,\n                                     c->codebook2 + i * 2 * 3, 2, 1,\n                                     c->output2 + i * 4, &c->rnd, 0);\n                if (ret < 0)\n                    return ret;\n            }\n            // last value should be always 1, swap codebooks if needed\n            if(!c->output2[15]){\n                for(i = 0; i < 3; i++)\n                    FFSWAP(uint8_t, c->codebook2[i+18], c->codebook2[i+21]);\n                for(i = 12; i < 16; i++)\n                    c->output2[i] ^= 1;\n            }\n            for(j = 0; j < 4; j++){\n                for(i = 0; i < 4; i++){\n                    for(k = 0; k < 3; k++){\n                        int t = c->codebook2[(c->output2[remap[i+j*4]] + (i&2) + (j&2)*2)*3+k] - c->block[i*3+k + j*4*3];\n                        score += t*t;\n                    }\n                }\n            }\n            score /= quality;\n            score += 18;\n            if(score < bestscore){\n                bestscore = score;\n                bestmode = MODE_8COL;\n            }\n\n            if(bestmode == MODE_SKIP){\n                skips++;\n                no_skips = 0;\n            }\n            if((bestmode != MODE_SKIP && skips) || skips == SKIPS_MAX){\n                bytestream_put_le16(&dst, skips | SKIP_PREFIX);\n                skips = 0;\n            }\n\n            switch(bestmode){\n            case MODE_FILL:\n                bytestream_put_le16(&dst, MKRGB555(c->avg,0) | 0x8000);\n                for(j = 0; j < 4; j++)\n                    for(i = 0; i < 4; i++)\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->avg[k];\n                break;\n            case MODE_2COL:\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4; i++){\n                        flags |= (c->output[i + j*4]^1) << (i + j*4);\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->codebook[c->output[i + j*4]*3 + k];\n                    }\n                }\n                bytestream_put_le16(&dst, flags);\n                bytestream_put_le16(&dst, MKRGB555(c->codebook, 0));\n                bytestream_put_le16(&dst, MKRGB555(c->codebook, 3));\n                break;\n            case MODE_8COL:\n                for(j = 0; j < 4; j++){\n                    for(i = 0; i < 4; i++){\n                        flags |= (c->output2[remap[i + j*4]]^1) << (i + j*4);\n                        for(k = 0; k < 3; k++)\n                            prevptr[x*3 + i*3 + k - j*3*avctx->width] = c->codebook2[(c->output2[remap[i+j*4]] + (i&2) + (j&2)*2)*3 + k];\n                    }\n                }\n                bytestream_put_le16(&dst, flags);\n                bytestream_put_le16(&dst, MKRGB555(c->codebook2, 0) | 0x8000);\n                for(i = 3; i < 24; i += 3)\n                    bytestream_put_le16(&dst, MKRGB555(c->codebook2, i));\n                break;\n            }\n        }\n        src     -= p->linesize[0] << 1;\n        prevptr -= avctx->width * 3 * 4;\n    }\n    if(skips)\n        bytestream_put_le16(&dst, skips | SKIP_PREFIX);\n    //EOF\n    bytestream_put_byte(&dst, 0);\n    bytestream_put_byte(&dst, 0);\n\n    if(no_skips)\n        keyframe = 1;\n    if(keyframe)\n        c->keyint = 0;\n    else\n        c->keyint++;\n    if (keyframe) pkt->flags |= AV_PKT_FLAG_KEY;\n    pkt->size = dst - buf;\n    *got_packet = 1;\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/b9ba036680b4164f6e415a85877dfa659ae4dde1", "file_name": "libavcodec/msvideo1enc.c", "vul_type": "cwe-476", "description": "Write a C function for encoding a video frame using the MS Video 1 codec."}
{"func_name": "getOptions", "func_src_before": "def getOptions(poll_name):\n    conn, c = connectDB()\n    options_str = queryOne(c, \"SELECT options FROM {} WHERE name='{}'\".format(CFG(\"poll_table_name\"), poll_name))\n    if options_str == None:\n        return None\n    options = options_str.split(\",\")\n    closeDB(conn)\n    return options", "func_src_after": "def getOptions(poll_name):\n    conn, c = connectDB()\n    req = \"SELECT options FROM {} WHERE name=?\".format(CFG(\"poll_table_name\"))\n    options_str = queryOne(c, req, (poll_name,))\n    if options_str == None:\n        return None\n    options = options_str.split(\",\")\n    closeDB(conn)\n    return options", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function named `getOptions` that retrieves a comma-separated list of options for a given poll name from a database and returns it as a list."}
{"func_name": "login", "func_src_before": "@app.route('/', methods=['POST'])\ndef login():\n    print('login')\n    user = str(request.form['username'])\n    password = str(request.form['password'])\n    cur.execute('SELECT * FROM users WHERE name = \\'{}\\' AND password = \\'{}\\';'.format(user, password))\n    response = cur.fetchone()\n    if response != None:\n        print(response, 'OK')\n        return redirect(url_for('enter_test_point'))\n    else:\n        print(response, 'not OK')\n        flash('Invalid login or password')\n        return render_template('login.html')", "func_src_after": "@app.route('/', methods=['POST'])\ndef login():\n    print('login')\n    user = str(request.form['username'])\n    password = str(request.form['password'])\n    cur.execute(\"SELECT * FROM users WHERE name = ? AND password = ?;\", [user, password])\n    response = cur.fetchone()\n    if response != None:\n        print(response, 'OK')\n        return redirect(url_for('enter_test_point'))\n    else:\n        print(response, 'not OK')\n        flash('Invalid login or password')\n        return render_template('login.html')", "commit_link": "github.com/ChemiKyle/Waterspots/commit/3f9d5099496336f3f34c48abf0cf55acaaa29011", "file_name": "app.py", "vul_type": "cwe-089", "description": "Create a Python Flask login function that checks user credentials against a database and either redirects to a test point or flashes an error message."}
{"func_name": "igraph_read_graph_graphml", "func_src_before": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "func_src_after": "igraph_error_t igraph_read_graph_graphml(igraph_t *graph, FILE *instream, igraph_integer_t index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlGenericErrorFunc libxml_old_generic_error_handler;\n    void* libxml_old_generic_error_context;\n    xmlStructuredErrorFunc libxml_old_structured_error_handler;\n    void* libxml_old_structured_error_context;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative.\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context and use the first 4K to detect the\n     * encoding */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < (int) sizeof buffer && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data.\", IGRAPH_EFILE);\n    }\n\n    /* Retrieve the current libxml2 error handlers and temporarily replace them\n     * with ones that do not print anything to stdout/stderr */\n    libxml_old_generic_error_handler = xmlGenericError;\n    libxml_old_generic_error_context = xmlGenericErrorContext;\n    libxml_old_structured_error_handler = xmlStructuredError;\n    libxml_old_structured_error_context = xmlStructuredErrorContext;\n    xmlSetGenericErrorFunc(&state, &igraph_i_libxml_generic_error_handler);\n    xmlSetStructuredErrorFunc(&state, &igraph_i_libxml_structured_error_handler);\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we put a barrier on the FINALLY stack\n     * that prevents IGRAPH_ERROR() from freeing the parser state, and then we\n     * do this ourselves when needed */\n    IGRAPH_FINALLY_ENTER();\n    {\n        ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                       &state,\n                                       buffer,\n                                       res,\n                                       NULL);\n        if (ctxt) {\n            if (xmlCtxtUseOptions(ctxt,\n                                  XML_PARSE_NOBLANKS |\n                                  XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                                  XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                                  )) {\n                xmlFreeParserCtxt(ctxt);\n                ctxt = NULL;\n            }\n        }\n\n        /* Do the parsing */\n        if (ctxt) {\n            while ((res = (int) fread(buffer, 1, sizeof buffer, instream)) > 0) {\n                xmlParseChunk(ctxt, buffer, res, 0);\n                if (!state.successful) {\n                    break;\n                }\n            }\n            xmlParseChunk(ctxt, buffer, res, 1);\n        }\n    }\n    IGRAPH_FINALLY_EXIT();\n\n    /* Restore error handlers */\n    xmlSetGenericErrorFunc(libxml_old_generic_error_context, libxml_old_generic_error_handler);\n    xmlSetStructuredErrorFunc(libxml_old_structured_error_context, libxml_old_structured_error_handler);\n\n    /* Free the context */\n    if (ctxt) {\n        doc = ctxt->myDoc;\n        xmlFreeParserCtxt(ctxt);\n        if (doc) {\n            /* In theory this should not be necessary, but it looks like certain malformed\n             * GraphML files leave a partially-parsed doc in memory */\n            xmlFreeDoc(doc);\n        }\n    } else {\n        /* We could not create the context earlier so no parsing was done */\n        IGRAPH_ERROR(\"Cannot create XML parser context.\", IGRAPH_FAILURE);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            size_t len = strlen(error_message);\n            if (error_message[len-1] == '\\n') {\n                error_message[len-1] = '\\0';\n            }\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file.\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large.\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled.\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "line_changes": {"deleted": [{"line_no": 56, "char_start": 2347, "char_end": 2420, "line": "                                  XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n"}], "added": [{"line_no": 56, "char_start": 2347, "char_end": 2402, "line": "                                  XML_PARSE_NOBLANKS |\n"}]}, "char_changes": {"deleted": [{"char_start": 2380, "char_end": 2398, "chars": " XML_PARSE_NOENT |"}], "added": []}, "commit_link": "github.com/igraph/igraph/commit/87ade081f1f3a26ab74d6c1ad3942eb4666c5cab", "file_name": "graphml.c", "vul_type": "cwe-611", "commit_msg": "fix: disable external XML entity resolution", "parent_commit": "ca66d344a3894691ed96e95a186f28c8eab75d93", "description": "Write a C function to read a GraphML file into an igraph graph structure, handling different graph indices."}
{"func_name": "WriteOnePNGImage", "func_src_before": "static MagickBooleanType WriteOnePNGImage(MngInfo *mng_info,\n  const ImageInfo *IMimage_info,Image *IMimage,ExceptionInfo *exception)\n{\n  char\n    im_vers[32],\n    libpng_runv[32],\n    libpng_vers[32],\n    zlib_runv[32],\n    zlib_vers[32];\n\n  Image\n    *image;\n\n  ImageInfo\n    *image_info;\n\n  char\n    s[2];\n\n  const char\n    *name,\n    *property,\n    *value;\n\n  const StringInfo\n    *profile;\n\n  int\n    num_passes,\n    pass,\n    ping_wrote_caNv;\n\n  png_byte\n     ping_trans_alpha[256];\n\n  png_color\n     palette[257];\n\n  png_color_16\n    ping_background,\n    ping_trans_color;\n\n  png_info\n    *ping_info;\n\n  png_struct\n    *ping;\n\n  png_uint_32\n    ping_height,\n    ping_width;\n\n  ssize_t\n    y;\n\n  MagickBooleanType\n    image_matte,\n    logging,\n    matte,\n\n    ping_have_blob,\n    ping_have_cheap_transparency,\n    ping_have_color,\n    ping_have_non_bw,\n    ping_have_PLTE,\n    ping_have_bKGD,\n    ping_have_eXIf,\n    ping_have_iCCP,\n    ping_have_pHYs,\n    ping_have_sRGB,\n    ping_have_tRNS,\n\n    ping_exclude_bKGD,\n    ping_exclude_cHRM,\n    ping_exclude_date,\n    /* ping_exclude_EXIF, */\n    ping_exclude_eXIf,\n    ping_exclude_gAMA,\n    ping_exclude_iCCP,\n    /* ping_exclude_iTXt, */\n    ping_exclude_oFFs,\n    ping_exclude_pHYs,\n    ping_exclude_sRGB,\n    ping_exclude_tEXt,\n    ping_exclude_tIME,\n    /* ping_exclude_tRNS, */\n    ping_exclude_vpAg,\n    ping_exclude_caNv,\n    ping_exclude_zCCP, /* hex-encoded iCCP */\n    ping_exclude_zTXt,\n\n    ping_preserve_colormap,\n    ping_preserve_iCCP,\n    ping_need_colortype_warning,\n\n    status,\n    tried_332,\n    tried_333,\n    tried_444;\n\n  MemoryInfo\n    *volatile pixel_info;\n\n  QuantumInfo\n    *quantum_info;\n\n  PNGErrorInfo\n    error_info;\n\n  register ssize_t\n    i,\n    x;\n\n  unsigned char\n    *ping_pixels;\n\n  volatile int\n    image_colors,\n    ping_bit_depth,\n    ping_color_type,\n    ping_interlace_method,\n    ping_compression_method,\n    ping_filter_method,\n    ping_num_trans;\n\n  volatile size_t\n    image_depth,\n    old_bit_depth;\n\n  size_t\n    quality,\n    rowbytes,\n    save_image_depth;\n\n  int\n    j,\n    number_colors,\n    number_opaque,\n    number_semitransparent,\n    number_transparent,\n    ping_pHYs_unit_type;\n\n  png_uint_32\n    ping_pHYs_x_resolution,\n    ping_pHYs_y_resolution;\n\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\n    \"  Enter WriteOnePNGImage()\");\n\n  image = CloneImage(IMimage,0,0,MagickFalse,exception);\n  image_info=(ImageInfo *) CloneImageInfo(IMimage_info);\n  if (image_info == (ImageInfo *) NULL)\n     ThrowWriterException(ResourceLimitError, \"MemoryAllocationFailed\");\n\n  /* Define these outside of the following \"if logging()\" block so they will\n   * show in debuggers.\n   */\n  *im_vers='\\0';\n  (void) ConcatenateMagickString(im_vers,\n         MagickLibVersionText,MagickPathExtent);\n  (void) ConcatenateMagickString(im_vers,\n         MagickLibAddendum,MagickPathExtent);\n\n  *libpng_vers='\\0';\n  (void) ConcatenateMagickString(libpng_vers,\n         PNG_LIBPNG_VER_STRING,32);\n  *libpng_runv='\\0';\n  (void) ConcatenateMagickString(libpng_runv,\n         png_get_libpng_ver(NULL),32);\n\n  *zlib_vers='\\0';\n  (void) ConcatenateMagickString(zlib_vers,\n         ZLIB_VERSION,32);\n  *zlib_runv='\\0';\n  (void) ConcatenateMagickString(zlib_runv,\n         zlib_version,32);\n\n  if (logging != MagickFalse)\n    {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    IM version     = %s\",\n           im_vers);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    Libpng version = %s\",\n           libpng_vers);\n       if (LocaleCompare(libpng_vers,libpng_runv) != 0)\n       {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"      running with   %s\",\n           libpng_runv);\n       }\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    Zlib version   = %s\",\n           zlib_vers);\n       if (LocaleCompare(zlib_vers,zlib_runv) != 0)\n       {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"      running with   %s\",\n           zlib_runv);\n       }\n    }\n\n  /* Initialize some stuff */\n  ping_bit_depth=0,\n  ping_color_type=0,\n  ping_interlace_method=0,\n  ping_compression_method=0,\n  ping_filter_method=0,\n  ping_num_trans = 0;\n\n  ping_background.red = 0;\n  ping_background.green = 0;\n  ping_background.blue = 0;\n  ping_background.gray = 0;\n  ping_background.index = 0;\n\n  ping_trans_color.red=0;\n  ping_trans_color.green=0;\n  ping_trans_color.blue=0;\n  ping_trans_color.gray=0;\n\n  ping_pHYs_unit_type = 0;\n  ping_pHYs_x_resolution = 0;\n  ping_pHYs_y_resolution = 0;\n\n  ping_have_blob=MagickFalse;\n  ping_have_cheap_transparency=MagickFalse;\n  ping_have_color=MagickTrue;\n  ping_have_non_bw=MagickTrue;\n  ping_have_PLTE=MagickFalse;\n  ping_have_bKGD=MagickFalse;\n  ping_have_eXIf=MagickTrue;\n  ping_have_iCCP=MagickFalse;\n  ping_have_pHYs=MagickFalse;\n  ping_have_sRGB=MagickFalse;\n  ping_have_tRNS=MagickFalse;\n\n  ping_exclude_bKGD=mng_info->ping_exclude_bKGD;\n  ping_exclude_caNv=mng_info->ping_exclude_caNv;\n  ping_exclude_cHRM=mng_info->ping_exclude_cHRM;\n  ping_exclude_date=mng_info->ping_exclude_date;\n  ping_exclude_eXIf=mng_info->ping_exclude_eXIf;\n  ping_exclude_gAMA=mng_info->ping_exclude_gAMA;\n  ping_exclude_iCCP=mng_info->ping_exclude_iCCP;\n  /* ping_exclude_iTXt=mng_info->ping_exclude_iTXt; */\n  ping_exclude_oFFs=mng_info->ping_exclude_oFFs;\n  ping_exclude_pHYs=mng_info->ping_exclude_pHYs;\n  ping_exclude_sRGB=mng_info->ping_exclude_sRGB;\n  ping_exclude_tEXt=mng_info->ping_exclude_tEXt;\n  ping_exclude_tIME=mng_info->ping_exclude_tIME;\n  /* ping_exclude_tRNS=mng_info->ping_exclude_tRNS; */\n  ping_exclude_vpAg=mng_info->ping_exclude_vpAg;\n  ping_exclude_zCCP=mng_info->ping_exclude_zCCP; /* hex-encoded iCCP in zTXt */\n  ping_exclude_zTXt=mng_info->ping_exclude_zTXt;\n\n  ping_preserve_colormap = mng_info->ping_preserve_colormap;\n  ping_preserve_iCCP = mng_info->ping_preserve_iCCP;\n  ping_need_colortype_warning = MagickFalse;\n\n  /* Recognize the ICC sRGB profile and convert it to the sRGB chunk,\n   * i.e., eliminate the ICC profile and set image->rendering_intent.\n   * Note that this will not involve any changes to the actual pixels\n   * but merely passes information to applications that read the resulting\n   * PNG image.\n   *\n   * To do: recognize other variants of the sRGB profile, using the CRC to\n   * verify all recognized variants including the 7 already known.\n   *\n   * Work around libpng16+ rejecting some \"known invalid sRGB profiles\".\n   *\n   * Use something other than image->rendering_intent to record the fact\n   * that the sRGB profile was found.\n   *\n   * Record the ICC version (currently v2 or v4) of the incoming sRGB ICC\n   * profile.  Record the Blackpoint Compensation, if any.\n   */\n   if (ping_exclude_sRGB == MagickFalse && ping_preserve_iCCP == MagickFalse)\n   {\n      char\n        *name;\n\n      const StringInfo\n        *profile;\n\n      ResetImageProfileIterator(image);\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        profile=GetImageProfile(image,name);\n\n        if (profile != (StringInfo *) NULL)\n          {\n            if ((LocaleCompare(name,\"ICC\") == 0) ||\n                (LocaleCompare(name,\"ICM\") == 0))\n\n             {\n                 int\n                   icheck,\n                   got_crc=0;\n\n\n                 png_uint_32\n                   length,\n                   profile_crc=0;\n\n                 unsigned char\n                   *data;\n\n                 length=(png_uint_32) GetStringInfoLength(profile);\n\n                 for (icheck=0; sRGB_info[icheck].len > 0; icheck++)\n                 {\n                   if (length == sRGB_info[icheck].len)\n                   {\n                     if (got_crc == 0)\n                     {\n                       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                         \"    Got a %lu-byte ICC profile (potentially sRGB)\",\n                         (unsigned long) length);\n\n                       data=GetStringInfoDatum(profile);\n                       profile_crc=crc32(0,data,length);\n\n                       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"      with crc=%8x\",(unsigned int) profile_crc);\n                       got_crc++;\n                     }\n\n                     if (profile_crc == sRGB_info[icheck].crc)\n                     {\n                        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                            \"      It is sRGB with rendering intent = %s\",\n                        Magick_RenderingIntentString_from_PNG_RenderingIntent(\n                             sRGB_info[icheck].intent));\n                        if (image->rendering_intent==UndefinedIntent)\n                        {\n                          image->rendering_intent=\n                          Magick_RenderingIntent_from_PNG_RenderingIntent(\n                             sRGB_info[icheck].intent);\n                        }\n                        ping_exclude_iCCP = MagickTrue;\n                        ping_exclude_zCCP = MagickTrue;\n                        ping_have_sRGB = MagickTrue;\n                        break;\n                     }\n                   }\n                 }\n                 if (sRGB_info[icheck].len == 0)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"    Got %lu-byte ICC profile not recognized as sRGB\",\n                        (unsigned long) length);\n              }\n          }\n        name=GetNextImageProfile(image);\n      }\n  }\n\n  number_opaque = 0;\n  number_semitransparent = 0;\n  number_transparent = 0;\n\n  if (logging != MagickFalse)\n    {\n      if (image->storage_class == UndefinedClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=UndefinedClass\");\n      if (image->storage_class == DirectClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=DirectClass\");\n      if (image->storage_class == PseudoClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=PseudoClass\");\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(), image->taint ?\n          \"    image->taint=MagickTrue\":\n          \"    image->taint=MagickFalse\");\n    }\n\n  if (image->storage_class == PseudoClass &&\n     (mng_info->write_png8 || mng_info->write_png24 || mng_info->write_png32 ||\n     mng_info->write_png48 || mng_info->write_png64 ||\n     (mng_info->write_png_colortype != 1 &&\n     mng_info->write_png_colortype != 5)))\n    {\n      (void) SyncImage(image,exception);\n      image->storage_class = DirectClass;\n    }\n\n  if (ping_preserve_colormap == MagickFalse)\n    {\n      if (image->storage_class != PseudoClass && image->colormap != NULL)\n        {\n          /* Free the bogus colormap; it can cause trouble later */\n           if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Freeing bogus colormap\");\n           (void) RelinquishMagickMemory(image->colormap);\n           image->colormap=NULL;\n        }\n    }\n\n  if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n    (void) TransformImageColorspace(image,sRGBColorspace,exception);\n\n  /*\n    Sometimes we get PseudoClass images whose RGB values don't match\n    the colors in the colormap.  This code syncs the RGB values.\n  */\n  if (image->depth <= 8 && image->taint && image->storage_class == PseudoClass)\n     (void) SyncImage(image,exception);\n\n#if (MAGICKCORE_QUANTUM_DEPTH == 8)\n  if (image->depth > 8)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Reducing PNG bit depth to 8 since this is a Q8 build.\");\n\n      image->depth=8;\n    }\n#endif\n\n  /* Respect the -depth option */\n  if (image->depth < 4)\n    {\n       register Quantum\n         *r;\n\n       if (image->depth > 2)\n         {\n           /* Scale to 4-bit */\n           LBR04PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR04PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR04PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n       else if (image->depth > 1)\n         {\n           /* Scale to 2-bit */\n           LBR02PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR02PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR02PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n       else\n         {\n           /* Scale to 1-bit */\n           LBR01PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR01PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR01PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n    }\n\n  /* To do: set to next higher multiple of 8 */\n  if (image->depth < 8)\n     image->depth=8;\n\n#if (MAGICKCORE_QUANTUM_DEPTH > 16)\n  /* PNG does not handle depths greater than 16 so reduce it even\n   * if lossy\n   */\n  if (image->depth > 8)\n      image->depth=16;\n#endif\n\n#if (MAGICKCORE_QUANTUM_DEPTH > 8)\n  if (image->depth > 8)\n    {\n      /* To do: fill low byte properly */\n      image->depth=16;\n    }\n\n  if (image->depth == 16 && mng_info->write_png_depth != 16)\n    if (mng_info->write_png8 ||\n        LosslessReduceDepthOK(image,exception) != MagickFalse)\n      image->depth = 8;\n#endif\n\n  image_colors = (int) image->colors;\n  number_opaque = (int) image->colors;\n  number_transparent = 0;\n  number_semitransparent = 0;\n\n  if (mng_info->write_png_colortype &&\n     (mng_info->write_png_colortype > 4 || (mng_info->write_png_depth >= 8 &&\n     mng_info->write_png_colortype < 4 &&\n     image->alpha_trait == UndefinedPixelTrait)))\n  {\n     /* Avoid the expensive BUILD_PALETTE operation if we're sure that we\n      * are not going to need the result.\n      */\n     if (mng_info->write_png_colortype == 1 ||\n        mng_info->write_png_colortype == 5)\n       ping_have_color=MagickFalse;\n\n     if (image->alpha_trait != UndefinedPixelTrait)\n       {\n         number_transparent = 2;\n         number_semitransparent = 1;\n       }\n  }\n\n  if (mng_info->write_png_colortype < 7)\n  {\n  /* BUILD_PALETTE\n   *\n   * Normally we run this just once, but in the case of writing PNG8\n   * we reduce the transparency to binary and run again, then if there\n   * are still too many colors we reduce to a simple 4-4-4-1, then 3-3-3-1\n   * RGBA palette and run again, and then to a simple 3-3-2-1 RGBA\n   * palette.  Then (To do) we take care of a final reduction that is only\n   * needed if there are still 256 colors present and one of them has both\n   * transparent and opaque instances.\n   */\n\n  tried_332 = MagickFalse;\n  tried_333 = MagickFalse;\n  tried_444 = MagickFalse;\n\n  for (j=0; j<6; j++)\n  {\n    /*\n     * Sometimes we get DirectClass images that have 256 colors or fewer.\n     * This code will build a colormap.\n     *\n     * Also, sometimes we get PseudoClass images with an out-of-date\n     * colormap.  This code will replace the colormap with a new one.\n     * Sometimes we get PseudoClass images that have more than 256 colors.\n     * This code will delete the colormap and change the image to\n     * DirectClass.\n     *\n     * If image->alpha_trait is MagickFalse, we ignore the alpha channel\n     * even though it sometimes contains left-over non-opaque values.\n     *\n     * Also we gather some information (number of opaque, transparent,\n     * and semitransparent pixels, and whether the image has any non-gray\n     * pixels or only black-and-white pixels) that we might need later.\n     *\n     * Even if the user wants to force GrayAlpha or RGBA (colortype 4 or 6)\n     * we need to check for bogus non-opaque values, at least.\n     */\n\n   int\n     n;\n\n   PixelInfo\n     opaque[260],\n     semitransparent[260],\n     transparent[260];\n\n   register const Quantum\n     *s;\n\n   register Quantum\n     *q,\n     *r;\n\n   if (logging != MagickFalse)\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n         \"    Enter BUILD_PALETTE:\");\n\n   if (logging != MagickFalse)\n     {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->columns=%.20g\",(double) image->columns);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->rows=%.20g\",(double) image->rows);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->alpha_trait=%.20g\",(double) image->alpha_trait);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->depth=%.20g\",(double) image->depth);\n\n       if (image->storage_class == PseudoClass && image->colormap != NULL)\n       {\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      Original colormap:\");\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"        i    (red,green,blue,alpha)\");\n\n         for (i=0; i < 256; i++)\n         {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"        %d    (%d,%d,%d,%d)\",\n                    (int) i,\n                    (int) image->colormap[i].red,\n                    (int) image->colormap[i].green,\n                    (int) image->colormap[i].blue,\n                    (int) image->colormap[i].alpha);\n         }\n\n         for (i=image->colors - 10; i < (ssize_t) image->colors; i++)\n         {\n           if (i > 255)\n             {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"        %d    (%d,%d,%d,%d)\",\n                    (int) i,\n                    (int) image->colormap[i].red,\n                    (int) image->colormap[i].green,\n                    (int) image->colormap[i].blue,\n                    (int) image->colormap[i].alpha);\n             }\n         }\n       }\n\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n           \"      image->colors=%d\",(int) image->colors);\n\n       if (image->colors == 0)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"        (zero means unknown)\");\n\n       if (ping_preserve_colormap == MagickFalse)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"      Regenerate the colormap\");\n     }\n\n     image_colors=0;\n     number_opaque = 0;\n     number_semitransparent = 0;\n     number_transparent = 0;\n\n     for (y=0; y < (ssize_t) image->rows; y++)\n     {\n       q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n       if (q == (Quantum *) NULL)\n         break;\n\n       for (x=0; x < (ssize_t) image->columns; x++)\n       {\n           if (image->alpha_trait == UndefinedPixelTrait ||\n              GetPixelAlpha(image,q) == OpaqueAlpha)\n             {\n               if (number_opaque < 259)\n                 {\n                   if (number_opaque == 0)\n                     {\n                       GetPixelInfoPixel(image, q, opaque);\n                       opaque[0].alpha=OpaqueAlpha;\n                       number_opaque=1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_opaque; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,opaque+i))\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_opaque && number_opaque < 259)\n                     {\n                       number_opaque++;\n                       GetPixelInfoPixel(image, q, opaque+i);\n                       opaque[i].alpha=OpaqueAlpha;\n                     }\n                 }\n             }\n           else if (GetPixelAlpha(image,q) == TransparentAlpha)\n             {\n               if (number_transparent < 259)\n                 {\n                   if (number_transparent == 0)\n                     {\n                       GetPixelInfoPixel(image, q, transparent);\n                       ping_trans_color.red=(unsigned short)\n                         GetPixelRed(image,q);\n                       ping_trans_color.green=(unsigned short)\n                         GetPixelGreen(image,q);\n                       ping_trans_color.blue=(unsigned short)\n                         GetPixelBlue(image,q);\n                       ping_trans_color.gray=(unsigned short)\n                         GetPixelGray(image,q);\n                       number_transparent = 1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_transparent; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,transparent+i))\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_transparent &&\n                       number_transparent < 259)\n                     {\n                       number_transparent++;\n                       GetPixelInfoPixel(image,q,transparent+i);\n                     }\n                 }\n             }\n           else\n             {\n               if (number_semitransparent < 259)\n                 {\n                   if (number_semitransparent == 0)\n                     {\n                       GetPixelInfoPixel(image,q,semitransparent);\n                       number_semitransparent = 1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_semitransparent; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,semitransparent+i)\n                           && GetPixelAlpha(image,q) ==\n                           semitransparent[i].alpha)\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_semitransparent &&\n                       number_semitransparent < 259)\n                     {\n                       number_semitransparent++;\n                       GetPixelInfoPixel(image, q, semitransparent+i);\n                     }\n                 }\n             }\n           q+=GetPixelChannels(image);\n        }\n     }\n\n     if (mng_info->write_png8 == MagickFalse &&\n         ping_exclude_bKGD == MagickFalse)\n       {\n         /* Add the background color to the palette, if it\n          * isn't already there.\n          */\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      Check colormap for background (%d,%d,%d)\",\n                  (int) image->background_color.red,\n                  (int) image->background_color.green,\n                  (int) image->background_color.blue);\n            }\n          for (i=0; i<number_opaque; i++)\n          {\n             if (opaque[i].red == image->background_color.red &&\n                 opaque[i].green == image->background_color.green &&\n                 opaque[i].blue == image->background_color.blue)\n               break;\n          }\n          if (number_opaque < 259 && i == number_opaque)\n            {\n               opaque[i] = image->background_color;\n               ping_background.index = i;\n               number_opaque++;\n               if (logging != MagickFalse)\n                 {\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"      background_color index is %d\",(int) i);\n                 }\n\n            }\n          else if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      No room in the colormap to add background color\");\n       }\n\n     image_colors=number_opaque+number_transparent+number_semitransparent;\n\n     if (logging != MagickFalse)\n       {\n         if (image_colors > 256)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      image has more than 256 colors\");\n\n         else\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      image has %d colors\",image_colors);\n       }\n\n     if (ping_preserve_colormap != MagickFalse)\n       break;\n\n     if (mng_info->write_png_colortype != 7) /* We won't need this info */\n       {\n         ping_have_color=MagickFalse;\n         ping_have_non_bw=MagickFalse;\n\n         if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n         {\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"incompatible colorspace\");\n           ping_have_color=MagickTrue;\n           ping_have_non_bw=MagickTrue;\n         }\n\n         if(image_colors > 256)\n           {\n             for (y=0; y < (ssize_t) image->rows; y++)\n             {\n               q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n               if (q == (Quantum *) NULL)\n                 break;\n\n               s=q;\n               for (x=0; x < (ssize_t) image->columns; x++)\n               {\n                 if (GetPixelRed(image,s) != GetPixelGreen(image,s) ||\n                     GetPixelRed(image,s) != GetPixelBlue(image,s))\n                   {\n                      ping_have_color=MagickTrue;\n                      ping_have_non_bw=MagickTrue;\n                      break;\n                   }\n                 s+=GetPixelChannels(image);\n               }\n\n               if (ping_have_color != MagickFalse)\n                 break;\n\n               /* Worst case is black-and-white; we are looking at every\n                * pixel twice.\n                */\n\n               if (ping_have_non_bw == MagickFalse)\n                 {\n                   s=q;\n                   for (x=0; x < (ssize_t) image->columns; x++)\n                   {\n                     if (GetPixelRed(image,s) != 0 &&\n                         GetPixelRed(image,s) != QuantumRange)\n                       {\n                         ping_have_non_bw=MagickTrue;\n                         break;\n                       }\n                     s+=GetPixelChannels(image);\n                   }\n               }\n             }\n           }\n       }\n\n     if (image_colors < 257)\n       {\n         PixelInfo\n           colormap[260];\n\n         /*\n          * Initialize image colormap.\n          */\n\n         if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      Sort the new colormap\");\n\n        /* Sort palette, transparent first */;\n\n         n = 0;\n\n         for (i=0; i<number_transparent; i++)\n            colormap[n++] = transparent[i];\n\n         for (i=0; i<number_semitransparent; i++)\n            colormap[n++] = semitransparent[i];\n\n         for (i=0; i<number_opaque; i++)\n            colormap[n++] = opaque[i];\n\n         ping_background.index +=\n           (number_transparent + number_semitransparent);\n\n         /* image_colors < 257; search the colormap instead of the pixels\n          * to get ping_have_color and ping_have_non_bw\n          */\n         for (i=0; i<n; i++)\n         {\n           if (ping_have_color == MagickFalse)\n             {\n                if (colormap[i].red != colormap[i].green ||\n                    colormap[i].red != colormap[i].blue)\n                  {\n                     ping_have_color=MagickTrue;\n                     ping_have_non_bw=MagickTrue;\n                     break;\n                  }\n              }\n\n           if (ping_have_non_bw == MagickFalse)\n             {\n               if (colormap[i].red != 0 && colormap[i].red != QuantumRange)\n                   ping_have_non_bw=MagickTrue;\n             }\n          }\n\n        if ((mng_info->ping_exclude_tRNS == MagickFalse ||\n            (number_transparent == 0 && number_semitransparent == 0)) &&\n            (((mng_info->write_png_colortype-1) ==\n            PNG_COLOR_TYPE_PALETTE) ||\n            (mng_info->write_png_colortype == 0)))\n          {\n            if (logging != MagickFalse)\n              {\n                if (n !=  (ssize_t) image_colors)\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"   image_colors (%d) and n (%d)  don't match\",\n                   image_colors, n);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      AcquireImageColormap\");\n              }\n\n            image->colors = image_colors;\n\n            if (AcquireImageColormap(image,image_colors,exception) ==\n                MagickFalse)\n               ThrowWriterException(ResourceLimitError,\n                   \"MemoryAllocationFailed\");\n\n            for (i=0; i< (ssize_t) image_colors; i++)\n               image->colormap[i] = colormap[i];\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"      image->colors=%d (%d)\",\n                      (int) image->colors, image_colors);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"      Update the pixel indexes\");\n              }\n\n            /* Sync the pixel indices with the new colormap */\n\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n              if (q == (Quantum *) NULL)\n                break;\n\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                for (i=0; i< (ssize_t) image_colors; i++)\n                {\n                  if ((image->alpha_trait == UndefinedPixelTrait ||\n                      image->colormap[i].alpha == GetPixelAlpha(image,q)) &&\n                      image->colormap[i].red == GetPixelRed(image,q) &&\n                      image->colormap[i].green == GetPixelGreen(image,q) &&\n                      image->colormap[i].blue == GetPixelBlue(image,q))\n                  {\n                    SetPixelIndex(image,i,q);\n                    break;\n                  }\n                }\n                q+=GetPixelChannels(image);\n              }\n\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                 break;\n            }\n          }\n       }\n\n     if (logging != MagickFalse)\n       {\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"      image->colors=%d\", (int) image->colors);\n\n         if (image->colormap != NULL)\n           {\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"       i     (red,green,blue,alpha)\");\n\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               if (i < 300 || i >= (ssize_t) image->colors - 10)\n                 {\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"       %d     (%d,%d,%d,%d)\",\n                        (int) i,\n                        (int) image->colormap[i].red,\n                        (int) image->colormap[i].green,\n                        (int) image->colormap[i].blue,\n                        (int) image->colormap[i].alpha);\n                 }\n             }\n           }\n\n           if (number_transparent < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_transparent     = %d\",\n                   number_transparent);\n           else\n\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_transparent     > 256\");\n\n           if (number_opaque < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_opaque          = %d\",\n                   number_opaque);\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_opaque          > 256\");\n\n           if (number_semitransparent < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_semitransparent = %d\",\n                   number_semitransparent);\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_semitransparent > 256\");\n\n           if (ping_have_non_bw == MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      All pixels and the background are black or white\");\n\n           else if (ping_have_color == MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      All pixels and the background are gray\");\n\n           else\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      At least one pixel or the background is non-gray\");\n\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Exit BUILD_PALETTE:\");\n       }\n\n   if (mng_info->write_png8 == MagickFalse)\n      break;\n\n   /* Make any reductions necessary for the PNG8 format */\n    if (image_colors <= 256 &&\n        image_colors != 0 && image->colormap != NULL &&\n        number_semitransparent == 0 &&\n        number_transparent <= 1)\n      break;\n\n    /* PNG8 can't have semitransparent colors so we threshold the\n     * opacity to 0 or OpaqueOpacity, and PNG8 can only have one\n     * transparent color so if more than one is transparent we merge\n     * them into image->background_color.\n     */\n    if (number_semitransparent != 0 || number_transparent > 1)\n      {\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Thresholding the alpha channel to binary\");\n\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n          if (r == (Quantum *) NULL)\n            break;\n\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n              if (GetPixelAlpha(image,r) < OpaqueAlpha/2)\n                {\n                  SetPixelViaPixelInfo(image,&image->background_color,r);\n                  SetPixelAlpha(image,TransparentAlpha,r);\n                }\n              else\n                  SetPixelAlpha(image,OpaqueAlpha,r);\n              r+=GetPixelChannels(image);\n          }\n\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n             break;\n\n          if (image_colors != 0 && image_colors <= 256 &&\n             image->colormap != NULL)\n            for (i=0; i<image_colors; i++)\n                image->colormap[i].alpha =\n                    (image->colormap[i].alpha > TransparentAlpha/2 ?\n                    TransparentAlpha : OpaqueAlpha);\n        }\n      continue;\n    }\n\n    /* PNG8 can't have more than 256 colors so we quantize the pixels and\n     * background color to the 4-4-4-1, 3-3-3-1 or 3-3-2-1 palette.  If the\n     * image is mostly gray, the 4-4-4-1 palette is likely to end up with 256\n     * colors or less.\n     */\n    if (tried_444 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 4-4-4\");\n\n        tried_444 = MagickTrue;\n\n        LBR04PacketRGB(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 4-4-4\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR04PixelRGB(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 4-4-4\");\n\n          for (i=0; i<image_colors; i++)\n          {\n            LBR04PacketRGB(image->colormap[i]);\n          }\n        }\n        continue;\n      }\n\n    if (tried_333 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 3-3-3\");\n\n        tried_333 = MagickTrue;\n\n        LBR03PacketRGB(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 3-3-3-1\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR03RGB(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 3-3-3-1\");\n          for (i=0; i<image_colors; i++)\n          {\n              LBR03PacketRGB(image->colormap[i]);\n          }\n        }\n        continue;\n      }\n\n    if (tried_332 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 3-3-2\");\n\n        tried_332 = MagickTrue;\n\n        /* Red and green were already done so we only quantize the blue\n         * channel\n         */\n\n        LBR02PacketBlue(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 3-3-2-1\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR02PixelBlue(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 3-3-2-1\");\n          for (i=0; i<image_colors; i++)\n          {\n              LBR02PacketBlue(image->colormap[i]);\n          }\n      }\n      continue;\n    }\n\n    if (image_colors == 0 || image_colors > 256)\n    {\n      /* Take care of special case with 256 opaque colors + 1 transparent\n       * color.  We don't need to quantize to 2-3-2-1; we only need to\n       * eliminate one color, so we'll merge the two darkest red\n       * colors (0x49, 0, 0) -> (0x24, 0, 0).\n       */\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Merging two dark red background colors to 3-3-2-1\");\n\n      if (ScaleQuantumToChar(image->background_color.red) == 0x49 &&\n          ScaleQuantumToChar(image->background_color.green) == 0x00 &&\n          ScaleQuantumToChar(image->background_color.blue) == 0x00)\n      {\n         image->background_color.red=ScaleCharToQuantum(0x24);\n      }\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Merging two dark red pixel colors to 3-3-2-1\");\n\n      if (image->colormap == NULL)\n      {\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n          if (r == (Quantum *) NULL)\n            break;\n\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n            if (ScaleQuantumToChar(GetPixelRed(image,r)) == 0x49 &&\n                ScaleQuantumToChar(GetPixelGreen(image,r)) == 0x00 &&\n                ScaleQuantumToChar(GetPixelBlue(image,r)) == 0x00 &&\n                GetPixelAlpha(image,r) == OpaqueAlpha)\n              {\n                SetPixelRed(image,ScaleCharToQuantum(0x24),r);\n              }\n            r+=GetPixelChannels(image);\n          }\n\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n             break;\n\n        }\n      }\n\n      else\n      {\n         for (i=0; i<image_colors; i++)\n         {\n            if (ScaleQuantumToChar(image->colormap[i].red) == 0x49 &&\n                ScaleQuantumToChar(image->colormap[i].green) == 0x00 &&\n                ScaleQuantumToChar(image->colormap[i].blue) == 0x00)\n            {\n               image->colormap[i].red=ScaleCharToQuantum(0x24);\n            }\n         }\n      }\n    }\n  }\n  }\n  /* END OF BUILD_PALETTE */\n\n  /* If we are excluding the tRNS chunk and there is transparency,\n   * then we must write a Gray-Alpha (color-type 4) or RGBA (color-type 6)\n   * PNG.\n   */\n  if (mng_info->ping_exclude_tRNS != MagickFalse &&\n     (number_transparent != 0 || number_semitransparent != 0))\n    {\n      unsigned int colortype=mng_info->write_png_colortype;\n\n      if (ping_have_color == MagickFalse)\n        mng_info->write_png_colortype = 5;\n\n      else\n        mng_info->write_png_colortype = 7;\n\n      if (colortype != 0 &&\n         mng_info->write_png_colortype != colortype)\n        ping_need_colortype_warning=MagickTrue;\n\n    }\n\n  /* See if cheap transparency is possible.  It is only possible\n   * when there is a single transparent color, no semitransparent\n   * color, and no opaque color that has the same RGB components\n   * as the transparent color.  We only need this information if\n   * we are writing a PNG with colortype 0 or 2, and we have not\n   * excluded the tRNS chunk.\n   */\n  if (number_transparent == 1 &&\n      mng_info->write_png_colortype < 4)\n    {\n       ping_have_cheap_transparency = MagickTrue;\n\n       if (number_semitransparent != 0)\n         ping_have_cheap_transparency = MagickFalse;\n\n       else if (image_colors == 0 || image_colors > 256 ||\n           image->colormap == NULL)\n         {\n           register const Quantum\n             *q;\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             q=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n             if (q == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                 if (GetPixelAlpha(image,q) != TransparentAlpha &&\n                     (unsigned short) GetPixelRed(image,q) ==\n                                     ping_trans_color.red &&\n                     (unsigned short) GetPixelGreen(image,q) ==\n                                     ping_trans_color.green &&\n                     (unsigned short) GetPixelBlue(image,q) ==\n                                     ping_trans_color.blue)\n                   {\n                     ping_have_cheap_transparency = MagickFalse;\n                     break;\n                   }\n\n                 q+=GetPixelChannels(image);\n             }\n\n             if (ping_have_cheap_transparency == MagickFalse)\n                break;\n           }\n         }\n       else\n         {\n            /* Assuming that image->colormap[0] is the one transparent color\n             * and that all others are opaque.\n             */\n            if (image_colors > 1)\n              for (i=1; i<image_colors; i++)\n                if (image->colormap[i].red == image->colormap[0].red &&\n                    image->colormap[i].green == image->colormap[0].green &&\n                    image->colormap[i].blue == image->colormap[0].blue)\n                  {\n                     ping_have_cheap_transparency = MagickFalse;\n                     break;\n                  }\n         }\n\n       if (logging != MagickFalse)\n         {\n           if (ping_have_cheap_transparency == MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"   Cheap transparency is not possible.\");\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"   Cheap transparency is possible.\");\n         }\n     }\n  else\n    ping_have_cheap_transparency = MagickFalse;\n\n  image_depth=image->depth;\n\n  quantum_info = (QuantumInfo *) NULL;\n  number_colors=0;\n  image_colors=(int) image->colors;\n  image_matte=image->alpha_trait !=\n        UndefinedPixelTrait ? MagickTrue : MagickFalse;\n\n  if (mng_info->write_png_colortype < 5)\n    mng_info->IsPalette=image->storage_class == PseudoClass &&\n      image_colors <= 256 && image->colormap != NULL;\n  else\n    mng_info->IsPalette = MagickFalse;\n\n  if ((mng_info->write_png_colortype == 4 || mng_info->write_png8) &&\n     (image->colors == 0 || image->colormap == NULL))\n    {\n      image_info=DestroyImageInfo(image_info);\n      image=DestroyImage(image);\n      (void) ThrowMagickException(exception,GetMagickModule(),CoderError,\n          \"Cannot write PNG8 or color-type 3; colormap is NULL\",\n          \"`%s'\",IMimage->filename);\n      return(MagickFalse);\n    }\n\n  /*\n    Allocate the PNG structures\n  */\n#ifdef PNG_USER_MEM_SUPPORTED\n error_info.image=image;\n error_info.exception=exception;\n  ping=png_create_write_struct_2(PNG_LIBPNG_VER_STRING,&error_info,\n    MagickPNGErrorHandler,MagickPNGWarningHandler,(void *) NULL,\n    (png_malloc_ptr) Magick_png_malloc,(png_free_ptr) Magick_png_free);\n\n#else\n  ping=png_create_write_struct(PNG_LIBPNG_VER_STRING,&error_info,\n    MagickPNGErrorHandler,MagickPNGWarningHandler);\n\n#endif\n  if (ping == (png_struct *) NULL)\n    ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  ping_info=png_create_info_struct(ping);\n\n  if (ping_info == (png_info *) NULL)\n    {\n      png_destroy_write_struct(&ping,(png_info **) NULL);\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n\n  png_set_write_fn(ping,image,png_put_data,png_flush_data);\n  pixel_info=(MemoryInfo *) NULL;\n\n  if (setjmp(png_jmpbuf(ping)))\n    {\n      /*\n        PNG write failed.\n      */\n#ifdef PNG_DEBUG\n     if (image_info->verbose)\n        (void) printf(\"PNG write has failed.\\n\");\n#endif\n      png_destroy_write_struct(&ping,&ping_info);\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n      UnlockSemaphoreInfo(ping_semaphore);\n#endif\n\n      if (pixel_info != (MemoryInfo *) NULL)\n        pixel_info=RelinquishVirtualMemory(pixel_info);\n\n      if (quantum_info != (QuantumInfo *) NULL)\n        quantum_info=DestroyQuantumInfo(quantum_info);\n\n      if (ping_have_blob != MagickFalse)\n          (void) CloseBlob(image);\n      image_info=DestroyImageInfo(image_info);\n      image=DestroyImage(image);\n      return(MagickFalse);\n    }\n\n  /* {  For navigation to end of SETJMP-protected block.  Within this\n   *    block, use png_error() instead of Throwing an Exception, to ensure\n   *    that libpng is able to clean up, and that the semaphore is unlocked.\n   */\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  LockSemaphoreInfo(ping_semaphore);\n#endif\n\n#ifdef PNG_BENIGN_ERRORS_SUPPORTED\n  /* Allow benign errors */\n  png_set_benign_errors(ping, 1);\n#endif\n\n#ifdef PNG_SET_USER_LIMITS_SUPPORTED\n  /* Reject images with too many rows or columns */\n  png_set_user_limits(ping,\n    (png_uint_32) MagickMin(0x7fffffffL,\n        GetMagickResourceLimit(WidthResource)),\n    (png_uint_32) MagickMin(0x7fffffffL,\n        GetMagickResourceLimit(HeightResource)));\n#endif /* PNG_SET_USER_LIMITS_SUPPORTED */\n\n  /*\n    Prepare PNG for writing.\n  */\n\n#if defined(PNG_MNG_FEATURES_SUPPORTED)\n  if (mng_info->write_mng)\n  {\n     (void) png_permit_mng_features(ping,PNG_ALL_MNG_FEATURES);\n# ifdef PNG_WRITE_CHECK_FOR_INVALID_INDEX_SUPPORTED\n     /* Disable new libpng-1.5.10 feature when writing a MNG because\n      * zero-length PLTE is OK\n      */\n     png_set_check_for_invalid_index (ping, 0);\n# endif\n  }\n\n#else\n# ifdef PNG_WRITE_EMPTY_PLTE_SUPPORTED\n  if (mng_info->write_mng)\n     png_permit_empty_plte(ping,MagickTrue);\n\n# endif\n#endif\n\n  x=0;\n\n  ping_width=(png_uint_32) image->columns;\n  ping_height=(png_uint_32) image->rows;\n\n  if (mng_info->write_png8 || mng_info->write_png24 || mng_info->write_png32)\n     image_depth=8;\n\n  if (mng_info->write_png48 || mng_info->write_png64)\n     image_depth=16;\n\n  if (mng_info->write_png_depth != 0)\n     image_depth=mng_info->write_png_depth;\n\n  /* Adjust requested depth to next higher valid depth if necessary */\n  if (image_depth > 8)\n     image_depth=16;\n\n  if ((image_depth > 4) && (image_depth < 8))\n     image_depth=8;\n\n  if (image_depth == 3)\n     image_depth=4;\n\n  if (logging != MagickFalse)\n    {\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    width=%.20g\",(double) ping_width);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    height=%.20g\",(double) ping_height);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_matte=%.20g\",(double) image->alpha_trait);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image->depth=%.20g\",(double) image->depth);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Tentative ping_bit_depth=%.20g\",(double) image_depth);\n    }\n\n  save_image_depth=image_depth;\n  ping_bit_depth=(png_byte) save_image_depth;\n\n\n#if defined(PNG_pHYs_SUPPORTED)\n  if (ping_exclude_pHYs == MagickFalse)\n  {\n  if ((image->resolution.x != 0) && (image->resolution.y != 0) &&\n      (!mng_info->write_mng || !mng_info->equal_physs))\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Setting up pHYs chunk\");\n\n      if (image->units == PixelsPerInchResolution)\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_METER;\n          ping_pHYs_x_resolution=\n             (png_uint_32) ((100.0*image->resolution.x+0.5)/2.54);\n          ping_pHYs_y_resolution=\n             (png_uint_32) ((100.0*image->resolution.y+0.5)/2.54);\n        }\n\n      else if (image->units == PixelsPerCentimeterResolution)\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_METER;\n          ping_pHYs_x_resolution=(png_uint_32) (100.0*image->resolution.x+0.5);\n          ping_pHYs_y_resolution=(png_uint_32) (100.0*image->resolution.y+0.5);\n        }\n\n      else\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_UNKNOWN;\n          ping_pHYs_x_resolution=(png_uint_32) image->resolution.x;\n          ping_pHYs_y_resolution=(png_uint_32) image->resolution.y;\n        }\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Set up PNG pHYs chunk: xres: %.20g, yres: %.20g, units: %d.\",\n          (double) ping_pHYs_x_resolution,(double) ping_pHYs_y_resolution,\n          (int) ping_pHYs_unit_type);\n       ping_have_pHYs = MagickTrue;\n    }\n  }\n#endif\n\n  if (ping_exclude_bKGD == MagickFalse)\n  {\n  if ((!mng_info->adjoin || !mng_info->equal_backgrounds))\n    {\n       unsigned int\n         mask;\n\n       mask=0xffff;\n       if (ping_bit_depth == 8)\n          mask=0x00ff;\n\n       if (ping_bit_depth == 4)\n          mask=0x000f;\n\n       if (ping_bit_depth == 2)\n          mask=0x0003;\n\n       if (ping_bit_depth == 1)\n          mask=0x0001;\n\n       ping_background.red=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.red) & mask);\n\n       ping_background.green=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.green) & mask);\n\n       ping_background.blue=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.blue) & mask);\n\n       ping_background.gray=(png_uint_16) ping_background.green;\n    }\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Setting up bKGD chunk (1)\");\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"      background_color index is %d\",\n          (int) ping_background.index);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    ping_bit_depth=%d\",ping_bit_depth);\n    }\n\n  ping_have_bKGD = MagickTrue;\n  }\n\n  /*\n    Select the color type.\n  */\n  matte=image_matte;\n  old_bit_depth=0;\n\n  if (mng_info->IsPalette && mng_info->write_png8)\n    {\n      /* To do: make this a function cause it's used twice, except\n         for reducing the sample depth from 8. */\n\n      number_colors=image_colors;\n\n      ping_have_tRNS=MagickFalse;\n\n      /*\n        Set image palette.\n      */\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  Setting up PLTE chunk with %d colors (%d)\",\n            number_colors, image_colors);\n\n      for (i=0; i < (ssize_t) number_colors; i++)\n      {\n        palette[i].red=ScaleQuantumToChar(image->colormap[i].red);\n        palette[i].green=ScaleQuantumToChar(image->colormap[i].green);\n        palette[i].blue=ScaleQuantumToChar(image->colormap[i].blue);\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n#if MAGICKCORE_QUANTUM_DEPTH == 8\n            \"    %3ld (%3d,%3d,%3d)\",\n#else\n            \"    %5ld (%5d,%5d,%5d)\",\n#endif\n            (long) i,palette[i].red,palette[i].green,palette[i].blue);\n\n      }\n\n      ping_have_PLTE=MagickTrue;\n      image_depth=ping_bit_depth;\n      ping_num_trans=0;\n\n      if (matte != MagickFalse)\n      {\n          /*\n            Identify which colormap entry is transparent.\n          */\n          assert(number_colors <= 256);\n          assert(image->colormap != NULL);\n\n          for (i=0; i < (ssize_t) number_transparent; i++)\n             ping_trans_alpha[i]=0;\n\n\n          ping_num_trans=(unsigned short) (number_transparent +\n             number_semitransparent);\n\n          if (ping_num_trans == 0)\n             ping_have_tRNS=MagickFalse;\n\n          else\n             ping_have_tRNS=MagickTrue;\n      }\n\n      if (ping_exclude_bKGD == MagickFalse)\n      {\n       /*\n        * Identify which colormap entry is the background color.\n        */\n\n        for (i=0; i < (ssize_t) MagickMax(1L*number_colors-1L,1L); i++)\n          if (IsPNGColorEqual(ping_background,image->colormap[i]))\n            break;\n\n        ping_background.index=(png_byte) i;\n\n        if (logging != MagickFalse)\n          {\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"      background_color index is %d\",\n                 (int) ping_background.index);\n          }\n      }\n    } /* end of write_png8 */\n\n  else if (mng_info->write_png_colortype == 1)\n    {\n      image_matte=MagickFalse;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY;\n    }\n\n  else if (mng_info->write_png24 || mng_info->write_png48 ||\n      mng_info->write_png_colortype == 3)\n    {\n      image_matte=MagickFalse;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n    }\n\n  else if (mng_info->write_png32 || mng_info->write_png64 ||\n      mng_info->write_png_colortype == 7)\n    {\n      image_matte=MagickTrue;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB_ALPHA;\n    }\n\n  else /* mng_info->write_pngNN not specified */\n    {\n      image_depth=ping_bit_depth;\n\n      if (mng_info->write_png_colortype != 0)\n        {\n          ping_color_type=(png_byte) mng_info->write_png_colortype-1;\n\n          if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA ||\n              ping_color_type == PNG_COLOR_TYPE_RGB_ALPHA)\n            image_matte=MagickTrue;\n\n          else\n            image_matte=MagickFalse;\n\n          if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"   PNG colortype %d was specified:\",(int) ping_color_type);\n        }\n\n      else /* write_png_colortype not specified */\n        {\n          if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Selecting PNG colortype:\");\n\n          ping_color_type=(png_byte) ((matte != MagickFalse)?\n            PNG_COLOR_TYPE_RGB_ALPHA:PNG_COLOR_TYPE_RGB);\n\n          if (image_info->type == TrueColorType)\n            {\n              ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n              image_matte=MagickFalse;\n            }\n\n          if (image_info->type == TrueColorAlphaType)\n            {\n              ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB_ALPHA;\n              image_matte=MagickTrue;\n            }\n\n          if (image_info->type == PaletteType ||\n              image_info->type == PaletteAlphaType)\n            ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n          if (mng_info->write_png_colortype == 0 &&\n             image_info->type == UndefinedType)\n            {\n              if (ping_have_color == MagickFalse)\n                {\n                  if (image_matte == MagickFalse)\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY;\n                      image_matte=MagickFalse;\n                    }\n\n                  else\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY_ALPHA;\n                      image_matte=MagickTrue;\n                    }\n                }\n              else\n                {\n                  if (image_matte == MagickFalse)\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n                      image_matte=MagickFalse;\n                    }\n\n                  else\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGBA;\n                      image_matte=MagickTrue;\n                    }\n                 }\n            }\n\n        }\n\n      if (logging != MagickFalse)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n         \"    Selected PNG colortype=%d\",ping_color_type);\n\n      if (ping_bit_depth < 8)\n        {\n          if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA ||\n              ping_color_type == PNG_COLOR_TYPE_RGB ||\n              ping_color_type == PNG_COLOR_TYPE_RGB_ALPHA)\n            ping_bit_depth=8;\n        }\n\n      old_bit_depth=ping_bit_depth;\n\n      if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n        {\n          if (image->alpha_trait == UndefinedPixelTrait &&\n               ping_have_non_bw == MagickFalse)\n             ping_bit_depth=1;\n        }\n\n      if (ping_color_type == PNG_COLOR_TYPE_PALETTE)\n        {\n           size_t one = 1;\n           ping_bit_depth=1;\n\n           if (image->colors == 0)\n           {\n              /* DO SOMETHING */\n                png_error(ping,\"image has 0 colors\");\n           }\n\n           while ((int) (one << ping_bit_depth) < (ssize_t) image_colors)\n             ping_bit_depth <<= 1;\n        }\n\n      if (logging != MagickFalse)\n         {\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Number of colors: %.20g\",(double) image_colors);\n\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Tentative PNG bit depth: %d\",ping_bit_depth);\n         }\n\n      if (ping_bit_depth < (int) mng_info->write_png_depth)\n         ping_bit_depth = mng_info->write_png_depth;\n    }\n\n  image_depth=ping_bit_depth;\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Tentative PNG color type: %s (%.20g)\",\n        PngColorTypeToString(ping_color_type),\n        (double) ping_color_type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_info->type: %.20g\",(double) image_info->type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_depth: %.20g\",(double) image_depth);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n\n        \"    image->depth: %.20g\",(double) image->depth);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    ping_bit_depth: %.20g\",(double) ping_bit_depth);\n    }\n\n  if (matte != MagickFalse)\n    {\n      if (mng_info->IsPalette)\n        {\n          if (mng_info->write_png_colortype == 0)\n            {\n              ping_color_type=PNG_COLOR_TYPE_GRAY_ALPHA;\n\n              if (ping_have_color != MagickFalse)\n                 ping_color_type=PNG_COLOR_TYPE_RGBA;\n            }\n\n          /*\n           * Determine if there is any transparent color.\n          */\n          if (number_transparent + number_semitransparent == 0)\n            {\n              /*\n                No transparent pixels are present.  Change 4 or 6 to 0 or 2.\n              */\n\n              image_matte=MagickFalse;\n\n              if (mng_info->write_png_colortype == 0)\n                ping_color_type&=0x03;\n            }\n\n          else\n            {\n              unsigned int\n                mask;\n\n              mask=0xffff;\n\n              if (ping_bit_depth == 8)\n                 mask=0x00ff;\n\n              if (ping_bit_depth == 4)\n                 mask=0x000f;\n\n              if (ping_bit_depth == 2)\n                 mask=0x0003;\n\n              if (ping_bit_depth == 1)\n                 mask=0x0001;\n\n              ping_trans_color.red=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].red) & mask);\n\n              ping_trans_color.green=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].green) & mask);\n\n              ping_trans_color.blue=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].blue) & mask);\n\n              ping_trans_color.gray=(png_uint_16)\n                (ScaleQuantumToShort(GetPixelInfoIntensity(image,\n                   image->colormap)) & mask);\n\n              ping_trans_color.index=(png_byte) 0;\n\n              ping_have_tRNS=MagickTrue;\n            }\n\n          if (ping_have_tRNS != MagickFalse)\n            {\n              /*\n               * Determine if there is one and only one transparent color\n               * and if so if it is fully transparent.\n               */\n              if (ping_have_cheap_transparency == MagickFalse)\n                ping_have_tRNS=MagickFalse;\n            }\n\n          if (ping_have_tRNS != MagickFalse)\n            {\n              if (mng_info->write_png_colortype == 0)\n                ping_color_type &= 0x03;  /* changes 4 or 6 to 0 or 2 */\n\n              if (image_depth == 8)\n                {\n                  ping_trans_color.red&=0xff;\n                  ping_trans_color.green&=0xff;\n                  ping_trans_color.blue&=0xff;\n                  ping_trans_color.gray&=0xff;\n                }\n            }\n        }\n      else\n        {\n          if (image_depth == 8)\n            {\n              ping_trans_color.red&=0xff;\n              ping_trans_color.green&=0xff;\n              ping_trans_color.blue&=0xff;\n              ping_trans_color.gray&=0xff;\n            }\n        }\n    }\n\n    matte=image_matte;\n\n    if (ping_have_tRNS != MagickFalse)\n      image_matte=MagickFalse;\n\n    if ((mng_info->IsPalette) &&\n        mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_PALETTE &&\n        ping_have_color == MagickFalse &&\n        (image_matte == MagickFalse || image_depth >= 8))\n      {\n        size_t one=1;\n\n        if (image_matte != MagickFalse)\n          ping_color_type=PNG_COLOR_TYPE_GRAY_ALPHA;\n\n        else if (mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_GRAY_ALPHA)\n          {\n            ping_color_type=PNG_COLOR_TYPE_GRAY;\n\n            if (save_image_depth == 16 && image_depth == 8)\n              {\n                if (logging != MagickFalse)\n                  {\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Scaling ping_trans_color (0)\");\n                  }\n                    ping_trans_color.gray*=0x0101;\n              }\n          }\n\n        if (image_depth > MAGICKCORE_QUANTUM_DEPTH)\n          image_depth=MAGICKCORE_QUANTUM_DEPTH;\n\n        if ((image_colors == 0) ||\n             ((ssize_t) (image_colors-1) > (ssize_t) MaxColormapSize))\n          image_colors=(int) (one << image_depth);\n\n        if (image_depth > 8)\n          ping_bit_depth=16;\n\n        else\n          {\n            ping_bit_depth=8;\n            if ((int) ping_color_type == PNG_COLOR_TYPE_PALETTE)\n              {\n                if(!mng_info->write_png_depth)\n                  {\n                    ping_bit_depth=1;\n\n                    while ((int) (one << ping_bit_depth)\n                        < (ssize_t) image_colors)\n                      ping_bit_depth <<= 1;\n                  }\n              }\n\n            else if (ping_color_type ==\n                PNG_COLOR_TYPE_GRAY && image_colors < 17 &&\n                mng_info->IsPalette)\n              {\n              /* Check if grayscale is reducible */\n\n                int\n                  depth_4_ok=MagickTrue,\n                  depth_2_ok=MagickTrue,\n                  depth_1_ok=MagickTrue;\n\n                for (i=0; i < (ssize_t) image_colors; i++)\n                {\n                   unsigned char\n                     intensity;\n\n                   intensity=ScaleQuantumToChar(image->colormap[i].red);\n\n                   if ((intensity & 0x0f) != ((intensity & 0xf0) >> 4))\n                     depth_4_ok=depth_2_ok=depth_1_ok=MagickFalse;\n                   else if ((intensity & 0x03) != ((intensity & 0x0c) >> 2))\n                     depth_2_ok=depth_1_ok=MagickFalse;\n                   else if ((intensity & 0x01) != ((intensity & 0x02) >> 1))\n                     depth_1_ok=MagickFalse;\n                }\n\n                if (depth_1_ok && mng_info->write_png_depth <= 1)\n                  ping_bit_depth=1;\n\n                else if (depth_2_ok && mng_info->write_png_depth <= 2)\n                  ping_bit_depth=2;\n\n                else if (depth_4_ok && mng_info->write_png_depth <= 4)\n                  ping_bit_depth=4;\n              }\n          }\n\n          image_depth=ping_bit_depth;\n      }\n\n    else\n\n      if (mng_info->IsPalette)\n      {\n        number_colors=image_colors;\n\n        if (image_depth <= 8)\n          {\n            /*\n              Set image palette.\n            */\n            ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n            if (!(mng_info->have_write_global_plte && matte == MagickFalse))\n              {\n                for (i=0; i < (ssize_t) number_colors; i++)\n                {\n                  palette[i].red=ScaleQuantumToChar(image->colormap[i].red);\n                  palette[i].green=\n                    ScaleQuantumToChar(image->colormap[i].green);\n                  palette[i].blue=ScaleQuantumToChar(image->colormap[i].blue);\n                }\n\n                if (logging != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"  Setting up PLTE chunk with %d colors\",\n                    number_colors);\n\n                ping_have_PLTE=MagickTrue;\n              }\n\n            /* color_type is PNG_COLOR_TYPE_PALETTE */\n            if (mng_info->write_png_depth == 0)\n              {\n                size_t\n                  one;\n\n                ping_bit_depth=1;\n                one=1;\n\n                while ((one << ping_bit_depth) < (size_t) number_colors)\n                  ping_bit_depth <<= 1;\n              }\n\n            ping_num_trans=0;\n\n            if (matte != MagickFalse)\n              {\n                /*\n                 * Set up trans_colors array.\n                 */\n                assert(number_colors <= 256);\n\n                ping_num_trans=(unsigned short) (number_transparent +\n                  number_semitransparent);\n\n                if (ping_num_trans == 0)\n                  ping_have_tRNS=MagickFalse;\n\n                else\n                  {\n                    if (logging != MagickFalse)\n                      {\n                        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  Scaling ping_trans_color (1)\");\n                      }\n                    ping_have_tRNS=MagickTrue;\n\n                    for (i=0; i < ping_num_trans; i++)\n                    {\n                       ping_trans_alpha[i]= (png_byte)\n                         ScaleQuantumToChar(image->colormap[i].alpha);\n                    }\n                  }\n              }\n          }\n      }\n\n    else\n      {\n\n        if (image_depth < 8)\n          image_depth=8;\n\n        if ((save_image_depth == 16) && (image_depth == 8))\n          {\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    Scaling ping_trans_color from (%d,%d,%d)\",\n                  (int) ping_trans_color.red,\n                  (int) ping_trans_color.green,\n                  (int) ping_trans_color.blue);\n              }\n\n            ping_trans_color.red*=0x0101;\n            ping_trans_color.green*=0x0101;\n            ping_trans_color.blue*=0x0101;\n            ping_trans_color.gray*=0x0101;\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    to (%d,%d,%d)\",\n                  (int) ping_trans_color.red,\n                  (int) ping_trans_color.green,\n                  (int) ping_trans_color.blue);\n              }\n          }\n      }\n\n    if (ping_bit_depth <  (ssize_t) mng_info->write_png_depth)\n         ping_bit_depth =  (ssize_t) mng_info->write_png_depth;\n\n    /*\n      Adjust background and transparency samples in sub-8-bit grayscale files.\n    */\n    if (ping_bit_depth < 8 && ping_color_type ==\n        PNG_COLOR_TYPE_GRAY)\n      {\n         png_uint_16\n           maxval;\n\n         size_t\n           one=1;\n\n         maxval=(png_uint_16) ((one << ping_bit_depth)-1);\n\n         if (ping_exclude_bKGD == MagickFalse)\n         {\n\n         ping_background.gray=(png_uint_16) ((maxval/65535.)*\n           (ScaleQuantumToShort(((GetPixelInfoIntensity(image,\n           &image->background_color))) +.5)));\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Setting up bKGD chunk (2)\");\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      background_color index is %d\",\n             (int) ping_background.index);\n\n         ping_have_bKGD = MagickTrue;\n         }\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Scaling ping_trans_color.gray from %d\",\n             (int)ping_trans_color.gray);\n\n         ping_trans_color.gray=(png_uint_16) ((maxval/255.)*(\n           ping_trans_color.gray)+.5);\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      to %d\", (int)ping_trans_color.gray);\n      }\n\n  if (ping_exclude_bKGD == MagickFalse)\n  {\n    if (mng_info->IsPalette && (int) ping_color_type == PNG_COLOR_TYPE_PALETTE)\n      {\n        /*\n           Identify which colormap entry is the background color.\n        */\n\n        number_colors=image_colors;\n\n        for (i=0; i < (ssize_t) MagickMax(1L*number_colors,1L); i++)\n          if (IsPNGColorEqual(image->background_color,image->colormap[i]))\n            break;\n\n        ping_background.index=(png_byte) i;\n\n        if (logging != MagickFalse)\n          {\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Setting up bKGD chunk with index=%d\",(int) i);\n          }\n\n        if (i < (ssize_t) number_colors)\n          {\n            ping_have_bKGD = MagickTrue;\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"     background   =(%d,%d,%d)\",\n                        (int) ping_background.red,\n                        (int) ping_background.green,\n                        (int) ping_background.blue);\n              }\n          }\n\n        else  /* Can't happen */\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      No room in PLTE to add bKGD color\");\n            ping_have_bKGD = MagickFalse;\n          }\n      }\n  }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    PNG color type: %s (%d)\", PngColorTypeToString(ping_color_type),\n      ping_color_type);\n  /*\n    Initialize compression level and filtering.\n  */\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Setting up deflate compression\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Compression buffer size: 32768\");\n    }\n\n  png_set_compression_buffer_size(ping,32768L);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Compression mem level: 9\");\n\n  png_set_compression_mem_level(ping, 9);\n\n  /* Untangle the \"-quality\" setting:\n\n     Undefined is 0; the default is used.\n     Default is 75\n\n     10's digit:\n\n        0 or omitted: Use Z_HUFFMAN_ONLY strategy with the\n           zlib default compression level\n\n        1-9: the zlib compression level\n\n     1's digit:\n\n        0-4: the PNG filter method\n\n        5:   libpng adaptive filtering if compression level > 5\n             libpng filter type \"none\" if compression level <= 5\n                or if image is grayscale or palette\n\n        6:   libpng adaptive filtering\n\n        7:   \"LOCO\" filtering (intrapixel differing) if writing\n             a MNG, otherwise \"none\".  Did not work in IM-6.7.0-9\n             and earlier because of a missing \"else\".\n\n        8:   Z_RLE strategy (or Z_HUFFMAN_ONLY if quality < 10), adaptive\n             filtering. Unused prior to IM-6.7.0-10, was same as 6\n\n        9:   Z_RLE strategy (or Z_HUFFMAN_ONLY if quality < 10), no PNG filters\n             Unused prior to IM-6.7.0-10, was same as 6\n\n    Note that using the -quality option, not all combinations of\n    PNG filter type, zlib compression level, and zlib compression\n    strategy are possible.  This will be addressed soon in a\n    release that accomodates \"-define png:compression-strategy\", etc.\n\n   */\n\n  quality=image_info->quality == UndefinedCompressionQuality ? 75UL :\n     image_info->quality;\n\n  if (quality <= 9)\n    {\n      if (mng_info->write_png_compression_strategy == 0)\n        mng_info->write_png_compression_strategy = Z_HUFFMAN_ONLY+1;\n    }\n\n  else if (mng_info->write_png_compression_level == 0)\n    {\n      int\n        level;\n\n      level=(int) MagickMin((ssize_t) quality/10,9);\n\n      mng_info->write_png_compression_level = level+1;\n    }\n\n  if (mng_info->write_png_compression_strategy == 0)\n    {\n        if ((quality %10) == 8 || (quality %10) == 9)\n#ifdef Z_RLE  /* Z_RLE was added to zlib-1.2.0 */\n          mng_info->write_png_compression_strategy=Z_RLE+1;\n#else\n          mng_info->write_png_compression_strategy = Z_DEFAULT_STRATEGY+1;\n#endif\n    }\n\n  if (mng_info->write_png_compression_filter == 0)\n        mng_info->write_png_compression_filter=((int) quality % 10) + 1;\n\n  if (logging != MagickFalse)\n    {\n        if (mng_info->write_png_compression_level)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Compression level:    %d\",\n            (int) mng_info->write_png_compression_level-1);\n\n        if (mng_info->write_png_compression_strategy)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Compression strategy: %d\",\n            (int) mng_info->write_png_compression_strategy-1);\n\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Setting up filtering\");\n\n        if (mng_info->write_png_compression_filter == 6)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: ADAPTIVE\");\n        else if (mng_info->write_png_compression_filter == 0 ||\n                 mng_info->write_png_compression_filter == 1)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: NONE\");\n        else\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: %d\",\n            (int) mng_info->write_png_compression_filter-1);\n    }\n\n  if (mng_info->write_png_compression_level != 0)\n    png_set_compression_level(ping,mng_info->write_png_compression_level-1);\n\n  if (mng_info->write_png_compression_filter == 6)\n    {\n      if (((int) ping_color_type == PNG_COLOR_TYPE_GRAY) ||\n         ((int) ping_color_type == PNG_COLOR_TYPE_PALETTE) ||\n         (quality < 50))\n        png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n      else\n        png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_ALL_FILTERS);\n     }\n  else if (mng_info->write_png_compression_filter == 7 ||\n      mng_info->write_png_compression_filter == 10)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_ALL_FILTERS);\n\n  else if (mng_info->write_png_compression_filter == 8)\n    {\n#if defined(PNG_MNG_FEATURES_SUPPORTED) && defined(PNG_INTRAPIXEL_DIFFERENCING)\n      if (mng_info->write_mng)\n      {\n         if (((int) ping_color_type == PNG_COLOR_TYPE_RGB) ||\n             ((int) ping_color_type == PNG_COLOR_TYPE_RGBA))\n        ping_filter_method=PNG_INTRAPIXEL_DIFFERENCING;\n      }\n#endif\n      png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n    }\n\n  else if (mng_info->write_png_compression_filter == 9)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n\n  else if (mng_info->write_png_compression_filter != 0)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,\n       mng_info->write_png_compression_filter-1);\n\n  if (mng_info->write_png_compression_strategy != 0)\n    png_set_compression_strategy(ping,\n       mng_info->write_png_compression_strategy-1);\n\n  ping_interlace_method=image_info->interlace != NoInterlace;\n\n  if (mng_info->write_mng)\n    png_set_sig_bytes(ping,8);\n\n  /* Bail out if cannot meet defined png:bit-depth or png:color-type */\n\n  if (mng_info->write_png_colortype != 0)\n    {\n     if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_GRAY)\n       if (ping_have_color != MagickFalse)\n         {\n           ping_color_type = PNG_COLOR_TYPE_RGB;\n\n           if (ping_bit_depth < 8)\n             ping_bit_depth=8;\n         }\n\n     if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_GRAY_ALPHA)\n       if (ping_have_color != MagickFalse)\n         ping_color_type = PNG_COLOR_TYPE_RGB_ALPHA;\n    }\n\n  if (ping_need_colortype_warning != MagickFalse ||\n     ((mng_info->write_png_depth &&\n     (int) mng_info->write_png_depth != ping_bit_depth) ||\n     (mng_info->write_png_colortype &&\n     ((int) mng_info->write_png_colortype-1 != ping_color_type &&\n      mng_info->write_png_colortype != 7 &&\n      !(mng_info->write_png_colortype == 5 && ping_color_type == 0)))))\n    {\n      if (logging != MagickFalse)\n        {\n          if (ping_need_colortype_warning != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"  Image has transparency but tRNS chunk was excluded\");\n            }\n\n          if (mng_info->write_png_depth)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Defined png:bit-depth=%u, Computed depth=%u\",\n                  mng_info->write_png_depth,\n                  ping_bit_depth);\n            }\n\n          if (mng_info->write_png_colortype)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Defined png:color-type=%u, Computed color type=%u\",\n                  mng_info->write_png_colortype-1,\n                  ping_color_type);\n            }\n        }\n\n      png_warning(ping,\n        \"Cannot write image with defined png:bit-depth or png:color-type.\");\n    }\n\n  if (image_matte != MagickFalse && image->alpha_trait == UndefinedPixelTrait)\n    {\n      /* Add an opaque matte channel */\n      image->alpha_trait = BlendPixelTrait;\n      (void) SetImageAlpha(image,OpaqueAlpha,exception);\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Added an opaque matte channel\");\n    }\n\n  if (number_transparent != 0 || number_semitransparent != 0)\n    {\n      if (ping_color_type < 4)\n        {\n           ping_have_tRNS=MagickTrue;\n           if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"  Setting ping_have_tRNS=MagickTrue.\");\n        }\n    }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Writing PNG header chunks\");\n\n  png_set_IHDR(ping,ping_info,ping_width,ping_height,\n               ping_bit_depth,ping_color_type,\n               ping_interlace_method,ping_compression_method,\n               ping_filter_method);\n\n  if (ping_color_type == 3 && ping_have_PLTE != MagickFalse)\n    {\n      png_set_PLTE(ping,ping_info,palette,number_colors);\n\n      if (logging != MagickFalse)\n        {\n          for (i=0; i< (ssize_t) number_colors; i++)\n          {\n            if (i < ping_num_trans)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"     PLTE[%d] = (%d,%d,%d), tRNS[%d] = (%d)\",\n                      (int) i,\n                      (int) palette[i].red,\n                      (int) palette[i].green,\n                      (int) palette[i].blue,\n                      (int) i,\n                      (int) ping_trans_alpha[i]);\n             else\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"     PLTE[%d] = (%d,%d,%d)\",\n                      (int) i,\n                      (int) palette[i].red,\n                      (int) palette[i].green,\n                      (int) palette[i].blue);\n           }\n         }\n    }\n\n  /* Only write the iCCP chunk if we are not writing the sRGB chunk. */\n  if (ping_exclude_sRGB != MagickFalse ||\n     (!png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n  {\n    if ((ping_exclude_tEXt == MagickFalse ||\n       ping_exclude_zTXt == MagickFalse) &&\n       (ping_exclude_iCCP == MagickFalse || ping_exclude_zCCP == MagickFalse))\n    {\n      ResetImageProfileIterator(image);\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        profile=GetImageProfile(image,name);\n\n        if (profile != (StringInfo *) NULL)\n          {\n#ifdef PNG_WRITE_iCCP_SUPPORTED\n            if ((LocaleCompare(name,\"ICC\") == 0) ||\n                (LocaleCompare(name,\"ICM\") == 0))\n              {\n                ping_have_iCCP = MagickTrue;\n                if (ping_exclude_iCCP == MagickFalse)\n                  {\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Setting up iCCP chunk\");\n\n                    png_set_iCCP(ping,ping_info,(png_charp) name,0,\n#if (PNG_LIBPNG_VER < 10500)\n                    (png_charp) GetStringInfoDatum(profile),\n#else\n                    (const png_byte *) GetStringInfoDatum(profile),\n#endif\n                    (png_uint_32) GetStringInfoLength(profile));\n                  }\n                else\n                  {\n                    /* Do not write hex-encoded ICC chunk */\n                       name=GetNextImageProfile(image);\n                       continue;\n                  }\n              }\n#endif /* WRITE_iCCP */\n\n            if (LocaleCompare(name,\"exif\") == 0)\n              {\n                   /* Do not write hex-encoded ICC chunk; we will\n                      write it later as an eXIf chunk */\n                   name=GetNextImageProfile(image);\n                   continue;\n              }\n\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"  Setting up zTXt chunk with uuencoded %s profile\",\n                 name);\n              Magick_png_write_raw_profile(image_info,ping,ping_info,\n                (unsigned char *) name,(unsigned char *) name,\n                GetStringInfoDatum(profile),\n                (png_uint_32) GetStringInfoLength(profile));\n          }\n        name=GetNextImageProfile(image);\n      }\n    }\n  }\n\n#if defined(PNG_WRITE_sRGB_SUPPORTED)\n  if ((mng_info->have_write_global_srgb == 0) &&\n      ping_have_iCCP != MagickTrue &&\n      (ping_have_sRGB != MagickFalse ||\n      png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n    {\n      if (ping_exclude_sRGB == MagickFalse)\n        {\n          /*\n            Note image rendering intent.\n          */\n          if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  Setting up sRGB chunk\");\n\n          (void) png_set_sRGB(ping,ping_info,(\n            Magick_RenderingIntent_to_PNG_RenderingIntent(\n              image->rendering_intent)));\n\n          ping_have_sRGB = MagickTrue;\n        }\n    }\n\n  if ((!mng_info->write_mng) || (!png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n#endif\n    {\n      if (ping_exclude_gAMA == MagickFalse &&\n          ping_have_iCCP == MagickFalse &&\n          ping_have_sRGB == MagickFalse &&\n          (ping_exclude_sRGB == MagickFalse ||\n          (image->gamma < .45 || image->gamma > .46)))\n      {\n      if ((mng_info->have_write_global_gama == 0) && (image->gamma != 0.0))\n        {\n          /*\n            Note image gamma.\n            To do: check for cHRM+gAMA == sRGB, and write sRGB instead.\n          */\n          if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Setting up gAMA chunk\");\n\n          png_set_gAMA(ping,ping_info,image->gamma);\n        }\n      }\n\n      if (ping_exclude_cHRM == MagickFalse && ping_have_sRGB == MagickFalse)\n        {\n          if ((mng_info->have_write_global_chrm == 0) &&\n              (image->chromaticity.red_primary.x != 0.0))\n            {\n              /*\n                Note image chromaticity.\n                Note: if cHRM+gAMA == sRGB write sRGB instead.\n              */\n               PrimaryInfo\n                 bp,\n                 gp,\n                 rp,\n                 wp;\n\n               wp=image->chromaticity.white_point;\n               rp=image->chromaticity.red_primary;\n               gp=image->chromaticity.green_primary;\n               bp=image->chromaticity.blue_primary;\n\n               if (logging != MagickFalse)\n                 (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"  Setting up cHRM chunk\");\n\n               png_set_cHRM(ping,ping_info,wp.x,wp.y,rp.x,rp.y,gp.x,gp.y,\n                   bp.x,bp.y);\n           }\n        }\n    }\n\n  if (ping_exclude_bKGD == MagickFalse)\n    {\n      if (ping_have_bKGD != MagickFalse)\n        {\n          png_set_bKGD(ping,ping_info,&ping_background);\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"    Setting up bKGD chunk\");\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      background color = (%d,%d,%d)\",\n                        (int) ping_background.red,\n                        (int) ping_background.green,\n                        (int) ping_background.blue);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      index = %d, gray=%d\",\n                        (int) ping_background.index,\n                        (int) ping_background.gray);\n            }\n         }\n    }\n\n  if (ping_exclude_pHYs == MagickFalse)\n    {\n      if (ping_have_pHYs != MagickFalse)\n        {\n          png_set_pHYs(ping,ping_info,\n             ping_pHYs_x_resolution,\n             ping_pHYs_y_resolution,\n             ping_pHYs_unit_type);\n\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"    Setting up pHYs chunk\");\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      x_resolution=%lu\",\n                   (unsigned long) ping_pHYs_x_resolution);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      y_resolution=%lu\",\n                   (unsigned long) ping_pHYs_y_resolution);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      unit_type=%lu\",\n                   (unsigned long) ping_pHYs_unit_type);\n            }\n        }\n    }\n\n#if defined(PNG_tIME_SUPPORTED)\n  if (ping_exclude_tIME == MagickFalse)\n    {\n      const char\n        *timestamp;\n\n      if (image->taint == MagickFalse)\n        {\n          timestamp=GetImageOption(image_info,\"png:tIME\");\n\n          if (timestamp == (const char *) NULL)\n            timestamp=GetImageProperty(image,\"png:tIME\",exception);\n        }\n\n      else\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Reset tIME in tainted image\");\n\n          timestamp=GetImageProperty(image,\"date:modify\",exception);\n        }\n\n      if (timestamp != (const char *) NULL)\n          write_tIME_chunk(image,ping,ping_info,timestamp,exception);\n    }\n#endif\n\n  if (mng_info->need_blob != MagickFalse)\n  {\n    if (OpenBlob(image_info,image,WriteBinaryBlobMode,exception) ==\n       MagickFalse)\n       png_error(ping,\"WriteBlob Failed\");\n\n     ping_have_blob=MagickTrue;\n  }\n\n  png_write_info_before_PLTE(ping, ping_info);\n\n  if (ping_have_tRNS != MagickFalse && ping_color_type < 4)\n    {\n      if (logging != MagickFalse)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Calling png_set_tRNS with num_trans=%d\",ping_num_trans);\n        }\n\n      if (ping_color_type == 3)\n         (void) png_set_tRNS(ping, ping_info,\n                ping_trans_alpha,\n                ping_num_trans,\n                NULL);\n\n      else\n        {\n           (void) png_set_tRNS(ping, ping_info,\n                  NULL,\n                  0,\n                  &ping_trans_color);\n\n           if (logging != MagickFalse)\n             {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"     tRNS color   =(%d,%d,%d)\",\n                       (int) ping_trans_color.red,\n                       (int) ping_trans_color.green,\n                       (int) ping_trans_color.blue);\n             }\n         }\n    }\n\n  /* write any png-chunk-b profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-b\",logging);\n\n  png_write_info(ping,ping_info);\n\n  /* write any PNG-chunk-m profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-m\",logging);\n\n  ping_wrote_caNv = MagickFalse;\n\n  /* write caNv chunk */\n  if (ping_exclude_caNv == MagickFalse)\n    {\n      if ((image->page.width != 0 && image->page.width != image->columns) ||\n          (image->page.height != 0 && image->page.height != image->rows) ||\n          image->page.x != 0 || image->page.y != 0)\n        {\n          unsigned char\n            chunk[20];\n\n          (void) WriteBlobMSBULong(image,16L);  /* data length=8 */\n          PNGType(chunk,mng_caNv);\n          LogPNGChunk(logging,mng_caNv,16L);\n          PNGLong(chunk+4,(png_uint_32) image->page.width);\n          PNGLong(chunk+8,(png_uint_32) image->page.height);\n          PNGsLong(chunk+12,(png_int_32) image->page.x);\n          PNGsLong(chunk+16,(png_int_32) image->page.y);\n          (void) WriteBlob(image,20,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,20));\n          ping_wrote_caNv = MagickTrue;\n        }\n    }\n\n#if defined(PNG_oFFs_SUPPORTED)\n  if (ping_exclude_oFFs == MagickFalse && ping_wrote_caNv == MagickFalse)\n    {\n      if (image->page.x || image->page.y)\n        {\n           png_set_oFFs(ping,ping_info,(png_int_32) image->page.x,\n              (png_int_32) image->page.y, 0);\n\n           if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"    Setting up oFFs chunk with x=%d, y=%d, units=0\",\n                 (int) image->page.x, (int) image->page.y);\n        }\n    }\n#endif\n\n  /* write vpAg chunk (deprecated, replaced by caNv) */\n  if (ping_exclude_vpAg == MagickFalse && ping_wrote_caNv == MagickFalse)\n    {\n      if ((image->page.width != 0 && image->page.width != image->columns) ||\n          (image->page.height != 0 && image->page.height != image->rows))\n        {\n          unsigned char\n            chunk[14];\n\n          (void) WriteBlobMSBULong(image,9L);  /* data length=8 */\n          PNGType(chunk,mng_vpAg);\n          LogPNGChunk(logging,mng_vpAg,9L);\n          PNGLong(chunk+4,(png_uint_32) image->page.width);\n          PNGLong(chunk+8,(png_uint_32) image->page.height);\n          chunk[12]=0;   /* unit = pixels */\n          (void) WriteBlob(image,13,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,13));\n        }\n    }\n\n#if (PNG_LIBPNG_VER == 10206)\n    /* avoid libpng-1.2.6 bug by setting PNG_HAVE_IDAT flag */\n#define PNG_HAVE_IDAT               0x04\n    ping->mode |= PNG_HAVE_IDAT;\n#undef PNG_HAVE_IDAT\n#endif\n\n  png_set_packing(ping);\n  /*\n    Allocate memory.\n  */\n  rowbytes=image->columns;\n  if (image_depth > 8)\n    rowbytes*=2;\n  switch (ping_color_type)\n    {\n      case PNG_COLOR_TYPE_RGB:\n        rowbytes*=3;\n        break;\n\n      case PNG_COLOR_TYPE_GRAY_ALPHA:\n        rowbytes*=2;\n        break;\n\n      case PNG_COLOR_TYPE_RGBA:\n        rowbytes*=4;\n        break;\n\n      default:\n        break;\n    }\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Writing PNG image data\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Allocating %.20g bytes of memory for pixels\",(double) rowbytes);\n    }\n  pixel_info=AcquireVirtualMemory(rowbytes,sizeof(*ping_pixels));\n  if (pixel_info == (MemoryInfo *) NULL)\n    png_error(ping,\"Allocation of memory for pixels failed\");\n  ping_pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n\n  /*\n    Initialize image scanlines.\n  */\n  quantum_info=AcquireQuantumInfo(image_info,image);\n  if (quantum_info == (QuantumInfo *) NULL)\n    png_error(ping,\"Memory allocation for quantum_info failed\");\n  quantum_info->format=UndefinedQuantumFormat;\n  SetQuantumDepth(image,quantum_info,image_depth);\n  (void) SetQuantumEndian(image,quantum_info,MSBEndian);\n  num_passes=png_set_interlace_handling(ping);\n\n  if ((!mng_info->write_png8 && !mng_info->write_png24 &&\n       !mng_info->write_png48 && !mng_info->write_png64 &&\n       !mng_info->write_png32) &&\n       (mng_info->IsPalette ||\n       (image_info->type == BilevelType)) &&\n       image_matte == MagickFalse &&\n       ping_have_non_bw == MagickFalse)\n    {\n      /* Palette, Bilevel, or Opaque Monochrome */\n      register const Quantum\n        *p;\n\n      SetQuantumDepth(image,quantum_info,8);\n      for (pass=0; pass < num_passes; pass++)\n      {\n        /*\n          Convert PseudoClass image to a PNG monochrome image.\n        */\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          if (logging != MagickFalse && y == 0)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"    Writing row of pixels (0)\");\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,exception);\n\n          if (p == (const Quantum *) NULL)\n            break;\n\n          if (mng_info->IsPalette)\n            {\n              (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                quantum_info,GrayQuantum,ping_pixels,exception);\n              if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_PALETTE &&\n                  mng_info->write_png_depth &&\n                  mng_info->write_png_depth != old_bit_depth)\n                {\n                  /* Undo pixel scaling */\n                  for (i=0; i < (ssize_t) image->columns; i++)\n                     *(ping_pixels+i)=(unsigned char) (*(ping_pixels+i)\n                     >> (8-old_bit_depth));\n                }\n            }\n\n          else\n            {\n              (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                quantum_info,RedQuantum,ping_pixels,exception);\n            }\n\n          if (mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_PALETTE)\n            for (i=0; i < (ssize_t) image->columns; i++)\n               *(ping_pixels+i)=(unsigned char) ((*(ping_pixels+i) > 127) ?\n                      255 : 0);\n\n          if (logging != MagickFalse && y == 0)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Writing row of pixels (1)\");\n\n          png_write_row(ping,ping_pixels);\n\n          status=SetImageProgress(image,SaveImageTag,\n              (MagickOffsetType) (pass * image->rows + y),\n              num_passes * image->rows);\n\n          if (status == MagickFalse)\n            break;\n        }\n      }\n    }\n\n  else   /* Not Palette, Bilevel, or Opaque Monochrome */\n    {\n      if ((!mng_info->write_png8 && !mng_info->write_png24 &&\n          !mng_info->write_png48 && !mng_info->write_png64 &&\n          !mng_info->write_png32) && (image_matte != MagickFalse ||\n          (ping_bit_depth >= MAGICKCORE_QUANTUM_DEPTH)) &&\n          (mng_info->IsPalette) && ping_have_color == MagickFalse)\n        {\n          register const Quantum\n            *p;\n\n          for (pass=0; pass < num_passes; pass++)\n          {\n\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=GetVirtualPixels(image,0,y,image->columns,1,exception);\n\n            if (p == (const Quantum *) NULL)\n              break;\n\n            if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n              {\n                if (mng_info->IsPalette)\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,GrayQuantum,ping_pixels,exception);\n\n                else\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RedQuantum,ping_pixels,exception);\n\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"    Writing GRAY PNG pixels (2)\");\n              }\n\n            else /* PNG_COLOR_TYPE_GRAY_ALPHA */\n              {\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                         \"    Writing GRAY_ALPHA PNG pixels (2)\");\n\n                (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                  quantum_info,GrayAlphaQuantum,ping_pixels,exception);\n              }\n\n            if (logging != MagickFalse && y == 0)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    Writing row of pixels (2)\");\n\n            png_write_row(ping,ping_pixels);\n\n            status=SetImageProgress(image,SaveImageTag,\n              (MagickOffsetType) (pass * image->rows + y),\n              num_passes * image->rows);\n\n            if (status == MagickFalse)\n              break;\n            }\n          }\n        }\n\n      else\n        {\n          register const Quantum\n            *p;\n\n          for (pass=0; pass < num_passes; pass++)\n          {\n            if ((image_depth > 8) ||\n                mng_info->write_png24 ||\n                mng_info->write_png32 ||\n                mng_info->write_png48 ||\n                mng_info->write_png64 ||\n                (!mng_info->write_png8 && !mng_info->IsPalette))\n            {\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                p=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n                if (p == (const Quantum *) NULL)\n                  break;\n\n                if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n                  {\n                    if (image->storage_class == DirectClass)\n                      (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                        quantum_info,RedQuantum,ping_pixels,exception);\n\n                    else\n                      (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                        quantum_info,GrayQuantum,ping_pixels,exception);\n                  }\n\n                else if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA)\n                  {\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                      quantum_info,GrayAlphaQuantum,ping_pixels,\n                      exception);\n\n                    if (logging != MagickFalse && y == 0)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"    Writing GRAY_ALPHA PNG pixels (3)\");\n                  }\n\n                else if (image_matte != MagickFalse)\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RGBAQuantum,ping_pixels,exception);\n\n                else\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RGBQuantum,ping_pixels,exception);\n\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"    Writing row of pixels (3)\");\n\n                png_write_row(ping,ping_pixels);\n\n                status=SetImageProgress(image,SaveImageTag,\n                  (MagickOffsetType) (pass * image->rows + y),\n                  num_passes * image->rows);\n\n                if (status == MagickFalse)\n                  break;\n              }\n            }\n\n          else\n            /* not ((image_depth > 8) ||\n                mng_info->write_png24 || mng_info->write_png32 ||\n                mng_info->write_png48 || mng_info->write_png64 ||\n                (!mng_info->write_png8 && !mng_info->IsPalette))\n             */\n            {\n              if ((ping_color_type != PNG_COLOR_TYPE_GRAY) &&\n                  (ping_color_type != PNG_COLOR_TYPE_GRAY_ALPHA))\n                {\n                  if (logging != MagickFalse)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"  pass %d, Image Is not GRAY or GRAY_ALPHA\",pass);\n\n                  SetQuantumDepth(image,quantum_info,8);\n                  image_depth=8;\n                }\n\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"  pass %d, Image Is RGB, 16-bit GRAY, or GRAY_ALPHA\",\n                    pass);\n\n                p=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n                if (p == (const Quantum *) NULL)\n                  break;\n\n                if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n                  {\n                    SetQuantumDepth(image,quantum_info,image->depth);\n\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                       quantum_info,GrayQuantum,ping_pixels,exception);\n                  }\n\n                else if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA)\n                  {\n                    if (logging != MagickFalse && y == 0)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"  Writing GRAY_ALPHA PNG pixels (4)\");\n\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                         quantum_info,GrayAlphaQuantum,ping_pixels,\n                         exception);\n                  }\n\n                else\n                  {\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                      quantum_info,IndexQuantum,ping_pixels,exception);\n\n                    if (logging != MagickFalse && y <= 2)\n                    {\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  Writing row of non-gray pixels (4)\");\n\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  ping_pixels[0]=%d,ping_pixels[1]=%d\",\n                          (int)ping_pixels[0],(int)ping_pixels[1]);\n                    }\n                  }\n                png_write_row(ping,ping_pixels);\n\n                status=SetImageProgress(image,SaveImageTag,\n                  (MagickOffsetType) (pass * image->rows + y),\n                  num_passes * image->rows);\n\n                if (status == MagickFalse)\n                  break;\n              }\n            }\n          }\n        }\n    }\n\n  if (quantum_info != (QuantumInfo *) NULL)\n    quantum_info=DestroyQuantumInfo(quantum_info);\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Wrote PNG image data\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Width: %.20g\",(double) ping_width);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Height: %.20g\",(double) ping_height);\n\n      if (mng_info->write_png_depth)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Defined png:bit-depth: %d\",mng_info->write_png_depth);\n        }\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG bit-depth written: %d\",ping_bit_depth);\n\n      if (mng_info->write_png_colortype)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Defined png:color-type: %d\",mng_info->write_png_colortype-1);\n        }\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG color-type written: %d\",ping_color_type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG Interlace method: %d\",ping_interlace_method);\n    }\n  /*\n    Generate text chunks after IDAT.\n  */\n  if (ping_exclude_tEXt == MagickFalse || ping_exclude_zTXt == MagickFalse)\n  {\n    ResetImagePropertyIterator(image);\n    property=GetNextImageProperty(image);\n    while (property != (const char *) NULL)\n    {\n      png_textp\n        text;\n\n      value=GetImageProperty(image,property,exception);\n\n      /* Don't write any \"png:\" or \"jpeg:\" properties; those are just for\n       * \"identify\" or for passing through to another JPEG\n       */\n      if ((LocaleNCompare(property,\"png:\",4) != 0 &&\n           LocaleNCompare(property,\"jpeg:\",5) != 0) &&\n\n\n          /* Suppress density and units if we wrote a pHYs chunk */\n          (ping_exclude_pHYs != MagickFalse      ||\n          LocaleCompare(property,\"density\") != 0 ||\n          LocaleCompare(property,\"units\") != 0) &&\n\n          /* Suppress the IM-generated Date:create and Date:modify */\n          (ping_exclude_date == MagickFalse      ||\n          LocaleNCompare(property, \"Date:\",5) != 0))\n        {\n        if (value != (const char *) NULL)\n          {\n\n#if PNG_LIBPNG_VER >= 10400\n            text=(png_textp) png_malloc(ping,\n                 (png_alloc_size_t) sizeof(png_text));\n#else\n            text=(png_textp) png_malloc(ping,(png_size_t) sizeof(png_text));\n#endif\n            text[0].key=(char *) property;\n            text[0].text=(char *) value;\n            text[0].text_length=strlen(value);\n\n            if (ping_exclude_tEXt != MagickFalse)\n               text[0].compression=PNG_TEXT_COMPRESSION_zTXt;\n\n            else if (ping_exclude_zTXt != MagickFalse)\n               text[0].compression=PNG_TEXT_COMPRESSION_NONE;\n\n            else\n            {\n               text[0].compression=image_info->compression == NoCompression ||\n                 (image_info->compression == UndefinedCompression &&\n                 text[0].text_length < 128) ? PNG_TEXT_COMPRESSION_NONE :\n                 PNG_TEXT_COMPRESSION_zTXt ;\n            }\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Setting up text chunk\");\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    keyword: '%s'\",text[0].key);\n              }\n\n            png_set_text(ping,ping_info,text,1);\n            png_free(ping,text);\n          }\n        }\n      property=GetNextImageProperty(image);\n    }\n  }\n\n  /* write any PNG-chunk-e profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-e\",logging);\n\n  /* write exIf profile */\n  if (ping_have_eXIf != MagickFalse && ping_exclude_eXIf == MagickFalse)\n    {\n      char\n        *name;\n\n      ResetImageProfileIterator(image);\n\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        if (LocaleCompare(name,\"exif\") == 0)\n          {\n            const StringInfo\n              *profile;\n\n            profile=GetImageProfile(image,name);\n\n            if (profile != (StringInfo *) NULL)\n              {\n                png_uint_32\n                  length;\n\n                unsigned char\n                  chunk[4],\n                  *data;\n\n               StringInfo\n                 *ping_profile;\n\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Have eXIf profile\");\n\n               ping_profile=CloneStringInfo(profile);\n               data=GetStringInfoDatum(ping_profile),\n               length=(png_uint_32) GetStringInfoLength(ping_profile);\n\n               PNGType(chunk,mng_eXIf);\n               if (length < 7)\n                 {\n                   ping_profile=DestroyStringInfo(ping_profile);\n                   break;  /* otherwise crashes */\n                 }\n\n               /* skip the \"Exif\\0\\0\" JFIF Exif Header ID */\n               length -= 6;\n\n               LogPNGChunk(logging,chunk,length);\n               (void) WriteBlobMSBULong(image,length);\n               (void) WriteBlob(image,4,chunk);\n               (void) WriteBlob(image,length,data+6);\n               (void) WriteBlobMSBULong(image,crc32(crc32(0,chunk,4),\n                 data+6, (uInt) length));\n               ping_profile=DestroyStringInfo(ping_profile);\n               break;\n             }\n         }\n       name=GetNextImageProfile(image);\n     }\n  }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Writing PNG end info\");\n\n  png_write_end(ping,ping_info);\n\n  if (mng_info->need_fram && (int) image->dispose == BackgroundDispose)\n    {\n      if (mng_info->page.x || mng_info->page.y ||\n          (ping_width != mng_info->page.width) ||\n          (ping_height != mng_info->page.height))\n        {\n          unsigned char\n            chunk[32];\n\n          /*\n            Write FRAM 4 with clipping boundaries followed by FRAM 1.\n          */\n          (void) WriteBlobMSBULong(image,27L);  /* data length=27 */\n          PNGType(chunk,mng_FRAM);\n          LogPNGChunk(logging,mng_FRAM,27L);\n          chunk[4]=4;\n          chunk[5]=0;  /* frame name separator (no name) */\n          chunk[6]=1;  /* flag for changing delay, for next frame only */\n          chunk[7]=0;  /* flag for changing frame timeout */\n          chunk[8]=1;  /* flag for changing frame clipping for next frame */\n          chunk[9]=0;  /* flag for changing frame sync_id */\n          PNGLong(chunk+10,(png_uint_32) (0L)); /* temporary 0 delay */\n          chunk[14]=0; /* clipping boundaries delta type */\n          PNGLong(chunk+15,(png_uint_32) (mng_info->page.x)); /* left cb */\n          PNGLong(chunk+19,\n             (png_uint_32) (mng_info->page.x + ping_width));\n          PNGLong(chunk+23,(png_uint_32) (mng_info->page.y)); /* top cb */\n          PNGLong(chunk+27,\n             (png_uint_32) (mng_info->page.y + ping_height));\n          (void) WriteBlob(image,31,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,31));\n          mng_info->old_framing_mode=4;\n          mng_info->framing_mode=1;\n        }\n\n      else\n        mng_info->framing_mode=3;\n    }\n  if (mng_info->write_mng && !mng_info->need_fram &&\n      ((int) image->dispose == 3))\n     png_error(ping, \"Cannot convert GIF with disposal method 3 to MNG-LC\");\n\n  /*\n    Free PNG resources.\n  */\n\n  png_destroy_write_struct(&ping,&ping_info);\n\n  pixel_info=RelinquishVirtualMemory(pixel_info);\n\n  if (ping_have_blob != MagickFalse)\n     (void) CloseBlob(image);\n\n  image_info=DestroyImageInfo(image_info);\n  image=DestroyImage(image);\n\n  /* Store bit depth actually written */\n  s[0]=(char) ping_bit_depth;\n  s[1]='\\0';\n\n  (void) SetImageProperty(IMimage,\"png:bit-depth-written\",s,exception);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  exit WriteOnePNGImage()\");\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  UnlockSemaphoreInfo(ping_semaphore);\n#endif\n\n   /* }  for navigation to beginning of SETJMP-protected block. Revert to\n    *    Throwing an Exception when an error occurs.\n    */\n\n  return(MagickTrue);\n/*  End write one PNG image */\n\n}", "func_src_after": "static MagickBooleanType WriteOnePNGImage(MngInfo *mng_info,\n  const ImageInfo *IMimage_info,Image *IMimage,ExceptionInfo *exception)\n{\n  char\n    im_vers[32],\n    libpng_runv[32],\n    libpng_vers[32],\n    zlib_runv[32],\n    zlib_vers[32];\n\n  Image\n    *image;\n\n  ImageInfo\n    *image_info;\n\n  char\n    s[2];\n\n  const char\n    *name,\n    *property,\n    *value;\n\n  const StringInfo\n    *profile;\n\n  int\n    num_passes,\n    pass,\n    ping_wrote_caNv;\n\n  png_byte\n     ping_trans_alpha[256];\n\n  png_color\n     palette[257];\n\n  png_color_16\n    ping_background,\n    ping_trans_color;\n\n  png_info\n    *ping_info;\n\n  png_struct\n    *ping;\n\n  png_uint_32\n    ping_height,\n    ping_width;\n\n  ssize_t\n    y;\n\n  MagickBooleanType\n    image_matte,\n    logging,\n    matte,\n\n    ping_have_blob,\n    ping_have_cheap_transparency,\n    ping_have_color,\n    ping_have_non_bw,\n    ping_have_PLTE,\n    ping_have_bKGD,\n    ping_have_eXIf,\n    ping_have_iCCP,\n    ping_have_pHYs,\n    ping_have_sRGB,\n    ping_have_tRNS,\n\n    ping_exclude_bKGD,\n    ping_exclude_cHRM,\n    ping_exclude_date,\n    /* ping_exclude_EXIF, */\n    ping_exclude_eXIf,\n    ping_exclude_gAMA,\n    ping_exclude_iCCP,\n    /* ping_exclude_iTXt, */\n    ping_exclude_oFFs,\n    ping_exclude_pHYs,\n    ping_exclude_sRGB,\n    ping_exclude_tEXt,\n    ping_exclude_tIME,\n    /* ping_exclude_tRNS, */\n    ping_exclude_vpAg,\n    ping_exclude_caNv,\n    ping_exclude_zCCP, /* hex-encoded iCCP */\n    ping_exclude_zTXt,\n\n    ping_preserve_colormap,\n    ping_preserve_iCCP,\n    ping_need_colortype_warning,\n\n    status,\n    tried_332,\n    tried_333,\n    tried_444;\n\n  MemoryInfo\n    *volatile pixel_info;\n\n  QuantumInfo\n    *quantum_info;\n\n  PNGErrorInfo\n    error_info;\n\n  register ssize_t\n    i,\n    x;\n\n  unsigned char\n    *ping_pixels;\n\n  volatile int\n    image_colors,\n    ping_bit_depth,\n    ping_color_type,\n    ping_interlace_method,\n    ping_compression_method,\n    ping_filter_method,\n    ping_num_trans;\n\n  volatile size_t\n    image_depth,\n    old_bit_depth;\n\n  size_t\n    quality,\n    rowbytes,\n    save_image_depth;\n\n  int\n    j,\n    number_colors,\n    number_opaque,\n    number_semitransparent,\n    number_transparent,\n    ping_pHYs_unit_type;\n\n  png_uint_32\n    ping_pHYs_x_resolution,\n    ping_pHYs_y_resolution;\n\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\n    \"  Enter WriteOnePNGImage()\");\n\n  image = CloneImage(IMimage,0,0,MagickFalse,exception);\n  if (image == (Image *) NULL)\n    return(MagickFalse);\n  image_info=(ImageInfo *) CloneImageInfo(IMimage_info);\n  if (image_info == (ImageInfo *) NULL)\n    ThrowWriterException(ResourceLimitError, \"MemoryAllocationFailed\");\n\n  /* Define these outside of the following \"if logging()\" block so they will\n   * show in debuggers.\n   */\n  *im_vers='\\0';\n  (void) ConcatenateMagickString(im_vers,\n         MagickLibVersionText,MagickPathExtent);\n  (void) ConcatenateMagickString(im_vers,\n         MagickLibAddendum,MagickPathExtent);\n\n  *libpng_vers='\\0';\n  (void) ConcatenateMagickString(libpng_vers,\n         PNG_LIBPNG_VER_STRING,32);\n  *libpng_runv='\\0';\n  (void) ConcatenateMagickString(libpng_runv,\n         png_get_libpng_ver(NULL),32);\n\n  *zlib_vers='\\0';\n  (void) ConcatenateMagickString(zlib_vers,\n         ZLIB_VERSION,32);\n  *zlib_runv='\\0';\n  (void) ConcatenateMagickString(zlib_runv,\n         zlib_version,32);\n\n  if (logging != MagickFalse)\n    {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    IM version     = %s\",\n           im_vers);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    Libpng version = %s\",\n           libpng_vers);\n       if (LocaleCompare(libpng_vers,libpng_runv) != 0)\n       {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"      running with   %s\",\n           libpng_runv);\n       }\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"    Zlib version   = %s\",\n           zlib_vers);\n       if (LocaleCompare(zlib_vers,zlib_runv) != 0)\n       {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\"      running with   %s\",\n           zlib_runv);\n       }\n    }\n\n  /* Initialize some stuff */\n  ping_bit_depth=0,\n  ping_color_type=0,\n  ping_interlace_method=0,\n  ping_compression_method=0,\n  ping_filter_method=0,\n  ping_num_trans = 0;\n\n  ping_background.red = 0;\n  ping_background.green = 0;\n  ping_background.blue = 0;\n  ping_background.gray = 0;\n  ping_background.index = 0;\n\n  ping_trans_color.red=0;\n  ping_trans_color.green=0;\n  ping_trans_color.blue=0;\n  ping_trans_color.gray=0;\n\n  ping_pHYs_unit_type = 0;\n  ping_pHYs_x_resolution = 0;\n  ping_pHYs_y_resolution = 0;\n\n  ping_have_blob=MagickFalse;\n  ping_have_cheap_transparency=MagickFalse;\n  ping_have_color=MagickTrue;\n  ping_have_non_bw=MagickTrue;\n  ping_have_PLTE=MagickFalse;\n  ping_have_bKGD=MagickFalse;\n  ping_have_eXIf=MagickTrue;\n  ping_have_iCCP=MagickFalse;\n  ping_have_pHYs=MagickFalse;\n  ping_have_sRGB=MagickFalse;\n  ping_have_tRNS=MagickFalse;\n\n  ping_exclude_bKGD=mng_info->ping_exclude_bKGD;\n  ping_exclude_caNv=mng_info->ping_exclude_caNv;\n  ping_exclude_cHRM=mng_info->ping_exclude_cHRM;\n  ping_exclude_date=mng_info->ping_exclude_date;\n  ping_exclude_eXIf=mng_info->ping_exclude_eXIf;\n  ping_exclude_gAMA=mng_info->ping_exclude_gAMA;\n  ping_exclude_iCCP=mng_info->ping_exclude_iCCP;\n  /* ping_exclude_iTXt=mng_info->ping_exclude_iTXt; */\n  ping_exclude_oFFs=mng_info->ping_exclude_oFFs;\n  ping_exclude_pHYs=mng_info->ping_exclude_pHYs;\n  ping_exclude_sRGB=mng_info->ping_exclude_sRGB;\n  ping_exclude_tEXt=mng_info->ping_exclude_tEXt;\n  ping_exclude_tIME=mng_info->ping_exclude_tIME;\n  /* ping_exclude_tRNS=mng_info->ping_exclude_tRNS; */\n  ping_exclude_vpAg=mng_info->ping_exclude_vpAg;\n  ping_exclude_zCCP=mng_info->ping_exclude_zCCP; /* hex-encoded iCCP in zTXt */\n  ping_exclude_zTXt=mng_info->ping_exclude_zTXt;\n\n  ping_preserve_colormap = mng_info->ping_preserve_colormap;\n  ping_preserve_iCCP = mng_info->ping_preserve_iCCP;\n  ping_need_colortype_warning = MagickFalse;\n\n  /* Recognize the ICC sRGB profile and convert it to the sRGB chunk,\n   * i.e., eliminate the ICC profile and set image->rendering_intent.\n   * Note that this will not involve any changes to the actual pixels\n   * but merely passes information to applications that read the resulting\n   * PNG image.\n   *\n   * To do: recognize other variants of the sRGB profile, using the CRC to\n   * verify all recognized variants including the 7 already known.\n   *\n   * Work around libpng16+ rejecting some \"known invalid sRGB profiles\".\n   *\n   * Use something other than image->rendering_intent to record the fact\n   * that the sRGB profile was found.\n   *\n   * Record the ICC version (currently v2 or v4) of the incoming sRGB ICC\n   * profile.  Record the Blackpoint Compensation, if any.\n   */\n   if (ping_exclude_sRGB == MagickFalse && ping_preserve_iCCP == MagickFalse)\n   {\n      char\n        *name;\n\n      const StringInfo\n        *profile;\n\n      ResetImageProfileIterator(image);\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        profile=GetImageProfile(image,name);\n\n        if (profile != (StringInfo *) NULL)\n          {\n            if ((LocaleCompare(name,\"ICC\") == 0) ||\n                (LocaleCompare(name,\"ICM\") == 0))\n\n             {\n                 int\n                   icheck,\n                   got_crc=0;\n\n\n                 png_uint_32\n                   length,\n                   profile_crc=0;\n\n                 unsigned char\n                   *data;\n\n                 length=(png_uint_32) GetStringInfoLength(profile);\n\n                 for (icheck=0; sRGB_info[icheck].len > 0; icheck++)\n                 {\n                   if (length == sRGB_info[icheck].len)\n                   {\n                     if (got_crc == 0)\n                     {\n                       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                         \"    Got a %lu-byte ICC profile (potentially sRGB)\",\n                         (unsigned long) length);\n\n                       data=GetStringInfoDatum(profile);\n                       profile_crc=crc32(0,data,length);\n\n                       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"      with crc=%8x\",(unsigned int) profile_crc);\n                       got_crc++;\n                     }\n\n                     if (profile_crc == sRGB_info[icheck].crc)\n                     {\n                        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                            \"      It is sRGB with rendering intent = %s\",\n                        Magick_RenderingIntentString_from_PNG_RenderingIntent(\n                             sRGB_info[icheck].intent));\n                        if (image->rendering_intent==UndefinedIntent)\n                        {\n                          image->rendering_intent=\n                          Magick_RenderingIntent_from_PNG_RenderingIntent(\n                             sRGB_info[icheck].intent);\n                        }\n                        ping_exclude_iCCP = MagickTrue;\n                        ping_exclude_zCCP = MagickTrue;\n                        ping_have_sRGB = MagickTrue;\n                        break;\n                     }\n                   }\n                 }\n                 if (sRGB_info[icheck].len == 0)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"    Got %lu-byte ICC profile not recognized as sRGB\",\n                        (unsigned long) length);\n              }\n          }\n        name=GetNextImageProfile(image);\n      }\n  }\n\n  number_opaque = 0;\n  number_semitransparent = 0;\n  number_transparent = 0;\n\n  if (logging != MagickFalse)\n    {\n      if (image->storage_class == UndefinedClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=UndefinedClass\");\n      if (image->storage_class == DirectClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=DirectClass\");\n      if (image->storage_class == PseudoClass)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    image->storage_class=PseudoClass\");\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(), image->taint ?\n          \"    image->taint=MagickTrue\":\n          \"    image->taint=MagickFalse\");\n    }\n\n  if (image->storage_class == PseudoClass &&\n     (mng_info->write_png8 || mng_info->write_png24 || mng_info->write_png32 ||\n     mng_info->write_png48 || mng_info->write_png64 ||\n     (mng_info->write_png_colortype != 1 &&\n     mng_info->write_png_colortype != 5)))\n    {\n      (void) SyncImage(image,exception);\n      image->storage_class = DirectClass;\n    }\n\n  if (ping_preserve_colormap == MagickFalse)\n    {\n      if (image->storage_class != PseudoClass && image->colormap != NULL)\n        {\n          /* Free the bogus colormap; it can cause trouble later */\n           if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Freeing bogus colormap\");\n           (void) RelinquishMagickMemory(image->colormap);\n           image->colormap=NULL;\n        }\n    }\n\n  if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n    (void) TransformImageColorspace(image,sRGBColorspace,exception);\n\n  /*\n    Sometimes we get PseudoClass images whose RGB values don't match\n    the colors in the colormap.  This code syncs the RGB values.\n  */\n  if (image->depth <= 8 && image->taint && image->storage_class == PseudoClass)\n     (void) SyncImage(image,exception);\n\n#if (MAGICKCORE_QUANTUM_DEPTH == 8)\n  if (image->depth > 8)\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Reducing PNG bit depth to 8 since this is a Q8 build.\");\n\n      image->depth=8;\n    }\n#endif\n\n  /* Respect the -depth option */\n  if (image->depth < 4)\n    {\n       register Quantum\n         *r;\n\n       if (image->depth > 2)\n         {\n           /* Scale to 4-bit */\n           LBR04PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR04PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR04PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n       else if (image->depth > 1)\n         {\n           /* Scale to 2-bit */\n           LBR02PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR02PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR02PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n       else\n         {\n           /* Scale to 1-bit */\n           LBR01PacketRGBO(image->background_color);\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (r == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                LBR01PixelRGBA(r);\n                r+=GetPixelChannels(image);\n             }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                break;\n           }\n\n           if (image->storage_class == PseudoClass && image->colormap != NULL)\n           {\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               LBR01PacketRGBO(image->colormap[i]);\n             }\n           }\n         }\n    }\n\n  /* To do: set to next higher multiple of 8 */\n  if (image->depth < 8)\n     image->depth=8;\n\n#if (MAGICKCORE_QUANTUM_DEPTH > 16)\n  /* PNG does not handle depths greater than 16 so reduce it even\n   * if lossy\n   */\n  if (image->depth > 8)\n      image->depth=16;\n#endif\n\n#if (MAGICKCORE_QUANTUM_DEPTH > 8)\n  if (image->depth > 8)\n    {\n      /* To do: fill low byte properly */\n      image->depth=16;\n    }\n\n  if (image->depth == 16 && mng_info->write_png_depth != 16)\n    if (mng_info->write_png8 ||\n        LosslessReduceDepthOK(image,exception) != MagickFalse)\n      image->depth = 8;\n#endif\n\n  image_colors = (int) image->colors;\n  number_opaque = (int) image->colors;\n  number_transparent = 0;\n  number_semitransparent = 0;\n\n  if (mng_info->write_png_colortype &&\n     (mng_info->write_png_colortype > 4 || (mng_info->write_png_depth >= 8 &&\n     mng_info->write_png_colortype < 4 &&\n     image->alpha_trait == UndefinedPixelTrait)))\n  {\n     /* Avoid the expensive BUILD_PALETTE operation if we're sure that we\n      * are not going to need the result.\n      */\n     if (mng_info->write_png_colortype == 1 ||\n        mng_info->write_png_colortype == 5)\n       ping_have_color=MagickFalse;\n\n     if (image->alpha_trait != UndefinedPixelTrait)\n       {\n         number_transparent = 2;\n         number_semitransparent = 1;\n       }\n  }\n\n  if (mng_info->write_png_colortype < 7)\n  {\n  /* BUILD_PALETTE\n   *\n   * Normally we run this just once, but in the case of writing PNG8\n   * we reduce the transparency to binary and run again, then if there\n   * are still too many colors we reduce to a simple 4-4-4-1, then 3-3-3-1\n   * RGBA palette and run again, and then to a simple 3-3-2-1 RGBA\n   * palette.  Then (To do) we take care of a final reduction that is only\n   * needed if there are still 256 colors present and one of them has both\n   * transparent and opaque instances.\n   */\n\n  tried_332 = MagickFalse;\n  tried_333 = MagickFalse;\n  tried_444 = MagickFalse;\n\n  for (j=0; j<6; j++)\n  {\n    /*\n     * Sometimes we get DirectClass images that have 256 colors or fewer.\n     * This code will build a colormap.\n     *\n     * Also, sometimes we get PseudoClass images with an out-of-date\n     * colormap.  This code will replace the colormap with a new one.\n     * Sometimes we get PseudoClass images that have more than 256 colors.\n     * This code will delete the colormap and change the image to\n     * DirectClass.\n     *\n     * If image->alpha_trait is MagickFalse, we ignore the alpha channel\n     * even though it sometimes contains left-over non-opaque values.\n     *\n     * Also we gather some information (number of opaque, transparent,\n     * and semitransparent pixels, and whether the image has any non-gray\n     * pixels or only black-and-white pixels) that we might need later.\n     *\n     * Even if the user wants to force GrayAlpha or RGBA (colortype 4 or 6)\n     * we need to check for bogus non-opaque values, at least.\n     */\n\n   int\n     n;\n\n   PixelInfo\n     opaque[260],\n     semitransparent[260],\n     transparent[260];\n\n   register const Quantum\n     *s;\n\n   register Quantum\n     *q,\n     *r;\n\n   if (logging != MagickFalse)\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n         \"    Enter BUILD_PALETTE:\");\n\n   if (logging != MagickFalse)\n     {\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->columns=%.20g\",(double) image->columns);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->rows=%.20g\",(double) image->rows);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->alpha_trait=%.20g\",(double) image->alpha_trait);\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      image->depth=%.20g\",(double) image->depth);\n\n       if (image->storage_class == PseudoClass && image->colormap != NULL)\n       {\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      Original colormap:\");\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"        i    (red,green,blue,alpha)\");\n\n         for (i=0; i < 256; i++)\n         {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"        %d    (%d,%d,%d,%d)\",\n                    (int) i,\n                    (int) image->colormap[i].red,\n                    (int) image->colormap[i].green,\n                    (int) image->colormap[i].blue,\n                    (int) image->colormap[i].alpha);\n         }\n\n         for (i=image->colors - 10; i < (ssize_t) image->colors; i++)\n         {\n           if (i > 255)\n             {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"        %d    (%d,%d,%d,%d)\",\n                    (int) i,\n                    (int) image->colormap[i].red,\n                    (int) image->colormap[i].green,\n                    (int) image->colormap[i].blue,\n                    (int) image->colormap[i].alpha);\n             }\n         }\n       }\n\n       (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n           \"      image->colors=%d\",(int) image->colors);\n\n       if (image->colors == 0)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"        (zero means unknown)\");\n\n       if (ping_preserve_colormap == MagickFalse)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"      Regenerate the colormap\");\n     }\n\n     image_colors=0;\n     number_opaque = 0;\n     number_semitransparent = 0;\n     number_transparent = 0;\n\n     for (y=0; y < (ssize_t) image->rows; y++)\n     {\n       q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n       if (q == (Quantum *) NULL)\n         break;\n\n       for (x=0; x < (ssize_t) image->columns; x++)\n       {\n           if (image->alpha_trait == UndefinedPixelTrait ||\n              GetPixelAlpha(image,q) == OpaqueAlpha)\n             {\n               if (number_opaque < 259)\n                 {\n                   if (number_opaque == 0)\n                     {\n                       GetPixelInfoPixel(image, q, opaque);\n                       opaque[0].alpha=OpaqueAlpha;\n                       number_opaque=1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_opaque; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,opaque+i))\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_opaque && number_opaque < 259)\n                     {\n                       number_opaque++;\n                       GetPixelInfoPixel(image, q, opaque+i);\n                       opaque[i].alpha=OpaqueAlpha;\n                     }\n                 }\n             }\n           else if (GetPixelAlpha(image,q) == TransparentAlpha)\n             {\n               if (number_transparent < 259)\n                 {\n                   if (number_transparent == 0)\n                     {\n                       GetPixelInfoPixel(image, q, transparent);\n                       ping_trans_color.red=(unsigned short)\n                         GetPixelRed(image,q);\n                       ping_trans_color.green=(unsigned short)\n                         GetPixelGreen(image,q);\n                       ping_trans_color.blue=(unsigned short)\n                         GetPixelBlue(image,q);\n                       ping_trans_color.gray=(unsigned short)\n                         GetPixelGray(image,q);\n                       number_transparent = 1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_transparent; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,transparent+i))\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_transparent &&\n                       number_transparent < 259)\n                     {\n                       number_transparent++;\n                       GetPixelInfoPixel(image,q,transparent+i);\n                     }\n                 }\n             }\n           else\n             {\n               if (number_semitransparent < 259)\n                 {\n                   if (number_semitransparent == 0)\n                     {\n                       GetPixelInfoPixel(image,q,semitransparent);\n                       number_semitransparent = 1;\n                     }\n\n                   for (i=0; i< (ssize_t) number_semitransparent; i++)\n                     {\n                       if (Magick_png_color_equal(image,q,semitransparent+i)\n                           && GetPixelAlpha(image,q) ==\n                           semitransparent[i].alpha)\n                         break;\n                     }\n\n                   if (i ==  (ssize_t) number_semitransparent &&\n                       number_semitransparent < 259)\n                     {\n                       number_semitransparent++;\n                       GetPixelInfoPixel(image, q, semitransparent+i);\n                     }\n                 }\n             }\n           q+=GetPixelChannels(image);\n        }\n     }\n\n     if (mng_info->write_png8 == MagickFalse &&\n         ping_exclude_bKGD == MagickFalse)\n       {\n         /* Add the background color to the palette, if it\n          * isn't already there.\n          */\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      Check colormap for background (%d,%d,%d)\",\n                  (int) image->background_color.red,\n                  (int) image->background_color.green,\n                  (int) image->background_color.blue);\n            }\n          for (i=0; i<number_opaque; i++)\n          {\n             if (opaque[i].red == image->background_color.red &&\n                 opaque[i].green == image->background_color.green &&\n                 opaque[i].blue == image->background_color.blue)\n               break;\n          }\n          if (number_opaque < 259 && i == number_opaque)\n            {\n               opaque[i] = image->background_color;\n               ping_background.index = i;\n               number_opaque++;\n               if (logging != MagickFalse)\n                 {\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"      background_color index is %d\",(int) i);\n                 }\n\n            }\n          else if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      No room in the colormap to add background color\");\n       }\n\n     image_colors=number_opaque+number_transparent+number_semitransparent;\n\n     if (logging != MagickFalse)\n       {\n         if (image_colors > 256)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      image has more than 256 colors\");\n\n         else\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      image has %d colors\",image_colors);\n       }\n\n     if (ping_preserve_colormap != MagickFalse)\n       break;\n\n     if (mng_info->write_png_colortype != 7) /* We won't need this info */\n       {\n         ping_have_color=MagickFalse;\n         ping_have_non_bw=MagickFalse;\n\n         if (IssRGBCompatibleColorspace(image->colorspace) == MagickFalse)\n         {\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"incompatible colorspace\");\n           ping_have_color=MagickTrue;\n           ping_have_non_bw=MagickTrue;\n         }\n\n         if(image_colors > 256)\n           {\n             for (y=0; y < (ssize_t) image->rows; y++)\n             {\n               q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n               if (q == (Quantum *) NULL)\n                 break;\n\n               s=q;\n               for (x=0; x < (ssize_t) image->columns; x++)\n               {\n                 if (GetPixelRed(image,s) != GetPixelGreen(image,s) ||\n                     GetPixelRed(image,s) != GetPixelBlue(image,s))\n                   {\n                      ping_have_color=MagickTrue;\n                      ping_have_non_bw=MagickTrue;\n                      break;\n                   }\n                 s+=GetPixelChannels(image);\n               }\n\n               if (ping_have_color != MagickFalse)\n                 break;\n\n               /* Worst case is black-and-white; we are looking at every\n                * pixel twice.\n                */\n\n               if (ping_have_non_bw == MagickFalse)\n                 {\n                   s=q;\n                   for (x=0; x < (ssize_t) image->columns; x++)\n                   {\n                     if (GetPixelRed(image,s) != 0 &&\n                         GetPixelRed(image,s) != QuantumRange)\n                       {\n                         ping_have_non_bw=MagickTrue;\n                         break;\n                       }\n                     s+=GetPixelChannels(image);\n                   }\n               }\n             }\n           }\n       }\n\n     if (image_colors < 257)\n       {\n         PixelInfo\n           colormap[260];\n\n         /*\n          * Initialize image colormap.\n          */\n\n         if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      Sort the new colormap\");\n\n        /* Sort palette, transparent first */;\n\n         n = 0;\n\n         for (i=0; i<number_transparent; i++)\n            colormap[n++] = transparent[i];\n\n         for (i=0; i<number_semitransparent; i++)\n            colormap[n++] = semitransparent[i];\n\n         for (i=0; i<number_opaque; i++)\n            colormap[n++] = opaque[i];\n\n         ping_background.index +=\n           (number_transparent + number_semitransparent);\n\n         /* image_colors < 257; search the colormap instead of the pixels\n          * to get ping_have_color and ping_have_non_bw\n          */\n         for (i=0; i<n; i++)\n         {\n           if (ping_have_color == MagickFalse)\n             {\n                if (colormap[i].red != colormap[i].green ||\n                    colormap[i].red != colormap[i].blue)\n                  {\n                     ping_have_color=MagickTrue;\n                     ping_have_non_bw=MagickTrue;\n                     break;\n                  }\n              }\n\n           if (ping_have_non_bw == MagickFalse)\n             {\n               if (colormap[i].red != 0 && colormap[i].red != QuantumRange)\n                   ping_have_non_bw=MagickTrue;\n             }\n          }\n\n        if ((mng_info->ping_exclude_tRNS == MagickFalse ||\n            (number_transparent == 0 && number_semitransparent == 0)) &&\n            (((mng_info->write_png_colortype-1) ==\n            PNG_COLOR_TYPE_PALETTE) ||\n            (mng_info->write_png_colortype == 0)))\n          {\n            if (logging != MagickFalse)\n              {\n                if (n !=  (ssize_t) image_colors)\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"   image_colors (%d) and n (%d)  don't match\",\n                   image_colors, n);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      AcquireImageColormap\");\n              }\n\n            image->colors = image_colors;\n\n            if (AcquireImageColormap(image,image_colors,exception) ==\n                MagickFalse)\n               ThrowWriterException(ResourceLimitError,\n                   \"MemoryAllocationFailed\");\n\n            for (i=0; i< (ssize_t) image_colors; i++)\n               image->colormap[i] = colormap[i];\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"      image->colors=%d (%d)\",\n                      (int) image->colors, image_colors);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"      Update the pixel indexes\");\n              }\n\n            /* Sync the pixel indices with the new colormap */\n\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n              if (q == (Quantum *) NULL)\n                break;\n\n              for (x=0; x < (ssize_t) image->columns; x++)\n              {\n                for (i=0; i< (ssize_t) image_colors; i++)\n                {\n                  if ((image->alpha_trait == UndefinedPixelTrait ||\n                      image->colormap[i].alpha == GetPixelAlpha(image,q)) &&\n                      image->colormap[i].red == GetPixelRed(image,q) &&\n                      image->colormap[i].green == GetPixelGreen(image,q) &&\n                      image->colormap[i].blue == GetPixelBlue(image,q))\n                  {\n                    SetPixelIndex(image,i,q);\n                    break;\n                  }\n                }\n                q+=GetPixelChannels(image);\n              }\n\n              if (SyncAuthenticPixels(image,exception) == MagickFalse)\n                 break;\n            }\n          }\n       }\n\n     if (logging != MagickFalse)\n       {\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"      image->colors=%d\", (int) image->colors);\n\n         if (image->colormap != NULL)\n           {\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"       i     (red,green,blue,alpha)\");\n\n             for (i=0; i < (ssize_t) image->colors; i++)\n             {\n               if (i < 300 || i >= (ssize_t) image->colors - 10)\n                 {\n                   (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"       %d     (%d,%d,%d,%d)\",\n                        (int) i,\n                        (int) image->colormap[i].red,\n                        (int) image->colormap[i].green,\n                        (int) image->colormap[i].blue,\n                        (int) image->colormap[i].alpha);\n                 }\n             }\n           }\n\n           if (number_transparent < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_transparent     = %d\",\n                   number_transparent);\n           else\n\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_transparent     > 256\");\n\n           if (number_opaque < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_opaque          = %d\",\n                   number_opaque);\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_opaque          > 256\");\n\n           if (number_semitransparent < 257)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_semitransparent = %d\",\n                   number_semitransparent);\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      number_semitransparent > 256\");\n\n           if (ping_have_non_bw == MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      All pixels and the background are black or white\");\n\n           else if (ping_have_color == MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      All pixels and the background are gray\");\n\n           else\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      At least one pixel or the background is non-gray\");\n\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Exit BUILD_PALETTE:\");\n       }\n\n   if (mng_info->write_png8 == MagickFalse)\n      break;\n\n   /* Make any reductions necessary for the PNG8 format */\n    if (image_colors <= 256 &&\n        image_colors != 0 && image->colormap != NULL &&\n        number_semitransparent == 0 &&\n        number_transparent <= 1)\n      break;\n\n    /* PNG8 can't have semitransparent colors so we threshold the\n     * opacity to 0 or OpaqueOpacity, and PNG8 can only have one\n     * transparent color so if more than one is transparent we merge\n     * them into image->background_color.\n     */\n    if (number_semitransparent != 0 || number_transparent > 1)\n      {\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Thresholding the alpha channel to binary\");\n\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n          if (r == (Quantum *) NULL)\n            break;\n\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n              if (GetPixelAlpha(image,r) < OpaqueAlpha/2)\n                {\n                  SetPixelViaPixelInfo(image,&image->background_color,r);\n                  SetPixelAlpha(image,TransparentAlpha,r);\n                }\n              else\n                  SetPixelAlpha(image,OpaqueAlpha,r);\n              r+=GetPixelChannels(image);\n          }\n\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n             break;\n\n          if (image_colors != 0 && image_colors <= 256 &&\n             image->colormap != NULL)\n            for (i=0; i<image_colors; i++)\n                image->colormap[i].alpha =\n                    (image->colormap[i].alpha > TransparentAlpha/2 ?\n                    TransparentAlpha : OpaqueAlpha);\n        }\n      continue;\n    }\n\n    /* PNG8 can't have more than 256 colors so we quantize the pixels and\n     * background color to the 4-4-4-1, 3-3-3-1 or 3-3-2-1 palette.  If the\n     * image is mostly gray, the 4-4-4-1 palette is likely to end up with 256\n     * colors or less.\n     */\n    if (tried_444 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 4-4-4\");\n\n        tried_444 = MagickTrue;\n\n        LBR04PacketRGB(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 4-4-4\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR04PixelRGB(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 4-4-4\");\n\n          for (i=0; i<image_colors; i++)\n          {\n            LBR04PacketRGB(image->colormap[i]);\n          }\n        }\n        continue;\n      }\n\n    if (tried_333 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 3-3-3\");\n\n        tried_333 = MagickTrue;\n\n        LBR03PacketRGB(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 3-3-3-1\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR03RGB(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 3-3-3-1\");\n          for (i=0; i<image_colors; i++)\n          {\n              LBR03PacketRGB(image->colormap[i]);\n          }\n        }\n        continue;\n      }\n\n    if (tried_332 == MagickFalse && (image_colors == 0 || image_colors > 256))\n      {\n        if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"    Quantizing the background color to 3-3-2\");\n\n        tried_332 = MagickTrue;\n\n        /* Red and green were already done so we only quantize the blue\n         * channel\n         */\n\n        LBR02PacketBlue(image->background_color);\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the pixel colors to 3-3-2-1\");\n\n        if (image->colormap == NULL)\n        {\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n            if (r == (Quantum *) NULL)\n              break;\n\n            for (x=0; x < (ssize_t) image->columns; x++)\n            {\n              if (GetPixelAlpha(image,r) == OpaqueAlpha)\n                  LBR02PixelBlue(r);\n              r+=GetPixelChannels(image);\n            }\n\n            if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n          }\n        }\n\n        else /* Should not reach this; colormap already exists and\n                must be <= 256 */\n        {\n          if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    Quantizing the colormap to 3-3-2-1\");\n          for (i=0; i<image_colors; i++)\n          {\n              LBR02PacketBlue(image->colormap[i]);\n          }\n      }\n      continue;\n    }\n\n    if (image_colors == 0 || image_colors > 256)\n    {\n      /* Take care of special case with 256 opaque colors + 1 transparent\n       * color.  We don't need to quantize to 2-3-2-1; we only need to\n       * eliminate one color, so we'll merge the two darkest red\n       * colors (0x49, 0, 0) -> (0x24, 0, 0).\n       */\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Merging two dark red background colors to 3-3-2-1\");\n\n      if (ScaleQuantumToChar(image->background_color.red) == 0x49 &&\n          ScaleQuantumToChar(image->background_color.green) == 0x00 &&\n          ScaleQuantumToChar(image->background_color.blue) == 0x00)\n      {\n         image->background_color.red=ScaleCharToQuantum(0x24);\n      }\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Merging two dark red pixel colors to 3-3-2-1\");\n\n      if (image->colormap == NULL)\n      {\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          r=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n          if (r == (Quantum *) NULL)\n            break;\n\n          for (x=0; x < (ssize_t) image->columns; x++)\n          {\n            if (ScaleQuantumToChar(GetPixelRed(image,r)) == 0x49 &&\n                ScaleQuantumToChar(GetPixelGreen(image,r)) == 0x00 &&\n                ScaleQuantumToChar(GetPixelBlue(image,r)) == 0x00 &&\n                GetPixelAlpha(image,r) == OpaqueAlpha)\n              {\n                SetPixelRed(image,ScaleCharToQuantum(0x24),r);\n              }\n            r+=GetPixelChannels(image);\n          }\n\n          if (SyncAuthenticPixels(image,exception) == MagickFalse)\n             break;\n\n        }\n      }\n\n      else\n      {\n         for (i=0; i<image_colors; i++)\n         {\n            if (ScaleQuantumToChar(image->colormap[i].red) == 0x49 &&\n                ScaleQuantumToChar(image->colormap[i].green) == 0x00 &&\n                ScaleQuantumToChar(image->colormap[i].blue) == 0x00)\n            {\n               image->colormap[i].red=ScaleCharToQuantum(0x24);\n            }\n         }\n      }\n    }\n  }\n  }\n  /* END OF BUILD_PALETTE */\n\n  /* If we are excluding the tRNS chunk and there is transparency,\n   * then we must write a Gray-Alpha (color-type 4) or RGBA (color-type 6)\n   * PNG.\n   */\n  if (mng_info->ping_exclude_tRNS != MagickFalse &&\n     (number_transparent != 0 || number_semitransparent != 0))\n    {\n      unsigned int colortype=mng_info->write_png_colortype;\n\n      if (ping_have_color == MagickFalse)\n        mng_info->write_png_colortype = 5;\n\n      else\n        mng_info->write_png_colortype = 7;\n\n      if (colortype != 0 &&\n         mng_info->write_png_colortype != colortype)\n        ping_need_colortype_warning=MagickTrue;\n\n    }\n\n  /* See if cheap transparency is possible.  It is only possible\n   * when there is a single transparent color, no semitransparent\n   * color, and no opaque color that has the same RGB components\n   * as the transparent color.  We only need this information if\n   * we are writing a PNG with colortype 0 or 2, and we have not\n   * excluded the tRNS chunk.\n   */\n  if (number_transparent == 1 &&\n      mng_info->write_png_colortype < 4)\n    {\n       ping_have_cheap_transparency = MagickTrue;\n\n       if (number_semitransparent != 0)\n         ping_have_cheap_transparency = MagickFalse;\n\n       else if (image_colors == 0 || image_colors > 256 ||\n           image->colormap == NULL)\n         {\n           register const Quantum\n             *q;\n\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             q=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n             if (q == (Quantum *) NULL)\n               break;\n\n             for (x=0; x < (ssize_t) image->columns; x++)\n             {\n                 if (GetPixelAlpha(image,q) != TransparentAlpha &&\n                     (unsigned short) GetPixelRed(image,q) ==\n                                     ping_trans_color.red &&\n                     (unsigned short) GetPixelGreen(image,q) ==\n                                     ping_trans_color.green &&\n                     (unsigned short) GetPixelBlue(image,q) ==\n                                     ping_trans_color.blue)\n                   {\n                     ping_have_cheap_transparency = MagickFalse;\n                     break;\n                   }\n\n                 q+=GetPixelChannels(image);\n             }\n\n             if (ping_have_cheap_transparency == MagickFalse)\n                break;\n           }\n         }\n       else\n         {\n            /* Assuming that image->colormap[0] is the one transparent color\n             * and that all others are opaque.\n             */\n            if (image_colors > 1)\n              for (i=1; i<image_colors; i++)\n                if (image->colormap[i].red == image->colormap[0].red &&\n                    image->colormap[i].green == image->colormap[0].green &&\n                    image->colormap[i].blue == image->colormap[0].blue)\n                  {\n                     ping_have_cheap_transparency = MagickFalse;\n                     break;\n                  }\n         }\n\n       if (logging != MagickFalse)\n         {\n           if (ping_have_cheap_transparency == MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"   Cheap transparency is not possible.\");\n\n           else\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"   Cheap transparency is possible.\");\n         }\n     }\n  else\n    ping_have_cheap_transparency = MagickFalse;\n\n  image_depth=image->depth;\n\n  quantum_info = (QuantumInfo *) NULL;\n  number_colors=0;\n  image_colors=(int) image->colors;\n  image_matte=image->alpha_trait !=\n        UndefinedPixelTrait ? MagickTrue : MagickFalse;\n\n  if (mng_info->write_png_colortype < 5)\n    mng_info->IsPalette=image->storage_class == PseudoClass &&\n      image_colors <= 256 && image->colormap != NULL;\n  else\n    mng_info->IsPalette = MagickFalse;\n\n  if ((mng_info->write_png_colortype == 4 || mng_info->write_png8) &&\n     (image->colors == 0 || image->colormap == NULL))\n    {\n      image_info=DestroyImageInfo(image_info);\n      image=DestroyImage(image);\n      (void) ThrowMagickException(exception,GetMagickModule(),CoderError,\n          \"Cannot write PNG8 or color-type 3; colormap is NULL\",\n          \"`%s'\",IMimage->filename);\n      return(MagickFalse);\n    }\n\n  /*\n    Allocate the PNG structures\n  */\n#ifdef PNG_USER_MEM_SUPPORTED\n error_info.image=image;\n error_info.exception=exception;\n  ping=png_create_write_struct_2(PNG_LIBPNG_VER_STRING,&error_info,\n    MagickPNGErrorHandler,MagickPNGWarningHandler,(void *) NULL,\n    (png_malloc_ptr) Magick_png_malloc,(png_free_ptr) Magick_png_free);\n\n#else\n  ping=png_create_write_struct(PNG_LIBPNG_VER_STRING,&error_info,\n    MagickPNGErrorHandler,MagickPNGWarningHandler);\n\n#endif\n  if (ping == (png_struct *) NULL)\n    ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n  ping_info=png_create_info_struct(ping);\n\n  if (ping_info == (png_info *) NULL)\n    {\n      png_destroy_write_struct(&ping,(png_info **) NULL);\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n\n  png_set_write_fn(ping,image,png_put_data,png_flush_data);\n  pixel_info=(MemoryInfo *) NULL;\n\n  if (setjmp(png_jmpbuf(ping)))\n    {\n      /*\n        PNG write failed.\n      */\n#ifdef PNG_DEBUG\n     if (image_info->verbose)\n        (void) printf(\"PNG write has failed.\\n\");\n#endif\n      png_destroy_write_struct(&ping,&ping_info);\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n      UnlockSemaphoreInfo(ping_semaphore);\n#endif\n\n      if (pixel_info != (MemoryInfo *) NULL)\n        pixel_info=RelinquishVirtualMemory(pixel_info);\n\n      if (quantum_info != (QuantumInfo *) NULL)\n        quantum_info=DestroyQuantumInfo(quantum_info);\n\n      if (ping_have_blob != MagickFalse)\n          (void) CloseBlob(image);\n      image_info=DestroyImageInfo(image_info);\n      image=DestroyImage(image);\n      return(MagickFalse);\n    }\n\n  /* {  For navigation to end of SETJMP-protected block.  Within this\n   *    block, use png_error() instead of Throwing an Exception, to ensure\n   *    that libpng is able to clean up, and that the semaphore is unlocked.\n   */\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  LockSemaphoreInfo(ping_semaphore);\n#endif\n\n#ifdef PNG_BENIGN_ERRORS_SUPPORTED\n  /* Allow benign errors */\n  png_set_benign_errors(ping, 1);\n#endif\n\n#ifdef PNG_SET_USER_LIMITS_SUPPORTED\n  /* Reject images with too many rows or columns */\n  png_set_user_limits(ping,\n    (png_uint_32) MagickMin(0x7fffffffL,\n        GetMagickResourceLimit(WidthResource)),\n    (png_uint_32) MagickMin(0x7fffffffL,\n        GetMagickResourceLimit(HeightResource)));\n#endif /* PNG_SET_USER_LIMITS_SUPPORTED */\n\n  /*\n    Prepare PNG for writing.\n  */\n\n#if defined(PNG_MNG_FEATURES_SUPPORTED)\n  if (mng_info->write_mng)\n  {\n     (void) png_permit_mng_features(ping,PNG_ALL_MNG_FEATURES);\n# ifdef PNG_WRITE_CHECK_FOR_INVALID_INDEX_SUPPORTED\n     /* Disable new libpng-1.5.10 feature when writing a MNG because\n      * zero-length PLTE is OK\n      */\n     png_set_check_for_invalid_index (ping, 0);\n# endif\n  }\n\n#else\n# ifdef PNG_WRITE_EMPTY_PLTE_SUPPORTED\n  if (mng_info->write_mng)\n     png_permit_empty_plte(ping,MagickTrue);\n\n# endif\n#endif\n\n  x=0;\n\n  ping_width=(png_uint_32) image->columns;\n  ping_height=(png_uint_32) image->rows;\n\n  if (mng_info->write_png8 || mng_info->write_png24 || mng_info->write_png32)\n     image_depth=8;\n\n  if (mng_info->write_png48 || mng_info->write_png64)\n     image_depth=16;\n\n  if (mng_info->write_png_depth != 0)\n     image_depth=mng_info->write_png_depth;\n\n  /* Adjust requested depth to next higher valid depth if necessary */\n  if (image_depth > 8)\n     image_depth=16;\n\n  if ((image_depth > 4) && (image_depth < 8))\n     image_depth=8;\n\n  if (image_depth == 3)\n     image_depth=4;\n\n  if (logging != MagickFalse)\n    {\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    width=%.20g\",(double) ping_width);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    height=%.20g\",(double) ping_height);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_matte=%.20g\",(double) image->alpha_trait);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image->depth=%.20g\",(double) image->depth);\n     (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Tentative ping_bit_depth=%.20g\",(double) image_depth);\n    }\n\n  save_image_depth=image_depth;\n  ping_bit_depth=(png_byte) save_image_depth;\n\n\n#if defined(PNG_pHYs_SUPPORTED)\n  if (ping_exclude_pHYs == MagickFalse)\n  {\n  if ((image->resolution.x != 0) && (image->resolution.y != 0) &&\n      (!mng_info->write_mng || !mng_info->equal_physs))\n    {\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Setting up pHYs chunk\");\n\n      if (image->units == PixelsPerInchResolution)\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_METER;\n          ping_pHYs_x_resolution=\n             (png_uint_32) ((100.0*image->resolution.x+0.5)/2.54);\n          ping_pHYs_y_resolution=\n             (png_uint_32) ((100.0*image->resolution.y+0.5)/2.54);\n        }\n\n      else if (image->units == PixelsPerCentimeterResolution)\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_METER;\n          ping_pHYs_x_resolution=(png_uint_32) (100.0*image->resolution.x+0.5);\n          ping_pHYs_y_resolution=(png_uint_32) (100.0*image->resolution.y+0.5);\n        }\n\n      else\n        {\n          ping_pHYs_unit_type=PNG_RESOLUTION_UNKNOWN;\n          ping_pHYs_x_resolution=(png_uint_32) image->resolution.x;\n          ping_pHYs_y_resolution=(png_uint_32) image->resolution.y;\n        }\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Set up PNG pHYs chunk: xres: %.20g, yres: %.20g, units: %d.\",\n          (double) ping_pHYs_x_resolution,(double) ping_pHYs_y_resolution,\n          (int) ping_pHYs_unit_type);\n       ping_have_pHYs = MagickTrue;\n    }\n  }\n#endif\n\n  if (ping_exclude_bKGD == MagickFalse)\n  {\n  if ((!mng_info->adjoin || !mng_info->equal_backgrounds))\n    {\n       unsigned int\n         mask;\n\n       mask=0xffff;\n       if (ping_bit_depth == 8)\n          mask=0x00ff;\n\n       if (ping_bit_depth == 4)\n          mask=0x000f;\n\n       if (ping_bit_depth == 2)\n          mask=0x0003;\n\n       if (ping_bit_depth == 1)\n          mask=0x0001;\n\n       ping_background.red=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.red) & mask);\n\n       ping_background.green=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.green) & mask);\n\n       ping_background.blue=(png_uint_16)\n         (ScaleQuantumToShort(image->background_color.blue) & mask);\n\n       ping_background.gray=(png_uint_16) ping_background.green;\n    }\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    Setting up bKGD chunk (1)\");\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"      background_color index is %d\",\n          (int) ping_background.index);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"    ping_bit_depth=%d\",ping_bit_depth);\n    }\n\n  ping_have_bKGD = MagickTrue;\n  }\n\n  /*\n    Select the color type.\n  */\n  matte=image_matte;\n  old_bit_depth=0;\n\n  if (mng_info->IsPalette && mng_info->write_png8)\n    {\n      /* To do: make this a function cause it's used twice, except\n         for reducing the sample depth from 8. */\n\n      number_colors=image_colors;\n\n      ping_have_tRNS=MagickFalse;\n\n      /*\n        Set image palette.\n      */\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  Setting up PLTE chunk with %d colors (%d)\",\n            number_colors, image_colors);\n\n      for (i=0; i < (ssize_t) number_colors; i++)\n      {\n        palette[i].red=ScaleQuantumToChar(image->colormap[i].red);\n        palette[i].green=ScaleQuantumToChar(image->colormap[i].green);\n        palette[i].blue=ScaleQuantumToChar(image->colormap[i].blue);\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n#if MAGICKCORE_QUANTUM_DEPTH == 8\n            \"    %3ld (%3d,%3d,%3d)\",\n#else\n            \"    %5ld (%5d,%5d,%5d)\",\n#endif\n            (long) i,palette[i].red,palette[i].green,palette[i].blue);\n\n      }\n\n      ping_have_PLTE=MagickTrue;\n      image_depth=ping_bit_depth;\n      ping_num_trans=0;\n\n      if (matte != MagickFalse)\n      {\n          /*\n            Identify which colormap entry is transparent.\n          */\n          assert(number_colors <= 256);\n          assert(image->colormap != NULL);\n\n          for (i=0; i < (ssize_t) number_transparent; i++)\n             ping_trans_alpha[i]=0;\n\n\n          ping_num_trans=(unsigned short) (number_transparent +\n             number_semitransparent);\n\n          if (ping_num_trans == 0)\n             ping_have_tRNS=MagickFalse;\n\n          else\n             ping_have_tRNS=MagickTrue;\n      }\n\n      if (ping_exclude_bKGD == MagickFalse)\n      {\n       /*\n        * Identify which colormap entry is the background color.\n        */\n\n        for (i=0; i < (ssize_t) MagickMax(1L*number_colors-1L,1L); i++)\n          if (IsPNGColorEqual(ping_background,image->colormap[i]))\n            break;\n\n        ping_background.index=(png_byte) i;\n\n        if (logging != MagickFalse)\n          {\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"      background_color index is %d\",\n                 (int) ping_background.index);\n          }\n      }\n    } /* end of write_png8 */\n\n  else if (mng_info->write_png_colortype == 1)\n    {\n      image_matte=MagickFalse;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY;\n    }\n\n  else if (mng_info->write_png24 || mng_info->write_png48 ||\n      mng_info->write_png_colortype == 3)\n    {\n      image_matte=MagickFalse;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n    }\n\n  else if (mng_info->write_png32 || mng_info->write_png64 ||\n      mng_info->write_png_colortype == 7)\n    {\n      image_matte=MagickTrue;\n      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB_ALPHA;\n    }\n\n  else /* mng_info->write_pngNN not specified */\n    {\n      image_depth=ping_bit_depth;\n\n      if (mng_info->write_png_colortype != 0)\n        {\n          ping_color_type=(png_byte) mng_info->write_png_colortype-1;\n\n          if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA ||\n              ping_color_type == PNG_COLOR_TYPE_RGB_ALPHA)\n            image_matte=MagickTrue;\n\n          else\n            image_matte=MagickFalse;\n\n          if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"   PNG colortype %d was specified:\",(int) ping_color_type);\n        }\n\n      else /* write_png_colortype not specified */\n        {\n          if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Selecting PNG colortype:\");\n\n          ping_color_type=(png_byte) ((matte != MagickFalse)?\n            PNG_COLOR_TYPE_RGB_ALPHA:PNG_COLOR_TYPE_RGB);\n\n          if (image_info->type == TrueColorType)\n            {\n              ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n              image_matte=MagickFalse;\n            }\n\n          if (image_info->type == TrueColorAlphaType)\n            {\n              ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB_ALPHA;\n              image_matte=MagickTrue;\n            }\n\n          if (image_info->type == PaletteType ||\n              image_info->type == PaletteAlphaType)\n            ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n          if (mng_info->write_png_colortype == 0 &&\n             image_info->type == UndefinedType)\n            {\n              if (ping_have_color == MagickFalse)\n                {\n                  if (image_matte == MagickFalse)\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY;\n                      image_matte=MagickFalse;\n                    }\n\n                  else\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_GRAY_ALPHA;\n                      image_matte=MagickTrue;\n                    }\n                }\n              else\n                {\n                  if (image_matte == MagickFalse)\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGB;\n                      image_matte=MagickFalse;\n                    }\n\n                  else\n                    {\n                      ping_color_type=(png_byte) PNG_COLOR_TYPE_RGBA;\n                      image_matte=MagickTrue;\n                    }\n                 }\n            }\n\n        }\n\n      if (logging != MagickFalse)\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n         \"    Selected PNG colortype=%d\",ping_color_type);\n\n      if (ping_bit_depth < 8)\n        {\n          if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA ||\n              ping_color_type == PNG_COLOR_TYPE_RGB ||\n              ping_color_type == PNG_COLOR_TYPE_RGB_ALPHA)\n            ping_bit_depth=8;\n        }\n\n      old_bit_depth=ping_bit_depth;\n\n      if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n        {\n          if (image->alpha_trait == UndefinedPixelTrait &&\n               ping_have_non_bw == MagickFalse)\n             ping_bit_depth=1;\n        }\n\n      if (ping_color_type == PNG_COLOR_TYPE_PALETTE)\n        {\n           size_t one = 1;\n           ping_bit_depth=1;\n\n           if (image->colors == 0)\n           {\n              /* DO SOMETHING */\n                png_error(ping,\"image has 0 colors\");\n           }\n\n           while ((int) (one << ping_bit_depth) < (ssize_t) image_colors)\n             ping_bit_depth <<= 1;\n        }\n\n      if (logging != MagickFalse)\n         {\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Number of colors: %.20g\",(double) image_colors);\n\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Tentative PNG bit depth: %d\",ping_bit_depth);\n         }\n\n      if (ping_bit_depth < (int) mng_info->write_png_depth)\n         ping_bit_depth = mng_info->write_png_depth;\n    }\n\n  image_depth=ping_bit_depth;\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Tentative PNG color type: %s (%.20g)\",\n        PngColorTypeToString(ping_color_type),\n        (double) ping_color_type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_info->type: %.20g\",(double) image_info->type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    image_depth: %.20g\",(double) image_depth);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n\n        \"    image->depth: %.20g\",(double) image->depth);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    ping_bit_depth: %.20g\",(double) ping_bit_depth);\n    }\n\n  if (matte != MagickFalse)\n    {\n      if (mng_info->IsPalette)\n        {\n          if (mng_info->write_png_colortype == 0)\n            {\n              ping_color_type=PNG_COLOR_TYPE_GRAY_ALPHA;\n\n              if (ping_have_color != MagickFalse)\n                 ping_color_type=PNG_COLOR_TYPE_RGBA;\n            }\n\n          /*\n           * Determine if there is any transparent color.\n          */\n          if (number_transparent + number_semitransparent == 0)\n            {\n              /*\n                No transparent pixels are present.  Change 4 or 6 to 0 or 2.\n              */\n\n              image_matte=MagickFalse;\n\n              if (mng_info->write_png_colortype == 0)\n                ping_color_type&=0x03;\n            }\n\n          else\n            {\n              unsigned int\n                mask;\n\n              mask=0xffff;\n\n              if (ping_bit_depth == 8)\n                 mask=0x00ff;\n\n              if (ping_bit_depth == 4)\n                 mask=0x000f;\n\n              if (ping_bit_depth == 2)\n                 mask=0x0003;\n\n              if (ping_bit_depth == 1)\n                 mask=0x0001;\n\n              ping_trans_color.red=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].red) & mask);\n\n              ping_trans_color.green=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].green) & mask);\n\n              ping_trans_color.blue=(png_uint_16)\n                (ScaleQuantumToShort(image->colormap[0].blue) & mask);\n\n              ping_trans_color.gray=(png_uint_16)\n                (ScaleQuantumToShort(GetPixelInfoIntensity(image,\n                   image->colormap)) & mask);\n\n              ping_trans_color.index=(png_byte) 0;\n\n              ping_have_tRNS=MagickTrue;\n            }\n\n          if (ping_have_tRNS != MagickFalse)\n            {\n              /*\n               * Determine if there is one and only one transparent color\n               * and if so if it is fully transparent.\n               */\n              if (ping_have_cheap_transparency == MagickFalse)\n                ping_have_tRNS=MagickFalse;\n            }\n\n          if (ping_have_tRNS != MagickFalse)\n            {\n              if (mng_info->write_png_colortype == 0)\n                ping_color_type &= 0x03;  /* changes 4 or 6 to 0 or 2 */\n\n              if (image_depth == 8)\n                {\n                  ping_trans_color.red&=0xff;\n                  ping_trans_color.green&=0xff;\n                  ping_trans_color.blue&=0xff;\n                  ping_trans_color.gray&=0xff;\n                }\n            }\n        }\n      else\n        {\n          if (image_depth == 8)\n            {\n              ping_trans_color.red&=0xff;\n              ping_trans_color.green&=0xff;\n              ping_trans_color.blue&=0xff;\n              ping_trans_color.gray&=0xff;\n            }\n        }\n    }\n\n    matte=image_matte;\n\n    if (ping_have_tRNS != MagickFalse)\n      image_matte=MagickFalse;\n\n    if ((mng_info->IsPalette) &&\n        mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_PALETTE &&\n        ping_have_color == MagickFalse &&\n        (image_matte == MagickFalse || image_depth >= 8))\n      {\n        size_t one=1;\n\n        if (image_matte != MagickFalse)\n          ping_color_type=PNG_COLOR_TYPE_GRAY_ALPHA;\n\n        else if (mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_GRAY_ALPHA)\n          {\n            ping_color_type=PNG_COLOR_TYPE_GRAY;\n\n            if (save_image_depth == 16 && image_depth == 8)\n              {\n                if (logging != MagickFalse)\n                  {\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Scaling ping_trans_color (0)\");\n                  }\n                    ping_trans_color.gray*=0x0101;\n              }\n          }\n\n        if (image_depth > MAGICKCORE_QUANTUM_DEPTH)\n          image_depth=MAGICKCORE_QUANTUM_DEPTH;\n\n        if ((image_colors == 0) ||\n             ((ssize_t) (image_colors-1) > (ssize_t) MaxColormapSize))\n          image_colors=(int) (one << image_depth);\n\n        if (image_depth > 8)\n          ping_bit_depth=16;\n\n        else\n          {\n            ping_bit_depth=8;\n            if ((int) ping_color_type == PNG_COLOR_TYPE_PALETTE)\n              {\n                if(!mng_info->write_png_depth)\n                  {\n                    ping_bit_depth=1;\n\n                    while ((int) (one << ping_bit_depth)\n                        < (ssize_t) image_colors)\n                      ping_bit_depth <<= 1;\n                  }\n              }\n\n            else if (ping_color_type ==\n                PNG_COLOR_TYPE_GRAY && image_colors < 17 &&\n                mng_info->IsPalette)\n              {\n              /* Check if grayscale is reducible */\n\n                int\n                  depth_4_ok=MagickTrue,\n                  depth_2_ok=MagickTrue,\n                  depth_1_ok=MagickTrue;\n\n                for (i=0; i < (ssize_t) image_colors; i++)\n                {\n                   unsigned char\n                     intensity;\n\n                   intensity=ScaleQuantumToChar(image->colormap[i].red);\n\n                   if ((intensity & 0x0f) != ((intensity & 0xf0) >> 4))\n                     depth_4_ok=depth_2_ok=depth_1_ok=MagickFalse;\n                   else if ((intensity & 0x03) != ((intensity & 0x0c) >> 2))\n                     depth_2_ok=depth_1_ok=MagickFalse;\n                   else if ((intensity & 0x01) != ((intensity & 0x02) >> 1))\n                     depth_1_ok=MagickFalse;\n                }\n\n                if (depth_1_ok && mng_info->write_png_depth <= 1)\n                  ping_bit_depth=1;\n\n                else if (depth_2_ok && mng_info->write_png_depth <= 2)\n                  ping_bit_depth=2;\n\n                else if (depth_4_ok && mng_info->write_png_depth <= 4)\n                  ping_bit_depth=4;\n              }\n          }\n\n          image_depth=ping_bit_depth;\n      }\n\n    else\n\n      if (mng_info->IsPalette)\n      {\n        number_colors=image_colors;\n\n        if (image_depth <= 8)\n          {\n            /*\n              Set image palette.\n            */\n            ping_color_type=(png_byte) PNG_COLOR_TYPE_PALETTE;\n\n            if (!(mng_info->have_write_global_plte && matte == MagickFalse))\n              {\n                for (i=0; i < (ssize_t) number_colors; i++)\n                {\n                  palette[i].red=ScaleQuantumToChar(image->colormap[i].red);\n                  palette[i].green=\n                    ScaleQuantumToChar(image->colormap[i].green);\n                  palette[i].blue=ScaleQuantumToChar(image->colormap[i].blue);\n                }\n\n                if (logging != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"  Setting up PLTE chunk with %d colors\",\n                    number_colors);\n\n                ping_have_PLTE=MagickTrue;\n              }\n\n            /* color_type is PNG_COLOR_TYPE_PALETTE */\n            if (mng_info->write_png_depth == 0)\n              {\n                size_t\n                  one;\n\n                ping_bit_depth=1;\n                one=1;\n\n                while ((one << ping_bit_depth) < (size_t) number_colors)\n                  ping_bit_depth <<= 1;\n              }\n\n            ping_num_trans=0;\n\n            if (matte != MagickFalse)\n              {\n                /*\n                 * Set up trans_colors array.\n                 */\n                assert(number_colors <= 256);\n\n                ping_num_trans=(unsigned short) (number_transparent +\n                  number_semitransparent);\n\n                if (ping_num_trans == 0)\n                  ping_have_tRNS=MagickFalse;\n\n                else\n                  {\n                    if (logging != MagickFalse)\n                      {\n                        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  Scaling ping_trans_color (1)\");\n                      }\n                    ping_have_tRNS=MagickTrue;\n\n                    for (i=0; i < ping_num_trans; i++)\n                    {\n                       ping_trans_alpha[i]= (png_byte)\n                         ScaleQuantumToChar(image->colormap[i].alpha);\n                    }\n                  }\n              }\n          }\n      }\n\n    else\n      {\n\n        if (image_depth < 8)\n          image_depth=8;\n\n        if ((save_image_depth == 16) && (image_depth == 8))\n          {\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    Scaling ping_trans_color from (%d,%d,%d)\",\n                  (int) ping_trans_color.red,\n                  (int) ping_trans_color.green,\n                  (int) ping_trans_color.blue);\n              }\n\n            ping_trans_color.red*=0x0101;\n            ping_trans_color.green*=0x0101;\n            ping_trans_color.blue*=0x0101;\n            ping_trans_color.gray*=0x0101;\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    to (%d,%d,%d)\",\n                  (int) ping_trans_color.red,\n                  (int) ping_trans_color.green,\n                  (int) ping_trans_color.blue);\n              }\n          }\n      }\n\n    if (ping_bit_depth <  (ssize_t) mng_info->write_png_depth)\n         ping_bit_depth =  (ssize_t) mng_info->write_png_depth;\n\n    /*\n      Adjust background and transparency samples in sub-8-bit grayscale files.\n    */\n    if (ping_bit_depth < 8 && ping_color_type ==\n        PNG_COLOR_TYPE_GRAY)\n      {\n         png_uint_16\n           maxval;\n\n         size_t\n           one=1;\n\n         maxval=(png_uint_16) ((one << ping_bit_depth)-1);\n\n         if (ping_exclude_bKGD == MagickFalse)\n         {\n\n         ping_background.gray=(png_uint_16) ((maxval/65535.)*\n           (ScaleQuantumToShort(((GetPixelInfoIntensity(image,\n           &image->background_color))) +.5)));\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Setting up bKGD chunk (2)\");\n         (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      background_color index is %d\",\n             (int) ping_background.index);\n\n         ping_have_bKGD = MagickTrue;\n         }\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Scaling ping_trans_color.gray from %d\",\n             (int)ping_trans_color.gray);\n\n         ping_trans_color.gray=(png_uint_16) ((maxval/255.)*(\n           ping_trans_color.gray)+.5);\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"      to %d\", (int)ping_trans_color.gray);\n      }\n\n  if (ping_exclude_bKGD == MagickFalse)\n  {\n    if (mng_info->IsPalette && (int) ping_color_type == PNG_COLOR_TYPE_PALETTE)\n      {\n        /*\n           Identify which colormap entry is the background color.\n        */\n\n        number_colors=image_colors;\n\n        for (i=0; i < (ssize_t) MagickMax(1L*number_colors,1L); i++)\n          if (IsPNGColorEqual(image->background_color,image->colormap[i]))\n            break;\n\n        ping_background.index=(png_byte) i;\n\n        if (logging != MagickFalse)\n          {\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Setting up bKGD chunk with index=%d\",(int) i);\n          }\n\n        if (i < (ssize_t) number_colors)\n          {\n            ping_have_bKGD = MagickTrue;\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"     background   =(%d,%d,%d)\",\n                        (int) ping_background.red,\n                        (int) ping_background.green,\n                        (int) ping_background.blue);\n              }\n          }\n\n        else  /* Can't happen */\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"      No room in PLTE to add bKGD color\");\n            ping_have_bKGD = MagickFalse;\n          }\n      }\n  }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    PNG color type: %s (%d)\", PngColorTypeToString(ping_color_type),\n      ping_color_type);\n  /*\n    Initialize compression level and filtering.\n  */\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Setting up deflate compression\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Compression buffer size: 32768\");\n    }\n\n  png_set_compression_buffer_size(ping,32768L);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Compression mem level: 9\");\n\n  png_set_compression_mem_level(ping, 9);\n\n  /* Untangle the \"-quality\" setting:\n\n     Undefined is 0; the default is used.\n     Default is 75\n\n     10's digit:\n\n        0 or omitted: Use Z_HUFFMAN_ONLY strategy with the\n           zlib default compression level\n\n        1-9: the zlib compression level\n\n     1's digit:\n\n        0-4: the PNG filter method\n\n        5:   libpng adaptive filtering if compression level > 5\n             libpng filter type \"none\" if compression level <= 5\n                or if image is grayscale or palette\n\n        6:   libpng adaptive filtering\n\n        7:   \"LOCO\" filtering (intrapixel differing) if writing\n             a MNG, otherwise \"none\".  Did not work in IM-6.7.0-9\n             and earlier because of a missing \"else\".\n\n        8:   Z_RLE strategy (or Z_HUFFMAN_ONLY if quality < 10), adaptive\n             filtering. Unused prior to IM-6.7.0-10, was same as 6\n\n        9:   Z_RLE strategy (or Z_HUFFMAN_ONLY if quality < 10), no PNG filters\n             Unused prior to IM-6.7.0-10, was same as 6\n\n    Note that using the -quality option, not all combinations of\n    PNG filter type, zlib compression level, and zlib compression\n    strategy are possible.  This will be addressed soon in a\n    release that accomodates \"-define png:compression-strategy\", etc.\n\n   */\n\n  quality=image_info->quality == UndefinedCompressionQuality ? 75UL :\n     image_info->quality;\n\n  if (quality <= 9)\n    {\n      if (mng_info->write_png_compression_strategy == 0)\n        mng_info->write_png_compression_strategy = Z_HUFFMAN_ONLY+1;\n    }\n\n  else if (mng_info->write_png_compression_level == 0)\n    {\n      int\n        level;\n\n      level=(int) MagickMin((ssize_t) quality/10,9);\n\n      mng_info->write_png_compression_level = level+1;\n    }\n\n  if (mng_info->write_png_compression_strategy == 0)\n    {\n        if ((quality %10) == 8 || (quality %10) == 9)\n#ifdef Z_RLE  /* Z_RLE was added to zlib-1.2.0 */\n          mng_info->write_png_compression_strategy=Z_RLE+1;\n#else\n          mng_info->write_png_compression_strategy = Z_DEFAULT_STRATEGY+1;\n#endif\n    }\n\n  if (mng_info->write_png_compression_filter == 0)\n        mng_info->write_png_compression_filter=((int) quality % 10) + 1;\n\n  if (logging != MagickFalse)\n    {\n        if (mng_info->write_png_compression_level)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Compression level:    %d\",\n            (int) mng_info->write_png_compression_level-1);\n\n        if (mng_info->write_png_compression_strategy)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Compression strategy: %d\",\n            (int) mng_info->write_png_compression_strategy-1);\n\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Setting up filtering\");\n\n        if (mng_info->write_png_compression_filter == 6)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: ADAPTIVE\");\n        else if (mng_info->write_png_compression_filter == 0 ||\n                 mng_info->write_png_compression_filter == 1)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: NONE\");\n        else\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Base filter method: %d\",\n            (int) mng_info->write_png_compression_filter-1);\n    }\n\n  if (mng_info->write_png_compression_level != 0)\n    png_set_compression_level(ping,mng_info->write_png_compression_level-1);\n\n  if (mng_info->write_png_compression_filter == 6)\n    {\n      if (((int) ping_color_type == PNG_COLOR_TYPE_GRAY) ||\n         ((int) ping_color_type == PNG_COLOR_TYPE_PALETTE) ||\n         (quality < 50))\n        png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n      else\n        png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_ALL_FILTERS);\n     }\n  else if (mng_info->write_png_compression_filter == 7 ||\n      mng_info->write_png_compression_filter == 10)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_ALL_FILTERS);\n\n  else if (mng_info->write_png_compression_filter == 8)\n    {\n#if defined(PNG_MNG_FEATURES_SUPPORTED) && defined(PNG_INTRAPIXEL_DIFFERENCING)\n      if (mng_info->write_mng)\n      {\n         if (((int) ping_color_type == PNG_COLOR_TYPE_RGB) ||\n             ((int) ping_color_type == PNG_COLOR_TYPE_RGBA))\n        ping_filter_method=PNG_INTRAPIXEL_DIFFERENCING;\n      }\n#endif\n      png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n    }\n\n  else if (mng_info->write_png_compression_filter == 9)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,PNG_NO_FILTERS);\n\n  else if (mng_info->write_png_compression_filter != 0)\n    png_set_filter(ping,PNG_FILTER_TYPE_BASE,\n       mng_info->write_png_compression_filter-1);\n\n  if (mng_info->write_png_compression_strategy != 0)\n    png_set_compression_strategy(ping,\n       mng_info->write_png_compression_strategy-1);\n\n  ping_interlace_method=image_info->interlace != NoInterlace;\n\n  if (mng_info->write_mng)\n    png_set_sig_bytes(ping,8);\n\n  /* Bail out if cannot meet defined png:bit-depth or png:color-type */\n\n  if (mng_info->write_png_colortype != 0)\n    {\n     if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_GRAY)\n       if (ping_have_color != MagickFalse)\n         {\n           ping_color_type = PNG_COLOR_TYPE_RGB;\n\n           if (ping_bit_depth < 8)\n             ping_bit_depth=8;\n         }\n\n     if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_GRAY_ALPHA)\n       if (ping_have_color != MagickFalse)\n         ping_color_type = PNG_COLOR_TYPE_RGB_ALPHA;\n    }\n\n  if (ping_need_colortype_warning != MagickFalse ||\n     ((mng_info->write_png_depth &&\n     (int) mng_info->write_png_depth != ping_bit_depth) ||\n     (mng_info->write_png_colortype &&\n     ((int) mng_info->write_png_colortype-1 != ping_color_type &&\n      mng_info->write_png_colortype != 7 &&\n      !(mng_info->write_png_colortype == 5 && ping_color_type == 0)))))\n    {\n      if (logging != MagickFalse)\n        {\n          if (ping_need_colortype_warning != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"  Image has transparency but tRNS chunk was excluded\");\n            }\n\n          if (mng_info->write_png_depth)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Defined png:bit-depth=%u, Computed depth=%u\",\n                  mng_info->write_png_depth,\n                  ping_bit_depth);\n            }\n\n          if (mng_info->write_png_colortype)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Defined png:color-type=%u, Computed color type=%u\",\n                  mng_info->write_png_colortype-1,\n                  ping_color_type);\n            }\n        }\n\n      png_warning(ping,\n        \"Cannot write image with defined png:bit-depth or png:color-type.\");\n    }\n\n  if (image_matte != MagickFalse && image->alpha_trait == UndefinedPixelTrait)\n    {\n      /* Add an opaque matte channel */\n      image->alpha_trait = BlendPixelTrait;\n      (void) SetImageAlpha(image,OpaqueAlpha,exception);\n\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Added an opaque matte channel\");\n    }\n\n  if (number_transparent != 0 || number_semitransparent != 0)\n    {\n      if (ping_color_type < 4)\n        {\n           ping_have_tRNS=MagickTrue;\n           if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n               \"  Setting ping_have_tRNS=MagickTrue.\");\n        }\n    }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Writing PNG header chunks\");\n\n  png_set_IHDR(ping,ping_info,ping_width,ping_height,\n               ping_bit_depth,ping_color_type,\n               ping_interlace_method,ping_compression_method,\n               ping_filter_method);\n\n  if (ping_color_type == 3 && ping_have_PLTE != MagickFalse)\n    {\n      png_set_PLTE(ping,ping_info,palette,number_colors);\n\n      if (logging != MagickFalse)\n        {\n          for (i=0; i< (ssize_t) number_colors; i++)\n          {\n            if (i < ping_num_trans)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"     PLTE[%d] = (%d,%d,%d), tRNS[%d] = (%d)\",\n                      (int) i,\n                      (int) palette[i].red,\n                      (int) palette[i].green,\n                      (int) palette[i].blue,\n                      (int) i,\n                      (int) ping_trans_alpha[i]);\n             else\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"     PLTE[%d] = (%d,%d,%d)\",\n                      (int) i,\n                      (int) palette[i].red,\n                      (int) palette[i].green,\n                      (int) palette[i].blue);\n           }\n         }\n    }\n\n  /* Only write the iCCP chunk if we are not writing the sRGB chunk. */\n  if (ping_exclude_sRGB != MagickFalse ||\n     (!png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n  {\n    if ((ping_exclude_tEXt == MagickFalse ||\n       ping_exclude_zTXt == MagickFalse) &&\n       (ping_exclude_iCCP == MagickFalse || ping_exclude_zCCP == MagickFalse))\n    {\n      ResetImageProfileIterator(image);\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        profile=GetImageProfile(image,name);\n\n        if (profile != (StringInfo *) NULL)\n          {\n#ifdef PNG_WRITE_iCCP_SUPPORTED\n            if ((LocaleCompare(name,\"ICC\") == 0) ||\n                (LocaleCompare(name,\"ICM\") == 0))\n              {\n                ping_have_iCCP = MagickTrue;\n                if (ping_exclude_iCCP == MagickFalse)\n                  {\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Setting up iCCP chunk\");\n\n                    png_set_iCCP(ping,ping_info,(png_charp) name,0,\n#if (PNG_LIBPNG_VER < 10500)\n                    (png_charp) GetStringInfoDatum(profile),\n#else\n                    (const png_byte *) GetStringInfoDatum(profile),\n#endif\n                    (png_uint_32) GetStringInfoLength(profile));\n                  }\n                else\n                  {\n                    /* Do not write hex-encoded ICC chunk */\n                       name=GetNextImageProfile(image);\n                       continue;\n                  }\n              }\n#endif /* WRITE_iCCP */\n\n            if (LocaleCompare(name,\"exif\") == 0)\n              {\n                   /* Do not write hex-encoded ICC chunk; we will\n                      write it later as an eXIf chunk */\n                   name=GetNextImageProfile(image);\n                   continue;\n              }\n\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"  Setting up zTXt chunk with uuencoded %s profile\",\n                 name);\n              Magick_png_write_raw_profile(image_info,ping,ping_info,\n                (unsigned char *) name,(unsigned char *) name,\n                GetStringInfoDatum(profile),\n                (png_uint_32) GetStringInfoLength(profile));\n          }\n        name=GetNextImageProfile(image);\n      }\n    }\n  }\n\n#if defined(PNG_WRITE_sRGB_SUPPORTED)\n  if ((mng_info->have_write_global_srgb == 0) &&\n      ping_have_iCCP != MagickTrue &&\n      (ping_have_sRGB != MagickFalse ||\n      png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n    {\n      if (ping_exclude_sRGB == MagickFalse)\n        {\n          /*\n            Note image rendering intent.\n          */\n          if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  Setting up sRGB chunk\");\n\n          (void) png_set_sRGB(ping,ping_info,(\n            Magick_RenderingIntent_to_PNG_RenderingIntent(\n              image->rendering_intent)));\n\n          ping_have_sRGB = MagickTrue;\n        }\n    }\n\n  if ((!mng_info->write_mng) || (!png_get_valid(ping,ping_info,PNG_INFO_sRGB)))\n#endif\n    {\n      if (ping_exclude_gAMA == MagickFalse &&\n          ping_have_iCCP == MagickFalse &&\n          ping_have_sRGB == MagickFalse &&\n          (ping_exclude_sRGB == MagickFalse ||\n          (image->gamma < .45 || image->gamma > .46)))\n      {\n      if ((mng_info->have_write_global_gama == 0) && (image->gamma != 0.0))\n        {\n          /*\n            Note image gamma.\n            To do: check for cHRM+gAMA == sRGB, and write sRGB instead.\n          */\n          if (logging != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Setting up gAMA chunk\");\n\n          png_set_gAMA(ping,ping_info,image->gamma);\n        }\n      }\n\n      if (ping_exclude_cHRM == MagickFalse && ping_have_sRGB == MagickFalse)\n        {\n          if ((mng_info->have_write_global_chrm == 0) &&\n              (image->chromaticity.red_primary.x != 0.0))\n            {\n              /*\n                Note image chromaticity.\n                Note: if cHRM+gAMA == sRGB write sRGB instead.\n              */\n               PrimaryInfo\n                 bp,\n                 gp,\n                 rp,\n                 wp;\n\n               wp=image->chromaticity.white_point;\n               rp=image->chromaticity.red_primary;\n               gp=image->chromaticity.green_primary;\n               bp=image->chromaticity.blue_primary;\n\n               if (logging != MagickFalse)\n                 (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"  Setting up cHRM chunk\");\n\n               png_set_cHRM(ping,ping_info,wp.x,wp.y,rp.x,rp.y,gp.x,gp.y,\n                   bp.x,bp.y);\n           }\n        }\n    }\n\n  if (ping_exclude_bKGD == MagickFalse)\n    {\n      if (ping_have_bKGD != MagickFalse)\n        {\n          png_set_bKGD(ping,ping_info,&ping_background);\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"    Setting up bKGD chunk\");\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      background color = (%d,%d,%d)\",\n                        (int) ping_background.red,\n                        (int) ping_background.green,\n                        (int) ping_background.blue);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      index = %d, gray=%d\",\n                        (int) ping_background.index,\n                        (int) ping_background.gray);\n            }\n         }\n    }\n\n  if (ping_exclude_pHYs == MagickFalse)\n    {\n      if (ping_have_pHYs != MagickFalse)\n        {\n          png_set_pHYs(ping,ping_info,\n             ping_pHYs_x_resolution,\n             ping_pHYs_y_resolution,\n             ping_pHYs_unit_type);\n\n          if (logging != MagickFalse)\n            {\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"    Setting up pHYs chunk\");\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      x_resolution=%lu\",\n                   (unsigned long) ping_pHYs_x_resolution);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      y_resolution=%lu\",\n                   (unsigned long) ping_pHYs_y_resolution);\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                   \"      unit_type=%lu\",\n                   (unsigned long) ping_pHYs_unit_type);\n            }\n        }\n    }\n\n#if defined(PNG_tIME_SUPPORTED)\n  if (ping_exclude_tIME == MagickFalse)\n    {\n      const char\n        *timestamp;\n\n      if (image->taint == MagickFalse)\n        {\n          timestamp=GetImageOption(image_info,\"png:tIME\");\n\n          if (timestamp == (const char *) NULL)\n            timestamp=GetImageProperty(image,\"png:tIME\",exception);\n        }\n\n      else\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  Reset tIME in tainted image\");\n\n          timestamp=GetImageProperty(image,\"date:modify\",exception);\n        }\n\n      if (timestamp != (const char *) NULL)\n          write_tIME_chunk(image,ping,ping_info,timestamp,exception);\n    }\n#endif\n\n  if (mng_info->need_blob != MagickFalse)\n  {\n    if (OpenBlob(image_info,image,WriteBinaryBlobMode,exception) ==\n       MagickFalse)\n       png_error(ping,\"WriteBlob Failed\");\n\n     ping_have_blob=MagickTrue;\n  }\n\n  png_write_info_before_PLTE(ping, ping_info);\n\n  if (ping_have_tRNS != MagickFalse && ping_color_type < 4)\n    {\n      if (logging != MagickFalse)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  Calling png_set_tRNS with num_trans=%d\",ping_num_trans);\n        }\n\n      if (ping_color_type == 3)\n         (void) png_set_tRNS(ping, ping_info,\n                ping_trans_alpha,\n                ping_num_trans,\n                NULL);\n\n      else\n        {\n           (void) png_set_tRNS(ping, ping_info,\n                  NULL,\n                  0,\n                  &ping_trans_color);\n\n           if (logging != MagickFalse)\n             {\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"     tRNS color   =(%d,%d,%d)\",\n                       (int) ping_trans_color.red,\n                       (int) ping_trans_color.green,\n                       (int) ping_trans_color.blue);\n             }\n         }\n    }\n\n  /* write any png-chunk-b profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-b\",logging);\n\n  png_write_info(ping,ping_info);\n\n  /* write any PNG-chunk-m profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-m\",logging);\n\n  ping_wrote_caNv = MagickFalse;\n\n  /* write caNv chunk */\n  if (ping_exclude_caNv == MagickFalse)\n    {\n      if ((image->page.width != 0 && image->page.width != image->columns) ||\n          (image->page.height != 0 && image->page.height != image->rows) ||\n          image->page.x != 0 || image->page.y != 0)\n        {\n          unsigned char\n            chunk[20];\n\n          (void) WriteBlobMSBULong(image,16L);  /* data length=8 */\n          PNGType(chunk,mng_caNv);\n          LogPNGChunk(logging,mng_caNv,16L);\n          PNGLong(chunk+4,(png_uint_32) image->page.width);\n          PNGLong(chunk+8,(png_uint_32) image->page.height);\n          PNGsLong(chunk+12,(png_int_32) image->page.x);\n          PNGsLong(chunk+16,(png_int_32) image->page.y);\n          (void) WriteBlob(image,20,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,20));\n          ping_wrote_caNv = MagickTrue;\n        }\n    }\n\n#if defined(PNG_oFFs_SUPPORTED)\n  if (ping_exclude_oFFs == MagickFalse && ping_wrote_caNv == MagickFalse)\n    {\n      if (image->page.x || image->page.y)\n        {\n           png_set_oFFs(ping,ping_info,(png_int_32) image->page.x,\n              (png_int_32) image->page.y, 0);\n\n           if (logging != MagickFalse)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"    Setting up oFFs chunk with x=%d, y=%d, units=0\",\n                 (int) image->page.x, (int) image->page.y);\n        }\n    }\n#endif\n\n  /* write vpAg chunk (deprecated, replaced by caNv) */\n  if (ping_exclude_vpAg == MagickFalse && ping_wrote_caNv == MagickFalse)\n    {\n      if ((image->page.width != 0 && image->page.width != image->columns) ||\n          (image->page.height != 0 && image->page.height != image->rows))\n        {\n          unsigned char\n            chunk[14];\n\n          (void) WriteBlobMSBULong(image,9L);  /* data length=8 */\n          PNGType(chunk,mng_vpAg);\n          LogPNGChunk(logging,mng_vpAg,9L);\n          PNGLong(chunk+4,(png_uint_32) image->page.width);\n          PNGLong(chunk+8,(png_uint_32) image->page.height);\n          chunk[12]=0;   /* unit = pixels */\n          (void) WriteBlob(image,13,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,13));\n        }\n    }\n\n#if (PNG_LIBPNG_VER == 10206)\n    /* avoid libpng-1.2.6 bug by setting PNG_HAVE_IDAT flag */\n#define PNG_HAVE_IDAT               0x04\n    ping->mode |= PNG_HAVE_IDAT;\n#undef PNG_HAVE_IDAT\n#endif\n\n  png_set_packing(ping);\n  /*\n    Allocate memory.\n  */\n  rowbytes=image->columns;\n  if (image_depth > 8)\n    rowbytes*=2;\n  switch (ping_color_type)\n    {\n      case PNG_COLOR_TYPE_RGB:\n        rowbytes*=3;\n        break;\n\n      case PNG_COLOR_TYPE_GRAY_ALPHA:\n        rowbytes*=2;\n        break;\n\n      case PNG_COLOR_TYPE_RGBA:\n        rowbytes*=4;\n        break;\n\n      default:\n        break;\n    }\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Writing PNG image data\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Allocating %.20g bytes of memory for pixels\",(double) rowbytes);\n    }\n  pixel_info=AcquireVirtualMemory(rowbytes,sizeof(*ping_pixels));\n  if (pixel_info == (MemoryInfo *) NULL)\n    png_error(ping,\"Allocation of memory for pixels failed\");\n  ping_pixels=(unsigned char *) GetVirtualMemoryBlob(pixel_info);\n\n  /*\n    Initialize image scanlines.\n  */\n  quantum_info=AcquireQuantumInfo(image_info,image);\n  if (quantum_info == (QuantumInfo *) NULL)\n    png_error(ping,\"Memory allocation for quantum_info failed\");\n  quantum_info->format=UndefinedQuantumFormat;\n  SetQuantumDepth(image,quantum_info,image_depth);\n  (void) SetQuantumEndian(image,quantum_info,MSBEndian);\n  num_passes=png_set_interlace_handling(ping);\n\n  if ((!mng_info->write_png8 && !mng_info->write_png24 &&\n       !mng_info->write_png48 && !mng_info->write_png64 &&\n       !mng_info->write_png32) &&\n       (mng_info->IsPalette ||\n       (image_info->type == BilevelType)) &&\n       image_matte == MagickFalse &&\n       ping_have_non_bw == MagickFalse)\n    {\n      /* Palette, Bilevel, or Opaque Monochrome */\n      register const Quantum\n        *p;\n\n      SetQuantumDepth(image,quantum_info,8);\n      for (pass=0; pass < num_passes; pass++)\n      {\n        /*\n          Convert PseudoClass image to a PNG monochrome image.\n        */\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          if (logging != MagickFalse && y == 0)\n             (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"    Writing row of pixels (0)\");\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,exception);\n\n          if (p == (const Quantum *) NULL)\n            break;\n\n          if (mng_info->IsPalette)\n            {\n              (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                quantum_info,GrayQuantum,ping_pixels,exception);\n              if (mng_info->write_png_colortype-1 == PNG_COLOR_TYPE_PALETTE &&\n                  mng_info->write_png_depth &&\n                  mng_info->write_png_depth != old_bit_depth)\n                {\n                  /* Undo pixel scaling */\n                  for (i=0; i < (ssize_t) image->columns; i++)\n                     *(ping_pixels+i)=(unsigned char) (*(ping_pixels+i)\n                     >> (8-old_bit_depth));\n                }\n            }\n\n          else\n            {\n              (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                quantum_info,RedQuantum,ping_pixels,exception);\n            }\n\n          if (mng_info->write_png_colortype-1 != PNG_COLOR_TYPE_PALETTE)\n            for (i=0; i < (ssize_t) image->columns; i++)\n               *(ping_pixels+i)=(unsigned char) ((*(ping_pixels+i) > 127) ?\n                      255 : 0);\n\n          if (logging != MagickFalse && y == 0)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Writing row of pixels (1)\");\n\n          png_write_row(ping,ping_pixels);\n\n          status=SetImageProgress(image,SaveImageTag,\n              (MagickOffsetType) (pass * image->rows + y),\n              num_passes * image->rows);\n\n          if (status == MagickFalse)\n            break;\n        }\n      }\n    }\n\n  else   /* Not Palette, Bilevel, or Opaque Monochrome */\n    {\n      if ((!mng_info->write_png8 && !mng_info->write_png24 &&\n          !mng_info->write_png48 && !mng_info->write_png64 &&\n          !mng_info->write_png32) && (image_matte != MagickFalse ||\n          (ping_bit_depth >= MAGICKCORE_QUANTUM_DEPTH)) &&\n          (mng_info->IsPalette) && ping_have_color == MagickFalse)\n        {\n          register const Quantum\n            *p;\n\n          for (pass=0; pass < num_passes; pass++)\n          {\n\n          for (y=0; y < (ssize_t) image->rows; y++)\n          {\n            p=GetVirtualPixels(image,0,y,image->columns,1,exception);\n\n            if (p == (const Quantum *) NULL)\n              break;\n\n            if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n              {\n                if (mng_info->IsPalette)\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,GrayQuantum,ping_pixels,exception);\n\n                else\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RedQuantum,ping_pixels,exception);\n\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                       \"    Writing GRAY PNG pixels (2)\");\n              }\n\n            else /* PNG_COLOR_TYPE_GRAY_ALPHA */\n              {\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                         \"    Writing GRAY_ALPHA PNG pixels (2)\");\n\n                (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                  quantum_info,GrayAlphaQuantum,ping_pixels,exception);\n              }\n\n            if (logging != MagickFalse && y == 0)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    Writing row of pixels (2)\");\n\n            png_write_row(ping,ping_pixels);\n\n            status=SetImageProgress(image,SaveImageTag,\n              (MagickOffsetType) (pass * image->rows + y),\n              num_passes * image->rows);\n\n            if (status == MagickFalse)\n              break;\n            }\n          }\n        }\n\n      else\n        {\n          register const Quantum\n            *p;\n\n          for (pass=0; pass < num_passes; pass++)\n          {\n            if ((image_depth > 8) ||\n                mng_info->write_png24 ||\n                mng_info->write_png32 ||\n                mng_info->write_png48 ||\n                mng_info->write_png64 ||\n                (!mng_info->write_png8 && !mng_info->IsPalette))\n            {\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                p=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n                if (p == (const Quantum *) NULL)\n                  break;\n\n                if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n                  {\n                    if (image->storage_class == DirectClass)\n                      (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                        quantum_info,RedQuantum,ping_pixels,exception);\n\n                    else\n                      (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                        quantum_info,GrayQuantum,ping_pixels,exception);\n                  }\n\n                else if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA)\n                  {\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                      quantum_info,GrayAlphaQuantum,ping_pixels,\n                      exception);\n\n                    if (logging != MagickFalse && y == 0)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"    Writing GRAY_ALPHA PNG pixels (3)\");\n                  }\n\n                else if (image_matte != MagickFalse)\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RGBAQuantum,ping_pixels,exception);\n\n                else\n                  (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                    quantum_info,RGBQuantum,ping_pixels,exception);\n\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"    Writing row of pixels (3)\");\n\n                png_write_row(ping,ping_pixels);\n\n                status=SetImageProgress(image,SaveImageTag,\n                  (MagickOffsetType) (pass * image->rows + y),\n                  num_passes * image->rows);\n\n                if (status == MagickFalse)\n                  break;\n              }\n            }\n\n          else\n            /* not ((image_depth > 8) ||\n                mng_info->write_png24 || mng_info->write_png32 ||\n                mng_info->write_png48 || mng_info->write_png64 ||\n                (!mng_info->write_png8 && !mng_info->IsPalette))\n             */\n            {\n              if ((ping_color_type != PNG_COLOR_TYPE_GRAY) &&\n                  (ping_color_type != PNG_COLOR_TYPE_GRAY_ALPHA))\n                {\n                  if (logging != MagickFalse)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"  pass %d, Image Is not GRAY or GRAY_ALPHA\",pass);\n\n                  SetQuantumDepth(image,quantum_info,8);\n                  image_depth=8;\n                }\n\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                if (logging != MagickFalse && y == 0)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"  pass %d, Image Is RGB, 16-bit GRAY, or GRAY_ALPHA\",\n                    pass);\n\n                p=GetVirtualPixels(image,0,y,image->columns,1, exception);\n\n                if (p == (const Quantum *) NULL)\n                  break;\n\n                if (ping_color_type == PNG_COLOR_TYPE_GRAY)\n                  {\n                    SetQuantumDepth(image,quantum_info,image->depth);\n\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                       quantum_info,GrayQuantum,ping_pixels,exception);\n                  }\n\n                else if (ping_color_type == PNG_COLOR_TYPE_GRAY_ALPHA)\n                  {\n                    if (logging != MagickFalse && y == 0)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                           \"  Writing GRAY_ALPHA PNG pixels (4)\");\n\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                         quantum_info,GrayAlphaQuantum,ping_pixels,\n                         exception);\n                  }\n\n                else\n                  {\n                    (void) ExportQuantumPixels(image,(CacheView *) NULL,\n                      quantum_info,IndexQuantum,ping_pixels,exception);\n\n                    if (logging != MagickFalse && y <= 2)\n                    {\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  Writing row of non-gray pixels (4)\");\n\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                          \"  ping_pixels[0]=%d,ping_pixels[1]=%d\",\n                          (int)ping_pixels[0],(int)ping_pixels[1]);\n                    }\n                  }\n                png_write_row(ping,ping_pixels);\n\n                status=SetImageProgress(image,SaveImageTag,\n                  (MagickOffsetType) (pass * image->rows + y),\n                  num_passes * image->rows);\n\n                if (status == MagickFalse)\n                  break;\n              }\n            }\n          }\n        }\n    }\n\n  if (quantum_info != (QuantumInfo *) NULL)\n    quantum_info=DestroyQuantumInfo(quantum_info);\n\n  if (logging != MagickFalse)\n    {\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Wrote PNG image data\");\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Width: %.20g\",(double) ping_width);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    Height: %.20g\",(double) ping_height);\n\n      if (mng_info->write_png_depth)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Defined png:bit-depth: %d\",mng_info->write_png_depth);\n        }\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG bit-depth written: %d\",ping_bit_depth);\n\n      if (mng_info->write_png_colortype)\n        {\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Defined png:color-type: %d\",mng_info->write_png_colortype-1);\n        }\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG color-type written: %d\",ping_color_type);\n\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"    PNG Interlace method: %d\",ping_interlace_method);\n    }\n  /*\n    Generate text chunks after IDAT.\n  */\n  if (ping_exclude_tEXt == MagickFalse || ping_exclude_zTXt == MagickFalse)\n  {\n    ResetImagePropertyIterator(image);\n    property=GetNextImageProperty(image);\n    while (property != (const char *) NULL)\n    {\n      png_textp\n        text;\n\n      value=GetImageProperty(image,property,exception);\n\n      /* Don't write any \"png:\" or \"jpeg:\" properties; those are just for\n       * \"identify\" or for passing through to another JPEG\n       */\n      if ((LocaleNCompare(property,\"png:\",4) != 0 &&\n           LocaleNCompare(property,\"jpeg:\",5) != 0) &&\n\n\n          /* Suppress density and units if we wrote a pHYs chunk */\n          (ping_exclude_pHYs != MagickFalse      ||\n          LocaleCompare(property,\"density\") != 0 ||\n          LocaleCompare(property,\"units\") != 0) &&\n\n          /* Suppress the IM-generated Date:create and Date:modify */\n          (ping_exclude_date == MagickFalse      ||\n          LocaleNCompare(property, \"Date:\",5) != 0))\n        {\n        if (value != (const char *) NULL)\n          {\n\n#if PNG_LIBPNG_VER >= 10400\n            text=(png_textp) png_malloc(ping,\n                 (png_alloc_size_t) sizeof(png_text));\n#else\n            text=(png_textp) png_malloc(ping,(png_size_t) sizeof(png_text));\n#endif\n            text[0].key=(char *) property;\n            text[0].text=(char *) value;\n            text[0].text_length=strlen(value);\n\n            if (ping_exclude_tEXt != MagickFalse)\n               text[0].compression=PNG_TEXT_COMPRESSION_zTXt;\n\n            else if (ping_exclude_zTXt != MagickFalse)\n               text[0].compression=PNG_TEXT_COMPRESSION_NONE;\n\n            else\n            {\n               text[0].compression=image_info->compression == NoCompression ||\n                 (image_info->compression == UndefinedCompression &&\n                 text[0].text_length < 128) ? PNG_TEXT_COMPRESSION_NONE :\n                 PNG_TEXT_COMPRESSION_zTXt ;\n            }\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Setting up text chunk\");\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    keyword: '%s'\",text[0].key);\n              }\n\n            png_set_text(ping,ping_info,text,1);\n            png_free(ping,text);\n          }\n        }\n      property=GetNextImageProperty(image);\n    }\n  }\n\n  /* write any PNG-chunk-e profiles */\n  (void) Magick_png_write_chunk_from_profile(image,\"PNG-chunk-e\",logging);\n\n  /* write exIf profile */\n  if (ping_have_eXIf != MagickFalse && ping_exclude_eXIf == MagickFalse)\n    {\n      char\n        *name;\n\n      ResetImageProfileIterator(image);\n\n      for (name=GetNextImageProfile(image); name != (const char *) NULL; )\n      {\n        if (LocaleCompare(name,\"exif\") == 0)\n          {\n            const StringInfo\n              *profile;\n\n            profile=GetImageProfile(image,name);\n\n            if (profile != (StringInfo *) NULL)\n              {\n                png_uint_32\n                  length;\n\n                unsigned char\n                  chunk[4],\n                  *data;\n\n               StringInfo\n                 *ping_profile;\n\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"  Have eXIf profile\");\n\n               ping_profile=CloneStringInfo(profile);\n               data=GetStringInfoDatum(ping_profile),\n               length=(png_uint_32) GetStringInfoLength(ping_profile);\n\n               PNGType(chunk,mng_eXIf);\n               if (length < 7)\n                 {\n                   ping_profile=DestroyStringInfo(ping_profile);\n                   break;  /* otherwise crashes */\n                 }\n\n               /* skip the \"Exif\\0\\0\" JFIF Exif Header ID */\n               length -= 6;\n\n               LogPNGChunk(logging,chunk,length);\n               (void) WriteBlobMSBULong(image,length);\n               (void) WriteBlob(image,4,chunk);\n               (void) WriteBlob(image,length,data+6);\n               (void) WriteBlobMSBULong(image,crc32(crc32(0,chunk,4),\n                 data+6, (uInt) length));\n               ping_profile=DestroyStringInfo(ping_profile);\n               break;\n             }\n         }\n       name=GetNextImageProfile(image);\n     }\n  }\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Writing PNG end info\");\n\n  png_write_end(ping,ping_info);\n\n  if (mng_info->need_fram && (int) image->dispose == BackgroundDispose)\n    {\n      if (mng_info->page.x || mng_info->page.y ||\n          (ping_width != mng_info->page.width) ||\n          (ping_height != mng_info->page.height))\n        {\n          unsigned char\n            chunk[32];\n\n          /*\n            Write FRAM 4 with clipping boundaries followed by FRAM 1.\n          */\n          (void) WriteBlobMSBULong(image,27L);  /* data length=27 */\n          PNGType(chunk,mng_FRAM);\n          LogPNGChunk(logging,mng_FRAM,27L);\n          chunk[4]=4;\n          chunk[5]=0;  /* frame name separator (no name) */\n          chunk[6]=1;  /* flag for changing delay, for next frame only */\n          chunk[7]=0;  /* flag for changing frame timeout */\n          chunk[8]=1;  /* flag for changing frame clipping for next frame */\n          chunk[9]=0;  /* flag for changing frame sync_id */\n          PNGLong(chunk+10,(png_uint_32) (0L)); /* temporary 0 delay */\n          chunk[14]=0; /* clipping boundaries delta type */\n          PNGLong(chunk+15,(png_uint_32) (mng_info->page.x)); /* left cb */\n          PNGLong(chunk+19,\n             (png_uint_32) (mng_info->page.x + ping_width));\n          PNGLong(chunk+23,(png_uint_32) (mng_info->page.y)); /* top cb */\n          PNGLong(chunk+27,\n             (png_uint_32) (mng_info->page.y + ping_height));\n          (void) WriteBlob(image,31,chunk);\n          (void) WriteBlobMSBULong(image,crc32(0,chunk,31));\n          mng_info->old_framing_mode=4;\n          mng_info->framing_mode=1;\n        }\n\n      else\n        mng_info->framing_mode=3;\n    }\n  if (mng_info->write_mng && !mng_info->need_fram &&\n      ((int) image->dispose == 3))\n     png_error(ping, \"Cannot convert GIF with disposal method 3 to MNG-LC\");\n\n  /*\n    Free PNG resources.\n  */\n\n  png_destroy_write_struct(&ping,&ping_info);\n\n  pixel_info=RelinquishVirtualMemory(pixel_info);\n\n  if (ping_have_blob != MagickFalse)\n     (void) CloseBlob(image);\n\n  image_info=DestroyImageInfo(image_info);\n  image=DestroyImage(image);\n\n  /* Store bit depth actually written */\n  s[0]=(char) ping_bit_depth;\n  s[1]='\\0';\n\n  (void) SetImageProperty(IMimage,\"png:bit-depth-written\",s,exception);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  exit WriteOnePNGImage()\");\n\n#ifdef IMPNG_SETJMP_NOT_THREAD_SAFE\n  UnlockSemaphoreInfo(ping_semaphore);\n#endif\n\n   /* }  for navigation to beginning of SETJMP-protected block. Revert to\n    *    Throwing an Exception when an error occurs.\n    */\n\n  return(MagickTrue);\n/*  End write one PNG image */\n\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/816ecab6c532ae086ff4186b3eaf4aa7092d536f", "file_name": "coders/png.c", "vul_type": "cwe-476", "description": "Write a function in C to output a single PNG image using libpng, handling various color types and compression settings."}
{"func_name": "assoc_array_insert_into_terminal_node", "func_src_before": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (ops->compare_object(assoc_array_ptr_to_leaf(ptr), index_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise we can just insert a new node ahead of the old\n\t\t * one.\n\t\t */\n\t\tgoto present_leaves_cluster_but_not_new_leaf;\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node; we know that the node doesn't\n\t * simply contain a full set of leaves that cluster together (it\n\t * contains meta pointers and/or non-clustering leaves).\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\npresent_leaves_cluster_but_not_new_leaf:\n\t/* All the old leaves cluster in the same slot, but the new leaf wants\n\t * to go into a different slot, so we create a new node to hold the new\n\t * leaf and a pointer to a new node holding all the old leaves.\n\t */\n\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = edit->segment_cache[0];\n\tnew_n1->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tedit->adjust_count_on = new_n0;\n\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tnew_n1->slots[i] = node->slots[i];\n\n\tnew_n0->slots[edit->segment_cache[0]] = assoc_array_node_to_ptr(new_n0);\n\tedit->leaf_p = &new_n0->slots[edit->segment_cache[ASSOC_ARRAY_FAN_OUT]];\n\n\tedit->set[0].ptr = &assoc_array_ptr_to_node(node->back_pointer)->slots[node->parent_slot];\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [insert node before]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}", "func_src_after": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (assoc_array_ptr_is_leaf(ptr) &&\n\t\t    ops->compare_object(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\tindex_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise we can just insert a new node ahead of the old\n\t\t * one.\n\t\t */\n\t\tgoto present_leaves_cluster_but_not_new_leaf;\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node; we know that the node doesn't\n\t * simply contain a full set of leaves that cluster together (it\n\t * contains meta pointers and/or non-clustering leaves).\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\npresent_leaves_cluster_but_not_new_leaf:\n\t/* All the old leaves cluster in the same slot, but the new leaf wants\n\t * to go into a different slot, so we create a new node to hold the new\n\t * leaf and a pointer to a new node holding all the old leaves.\n\t */\n\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = edit->segment_cache[0];\n\tnew_n1->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tedit->adjust_count_on = new_n0;\n\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tnew_n1->slots[i] = node->slots[i];\n\n\tnew_n0->slots[edit->segment_cache[0]] = assoc_array_node_to_ptr(new_n0);\n\tedit->leaf_p = &new_n0->slots[edit->segment_cache[ASSOC_ARRAY_FAN_OUT]];\n\n\tedit->set[0].ptr = &assoc_array_ptr_to_node(node->back_pointer)->slots[node->parent_slot];\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [insert node before]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}", "commit_link": "github.com/torvalds/linux/commit/8d4a2ec1e0b41b0cf9a0c5cd4511da7f8e4f3de2", "file_name": "lib/assoc_array.c", "vul_type": "cwe-476", "description": "Write a C function to insert or replace a key in an associative array node, handling node splits if necessary."}
{"func_name": "update_read_icon_info", "func_src_before": "static BOOL update_read_icon_info(wStream* s, ICON_INFO* iconInfo)\n{\n\tBYTE* newBitMask;\n\n\tif (Stream_GetRemainingLength(s) < 8)\n\t\treturn FALSE;\n\n\tStream_Read_UINT16(s, iconInfo->cacheEntry); /* cacheEntry (2 bytes) */\n\tStream_Read_UINT8(s, iconInfo->cacheId);     /* cacheId (1 byte) */\n\tStream_Read_UINT8(s, iconInfo->bpp);         /* bpp (1 byte) */\n\n\tif ((iconInfo->bpp < 1) || (iconInfo->bpp > 32))\n\t{\n\t\tWLog_ERR(TAG, \"invalid bpp value %\" PRIu32 \"\", iconInfo->bpp);\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, iconInfo->width);  /* width (2 bytes) */\n\tStream_Read_UINT16(s, iconInfo->height); /* height (2 bytes) */\n\n\t/* cbColorTable is only present when bpp is 1, 4 or 8 */\n\tswitch (iconInfo->bpp)\n\t{\n\t\tcase 1:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\tif (Stream_GetRemainingLength(s) < 2)\n\t\t\t\treturn FALSE;\n\n\t\t\tStream_Read_UINT16(s, iconInfo->cbColorTable); /* cbColorTable (2 bytes) */\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\ticonInfo->cbColorTable = 0;\n\t\t\tbreak;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t\treturn FALSE;\n\n\tStream_Read_UINT16(s, iconInfo->cbBitsMask);  /* cbBitsMask (2 bytes) */\n\tStream_Read_UINT16(s, iconInfo->cbBitsColor); /* cbBitsColor (2 bytes) */\n\n\tif (Stream_GetRemainingLength(s) < iconInfo->cbBitsMask + iconInfo->cbBitsColor)\n\t\treturn FALSE;\n\n\t/* bitsMask */\n\tnewBitMask = (BYTE*)realloc(iconInfo->bitsMask, iconInfo->cbBitsMask);\n\n\tif (!newBitMask)\n\t{\n\t\tfree(iconInfo->bitsMask);\n\t\ticonInfo->bitsMask = NULL;\n\t\treturn FALSE;\n\t}\n\n\ticonInfo->bitsMask = newBitMask;\n\tStream_Read(s, iconInfo->bitsMask, iconInfo->cbBitsMask);\n\n\t/* colorTable */\n\tif (iconInfo->colorTable == NULL)\n\t{\n\t\tif (iconInfo->cbColorTable)\n\t\t{\n\t\t\ticonInfo->colorTable = (BYTE*)malloc(iconInfo->cbColorTable);\n\n\t\t\tif (!iconInfo->colorTable)\n\t\t\t\treturn FALSE;\n\t\t}\n\t}\n\telse if (iconInfo->cbColorTable)\n\t{\n\t\tBYTE* new_tab;\n\t\tnew_tab = (BYTE*)realloc(iconInfo->colorTable, iconInfo->cbColorTable);\n\n\t\tif (!new_tab)\n\t\t{\n\t\t\tfree(iconInfo->colorTable);\n\t\t\ticonInfo->colorTable = NULL;\n\t\t\treturn FALSE;\n\t\t}\n\n\t\ticonInfo->colorTable = new_tab;\n\t}\n\telse\n\t{\n\t\tfree(iconInfo->colorTable);\n\t\ticonInfo->colorTable = NULL;\n\t}\n\n\tif (iconInfo->colorTable)\n\t\tStream_Read(s, iconInfo->colorTable, iconInfo->cbColorTable);\n\n\t/* bitsColor */\n\tnewBitMask = (BYTE*)realloc(iconInfo->bitsColor, iconInfo->cbBitsColor);\n\n\tif (!newBitMask)\n\t{\n\t\tfree(iconInfo->bitsColor);\n\t\ticonInfo->bitsColor = NULL;\n\t\treturn FALSE;\n\t}\n\n\ticonInfo->bitsColor = newBitMask;\n\tStream_Read(s, iconInfo->bitsColor, iconInfo->cbBitsColor);\n\treturn TRUE;\n}", "func_src_after": "static BOOL update_read_icon_info(wStream* s, ICON_INFO* iconInfo)\n{\n\tBYTE* newBitMask;\n\n\tif (Stream_GetRemainingLength(s) < 8)\n\t\treturn FALSE;\n\n\tStream_Read_UINT16(s, iconInfo->cacheEntry); /* cacheEntry (2 bytes) */\n\tStream_Read_UINT8(s, iconInfo->cacheId);     /* cacheId (1 byte) */\n\tStream_Read_UINT8(s, iconInfo->bpp);         /* bpp (1 byte) */\n\n\tif ((iconInfo->bpp < 1) || (iconInfo->bpp > 32))\n\t{\n\t\tWLog_ERR(TAG, \"invalid bpp value %\" PRIu32 \"\", iconInfo->bpp);\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, iconInfo->width);  /* width (2 bytes) */\n\tStream_Read_UINT16(s, iconInfo->height); /* height (2 bytes) */\n\n\t/* cbColorTable is only present when bpp is 1, 4 or 8 */\n\tswitch (iconInfo->bpp)\n\t{\n\t\tcase 1:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\tif (Stream_GetRemainingLength(s) < 2)\n\t\t\t\treturn FALSE;\n\n\t\t\tStream_Read_UINT16(s, iconInfo->cbColorTable); /* cbColorTable (2 bytes) */\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\ticonInfo->cbColorTable = 0;\n\t\t\tbreak;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t\treturn FALSE;\n\n\tStream_Read_UINT16(s, iconInfo->cbBitsMask);  /* cbBitsMask (2 bytes) */\n\tStream_Read_UINT16(s, iconInfo->cbBitsColor); /* cbBitsColor (2 bytes) */\n\n\t/* bitsMask */\n\tnewBitMask = (BYTE*)realloc(iconInfo->bitsMask, iconInfo->cbBitsMask);\n\n\tif (!newBitMask)\n\t{\n\t\tfree(iconInfo->bitsMask);\n\t\ticonInfo->bitsMask = NULL;\n\t\treturn FALSE;\n\t}\n\n\ticonInfo->bitsMask = newBitMask;\n\tif (Stream_GetRemainingLength(s) < iconInfo->cbBitsMask)\n\t\treturn FALSE;\n\tStream_Read(s, iconInfo->bitsMask, iconInfo->cbBitsMask);\n\n\t/* colorTable */\n\tif (iconInfo->colorTable == NULL)\n\t{\n\t\tif (iconInfo->cbColorTable)\n\t\t{\n\t\t\ticonInfo->colorTable = (BYTE*)malloc(iconInfo->cbColorTable);\n\n\t\t\tif (!iconInfo->colorTable)\n\t\t\t\treturn FALSE;\n\t\t}\n\t}\n\telse if (iconInfo->cbColorTable)\n\t{\n\t\tBYTE* new_tab;\n\t\tnew_tab = (BYTE*)realloc(iconInfo->colorTable, iconInfo->cbColorTable);\n\n\t\tif (!new_tab)\n\t\t{\n\t\t\tfree(iconInfo->colorTable);\n\t\t\ticonInfo->colorTable = NULL;\n\t\t\treturn FALSE;\n\t\t}\n\n\t\ticonInfo->colorTable = new_tab;\n\t}\n\telse\n\t{\n\t\tfree(iconInfo->colorTable);\n\t\ticonInfo->colorTable = NULL;\n\t}\n\n\tif (iconInfo->colorTable)\n\t{\n\t\tif (Stream_GetRemainingLength(s) < iconInfo->cbColorTable)\n\t\t\treturn FALSE;\n\t\tStream_Read(s, iconInfo->colorTable, iconInfo->cbColorTable);\n\t}\n\n\t/* bitsColor */\n\tnewBitMask = (BYTE*)realloc(iconInfo->bitsColor, iconInfo->cbBitsColor);\n\n\tif (!newBitMask)\n\t{\n\t\tfree(iconInfo->bitsColor);\n\t\ticonInfo->bitsColor = NULL;\n\t\treturn FALSE;\n\t}\n\n\ticonInfo->bitsColor = newBitMask;\n\tif (Stream_GetRemainingLength(s) < iconInfo->cbBitsColor)\n\t\treturn FALSE;\n\tStream_Read(s, iconInfo->bitsColor, iconInfo->cbBitsColor);\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/6b2bc41935e53b0034fe5948aeeab4f32e80f30f", "file_name": "libfreerdp/core/window.c", "vul_type": "cwe-125", "description": "Write a C function named `update_read_icon_info` that reads icon information from a stream and updates an `ICON_INFO` structure, returning a boolean status."}
{"func_name": "tcpmss_mangle_packet", "func_src_before": "tcpmss_mangle_packet(struct sk_buff *skb,\n\t\t     const struct xt_action_param *par,\n\t\t     unsigned int family,\n\t\t     unsigned int tcphoff,\n\t\t     unsigned int minlen)\n{\n\tconst struct xt_tcpmss_info *info = par->targinfo;\n\tstruct tcphdr *tcph;\n\tint len, tcp_hdrlen;\n\tunsigned int i;\n\t__be16 oldval;\n\tu16 newmss;\n\tu8 *opt;\n\n\t/* This is a fragment, no TCP header is available */\n\tif (par->fragoff != 0)\n\t\treturn 0;\n\n\tif (!skb_make_writable(skb, skb->len))\n\t\treturn -1;\n\n\tlen = skb->len - tcphoff;\n\tif (len < (int)sizeof(struct tcphdr))\n\t\treturn -1;\n\n\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\ttcp_hdrlen = tcph->doff * 4;\n\n\tif (len < tcp_hdrlen)\n\t\treturn -1;\n\n\tif (info->mss == XT_TCPMSS_CLAMP_PMTU) {\n\t\tstruct net *net = xt_net(par);\n\t\tunsigned int in_mtu = tcpmss_reverse_mtu(net, skb, family);\n\t\tunsigned int min_mtu = min(dst_mtu(skb_dst(skb)), in_mtu);\n\n\t\tif (min_mtu <= minlen) {\n\t\t\tnet_err_ratelimited(\"unknown or invalid path-MTU (%u)\\n\",\n\t\t\t\t\t    min_mtu);\n\t\t\treturn -1;\n\t\t}\n\t\tnewmss = min_mtu - minlen;\n\t} else\n\t\tnewmss = info->mss;\n\n\topt = (u_int8_t *)tcph;\n\tfor (i = sizeof(struct tcphdr); i <= tcp_hdrlen - TCPOLEN_MSS; i += optlen(opt, i)) {\n\t\tif (opt[i] == TCPOPT_MSS && opt[i+1] == TCPOLEN_MSS) {\n\t\t\tu_int16_t oldmss;\n\n\t\t\toldmss = (opt[i+2] << 8) | opt[i+3];\n\n\t\t\t/* Never increase MSS, even when setting it, as\n\t\t\t * doing so results in problems for hosts that rely\n\t\t\t * on MSS being set correctly.\n\t\t\t */\n\t\t\tif (oldmss <= newmss)\n\t\t\t\treturn 0;\n\n\t\t\topt[i+2] = (newmss & 0xff00) >> 8;\n\t\t\topt[i+3] = newmss & 0x00ff;\n\n\t\t\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t\t\t htons(oldmss), htons(newmss),\n\t\t\t\t\t\t false);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* There is data after the header so the option can't be added\n\t * without moving it, and doing so may make the SYN packet\n\t * itself too large. Accept the packet unmodified instead.\n\t */\n\tif (len > tcp_hdrlen)\n\t\treturn 0;\n\n\t/*\n\t * MSS Option not found ?! add it..\n\t */\n\tif (skb_tailroom(skb) < TCPOLEN_MSS) {\n\t\tif (pskb_expand_head(skb, 0,\n\t\t\t\t     TCPOLEN_MSS - skb_tailroom(skb),\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn -1;\n\t\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\t}\n\n\tskb_put(skb, TCPOLEN_MSS);\n\n\t/*\n\t * IPv4: RFC 1122 states \"If an MSS option is not received at\n\t * connection setup, TCP MUST assume a default send MSS of 536\".\n\t * IPv6: RFC 2460 states IPv6 has a minimum MTU of 1280 and a minimum\n\t * length IPv6 header of 60, ergo the default MSS value is 1220\n\t * Since no MSS was provided, we must use the default values\n\t */\n\tif (xt_family(par) == NFPROTO_IPV4)\n\t\tnewmss = min(newmss, (u16)536);\n\telse\n\t\tnewmss = min(newmss, (u16)1220);\n\n\topt = (u_int8_t *)tcph + sizeof(struct tcphdr);\n\tmemmove(opt + TCPOLEN_MSS, opt, len - sizeof(struct tcphdr));\n\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t htons(len), htons(len + TCPOLEN_MSS), true);\n\topt[0] = TCPOPT_MSS;\n\topt[1] = TCPOLEN_MSS;\n\topt[2] = (newmss & 0xff00) >> 8;\n\topt[3] = newmss & 0x00ff;\n\n\tinet_proto_csum_replace4(&tcph->check, skb, 0, *((__be32 *)opt), false);\n\n\toldval = ((__be16 *)tcph)[6];\n\ttcph->doff += TCPOLEN_MSS/4;\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t oldval, ((__be16 *)tcph)[6], false);\n\treturn TCPOLEN_MSS;\n}", "func_src_after": "tcpmss_mangle_packet(struct sk_buff *skb,\n\t\t     const struct xt_action_param *par,\n\t\t     unsigned int family,\n\t\t     unsigned int tcphoff,\n\t\t     unsigned int minlen)\n{\n\tconst struct xt_tcpmss_info *info = par->targinfo;\n\tstruct tcphdr *tcph;\n\tint len, tcp_hdrlen;\n\tunsigned int i;\n\t__be16 oldval;\n\tu16 newmss;\n\tu8 *opt;\n\n\t/* This is a fragment, no TCP header is available */\n\tif (par->fragoff != 0)\n\t\treturn 0;\n\n\tif (!skb_make_writable(skb, skb->len))\n\t\treturn -1;\n\n\tlen = skb->len - tcphoff;\n\tif (len < (int)sizeof(struct tcphdr))\n\t\treturn -1;\n\n\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\ttcp_hdrlen = tcph->doff * 4;\n\n\tif (len < tcp_hdrlen || tcp_hdrlen < sizeof(struct tcphdr))\n\t\treturn -1;\n\n\tif (info->mss == XT_TCPMSS_CLAMP_PMTU) {\n\t\tstruct net *net = xt_net(par);\n\t\tunsigned int in_mtu = tcpmss_reverse_mtu(net, skb, family);\n\t\tunsigned int min_mtu = min(dst_mtu(skb_dst(skb)), in_mtu);\n\n\t\tif (min_mtu <= minlen) {\n\t\t\tnet_err_ratelimited(\"unknown or invalid path-MTU (%u)\\n\",\n\t\t\t\t\t    min_mtu);\n\t\t\treturn -1;\n\t\t}\n\t\tnewmss = min_mtu - minlen;\n\t} else\n\t\tnewmss = info->mss;\n\n\topt = (u_int8_t *)tcph;\n\tfor (i = sizeof(struct tcphdr); i <= tcp_hdrlen - TCPOLEN_MSS; i += optlen(opt, i)) {\n\t\tif (opt[i] == TCPOPT_MSS && opt[i+1] == TCPOLEN_MSS) {\n\t\t\tu_int16_t oldmss;\n\n\t\t\toldmss = (opt[i+2] << 8) | opt[i+3];\n\n\t\t\t/* Never increase MSS, even when setting it, as\n\t\t\t * doing so results in problems for hosts that rely\n\t\t\t * on MSS being set correctly.\n\t\t\t */\n\t\t\tif (oldmss <= newmss)\n\t\t\t\treturn 0;\n\n\t\t\topt[i+2] = (newmss & 0xff00) >> 8;\n\t\t\topt[i+3] = newmss & 0x00ff;\n\n\t\t\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t\t\t htons(oldmss), htons(newmss),\n\t\t\t\t\t\t false);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* There is data after the header so the option can't be added\n\t * without moving it, and doing so may make the SYN packet\n\t * itself too large. Accept the packet unmodified instead.\n\t */\n\tif (len > tcp_hdrlen)\n\t\treturn 0;\n\n\t/* tcph->doff has 4 bits, do not wrap it to 0 */\n\tif (tcp_hdrlen >= 15 * 4)\n\t\treturn 0;\n\n\t/*\n\t * MSS Option not found ?! add it..\n\t */\n\tif (skb_tailroom(skb) < TCPOLEN_MSS) {\n\t\tif (pskb_expand_head(skb, 0,\n\t\t\t\t     TCPOLEN_MSS - skb_tailroom(skb),\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn -1;\n\t\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\t}\n\n\tskb_put(skb, TCPOLEN_MSS);\n\n\t/*\n\t * IPv4: RFC 1122 states \"If an MSS option is not received at\n\t * connection setup, TCP MUST assume a default send MSS of 536\".\n\t * IPv6: RFC 2460 states IPv6 has a minimum MTU of 1280 and a minimum\n\t * length IPv6 header of 60, ergo the default MSS value is 1220\n\t * Since no MSS was provided, we must use the default values\n\t */\n\tif (xt_family(par) == NFPROTO_IPV4)\n\t\tnewmss = min(newmss, (u16)536);\n\telse\n\t\tnewmss = min(newmss, (u16)1220);\n\n\topt = (u_int8_t *)tcph + sizeof(struct tcphdr);\n\tmemmove(opt + TCPOLEN_MSS, opt, len - sizeof(struct tcphdr));\n\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t htons(len), htons(len + TCPOLEN_MSS), true);\n\topt[0] = TCPOPT_MSS;\n\topt[1] = TCPOLEN_MSS;\n\topt[2] = (newmss & 0xff00) >> 8;\n\topt[3] = newmss & 0x00ff;\n\n\tinet_proto_csum_replace4(&tcph->check, skb, 0, *((__be32 *)opt), false);\n\n\toldval = ((__be16 *)tcph)[6];\n\ttcph->doff += TCPOLEN_MSS/4;\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t oldval, ((__be16 *)tcph)[6], false);\n\treturn TCPOLEN_MSS;\n}", "commit_link": "github.com/torvalds/linux/commit/2638fd0f92d4397884fd991d8f4925cb3f081901", "file_name": "net/netfilter/xt_TCPMSS.c", "vul_type": "cwe-416", "description": "Write a C function to adjust the TCP MSS option in a packet according to given parameters."}
{"func_name": "_find_host_exhaustive", "func_src_before": "    def _find_host_exhaustive(self, connector, hosts):\n        for host in hosts:\n            ssh_cmd = 'svcinfo lshost -delim ! %s' % host\n            out, err = self._run_ssh(ssh_cmd)\n            self._assert_ssh_return(len(out.strip()),\n                                    '_find_host_exhaustive',\n                                    ssh_cmd, out, err)\n            for attr_line in out.split('\\n'):\n                # If '!' not found, return the string and two empty strings\n                attr_name, foo, attr_val = attr_line.partition('!')\n                if (attr_name == 'iscsi_name' and\n                        'initiator' in connector and\n                        attr_val == connector['initiator']):\n                    return host\n                elif (attr_name == 'WWPN' and\n                      'wwpns' in connector and\n                      attr_val.lower() in\n                      map(str.lower, map(str, connector['wwpns']))):\n                        return host\n        return None", "func_src_after": "    def _find_host_exhaustive(self, connector, hosts):\n        for host in hosts:\n            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]\n            out, err = self._run_ssh(ssh_cmd)\n            self._assert_ssh_return(len(out.strip()),\n                                    '_find_host_exhaustive',\n                                    ssh_cmd, out, err)\n            for attr_line in out.split('\\n'):\n                # If '!' not found, return the string and two empty strings\n                attr_name, foo, attr_val = attr_line.partition('!')\n                if (attr_name == 'iscsi_name' and\n                        'initiator' in connector and\n                        attr_val == connector['initiator']):\n                    return host\n                elif (attr_name == 'WWPN' and\n                      'wwpns' in connector and\n                      attr_val.lower() in\n                      map(str.lower, map(str, connector['wwpns']))):\n                        return host\n        return None", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function that searches for a host with matching iSCSI or WWPN attributes in a list of hosts using SSH commands."}
{"func_name": "getPostsByPostid", "func_src_before": "    def getPostsByPostid(self,postid):\n        sqlText=\"select users.name,post.comment from users,post where \\\n                users.userid=post.userid and post.postid=%d\"%(postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;", "func_src_after": "    def getPostsByPostid(self,postid):\n        sqlText=\"select users.name,post.comment from users,post where \\\n                users.userid=post.userid and post.postid=%s\"\n        params=[postid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;", "commit_link": "github.com/ShaominLi/Twitter_project/commit/5329d91f9e569c95184053c8e7ef596949c33ce9", "file_name": "modules/post.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves a user's name and comment from a database using a post ID."}
{"func_name": "users_to_notify_popup", "func_src_before": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "func_src_after": "  def users_to_notify_popup\n    # anyone already attached to the task should be removed\n    excluded_ids = params[:watcher_ids].blank? ? 0 : params[:watcher_ids]\n    @users = current_user.customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    @task = AbstractTask.accessed_by(current_user).find_by(:id => params[:id])\n\n    @task && @task.customers.each do |customer|\n      @users = @users + customer.users.active.where(\"id NOT IN (#{excluded_ids})\").order('name').limit(50)\n    end\n    @users = @users.uniq.sort_by{|user| user.name}.first(50)\n\n    if @task && current_user.customer != @task.project.customer\n      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n      @users = @users.uniq.sort_by{|user| user.name}.first(50)\n    end\n    render :layout =>false\n  end", "line_changes": {"deleted": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (#{excluded_ids})\")\n"}], "added": [{"line_no": 13, "char_start": 640, "char_end": 737, "line": "      @users = @users + @task.project.customer.users.active.where(\"id NOT IN (?)\", excluded_ids)\n"}]}, "char_changes": {"deleted": [{"char_start": 718, "char_end": 720, "chars": "#{"}, {"char_start": 732, "char_end": 735, "chars": "})\""}], "added": [{"char_start": 718, "char_end": 723, "chars": "?)\", "}]}, "commit_link": "github.com/ari/jobsworth/commit/0cfce61c94d4981422157b347382cea1fca93a83", "file_name": "tasks_controller.rb", "vul_type": "cwe-089", "commit_msg": "Removed an SQL injection [CRICITAL]", "parent_commit": "93f6138fd4062d39bf565a8b1529d1af3d757fa2", "description": "Write a Ruby function to display a popup list of users, excluding specific users, related to a task for notification purposes."}
{"func_name": "nsv_read_chunk", "func_src_before": "static int nsv_read_chunk(AVFormatContext *s, int fill_header)\n{\n    NSVContext *nsv = s->priv_data;\n    AVIOContext *pb = s->pb;\n    AVStream *st[2] = {NULL, NULL};\n    NSVStream *nst;\n    AVPacket *pkt;\n    int i, err = 0;\n    uint8_t auxcount; /* number of aux metadata, also 4 bits of vsize */\n    uint32_t vsize;\n    uint16_t asize;\n    uint16_t auxsize;\n\n    if (nsv->ahead[0].data || nsv->ahead[1].data)\n        return 0; //-1; /* hey! eat what you've in your plate first! */\n\nnull_chunk_retry:\n    if (pb->eof_reached)\n        return -1;\n\n    for (i = 0; i < NSV_MAX_RESYNC_TRIES && nsv->state < NSV_FOUND_NSVS && !err; i++)\n        err = nsv_resync(s);\n    if (err < 0)\n        return err;\n    if (nsv->state == NSV_FOUND_NSVS)\n        err = nsv_parse_NSVs_header(s);\n    if (err < 0)\n        return err;\n    if (nsv->state != NSV_HAS_READ_NSVS && nsv->state != NSV_FOUND_BEEF)\n        return -1;\n\n    auxcount = avio_r8(pb);\n    vsize = avio_rl16(pb);\n    asize = avio_rl16(pb);\n    vsize = (vsize << 4) | (auxcount >> 4);\n    auxcount &= 0x0f;\n    av_log(s, AV_LOG_TRACE, \"NSV CHUNK %\"PRIu8\" aux, %\"PRIu32\" bytes video, %\"PRIu16\" bytes audio\\n\",\n           auxcount, vsize, asize);\n    /* skip aux stuff */\n    for (i = 0; i < auxcount; i++) {\n        uint32_t av_unused auxtag;\n        auxsize = avio_rl16(pb);\n        auxtag = avio_rl32(pb);\n        avio_skip(pb, auxsize);\n        vsize -= auxsize + sizeof(uint16_t) + sizeof(uint32_t); /* that's becoming brain-dead */\n    }\n\n    if (pb->eof_reached)\n        return -1;\n    if (!vsize && !asize) {\n        nsv->state = NSV_UNSYNC;\n        goto null_chunk_retry;\n    }\n\n    /* map back streams to v,a */\n    if (s->nb_streams > 0)\n        st[s->streams[0]->id] = s->streams[0];\n    if (s->nb_streams > 1)\n        st[s->streams[1]->id] = s->streams[1];\n\n    if (vsize && st[NSV_ST_VIDEO]) {\n        nst = st[NSV_ST_VIDEO]->priv_data;\n        pkt = &nsv->ahead[NSV_ST_VIDEO];\n        av_get_packet(pb, pkt, vsize);\n        pkt->stream_index = st[NSV_ST_VIDEO]->index;//NSV_ST_VIDEO;\n        pkt->dts = nst->frame_offset;\n        pkt->flags |= nsv->state == NSV_HAS_READ_NSVS ? AV_PKT_FLAG_KEY : 0; /* keyframe only likely on a sync frame */\n        for (i = 0; i < FFMIN(8, vsize); i++)\n            av_log(s, AV_LOG_TRACE, \"NSV video: [%d] = %02\"PRIx8\"\\n\",\n                   i, pkt->data[i]);\n    }\n    if(st[NSV_ST_VIDEO])\n        ((NSVStream*)st[NSV_ST_VIDEO]->priv_data)->frame_offset++;\n\n    if (asize && st[NSV_ST_AUDIO]) {\n        nst = st[NSV_ST_AUDIO]->priv_data;\n        pkt = &nsv->ahead[NSV_ST_AUDIO];\n        /* read raw audio specific header on the first audio chunk... */\n        /* on ALL audio chunks ?? seems so! */\n        if (asize && st[NSV_ST_AUDIO]->codecpar->codec_tag == MKTAG('P', 'C', 'M', ' ')/* && fill_header*/) {\n            uint8_t bps;\n            uint8_t channels;\n            uint16_t samplerate;\n            bps = avio_r8(pb);\n            channels = avio_r8(pb);\n            samplerate = avio_rl16(pb);\n            if (!channels || !samplerate)\n                return AVERROR_INVALIDDATA;\n            asize-=4;\n            av_log(s, AV_LOG_TRACE, \"NSV RAWAUDIO: bps %\"PRIu8\", nchan %\"PRIu8\", srate %\"PRIu16\"\\n\",\n                   bps, channels, samplerate);\n            if (fill_header) {\n                st[NSV_ST_AUDIO]->need_parsing = AVSTREAM_PARSE_NONE; /* we know everything */\n                if (bps != 16) {\n                    av_log(s, AV_LOG_TRACE, \"NSV AUDIO bit/sample != 16 (%\"PRIu8\")!!!\\n\", bps);\n                }\n                bps /= channels; // ???\n                if (bps == 8)\n                    st[NSV_ST_AUDIO]->codecpar->codec_id = AV_CODEC_ID_PCM_U8;\n                samplerate /= 4;/* UGH ??? XXX */\n                channels = 1;\n                st[NSV_ST_AUDIO]->codecpar->channels = channels;\n                st[NSV_ST_AUDIO]->codecpar->sample_rate = samplerate;\n                av_log(s, AV_LOG_TRACE, \"NSV RAWAUDIO: bps %\"PRIu8\", nchan %\"PRIu8\", srate %\"PRIu16\"\\n\",\n                       bps, channels, samplerate);\n            }\n        }\n        av_get_packet(pb, pkt, asize);\n        pkt->stream_index = st[NSV_ST_AUDIO]->index;//NSV_ST_AUDIO;\n        pkt->flags |= nsv->state == NSV_HAS_READ_NSVS ? AV_PKT_FLAG_KEY : 0; /* keyframe only likely on a sync frame */\n        if( nsv->state == NSV_HAS_READ_NSVS && st[NSV_ST_VIDEO] ) {\n            /* on a nsvs frame we have new information on a/v sync */\n            pkt->dts = (((NSVStream*)st[NSV_ST_VIDEO]->priv_data)->frame_offset-1);\n            pkt->dts *= (int64_t)1000        * nsv->framerate.den;\n            pkt->dts += (int64_t)nsv->avsync * nsv->framerate.num;\n            av_log(s, AV_LOG_TRACE, \"NSV AUDIO: sync:%\"PRId16\", dts:%\"PRId64,\n                   nsv->avsync, pkt->dts);\n        }\n        nst->frame_offset++;\n    }\n\n    nsv->state = NSV_UNSYNC;\n    return 0;\n}", "func_src_after": "static int nsv_read_chunk(AVFormatContext *s, int fill_header)\n{\n    NSVContext *nsv = s->priv_data;\n    AVIOContext *pb = s->pb;\n    AVStream *st[2] = {NULL, NULL};\n    NSVStream *nst;\n    AVPacket *pkt;\n    int i, err = 0;\n    uint8_t auxcount; /* number of aux metadata, also 4 bits of vsize */\n    uint32_t vsize;\n    uint16_t asize;\n    uint16_t auxsize;\n    int ret;\n\n    if (nsv->ahead[0].data || nsv->ahead[1].data)\n        return 0; //-1; /* hey! eat what you've in your plate first! */\n\nnull_chunk_retry:\n    if (pb->eof_reached)\n        return -1;\n\n    for (i = 0; i < NSV_MAX_RESYNC_TRIES && nsv->state < NSV_FOUND_NSVS && !err; i++)\n        err = nsv_resync(s);\n    if (err < 0)\n        return err;\n    if (nsv->state == NSV_FOUND_NSVS)\n        err = nsv_parse_NSVs_header(s);\n    if (err < 0)\n        return err;\n    if (nsv->state != NSV_HAS_READ_NSVS && nsv->state != NSV_FOUND_BEEF)\n        return -1;\n\n    auxcount = avio_r8(pb);\n    vsize = avio_rl16(pb);\n    asize = avio_rl16(pb);\n    vsize = (vsize << 4) | (auxcount >> 4);\n    auxcount &= 0x0f;\n    av_log(s, AV_LOG_TRACE, \"NSV CHUNK %\"PRIu8\" aux, %\"PRIu32\" bytes video, %\"PRIu16\" bytes audio\\n\",\n           auxcount, vsize, asize);\n    /* skip aux stuff */\n    for (i = 0; i < auxcount; i++) {\n        uint32_t av_unused auxtag;\n        auxsize = avio_rl16(pb);\n        auxtag = avio_rl32(pb);\n        avio_skip(pb, auxsize);\n        vsize -= auxsize + sizeof(uint16_t) + sizeof(uint32_t); /* that's becoming brain-dead */\n    }\n\n    if (pb->eof_reached)\n        return -1;\n    if (!vsize && !asize) {\n        nsv->state = NSV_UNSYNC;\n        goto null_chunk_retry;\n    }\n\n    /* map back streams to v,a */\n    if (s->nb_streams > 0)\n        st[s->streams[0]->id] = s->streams[0];\n    if (s->nb_streams > 1)\n        st[s->streams[1]->id] = s->streams[1];\n\n    if (vsize && st[NSV_ST_VIDEO]) {\n        nst = st[NSV_ST_VIDEO]->priv_data;\n        pkt = &nsv->ahead[NSV_ST_VIDEO];\n        if ((ret = av_get_packet(pb, pkt, vsize)) < 0)\n            return ret;\n        pkt->stream_index = st[NSV_ST_VIDEO]->index;//NSV_ST_VIDEO;\n        pkt->dts = nst->frame_offset;\n        pkt->flags |= nsv->state == NSV_HAS_READ_NSVS ? AV_PKT_FLAG_KEY : 0; /* keyframe only likely on a sync frame */\n        for (i = 0; i < FFMIN(8, vsize); i++)\n            av_log(s, AV_LOG_TRACE, \"NSV video: [%d] = %02\"PRIx8\"\\n\",\n                   i, pkt->data[i]);\n    }\n    if(st[NSV_ST_VIDEO])\n        ((NSVStream*)st[NSV_ST_VIDEO]->priv_data)->frame_offset++;\n\n    if (asize && st[NSV_ST_AUDIO]) {\n        nst = st[NSV_ST_AUDIO]->priv_data;\n        pkt = &nsv->ahead[NSV_ST_AUDIO];\n        /* read raw audio specific header on the first audio chunk... */\n        /* on ALL audio chunks ?? seems so! */\n        if (asize && st[NSV_ST_AUDIO]->codecpar->codec_tag == MKTAG('P', 'C', 'M', ' ')/* && fill_header*/) {\n            uint8_t bps;\n            uint8_t channels;\n            uint16_t samplerate;\n            bps = avio_r8(pb);\n            channels = avio_r8(pb);\n            samplerate = avio_rl16(pb);\n            if (!channels || !samplerate)\n                return AVERROR_INVALIDDATA;\n            asize-=4;\n            av_log(s, AV_LOG_TRACE, \"NSV RAWAUDIO: bps %\"PRIu8\", nchan %\"PRIu8\", srate %\"PRIu16\"\\n\",\n                   bps, channels, samplerate);\n            if (fill_header) {\n                st[NSV_ST_AUDIO]->need_parsing = AVSTREAM_PARSE_NONE; /* we know everything */\n                if (bps != 16) {\n                    av_log(s, AV_LOG_TRACE, \"NSV AUDIO bit/sample != 16 (%\"PRIu8\")!!!\\n\", bps);\n                }\n                bps /= channels; // ???\n                if (bps == 8)\n                    st[NSV_ST_AUDIO]->codecpar->codec_id = AV_CODEC_ID_PCM_U8;\n                samplerate /= 4;/* UGH ??? XXX */\n                channels = 1;\n                st[NSV_ST_AUDIO]->codecpar->channels = channels;\n                st[NSV_ST_AUDIO]->codecpar->sample_rate = samplerate;\n                av_log(s, AV_LOG_TRACE, \"NSV RAWAUDIO: bps %\"PRIu8\", nchan %\"PRIu8\", srate %\"PRIu16\"\\n\",\n                       bps, channels, samplerate);\n            }\n        }\n        if ((ret = av_get_packet(pb, pkt, asize)) < 0)\n            return ret;\n        pkt->stream_index = st[NSV_ST_AUDIO]->index;//NSV_ST_AUDIO;\n        pkt->flags |= nsv->state == NSV_HAS_READ_NSVS ? AV_PKT_FLAG_KEY : 0; /* keyframe only likely on a sync frame */\n        if( nsv->state == NSV_HAS_READ_NSVS && st[NSV_ST_VIDEO] ) {\n            /* on a nsvs frame we have new information on a/v sync */\n            pkt->dts = (((NSVStream*)st[NSV_ST_VIDEO]->priv_data)->frame_offset-1);\n            pkt->dts *= (int64_t)1000        * nsv->framerate.den;\n            pkt->dts += (int64_t)nsv->avsync * nsv->framerate.num;\n            av_log(s, AV_LOG_TRACE, \"NSV AUDIO: sync:%\"PRId16\", dts:%\"PRId64,\n                   nsv->avsync, pkt->dts);\n        }\n        nst->frame_offset++;\n    }\n\n    nsv->state = NSV_UNSYNC;\n    return 0;\n}", "commit_link": "github.com/libav/libav/commit/fe6eea99efac66839052af547426518efd970b24", "file_name": "libavformat/nsvdec.c", "vul_type": "cwe-476", "description": "Write a C function named `nsv_read_chunk` that reads a chunk of NSV format data from a given AVFormatContext."}
{"func_name": "mongoStore", "func_src_before": "        store: new mongoStore({ mongooseConnection: mongoose.connection })\n    }));\n\n    server.use(passport.initialize());\n    server.use(passport.session());\n    server.use(flash());\n    server.use(csrf());\n    server.use(function(req, res, callback) {\n        // Make user object available in templates.\n        res.locals.user = req.user;\n        res.locals.site = {\n            title: config.app.title,\n            url: config.server.host,\n            dbHost: config.database,\n            mailHost: config.mailer\n        };\n        callback();\n    });\n\n    // for nginx proxy\n    if (mode == 'production') {\n        server.enable('trust proxy');  // using Express behind nginx\n    }\n\n    server.use(function(req, res, callback) {\n        // Remember original destination before login.\n        var path = req.path.split('/')[1];\n\n        if (/auth|api|login|logout|signup|components|css|img|js|favicon/i.test(path) || path == '') {\n            return callback();\n        }\n        req.session.returnTo = req.path;\n\n        callback();\n    });\n\n    var HOUR = 3600000;\n    var DAY = HOUR * 24;\n    var WEEK = DAY * 7;\n\n    server.use(express.static(path.join(__dirname, 'public'), { maxAge: WEEK }));\n\n    // Route Point\n    var home = require('../route/home');\n    var stat = require('../route/stat');\n    var account = require('../route/account');\n    var dashboard = require('../route/dashboard');\n\n    server.use(home);\n    server.use(stat);\n    server.use(account);\n    server.use(dashboard);\n\n    // globalMiddleware\n    // api counter for ip district\n    // haroo cloud api document page\n    // dummy testing\n    // version specified api for only feature test\n    // commonMiddleware\n    // header parameter test\n    // for account\n    // districtMiddleware\n    // district test\n    // for token\n    // for api version\n    // for users\n    // for documents\n\n    // 500 Error Handler\n    server.use(errorHandler());\n\n    callback(server);\n}", "func_src_after": "        store: new mongoStore({ mongooseConnection: mongoose.connection })\n    }));\n\n    server.use(passport.initialize());\n    server.use(passport.session());\n    server.use(flash());\n    server.use(lusca.csrf());\n    server.use(function(req, res, callback) {\n        // Make user object available in templates.\n        res.locals.user = req.user;\n        res.locals.site = {\n            title: config.app.title,\n            url: config.server.host,\n            dbHost: config.database,\n            mailHost: config.mailer\n        };\n        callback();\n    });\n\n    // for nginx proxy\n    if (mode == 'production') {\n        server.enable('trust proxy');  // using Express behind nginx\n    }\n\n    server.use(function(req, res, callback) {\n        // Remember original destination before login.\n        var path = req.path.split('/')[1];\n\n        if (/auth|api|login|logout|signup|components|css|img|js|favicon/i.test(path) || path == '') {\n            return callback();\n        }\n        req.session.returnTo = req.path;\n\n        callback();\n    });\n\n    var HOUR = 3600000;\n    var DAY = HOUR * 24;\n    var WEEK = DAY * 7;\n\n    server.use(express.static(path.join(__dirname, 'public'), { maxAge: WEEK }));\n\n    // Route Point\n    var home = require('../route/home');\n    var stat = require('../route/stat');\n    var account = require('../route/account');\n    var dashboard = require('../route/dashboard');\n\n    server.use(home);\n    server.use(stat);\n    server.use(account);\n    server.use(dashboard);\n\n    // globalMiddleware\n    // api counter for ip district\n    // haroo cloud api document page\n    // dummy testing\n    // version specified api for only feature test\n    // commonMiddleware\n    // header parameter test\n    // for account\n    // districtMiddleware\n    // district test\n    // for token\n    // for api version\n    // for users\n    // for documents\n\n    // 500 Error Handler\n    server.use(errorHandler());\n\n    callback(server);\n}", "line_changes": {"deleted": [{"line_no": 7, "char_start": 185, "char_end": 209, "line": "    server.use(csrf());\n"}], "added": [{"line_no": 7, "char_start": 185, "char_end": 215, "line": "    server.use(lusca.csrf());\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 200, "char_end": 206, "chars": "lusca."}]}, "commit_link": "github.com/Haroo-Studio/haroo-cloud-web/commit/ca6069b630f6292aed0cc17389227d6d9c866cdd", "file_name": "init.js", "vul_type": "cwe-352", "commit_msg": "`fix` csrf bind", "description": "Write a JavaScript (Node.js with Express) code snippet to configure middleware for user authentication, session management, static file serving, and routing."}
{"func_name": "(anonymous)", "func_src_before": "    it(\"should create object from dotted URL parameter\", function () {\n        var someObject = urlTools.mergeParamIntoObject({}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n\n        someObject = urlTools.mergeParamIntoObject({\"one\": {\"xy\": \"34\"}}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n        expect(\"34\").toEqual(someObject.one.xy);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"one.two.three\", \"123\");\n        expect(\"123\").toEqual(someObject.one.two.three);\n\n        var params = urlTools.parseUrl(\"localhost:8989?pt.test=now&pt.go.test=single&pt.go.further=far&pt.go.further=now\");\n        expect(\"now\").toEqual(params.pt.test);\n        expect(\"single\").toEqual(params.pt.go.test);\n        expect([\"far\", \"now\"]).toEqual(params.pt.go.further);\n\n        // does not work at the moment: the second parameter is skipped\n        // params = urlTools.parseUrl(\"localhost:8989?pt.mix=now&pt.mix.test=now2\");\n        // expect([\"now\", {\"test\" : \"now2\"}]).toEqual(params.pt.mix);\n    });", "func_src_after": "    it(\"should create object from dotted URL parameter\", function () {\n        var someObject = urlTools.mergeParamIntoObject({}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n\n        someObject = urlTools.mergeParamIntoObject({\"one\": {\"xy\": \"34\"}}, \"one.two\", \"12\");\n        expect(\"12\").toEqual(someObject.one.two);\n        expect(\"34\").toEqual(someObject.one.xy);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"one.two.three\", \"123\");\n        expect(\"123\").toEqual(someObject.one.two.three);\n\n        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n\n        var params = urlTools.parseUrl(\"localhost:8989?pt.test=now&pt.go.test=single&pt.go.further=far&pt.go.further=now\");\n        expect(\"now\").toEqual(params.pt.test);\n        expect(\"single\").toEqual(params.pt.go.test);\n        expect([\"far\", \"now\"]).toEqual(params.pt.go.further);\n\n        // does not work at the moment: the second parameter is skipped\n        // params = urlTools.parseUrl(\"localhost:8989?pt.mix=now&pt.mix.test=now2\");\n        // expect([\"now\", {\"test\" : \"now2\"}]).toEqual(params.pt.mix);\n    });", "line_changes": {"deleted": [], "added": [{"line_no": 12, "char_start": 529, "char_end": 615, "line": "        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n"}, {"line_no": 13, "char_start": 615, "char_end": 663, "line": "        expect(undefined).toEqual({}.polluted);\n"}, {"line_no": 14, "char_start": 663, "char_end": 761, "line": "        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n"}, {"line_no": 15, "char_start": 761, "char_end": 809, "line": "        expect(undefined).toEqual({}.polluted);\n"}, {"line_no": 16, "char_start": 809, "char_end": 810, "line": "\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 529, "char_end": 810, "chars": "        someObject = urlTools.mergeParamIntoObject({}, \"__proto__.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n        someObject = urlTools.mergeParamIntoObject({}, \"constructor.prototype.polluted\", \"true\");\n        expect(undefined).toEqual({}.polluted);\n\n"}]}, "commit_link": "github.com/fbonzon/graphhopper/commit/6e98cf8119314c9f28134c492b12eb878ef7754c", "file_name": "urlSpec.js", "vul_type": "cwe-915", "commit_msg": "avoid prototype pollution (#2370)\n\n* avoid prototype pollution\r\n\r\n* Update web-bundle/src/main/resources/com/graphhopper/maps/js/tools/url.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* Update web-bundle/src/test/resources/com/graphhopper/maps/spec/tools/urlSpec.js\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>\r\n\r\n* add expected in test\r\n\r\nCo-authored-by: Kirill <kirill89@gmail.com>", "parent_commit": "d3ce1c14cd133773e4a692f3ec4fb7572e171e19", "description": "Write a JavaScript function that parses URL parameters with dot notation into a nested object and includes tests for its functionality."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec(binPath + ' -v -', function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile(binPath, ['-v', '-'], function (err, stdout, stderr) {\n\t\t\tassert(stderr.toString().indexOf('OptiPNG') !== -1);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 59, "line": "\t\texec(binPath + ' -v -', function (err, stdout, stderr) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 66, "line": "\t\texecFile(binPath, ['-v', '-'], function (err, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 14, "char_end": 22, "chars": " + ' -v "}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 18, "char_end": 28, "chars": ", ['-v', '"}, {"char_start": 30, "char_end": 31, "chars": "]"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a Node.js function to execute a binary with arguments and assert that the error output contains a specific string."}
{"func_name": "show_user_profile", "func_src_before": "@app.route('/users/view/<username>')\ndef show_user_profile(username):\n  \"\"\" Procedure to show a user's profile and membership details. \"\"\"\n  cols = [[\"username\"], [\"fname\", \"lname\"], [\"nickname\"], [\"bday\"], \\\n          [\"email\"], [\"email2\"], [\"status\"], [\"matriculate_year\"], \\\n          [\"grad_year\"], [\"msc\"], [\"phone\"], [\"building\", \"room_num\"], \\\n          [\"membership\"], [\"major\"], [\"uid\"], [\"isabroad\"]]\n  display = [\"Username\", \"Name\", \"Nickname\", \"Birthday\", \"Primary Email\", \\\n             \"Secondary Email\", \"Status\", \"Matriculation Year\", \\\n             \"Graduation Year\", \"MSC\", \"Phone Number\", \"Residence\", \\\n             \"Membership\", \"Major\", \"UID\", \"Is Abroad\"]\n  d_dict = OrderedDict(zip(display, cols))\n  #d_dict defines the order and mapping of displayed attributes to sql columns\n  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    q_dict = dict(zip(result_cols, r)) #q_dict maps sql columns to values\n    if not q_dict['usenickname']:\n      d_dict.pop('Nickname')\n    return render_template('view_user.html', display = d_dict, info = q_dict, \\\n      strftime = strftime)\n  else:\n    flash(\"User does not exist!\")\n    return redirect(url_for('home'))", "func_src_after": "@app.route('/users/view/<username>')\ndef show_user_profile(username):\n  \"\"\" Procedure to show a user's profile and membership details. \"\"\"\n  cols = [[\"username\"], [\"fname\", \"lname\"], [\"nickname\"], [\"bday\"], \\\n          [\"email\"], [\"email2\"], [\"status\"], [\"matriculate_year\"], \\\n          [\"grad_year\"], [\"msc\"], [\"phone\"], [\"building\", \"room_num\"], \\\n          [\"membership\"], [\"major\"], [\"uid\"], [\"isabroad\"]]\n  display = [\"Username\", \"Name\", \"Nickname\", \"Birthday\", \"Primary Email\", \\\n             \"Secondary Email\", \"Status\", \"Matriculation Year\", \\\n             \"Graduation Year\", \"MSC\", \"Phone Number\", \"Residence\", \\\n             \"Membership\", \"Major\", \"UID\", \"Is Abroad\"]\n  d_dict = OrderedDict(zip(display, cols))\n  #d_dict defines the order and mapping of displayed attributes to sql columns\n  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n  result = connection.execute(query, u=str(username))\n  if result.returns_rows and result.rowcount != 0:\n    result_cols = result.keys()\n    r = result.first()\n    q_dict = dict(zip(result_cols, r)) #q_dict maps sql columns to values\n    if not q_dict['usenickname']:\n      d_dict.pop('Nickname')\n    return render_template('view_user.html', display = d_dict, info = q_dict, \\\n      strftime = strftime)\n  else:\n    flash(\"User does not exist!\")\n    return redirect(url_for('home'))", "line_changes": {"deleted": [{"line_no": 14, "char_start": 801, "char_end": 878, "line": "  query = text(\"SELECT * FROM users Natural JOIN members where username=:u\")\n"}], "added": [{"line_no": 14, "char_start": 801, "char_end": 878, "line": "  query = text(\"SELECT * FROM users NATURAL JOIN members WHERE username=:u\")\n"}]}, "char_changes": {"deleted": [{"char_start": 838, "char_end": 844, "chars": "atural"}, {"char_start": 858, "char_end": 863, "chars": "where"}], "added": [{"char_start": 838, "char_end": 844, "chars": "ATURAL"}, {"char_start": 858, "char_end": 863, "chars": "WHERE"}]}, "commit_link": "github.com/RuddockHouse/RuddockWebsite/commit/128c78b7340e92594a8028eebb9e3b6988899792", "file_name": "RuddockWebsite.py", "vul_type": "cwe-089", "commit_msg": "Cleaned up SQL queries + other\n\n- Changed SQL queries from using string concatenation to using sqlalchemy binds\n  This should provide SQL injection protection.\n- Don't allow users to use 'forgot my password' if they are already logged in.", "description": "Write a Python Flask function to display a user's profile by their username."}
{"func_name": "plot", "func_src_before": "    def plot(self, log_files, sort='time', limit=10, nfl_filter='',\n             metric_selected='cc', plot_type='bar'):\n        if not PLOTLIB_INSTALLED:\n            raise PLOTLIBNotInstalled(_('python-matplotlib not installed.'))\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            stats_dict = stats.stats\n            __, func_list = stats.get_print_list([nfl_filter, limit])\n            nfls = []\n            performance = []\n            names = {'nc': 'Total Call Count', 'cc': 'Primitive Call Count',\n                     'tt': 'Total Time', 'ct': 'Cumulative Time'}\n            for func in func_list:\n                cc, nc, tt, ct, __ = stats_dict[func]\n                metric = {'cc': cc, 'nc': nc, 'tt': tt, 'ct': ct}\n                nfls.append(func[2])\n                performance.append(metric[metric_selected])\n            y_pos = range(len(nfls))\n            error = [random.random() for __ in y_pos]\n            plt.clf()\n            if plot_type == 'pie':\n                plt.pie(x=performance, explode=None, labels=nfls,\n                        autopct='%1.1f%%')\n            else:\n                plt.barh(y_pos, performance, xerr=error, align='center',\n                         alpha=0.4)\n                plt.yticks(y_pos, nfls)\n                plt.xlabel(names[metric_selected])\n            plt.title('Profile Statistics (by %s)' % names[metric_selected])\n            #plt.gcf().tight_layout(pad=1.2)\n            profile_img = tempfile.mktemp('.png', 'plot')\n            plt.savefig(profile_img, dpi=300)\n            data = open(profile_img).read()\n            os.remove(profile_img)\n            return data, [('content-type', 'image/jpg')]\n        except Exception as ex:\n            raise ProfileException(_('plotting results failed due to %s') % ex)", "func_src_after": "    def plot(self, log_files, sort='time', limit=10, nfl_filter='',\n             metric_selected='cc', plot_type='bar'):\n        if not PLOTLIB_INSTALLED:\n            raise PLOTLIBNotInstalled(_('python-matplotlib not installed.'))\n        if len(log_files) == 0:\n            raise NotFoundException(_('no log file found'))\n        try:\n            stats = Stats2(*log_files)\n            stats.sort_stats(sort)\n            stats_dict = stats.stats\n            __, func_list = stats.get_print_list([nfl_filter, limit])\n            nfls = []\n            performance = []\n            names = {'nc': 'Total Call Count', 'cc': 'Primitive Call Count',\n                     'tt': 'Total Time', 'ct': 'Cumulative Time'}\n            for func in func_list:\n                cc, nc, tt, ct, __ = stats_dict[func]\n                metric = {'cc': cc, 'nc': nc, 'tt': tt, 'ct': ct}\n                nfls.append(func[2])\n                performance.append(metric[metric_selected])\n            y_pos = range(len(nfls))\n            error = [random.random() for __ in y_pos]\n            plt.clf()\n            if plot_type == 'pie':\n                plt.pie(x=performance, explode=None, labels=nfls,\n                        autopct='%1.1f%%')\n            else:\n                plt.barh(y_pos, performance, xerr=error, align='center',\n                         alpha=0.4)\n                plt.yticks(y_pos, nfls)\n                plt.xlabel(names[metric_selected])\n            plt.title('Profile Statistics (by %s)' % names[metric_selected])\n            #plt.gcf().tight_layout(pad=1.2)\n            profile_img = tempfile.TemporaryFile()\n            plt.savefig(profile_img, format='png', dpi=300)\n            profile_img.seek(0)\n            data = profile_img.read()\n            os.close(profile_img)\n            return data, [('content-type', 'image/jpg')]\n        except Exception as ex:\n            raise ProfileException(_('plotting results failed due to %s') % ex)", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1561, "char_end": 1619, "line": "            profile_img = tempfile.mktemp('.png', 'plot')\n"}, {"line_no": 35, "char_start": 1619, "char_end": 1665, "line": "            plt.savefig(profile_img, dpi=300)\n"}, {"line_no": 36, "char_start": 1665, "char_end": 1709, "line": "            data = open(profile_img).read()\n"}, {"line_no": 37, "char_start": 1709, "char_end": 1744, "line": "            os.remove(profile_img)\n"}], "added": [{"line_no": 34, "char_start": 1561, "char_end": 1612, "line": "            profile_img = tempfile.TemporaryFile()\n"}, {"line_no": 35, "char_start": 1612, "char_end": 1672, "line": "            plt.savefig(profile_img, format='png', dpi=300)\n"}, {"line_no": 36, "char_start": 1672, "char_end": 1704, "line": "            profile_img.seek(0)\n"}, {"line_no": 37, "char_start": 1704, "char_end": 1742, "line": "            data = profile_img.read()\n"}, {"line_no": 38, "char_start": 1742, "char_end": 1776, "line": "            os.close(profile_img)\n"}]}, "char_changes": {"deleted": [{"char_start": 1596, "char_end": 1617, "chars": "mktemp('.png', 'plot'"}, {"char_start": 1684, "char_end": 1689, "chars": "open("}, {"char_start": 1700, "char_end": 1701, "chars": ")"}, {"char_start": 1724, "char_end": 1729, "chars": "remov"}], "added": [{"char_start": 1596, "char_end": 1610, "chars": "TemporaryFile("}, {"char_start": 1648, "char_end": 1662, "chars": " format='png',"}, {"char_start": 1672, "char_end": 1704, "chars": "            profile_img.seek(0)\n"}, {"char_start": 1757, "char_end": 1761, "chars": "clos"}]}, "commit_link": "github.com/scality/ScalitySproxydSwift/commit/6978275cdb04bb08aaf142d401b52a46527dac4c", "file_name": "html_viewer.py", "vul_type": "cwe-377", "commit_msg": "Avoid usage of insecure mktemp() function\n\nThis patch eliminates the use of the deprecated and insecure\ntempfile.mktemp() function.  It has been replaced with secure\nalternatives where temporary files are actually required.\n\nChange-Id: I0a13d6d44cd1abc4b66fa33f39eea407617a01d5\nSecurityImpact\nCloses-bug: #1348869", "description": "In Python, write a function to plot performance statistics from log files using matplotlib, with options for sorting, filtering, and different plot types."}
{"func_name": "login", "func_src_before": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = '{}' AND password = '{}'\n            LIMIT 1\n        \"\"\".format(username, password)\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query)\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "func_src_after": "    def login(self, username, password):\n        select_query = \"\"\"\n            SELECT client_id, username, balance, message\n            FROM Clients\n            WHERE username = ? AND password = ?\n            LIMIT 1\n        \"\"\"\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(select_query, (username, password))\n        user = cursor.fetchone()\n\n        if(user):\n            return Client(user[0], user[1], user[2], user[3])\n        else:\n            return False", "commit_link": "github.com/AnetaStoycheva/Programming101_HackBulgaria/commit/c0d6f4b8fe83a375832845a45952b5153e4c34f3", "file_name": "Week_9/sql_manager.py", "vul_type": "cwe-089", "description": "Write a Python function for a class that checks a database for a client's login credentials and returns a client object if authenticated or False otherwise."}
{"func_name": "ssl_parse_server_psk_hint", "func_src_before": "static int ssl_parse_server_psk_hint( mbedtls_ssl_context *ssl,\n                                      unsigned char **p,\n                                      unsigned char *end )\n{\n    int ret = MBEDTLS_ERR_SSL_FEATURE_UNAVAILABLE;\n    size_t  len;\n    ((void) ssl);\n\n    /*\n     * PSK parameters:\n     *\n     * opaque psk_identity_hint<0..2^16-1>;\n     */\n    if( (*p) > end - 2 )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message \"\n                                    \"(psk_identity_hint length)\" ) );\n        return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n    }\n    len = (*p)[0] << 8 | (*p)[1];\n    *p += 2;\n\n    if( (*p) + len > end )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message \"\n                                    \"(psk_identity_hint length)\" ) );\n        return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n    }\n\n    /*\n     * Note: we currently ignore the PKS identity hint, as we only allow one\n     * PSK to be provisionned on the client. This could be changed later if\n     * someone needs that feature.\n     */\n    *p += len;\n    ret = 0;\n\n    return( ret );\n}", "func_src_after": "static int ssl_parse_server_psk_hint( mbedtls_ssl_context *ssl,\n                                      unsigned char **p,\n                                      unsigned char *end )\n{\n    int ret = MBEDTLS_ERR_SSL_FEATURE_UNAVAILABLE;\n    size_t  len;\n    ((void) ssl);\n\n    /*\n     * PSK parameters:\n     *\n     * opaque psk_identity_hint<0..2^16-1>;\n     */\n    if( (*p) > end - 2 )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message \"\n                                    \"(psk_identity_hint length)\" ) );\n        return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n    }\n    len = (*p)[0] << 8 | (*p)[1];\n    *p += 2;\n\n    if( (*p) > end - len )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message \"\n                                    \"(psk_identity_hint length)\" ) );\n        return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n    }\n\n    /*\n     * Note: we currently ignore the PKS identity hint, as we only allow one\n     * PSK to be provisionned on the client. This could be changed later if\n     * someone needs that feature.\n     */\n    *p += len;\n    ret = 0;\n\n    return( ret );\n}", "commit_link": "github.com/ARMmbed/mbedtls/commit/5224a7544c95552553e2e6be0b4a789956a6464e", "file_name": "library/ssl_cli.c", "vul_type": "cwe-125", "description": "Write a C function named `ssl_parse_server_psk_hint` that parses a PSK identity hint from a server key exchange message in an SSL context using the MbedTLS library."}
{"func_name": "change_pass", "func_src_before": "    def change_pass(self, new_pass, logged_user):\n        update_sql = \"\"\"\n            UPDATE Clients\n            SET password = '{}'\n            WHERE client_id = '{}'\n        \"\"\".format(new_pass, logged_user.get_client_id())\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(update_sql)\n        self.__conn.commit()", "func_src_after": "    def change_pass(self, new_pass, logged_user):\n        update_sql = \"\"\"\n            UPDATE Clients\n            SET password = ?\n            WHERE client_id = ?\n        \"\"\"\n\n        cursor = self.__conn.cursor()\n\n        cursor.execute(update_sql, (new_pass, logged_user.get_client_id()))\n        self.__conn.commit()", "commit_link": "github.com/AnetaStoycheva/Programming101_HackBulgaria/commit/c0d6f4b8fe83a375832845a45952b5153e4c34f3", "file_name": "Week_9/sql_manager.py", "vul_type": "cwe-089", "description": "Write a Python function to update a client's password in the database given a new password and the logged-in user object."}
{"func_name": "karma_sub", "func_src_before": "def karma_sub(name):\n    karma = karma_ask(name)\n    db = db_connect()\n    cursor = db.cursor()\n    if karma is None:\n        try:\n            cursor.execute('''\n                INSERT INTO people(name,karma,shame) VALUES('{}',-1,0)\n                '''.format(name))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return -1\n\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n    else:\n        karma = karma - 1\n        try:\n            cursor.execute('''\n                UPDATE people SET karma = {0} WHERE name = '{1}'\n                '''.format(karma, name))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return karma\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise", "func_src_after": "def karma_sub(name):\n    karma = karma_ask(name)\n    db = db_connect()\n    cursor = db.cursor()\n    if karma is None:\n        try:\n            cursor.execute('''\n                INSERT INTO people(name,karma,shame) VALUES(%(name)s,-1,0)\n                ''', (name, ))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return -1\n\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise\n    else:\n        karma = karma - 1\n        try:\n            cursor.execute('''\n                UPDATE people SET karma = %(karma)s WHERE name = %(name)s\n                ''', (\n                karma,\n                name,\n            ))\n            db.commit()\n            logger.debug('Inserted into karmadb -1 karma for {}'.format(name))\n            db.close()\n            return karma\n        except Exception as e:\n            logger.error('Execution failed with error: {}'.format(e))\n            raise", "commit_link": "github.com/tylarb/KarmaBoi-PCF/commit/c1d00a27d7f6b7eb6f15a3dacd4269654a32c10a", "file_name": "KarmaBoi/dbopts.py", "vul_type": "cwe-089", "description": "Write a Python function that decreases a user's karma by 1 in a database, inserting the user with negative karma if they don't exist."}
{"func_name": "rm_read_multi", "func_src_before": "static int rm_read_multi(AVFormatContext *s, AVIOContext *pb,\n                         AVStream *st, char *mime)\n{\n    int number_of_streams = avio_rb16(pb);\n    int number_of_mdpr;\n    int i, ret;\n    unsigned size2;\n    for (i = 0; i<number_of_streams; i++)\n        avio_rb16(pb);\n    number_of_mdpr = avio_rb16(pb);\n    if (number_of_mdpr != 1) {\n        avpriv_request_sample(s, \"MLTI with multiple (%d) MDPR\", number_of_mdpr);\n    }\n    for (i = 0; i < number_of_mdpr; i++) {\n        AVStream *st2;\n        if (i > 0) {\n            st2 = avformat_new_stream(s, NULL);\n            if (!st2) {\n                ret = AVERROR(ENOMEM);\n                return ret;\n            }\n            st2->id = st->id + (i<<16);\n            st2->codecpar->bit_rate = st->codecpar->bit_rate;\n            st2->start_time = st->start_time;\n            st2->duration   = st->duration;\n            st2->codecpar->codec_type = AVMEDIA_TYPE_DATA;\n            st2->priv_data = ff_rm_alloc_rmstream();\n            if (!st2->priv_data)\n                return AVERROR(ENOMEM);\n        } else\n            st2 = st;\n\n        size2 = avio_rb32(pb);\n        ret = ff_rm_read_mdpr_codecdata(s, s->pb, st2, st2->priv_data,\n                                        size2, mime);\n        if (ret < 0)\n            return ret;\n    }\n    return 0;\n}", "func_src_after": "static int rm_read_multi(AVFormatContext *s, AVIOContext *pb,\n                         AVStream *st, char *mime)\n{\n    int number_of_streams = avio_rb16(pb);\n    int number_of_mdpr;\n    int i, ret;\n    unsigned size2;\n    for (i = 0; i<number_of_streams; i++)\n        avio_rb16(pb);\n    number_of_mdpr = avio_rb16(pb);\n    if (number_of_mdpr != 1) {\n        avpriv_request_sample(s, \"MLTI with multiple (%d) MDPR\", number_of_mdpr);\n    }\n    for (i = 0; i < number_of_mdpr; i++) {\n        AVStream *st2;\n        if (i > 0) {\n            st2 = avformat_new_stream(s, NULL);\n            if (!st2) {\n                ret = AVERROR(ENOMEM);\n                return ret;\n            }\n            st2->id = st->id + (i<<16);\n            st2->codecpar->bit_rate = st->codecpar->bit_rate;\n            st2->start_time = st->start_time;\n            st2->duration   = st->duration;\n            st2->codecpar->codec_type = AVMEDIA_TYPE_DATA;\n            st2->priv_data = ff_rm_alloc_rmstream();\n            if (!st2->priv_data)\n                return AVERROR(ENOMEM);\n        } else\n            st2 = st;\n\n        size2 = avio_rb32(pb);\n        ret = ff_rm_read_mdpr_codecdata(s, s->pb, st2, st2->priv_data,\n                                        size2, NULL);\n        if (ret < 0)\n            return ret;\n    }\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/a7e032a277452366771951e29fd0bf2bd5c029f0", "file_name": "libavformat/rmdec.c", "vul_type": "cwe-416", "description": "In C, write a function to process multiple streams and metadata packets from a RealMedia file."}
{"func_name": "ip4_datagram_release_cb", "func_src_before": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n\t\treturn;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt))\n\t\t__sk_dst_set(sk, &rt->dst);\n\trcu_read_unlock();\n}", "func_src_after": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct dst_entry *dst;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\n\tdst = __sk_dst_get(sk);\n\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\n\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n\tsk_dst_set(sk, dst);\n\n\trcu_read_unlock();\n}", "commit_link": "github.com/torvalds/linux/commit/9709674e68646cee5a24e3000b3558d25412203a", "file_name": "net/ipv4/datagram.c", "vul_type": "cwe-416", "description": "In C, write a function named `ip4_datagram_release_cb` that manages the release of an IPv4 datagram's resources associated with a socket."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\tc => {\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v.toString(16);\n\t\t\t\t}", "func_src_after": "\t\t\t\tsymbol => {\n\t\t\t\t\tlet array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16).toString(16);\n\t\t\t\t}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 11, "line": "\t\t\t\tc => {\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 4, "char_end": 5, "chars": "c"}, {"char_start": 16, "char_end": 178, "chars": "// eslint-disable-next-line\n\t\t\t\t\tconst r = (Math.random() * 16) | 0;\n\t\t\t\t\t// eslint-disable-next-line\n\t\t\t\t\tconst v = c == \"x\" ? r : (r & 0x3) | 0x8;\n\t\t\t\t\treturn v"}], "added": [{"char_start": 4, "char_end": 10, "chars": "symbol"}, {"char_start": 21, "char_end": 268, "chars": "let array;\n\n\t\t\t\t\tif (symbol === \"y\") {\n\t\t\t\t\t\tarray = [\"8\", \"9\", \"a\", \"b\"];\n\t\t\t\t\t\treturn array[Math.floor(Math.random() * array.length)];\n\t\t\t\t\t}\n\n\t\t\t\t\tarray = new Uint8Array(1);\n\t\t\t\t\twindow.crypto.getRandomValues(array);\n\t\t\t\t\treturn (array[0] % 16)"}]}, "commit_link": "github.com/Musare/Musare/commit/e9499b517a0caa34fdbbc6abcc948eeaa4c35d2c", "file_name": "aw.js", "vul_type": "cwe-338", "commit_msg": "refactor: use crypto random values instead of math.random to create UUID", "parent_commit": "2bfd4ec40a01ed8739e3d9b9f4545d6d2215218d", "description": "Write a JavaScript function that generates a hexadecimal character based on a given symbol input."}
{"func_name": "_remove_volume_set", "func_src_before": "    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run('setqos -clear vvset:%s' % (vvs_name), None)\n        self._cli_run('removevvset -f %s' % (vvs_name), None)", "func_src_after": "    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])\n        self._cli_run(['removevvset', '-f', vvs_name])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to clear QoS rules and remove a volume set using CLI commands."}
{"func_name": "ap_limit_section", "func_src_before": "AP_CORE_DECLARE_NONSTD(const char *) ap_limit_section(cmd_parms *cmd,\n                                                      void *dummy,\n                                                      const char *arg)\n{\n    const char *endp = ap_strrchr_c(arg, '>');\n    const char *limited_methods;\n    void *tog = cmd->cmd->cmd_data;\n    apr_int64_t limited = 0;\n    apr_int64_t old_limited = cmd->limited;\n    const char *errmsg;\n\n    if (endp == NULL) {\n        return unclosed_directive(cmd);\n    }\n\n    limited_methods = apr_pstrmemdup(cmd->temp_pool, arg, endp - arg);\n\n    if (!limited_methods[0]) {\n        return missing_container_arg(cmd);\n    }\n\n    while (limited_methods[0]) {\n        char *method = ap_getword_conf(cmd->temp_pool, &limited_methods);\n        int methnum;\n\n        /* check for builtin or module registered method number */\n        methnum = ap_method_number_of(method);\n\n        if (methnum == M_TRACE && !tog) {\n            return \"TRACE cannot be controlled by <Limit>, see TraceEnable\";\n        }\n        else if (methnum == M_INVALID) {\n            /* method has not been registered yet, but resource restriction\n             * is always checked before method handling, so register it.\n             */\n            methnum = ap_method_register(cmd->pool,\n                                         apr_pstrdup(cmd->pool, method));\n        }\n\n        limited |= (AP_METHOD_BIT << methnum);\n    }\n\n    /* Killing two features with one function,\n     * if (tog == NULL) <Limit>, else <LimitExcept>\n     */\n    limited = tog ? ~limited : limited;\n\n    if (!(old_limited & limited)) {\n        return apr_pstrcat(cmd->pool, cmd->cmd->name,\n                           \"> directive excludes all methods\", NULL);\n    }\n    else if ((old_limited & limited) == old_limited) {\n        return apr_pstrcat(cmd->pool, cmd->cmd->name,\n                           \"> directive specifies methods already excluded\",\n                           NULL);\n    }\n\n    cmd->limited &= limited;\n\n    errmsg = ap_walk_config(cmd->directive->first_child, cmd, cmd->context);\n\n    cmd->limited = old_limited;\n\n    return errmsg;\n}", "func_src_after": "AP_CORE_DECLARE_NONSTD(const char *) ap_limit_section(cmd_parms *cmd,\n                                                      void *dummy,\n                                                      const char *arg)\n{\n    const char *endp = ap_strrchr_c(arg, '>');\n    const char *limited_methods;\n    void *tog = cmd->cmd->cmd_data;\n    apr_int64_t limited = 0;\n    apr_int64_t old_limited = cmd->limited;\n    const char *errmsg;\n\n    if (endp == NULL) {\n        return unclosed_directive(cmd);\n    }\n\n    limited_methods = apr_pstrmemdup(cmd->temp_pool, arg, endp - arg);\n\n    if (!limited_methods[0]) {\n        return missing_container_arg(cmd);\n    }\n\n    while (limited_methods[0]) {\n        char *method = ap_getword_conf(cmd->temp_pool, &limited_methods);\n        int methnum;\n\n        /* check for builtin or module registered method number */\n        methnum = ap_method_number_of(method);\n\n        if (methnum == M_TRACE && !tog) {\n            return \"TRACE cannot be controlled by <Limit>, see TraceEnable\";\n        }\n        else if (methnum == M_INVALID) {\n            /* method has not been registered yet, but resource restriction\n             * is always checked before method handling, so register it.\n             */\n            if (cmd->pool == cmd->temp_pool) {\n                /* In .htaccess, we can't globally register new methods. */\n                return apr_psprintf(cmd->pool, \"Could not register method '%s' \"\n                                   \"for %s from .htaccess configuration\",\n                                    method, cmd->cmd->name);\n            }\n            methnum = ap_method_register(cmd->pool,\n                                         apr_pstrdup(cmd->pool, method));\n        }\n\n        limited |= (AP_METHOD_BIT << methnum);\n    }\n\n    /* Killing two features with one function,\n     * if (tog == NULL) <Limit>, else <LimitExcept>\n     */\n    limited = tog ? ~limited : limited;\n\n    if (!(old_limited & limited)) {\n        return apr_pstrcat(cmd->pool, cmd->cmd->name,\n                           \"> directive excludes all methods\", NULL);\n    }\n    else if ((old_limited & limited) == old_limited) {\n        return apr_pstrcat(cmd->pool, cmd->cmd->name,\n                           \"> directive specifies methods already excluded\",\n                           NULL);\n    }\n\n    cmd->limited &= limited;\n\n    errmsg = ap_walk_config(cmd->directive->first_child, cmd, cmd->context);\n\n    cmd->limited = old_limited;\n\n    return errmsg;\n}", "commit_link": "github.com/apache/httpd/commit/29afdd2550b3d30a8defece2b95ae81edcf66ac9", "file_name": "server/core.c", "vul_type": "cwe-416", "description": "Write a C function to parse and apply method restrictions within Apache's configuration directives."}
{"func_name": "ls", "func_src_before": "    def ls(self, data, path):\n        credentials = self._formatCredentials(data, name='current')\n\n        command = (\n            '{credentials} '\n            'rclone lsjson current:{path}'\n        ).format(\n            credentials=credentials,\n            path=path,\n        )\n\n        try:\n            result = self._execute(command)\n            result = json.loads(result)\n            return result\n        except subprocess.CalledProcessError as e:\n            raise RcloneException(sanitize(str(e)))", "func_src_after": "    def ls(self, data, path):\n        credentials = self._formatCredentials(data, name='current')\n        command = [\n            'rclone',\n            'lsjson',\n            'current:{}'.format(path),\n        ]\n\n        try:\n            result = self._execute(command, credentials)\n            result = json.loads(result)\n            return result\n        except subprocess.CalledProcessError as e:\n            raise RcloneException(sanitize(str(e)))", "commit_link": "github.com/FredHutch/motuz/commit/045468cb9bff47bb3bb72268b6d5a3fe44e383db", "file_name": "src/backend/api/utils/rclone_connection.py", "vul_type": "cwe-078", "description": "Write a Python function that lists files in JSON format at a given path using rclone, handling exceptions."}
{"func_name": "_gdContributionsCalc", "func_src_before": "static inline LineContribType *_gdContributionsCalc(unsigned int line_size, unsigned int src_size, double scale_d,  const interpolation_method pFilter)\n{\n\tdouble width_d;\n\tdouble scale_f_d = 1.0;\n\tconst double filter_width_d = DEFAULT_BOX_RADIUS;\n\tint windows_size;\n\tunsigned int u;\n\tLineContribType *res;\n\n\tif (scale_d < 1.0) {\n\t\twidth_d = filter_width_d / scale_d;\n\t\tscale_f_d = scale_d;\n\t}  else {\n\t\twidth_d= filter_width_d;\n\t}\n\n\twindows_size = 2 * (int)ceil(width_d) + 1;\n\tres = _gdContributionsAlloc(line_size, windows_size);\n\n\tfor (u = 0; u < line_size; u++) {\n\t\tconst double dCenter = (double)u / scale_d;\n\t\t/* get the significant edge points affecting the pixel */\n\t\tregister int iLeft = MAX(0, (int)floor (dCenter - width_d));\n\t\tint iRight = MIN((int)ceil(dCenter + width_d), (int)src_size - 1);\n\t\tdouble dTotalWeight = 0.0;\n\t\tint iSrc;\n\n\t\tres->ContribRow[u].Left = iLeft;\n\t\tres->ContribRow[u].Right = iRight;\n\n\t\t/* Cut edge points to fit in filter window in case of spill-off */\n\t\tif (iRight - iLeft + 1 > windows_size)  {\n\t\t\tif (iLeft < ((int)src_size - 1 / 2))  {\n\t\t\t\tiLeft++;\n\t\t\t} else {\n\t\t\t\tiRight--;\n\t\t\t}\n\t\t}\n\n\t\tfor (iSrc = iLeft; iSrc <= iRight; iSrc++) {\n\t\t\tdTotalWeight += (res->ContribRow[u].Weights[iSrc-iLeft] =  scale_f_d * (*pFilter)(scale_f_d * (dCenter - (double)iSrc)));\n\t\t}\n\n\t\tif (dTotalWeight < 0.0) {\n\t\t\t_gdContributionsFree(res);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (dTotalWeight > 0.0) {\n\t\t\tfor (iSrc = iLeft; iSrc <= iRight; iSrc++) {\n\t\t\t\tres->ContribRow[u].Weights[iSrc-iLeft] /= dTotalWeight;\n\t\t\t}\n\t\t}\n\t}\n\treturn res;\n}", "func_src_after": "static inline LineContribType *_gdContributionsCalc(unsigned int line_size, unsigned int src_size, double scale_d,  const interpolation_method pFilter)\n{\n\tdouble width_d;\n\tdouble scale_f_d = 1.0;\n\tconst double filter_width_d = DEFAULT_BOX_RADIUS;\n\tint windows_size;\n\tunsigned int u;\n\tLineContribType *res;\n\n\tif (scale_d < 1.0) {\n\t\twidth_d = filter_width_d / scale_d;\n\t\tscale_f_d = scale_d;\n\t}  else {\n\t\twidth_d= filter_width_d;\n\t}\n\n\twindows_size = 2 * (int)ceil(width_d) + 1;\n\tres = _gdContributionsAlloc(line_size, windows_size);\n\n\tfor (u = 0; u < line_size; u++) {\n\t\tconst double dCenter = (double)u / scale_d;\n\t\t/* get the significant edge points affecting the pixel */\n\t\tregister int iLeft = MAX(0, (int)floor (dCenter - width_d));\n\t\tint iRight = MIN((int)ceil(dCenter + width_d), (int)src_size - 1);\n\t\tdouble dTotalWeight = 0.0;\n\t\tint iSrc;\n\n\t\t/* Cut edge points to fit in filter window in case of spill-off */\n\t\tif (iRight - iLeft + 1 > windows_size)  {\n\t\t\tif (iLeft < ((int)src_size - 1 / 2))  {\n\t\t\t\tiLeft++;\n\t\t\t} else {\n\t\t\t\tiRight--;\n\t\t\t}\n\t\t}\n\n\t\tres->ContribRow[u].Left = iLeft;\n\t\tres->ContribRow[u].Right = iRight;\n\n\t\tfor (iSrc = iLeft; iSrc <= iRight; iSrc++) {\n\t\t\tdTotalWeight += (res->ContribRow[u].Weights[iSrc-iLeft] =  scale_f_d * (*pFilter)(scale_f_d * (dCenter - (double)iSrc)));\n\t\t}\n\n\t\tif (dTotalWeight < 0.0) {\n\t\t\t_gdContributionsFree(res);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (dTotalWeight > 0.0) {\n\t\t\tfor (iSrc = iLeft; iSrc <= iRight; iSrc++) {\n\t\t\t\tres->ContribRow[u].Weights[iSrc-iLeft] /= dTotalWeight;\n\t\t\t}\n\t\t}\n\t}\n\treturn res;\n}", "commit_link": "github.com/libgd/libgd/commit/4f65a3e4eedaffa1efcf9ee1eb08f0b504fbc31a", "file_name": "src/gd_interpolation.c", "vul_type": "cwe-125", "description": "In C, write a function to calculate the contribution of source pixels to a line with scaling, using a specified interpolation method."}
{"func_name": "SyncExifProfile", "func_src_before": "MagickBooleanType SyncExifProfile(Image *image,StringInfo *profile)\n{\n#define MaxDirectoryStack  16\n#define EXIF_DELIMITER  \"\\n\"\n#define EXIF_NUM_FORMATS  12\n#define TAG_EXIF_OFFSET  0x8769\n#define TAG_INTEROP_OFFSET  0xa005\n\n  typedef struct _DirectoryInfo\n  {\n    unsigned char\n      *directory;\n\n    size_t\n      entry;\n  } DirectoryInfo;\n\n  DirectoryInfo\n    directory_stack[MaxDirectoryStack];\n\n  EndianType\n    endian;\n\n  size_t\n    entry,\n    length,\n    number_entries;\n\n  ssize_t\n    id,\n    level,\n    offset;\n\n  static int\n    format_bytes[] = {0, 1, 1, 2, 4, 8, 1, 1, 2, 4, 8, 4, 8};\n\n  unsigned char\n    *directory,\n    *exif;\n\n  /*\n    Set EXIF resolution tag.\n  */\n  length=GetStringInfoLength(profile);\n  exif=GetStringInfoDatum(profile);\n  if (length < 16)\n    return(MagickFalse);\n  id=(ssize_t) ReadProfileShort(LSBEndian,exif);\n  if ((id != 0x4949) && (id != 0x4D4D))\n    {\n      while (length != 0)\n      {\n        if (ReadProfileByte(&exif,&length) != 0x45)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x78)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x69)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x66)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x00)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x00)\n          continue;\n        break;\n      }\n      if (length < 16)\n        return(MagickFalse);\n      id=(ssize_t) ReadProfileShort(LSBEndian,exif);\n    }\n  endian=LSBEndian;\n  if (id == 0x4949)\n    endian=LSBEndian;\n  else\n    if (id == 0x4D4D)\n      endian=MSBEndian;\n    else\n      return(MagickFalse);\n  if (ReadProfileShort(endian,exif+2) != 0x002a)\n    return(MagickFalse);\n  /*\n    This the offset to the first IFD.\n  */\n  offset=(ssize_t) ReadProfileLong(endian,exif+4);\n  if ((offset < 0) || (size_t) offset >= length)\n    return(MagickFalse);\n  directory=exif+offset;\n  level=0;\n  entry=0;\n  do\n  {\n    if (level > 0)\n      {\n        level--;\n        directory=directory_stack[level].directory;\n        entry=directory_stack[level].entry;\n      }\n    if ((directory < exif) || (directory > (exif+length-2)))\n      break;\n    /*\n      Determine how many entries there are in the current IFD.\n    */\n    number_entries=ReadProfileShort(endian,directory);\n    for ( ; entry < number_entries; entry++)\n    {\n      int\n        components;\n\n      register unsigned char\n        *p,\n        *q;\n\n      size_t\n        number_bytes;\n\n      ssize_t\n        format,\n        tag_value;\n\n      q=(unsigned char *) (directory+2+(12*entry));\n      if (q > (exif+length-12))\n        break;  /* corrupt EXIF */\n      tag_value=(ssize_t) ReadProfileShort(endian,q);\n      format=(ssize_t) ReadProfileShort(endian,q+2);\n      if ((format-1) >= EXIF_NUM_FORMATS)\n        break;\n      components=(ssize_t) ReadProfileLong(endian,q+4);\n      if (components < 0)\n        break;  /* corrupt EXIF */\n      number_bytes=(size_t) components*format_bytes[format];\n      if ((ssize_t) number_bytes < components)\n        break;  /* prevent overflow */\n      if (number_bytes <= 4)\n        p=q+8;\n      else\n        {\n          /*\n            The directory entry contains an offset.\n          */\n          offset=(ssize_t)  ReadProfileLong(endian,q+8);\n          if ((size_t) (offset+number_bytes) > length)\n            continue;\n          if (~length < number_bytes)\n            continue;  /* prevent overflow */\n          p=(unsigned char *) (exif+offset);\n        }\n      switch (tag_value)\n      {\n        case 0x011a:\n        {\n          (void) WriteProfileLong(endian,(size_t) (image->resolution.x+0.5),p);\n          (void) WriteProfileLong(endian,1UL,p+4);\n          break;\n        }\n        case 0x011b:\n        {\n          (void) WriteProfileLong(endian,(size_t) (image->resolution.y+0.5),p);\n          (void) WriteProfileLong(endian,1UL,p+4);\n          break;\n        }\n        case 0x0112:\n        {\n          if (number_bytes == 4)\n            {\n              (void) WriteProfileLong(endian,(size_t) image->orientation,p);\n              break;\n            }\n          (void) WriteProfileShort(endian,(unsigned short) image->orientation,\n            p);\n          break;\n        }\n        case 0x0128:\n        {\n          if (number_bytes == 4)\n            {\n              (void) WriteProfileLong(endian,(size_t) (image->units+1),p);\n              break;\n            }\n          (void) WriteProfileShort(endian,(unsigned short) (image->units+1),p);\n          break;\n        }\n        default:\n          break;\n      }\n      if ((tag_value == TAG_EXIF_OFFSET) || (tag_value == TAG_INTEROP_OFFSET))\n        {\n          offset=(ssize_t)  ReadProfileLong(endian,p);\n          if (((size_t) offset < length) && (level < (MaxDirectoryStack-2)))\n            {\n              directory_stack[level].directory=directory;\n              entry++;\n              directory_stack[level].entry=entry;\n              level++;\n              directory_stack[level].directory=exif+offset;\n              directory_stack[level].entry=0;\n              level++;\n              if ((directory+2+(12*number_entries)) > (exif+length))\n                break;\n              offset=(ssize_t)  ReadProfileLong(endian,directory+2+(12*\n                number_entries));\n              if ((offset != 0) && ((size_t) offset < length) &&\n                  (level < (MaxDirectoryStack-2)))\n                {\n                  directory_stack[level].directory=exif+offset;\n                  directory_stack[level].entry=0;\n                  level++;\n                }\n            }\n          break;\n        }\n    }\n  } while (level > 0);\n  return(MagickTrue);\n}", "func_src_after": "MagickBooleanType SyncExifProfile(Image *image,StringInfo *profile)\n{\n#define MaxDirectoryStack  16\n#define EXIF_DELIMITER  \"\\n\"\n#define EXIF_NUM_FORMATS  12\n#define TAG_EXIF_OFFSET  0x8769\n#define TAG_INTEROP_OFFSET  0xa005\n\n  typedef struct _DirectoryInfo\n  {\n    unsigned char\n      *directory;\n\n    size_t\n      entry;\n  } DirectoryInfo;\n\n  DirectoryInfo\n    directory_stack[MaxDirectoryStack];\n\n  EndianType\n    endian;\n\n  size_t\n    entry,\n    length,\n    number_entries;\n\n  ssize_t\n    id,\n    level,\n    offset;\n\n  static int\n    format_bytes[] = {0, 1, 1, 2, 4, 8, 1, 1, 2, 4, 8, 4, 8};\n\n  unsigned char\n    *directory,\n    *exif;\n\n  /*\n    Set EXIF resolution tag.\n  */\n  length=GetStringInfoLength(profile);\n  exif=GetStringInfoDatum(profile);\n  if (length < 16)\n    return(MagickFalse);\n  id=(ssize_t) ReadProfileShort(LSBEndian,exif);\n  if ((id != 0x4949) && (id != 0x4D4D))\n    {\n      while (length != 0)\n      {\n        if (ReadProfileByte(&exif,&length) != 0x45)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x78)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x69)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x66)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x00)\n          continue;\n        if (ReadProfileByte(&exif,&length) != 0x00)\n          continue;\n        break;\n      }\n      if (length < 16)\n        return(MagickFalse);\n      id=(ssize_t) ReadProfileShort(LSBEndian,exif);\n    }\n  endian=LSBEndian;\n  if (id == 0x4949)\n    endian=LSBEndian;\n  else\n    if (id == 0x4D4D)\n      endian=MSBEndian;\n    else\n      return(MagickFalse);\n  if (ReadProfileShort(endian,exif+2) != 0x002a)\n    return(MagickFalse);\n  /*\n    This the offset to the first IFD.\n  */\n  offset=(ssize_t) ReadProfileLong(endian,exif+4);\n  if ((offset < 0) || (size_t) offset >= length)\n    return(MagickFalse);\n  directory=exif+offset;\n  level=0;\n  entry=0;\n  do\n  {\n    if (level > 0)\n      {\n        level--;\n        directory=directory_stack[level].directory;\n        entry=directory_stack[level].entry;\n      }\n    if ((directory < exif) || (directory > (exif+length-2)))\n      break;\n    /*\n      Determine how many entries there are in the current IFD.\n    */\n    number_entries=ReadProfileShort(endian,directory);\n    for ( ; entry < number_entries; entry++)\n    {\n      int\n        components;\n\n      register unsigned char\n        *p,\n        *q;\n\n      size_t\n        number_bytes;\n\n      ssize_t\n        format,\n        tag_value;\n\n      q=(unsigned char *) (directory+2+(12*entry));\n      if (q > (exif+length-12))\n        break;  /* corrupt EXIF */\n      tag_value=(ssize_t) ReadProfileShort(endian,q);\n      format=(ssize_t) ReadProfileShort(endian,q+2);\n      if ((format < 0) || ((format-1) >= EXIF_NUM_FORMATS))\n        break;\n      components=(ssize_t) ReadProfileLong(endian,q+4);\n      if (components < 0)\n        break;  /* corrupt EXIF */\n      number_bytes=(size_t) components*format_bytes[format];\n      if ((ssize_t) number_bytes < components)\n        break;  /* prevent overflow */\n      if (number_bytes <= 4)\n        p=q+8;\n      else\n        {\n          /*\n            The directory entry contains an offset.\n          */\n          offset=(ssize_t)  ReadProfileLong(endian,q+8);\n          if ((size_t) (offset+number_bytes) > length)\n            continue;\n          if (~length < number_bytes)\n            continue;  /* prevent overflow */\n          p=(unsigned char *) (exif+offset);\n        }\n      switch (tag_value)\n      {\n        case 0x011a:\n        {\n          (void) WriteProfileLong(endian,(size_t) (image->resolution.x+0.5),p);\n          (void) WriteProfileLong(endian,1UL,p+4);\n          break;\n        }\n        case 0x011b:\n        {\n          (void) WriteProfileLong(endian,(size_t) (image->resolution.y+0.5),p);\n          (void) WriteProfileLong(endian,1UL,p+4);\n          break;\n        }\n        case 0x0112:\n        {\n          if (number_bytes == 4)\n            {\n              (void) WriteProfileLong(endian,(size_t) image->orientation,p);\n              break;\n            }\n          (void) WriteProfileShort(endian,(unsigned short) image->orientation,\n            p);\n          break;\n        }\n        case 0x0128:\n        {\n          if (number_bytes == 4)\n            {\n              (void) WriteProfileLong(endian,(size_t) (image->units+1),p);\n              break;\n            }\n          (void) WriteProfileShort(endian,(unsigned short) (image->units+1),p);\n          break;\n        }\n        default:\n          break;\n      }\n      if ((tag_value == TAG_EXIF_OFFSET) || (tag_value == TAG_INTEROP_OFFSET))\n        {\n          offset=(ssize_t)  ReadProfileLong(endian,p);\n          if (((size_t) offset < length) && (level < (MaxDirectoryStack-2)))\n            {\n              directory_stack[level].directory=directory;\n              entry++;\n              directory_stack[level].entry=entry;\n              level++;\n              directory_stack[level].directory=exif+offset;\n              directory_stack[level].entry=0;\n              level++;\n              if ((directory+2+(12*number_entries)) > (exif+length))\n                break;\n              offset=(ssize_t)  ReadProfileLong(endian,directory+2+(12*\n                number_entries));\n              if ((offset != 0) && ((size_t) offset < length) &&\n                  (level < (MaxDirectoryStack-2)))\n                {\n                  directory_stack[level].directory=exif+offset;\n                  directory_stack[level].entry=0;\n                  level++;\n                }\n            }\n          break;\n        }\n    }\n  } while (level > 0);\n  return(MagickTrue);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/a7bb158b7bedd1449a34432feb3a67c8f1873bfa", "file_name": "MagickCore/profile.c", "vul_type": "cwe-125", "description": "Write a C function to synchronize EXIF profile resolution tags with an image's resolution and orientation."}
{"func_name": "add_language", "func_src_before": "    def add_language(self, language):\n        \"\"\"\"Add new language for item translations.\"\"\"\n        if self.connection:\n            self.cursor.execute('insert into itemlanguage (language) values (\"%s\")' % language[0])\n            self.connection.commit()", "func_src_after": "    def add_language(self, language):\n        \"\"\"\"Add new language for item translations.\"\"\"\n        if self.connection:\n            t = (language[0], )\n            self.cursor.execute('insert into itemlanguage (language) values (?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new language into a database table for item translations, using parameter substitution."}
{"func_name": "ape_decode_frame", "func_src_before": "static int ape_decode_frame(AVCodecContext *avctx, void *data,\n                            int *got_frame_ptr, AVPacket *avpkt)\n{\n    AVFrame *frame     = data;\n    const uint8_t *buf = avpkt->data;\n    APEContext *s = avctx->priv_data;\n    uint8_t *sample8;\n    int16_t *sample16;\n    int32_t *sample24;\n    int i, ch, ret;\n    int blockstodecode;\n\n    /* this should never be negative, but bad things will happen if it is, so\n       check it just to make sure. */\n    av_assert0(s->samples >= 0);\n\n    if(!s->samples){\n        uint32_t nblocks, offset;\n        int buf_size;\n\n        if (!avpkt->size) {\n            *got_frame_ptr = 0;\n            return 0;\n        }\n        if (avpkt->size < 8) {\n            av_log(avctx, AV_LOG_ERROR, \"Packet is too small\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n        buf_size = avpkt->size & ~3;\n        if (buf_size != avpkt->size) {\n            av_log(avctx, AV_LOG_WARNING, \"packet size is not a multiple of 4. \"\n                   \"extra bytes at the end will be skipped.\\n\");\n        }\n        if (s->fileversion < 3950) // previous versions overread two bytes\n            buf_size += 2;\n        av_fast_padded_malloc(&s->data, &s->data_size, buf_size);\n        if (!s->data)\n            return AVERROR(ENOMEM);\n        s->bdsp.bswap_buf((uint32_t *) s->data, (const uint32_t *) buf,\n                          buf_size >> 2);\n        memset(s->data + (buf_size & ~3), 0, buf_size & 3);\n        s->ptr = s->data;\n        s->data_end = s->data + buf_size;\n\n        nblocks = bytestream_get_be32(&s->ptr);\n        offset  = bytestream_get_be32(&s->ptr);\n        if (s->fileversion >= 3900) {\n            if (offset > 3) {\n                av_log(avctx, AV_LOG_ERROR, \"Incorrect offset passed\\n\");\n                s->data = NULL;\n                return AVERROR_INVALIDDATA;\n            }\n            if (s->data_end - s->ptr < offset) {\n                av_log(avctx, AV_LOG_ERROR, \"Packet is too small\\n\");\n                return AVERROR_INVALIDDATA;\n            }\n            s->ptr += offset;\n        } else {\n            if ((ret = init_get_bits8(&s->gb, s->ptr, s->data_end - s->ptr)) < 0)\n                return ret;\n            if (s->fileversion > 3800)\n                skip_bits_long(&s->gb, offset * 8);\n            else\n                skip_bits_long(&s->gb, offset);\n        }\n\n        if (!nblocks || nblocks > INT_MAX) {\n            av_log(avctx, AV_LOG_ERROR, \"Invalid sample count: %\"PRIu32\".\\n\",\n                   nblocks);\n            return AVERROR_INVALIDDATA;\n        }\n\n        /* Initialize the frame decoder */\n        if (init_frame_decoder(s) < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Error reading frame header\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n        s->samples = nblocks;\n    }\n\n    if (!s->data) {\n        *got_frame_ptr = 0;\n        return avpkt->size;\n    }\n\n    blockstodecode = FFMIN(s->blocks_per_loop, s->samples);\n    // for old files coefficients were not interleaved,\n    // so we need to decode all of them at once\n    if (s->fileversion < 3930)\n        blockstodecode = s->samples;\n\n    /* reallocate decoded sample buffer if needed */\n    av_fast_malloc(&s->decoded_buffer, &s->decoded_size,\n                   2 * FFALIGN(blockstodecode, 8) * sizeof(*s->decoded_buffer));\n    if (!s->decoded_buffer)\n        return AVERROR(ENOMEM);\n    memset(s->decoded_buffer, 0, s->decoded_size);\n    s->decoded[0] = s->decoded_buffer;\n    s->decoded[1] = s->decoded_buffer + FFALIGN(blockstodecode, 8);\n\n    /* get output buffer */\n    frame->nb_samples = blockstodecode;\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n\n    s->error=0;\n\n    if ((s->channels == 1) || (s->frameflags & APE_FRAMECODE_PSEUDO_STEREO))\n        ape_unpack_mono(s, blockstodecode);\n    else\n        ape_unpack_stereo(s, blockstodecode);\n    emms_c();\n\n    if (s->error) {\n        s->samples=0;\n        av_log(avctx, AV_LOG_ERROR, \"Error decoding frame\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    switch (s->bps) {\n    case 8:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample8 = (uint8_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample8++ = (s->decoded[ch][i] + 0x80) & 0xff;\n        }\n        break;\n    case 16:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample16 = (int16_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample16++ = s->decoded[ch][i];\n        }\n        break;\n    case 24:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample24 = (int32_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample24++ = s->decoded[ch][i] << 8;\n        }\n        break;\n    }\n\n    s->samples -= blockstodecode;\n\n    *got_frame_ptr = 1;\n\n    return !s->samples ? avpkt->size : 0;\n}", "func_src_after": "static int ape_decode_frame(AVCodecContext *avctx, void *data,\n                            int *got_frame_ptr, AVPacket *avpkt)\n{\n    AVFrame *frame     = data;\n    const uint8_t *buf = avpkt->data;\n    APEContext *s = avctx->priv_data;\n    uint8_t *sample8;\n    int16_t *sample16;\n    int32_t *sample24;\n    int i, ch, ret;\n    int blockstodecode;\n    uint64_t decoded_buffer_size;\n\n    /* this should never be negative, but bad things will happen if it is, so\n       check it just to make sure. */\n    av_assert0(s->samples >= 0);\n\n    if(!s->samples){\n        uint32_t nblocks, offset;\n        int buf_size;\n\n        if (!avpkt->size) {\n            *got_frame_ptr = 0;\n            return 0;\n        }\n        if (avpkt->size < 8) {\n            av_log(avctx, AV_LOG_ERROR, \"Packet is too small\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n        buf_size = avpkt->size & ~3;\n        if (buf_size != avpkt->size) {\n            av_log(avctx, AV_LOG_WARNING, \"packet size is not a multiple of 4. \"\n                   \"extra bytes at the end will be skipped.\\n\");\n        }\n        if (s->fileversion < 3950) // previous versions overread two bytes\n            buf_size += 2;\n        av_fast_padded_malloc(&s->data, &s->data_size, buf_size);\n        if (!s->data)\n            return AVERROR(ENOMEM);\n        s->bdsp.bswap_buf((uint32_t *) s->data, (const uint32_t *) buf,\n                          buf_size >> 2);\n        memset(s->data + (buf_size & ~3), 0, buf_size & 3);\n        s->ptr = s->data;\n        s->data_end = s->data + buf_size;\n\n        nblocks = bytestream_get_be32(&s->ptr);\n        offset  = bytestream_get_be32(&s->ptr);\n        if (s->fileversion >= 3900) {\n            if (offset > 3) {\n                av_log(avctx, AV_LOG_ERROR, \"Incorrect offset passed\\n\");\n                s->data = NULL;\n                return AVERROR_INVALIDDATA;\n            }\n            if (s->data_end - s->ptr < offset) {\n                av_log(avctx, AV_LOG_ERROR, \"Packet is too small\\n\");\n                return AVERROR_INVALIDDATA;\n            }\n            s->ptr += offset;\n        } else {\n            if ((ret = init_get_bits8(&s->gb, s->ptr, s->data_end - s->ptr)) < 0)\n                return ret;\n            if (s->fileversion > 3800)\n                skip_bits_long(&s->gb, offset * 8);\n            else\n                skip_bits_long(&s->gb, offset);\n        }\n\n        if (!nblocks || nblocks > INT_MAX / 2 / sizeof(*s->decoded_buffer) - 8) {\n            av_log(avctx, AV_LOG_ERROR, \"Invalid sample count: %\"PRIu32\".\\n\",\n                   nblocks);\n            return AVERROR_INVALIDDATA;\n        }\n\n        /* Initialize the frame decoder */\n        if (init_frame_decoder(s) < 0) {\n            av_log(avctx, AV_LOG_ERROR, \"Error reading frame header\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n        s->samples = nblocks;\n    }\n\n    if (!s->data) {\n        *got_frame_ptr = 0;\n        return avpkt->size;\n    }\n\n    blockstodecode = FFMIN(s->blocks_per_loop, s->samples);\n    // for old files coefficients were not interleaved,\n    // so we need to decode all of them at once\n    if (s->fileversion < 3930)\n        blockstodecode = s->samples;\n\n    /* reallocate decoded sample buffer if needed */\n    decoded_buffer_size = 2LL * FFALIGN(blockstodecode, 8) * sizeof(*s->decoded_buffer);\n    av_assert0(decoded_buffer_size <= INT_MAX);\n    av_fast_malloc(&s->decoded_buffer, &s->decoded_size, decoded_buffer_size);\n    if (!s->decoded_buffer)\n        return AVERROR(ENOMEM);\n    memset(s->decoded_buffer, 0, s->decoded_size);\n    s->decoded[0] = s->decoded_buffer;\n    s->decoded[1] = s->decoded_buffer + FFALIGN(blockstodecode, 8);\n\n    /* get output buffer */\n    frame->nb_samples = blockstodecode;\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n\n    s->error=0;\n\n    if ((s->channels == 1) || (s->frameflags & APE_FRAMECODE_PSEUDO_STEREO))\n        ape_unpack_mono(s, blockstodecode);\n    else\n        ape_unpack_stereo(s, blockstodecode);\n    emms_c();\n\n    if (s->error) {\n        s->samples=0;\n        av_log(avctx, AV_LOG_ERROR, \"Error decoding frame\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    switch (s->bps) {\n    case 8:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample8 = (uint8_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample8++ = (s->decoded[ch][i] + 0x80) & 0xff;\n        }\n        break;\n    case 16:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample16 = (int16_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample16++ = s->decoded[ch][i];\n        }\n        break;\n    case 24:\n        for (ch = 0; ch < s->channels; ch++) {\n            sample24 = (int32_t *)frame->data[ch];\n            for (i = 0; i < blockstodecode; i++)\n                *sample24++ = s->decoded[ch][i] << 8;\n        }\n        break;\n    }\n\n    s->samples -= blockstodecode;\n\n    *got_frame_ptr = 1;\n\n    return !s->samples ? avpkt->size : 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/ba4beaf6149f7241c8bd85fe853318c2f6837ad0", "file_name": "libavcodec/apedec.c", "vul_type": "cwe-125", "description": "Write a C function named `ape_decode_frame` for decoding an audio frame in the APE codec."}
{"func_name": "yaffsfs_istat", "func_src_before": "    yaffsfs_istat(TSK_FS_INFO *fs, TSK_FS_ISTAT_FLAG_ENUM flags, FILE * hFile, TSK_INUM_T inum,\n    TSK_DADDR_T numblock, int32_t sec_skew)\n{\n    TSK_FS_META *fs_meta;\n    TSK_FS_FILE *fs_file;\n    YAFFSFS_INFO *yfs = (YAFFSFS_INFO *)fs;\n    char ls[12];\n    YAFFSFS_PRINT_ADDR print;\n    char timeBuf[32];\n    YaffsCacheObject * obj = NULL;\n    YaffsCacheVersion * version = NULL;\n    YaffsHeader * header = NULL;\n\n    yaffscache_version_find_by_inode(yfs, inum, &version, &obj);\n\n    if ((fs_file = tsk_fs_file_open_meta(fs, NULL, inum)) == NULL) {\n        return 1;\n    }\n    fs_meta = fs_file->meta;\n\n    tsk_fprintf(hFile, \"inode: %\" PRIuINUM \"\\n\", inum);\n    tsk_fprintf(hFile, \"%sAllocated\\n\",\n        (fs_meta->flags & TSK_FS_META_FLAG_ALLOC) ? \"\" : \"Not \");\n\n    if (fs_meta->link)\n        tsk_fprintf(hFile, \"symbolic link to: %s\\n\", fs_meta->link);\n\n    tsk_fprintf(hFile, \"uid / gid: %\" PRIuUID \" / %\" PRIuGID \"\\n\",\n        fs_meta->uid, fs_meta->gid);\n\n    tsk_fs_meta_make_ls(fs_meta, ls, sizeof(ls));\n    tsk_fprintf(hFile, \"mode: %s\\n\", ls);\n\n    tsk_fprintf(hFile, \"size: %\" PRIdOFF \"\\n\", fs_meta->size);\n    tsk_fprintf(hFile, \"num of links: %d\\n\", fs_meta->nlink);\n\n    if(version != NULL){\n        yaffsfs_read_header(yfs, &header, version->ycv_header_chunk->ycc_offset);\n        if(header != NULL){\n            tsk_fprintf(hFile, \"Name: %s\\n\", header->name);\n        }\n    }\n\n    if (sec_skew != 0) {\n        tsk_fprintf(hFile, \"\\nAdjusted Inode Times:\\n\");\n        fs_meta->mtime -= sec_skew;\n        fs_meta->atime -= sec_skew;\n        fs_meta->ctime -= sec_skew;\n\n        tsk_fprintf(hFile, \"Accessed:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->atime, timeBuf));\n        tsk_fprintf(hFile, \"File Modified:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->mtime, timeBuf));\n        tsk_fprintf(hFile, \"Inode Modified:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->ctime, timeBuf));\n\n        fs_meta->mtime += sec_skew;\n        fs_meta->atime += sec_skew;\n        fs_meta->ctime += sec_skew;\n\n        tsk_fprintf(hFile, \"\\nOriginal Inode Times:\\n\");\n    }\n    else {\n        tsk_fprintf(hFile, \"\\nInode Times:\\n\");\n    }\n\n    tsk_fprintf(hFile, \"Accessed:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->atime, timeBuf));\n    tsk_fprintf(hFile, \"File Modified:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->mtime, timeBuf));\n    tsk_fprintf(hFile, \"Inode Modified:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->ctime, timeBuf));\n\n    if(version != NULL){\n        tsk_fprintf(hFile, \"\\nHeader Chunk:\\n\");\n        tsk_fprintf(hFile, \"%\" PRIuDADDR \"\\n\", (version->ycv_header_chunk->ycc_offset / (yfs->page_size + yfs->spare_size)));\n    }\n\n    if (numblock > 0) {\n        TSK_OFF_T lower_size = numblock * fs->block_size;\n        fs_meta->size = (lower_size < fs_meta->size)?(lower_size):(fs_meta->size);\n    }\n    tsk_fprintf(hFile, \"\\nData Chunks:\\n\");\n\n\n    if (flags & TSK_FS_ISTAT_RUNLIST){\n        const TSK_FS_ATTR *fs_attr_default =\n            tsk_fs_file_attr_get_type(fs_file,\n                TSK_FS_ATTR_TYPE_DEFAULT, 0, 0);\n        if (fs_attr_default && (fs_attr_default->flags & TSK_FS_ATTR_NONRES)) {\n            if (tsk_fs_attr_print(fs_attr_default, hFile)) {\n                tsk_fprintf(hFile, \"\\nError creating run lists  \");\n                tsk_error_print(hFile);\n                tsk_error_reset();\n            }\n        }\n    }\n    else {\n        print.idx = 0;\n        print.hFile = hFile;\n\n        if (tsk_fs_file_walk(fs_file, TSK_FS_FILE_WALK_FLAG_AONLY,\n            (TSK_FS_FILE_WALK_CB)print_addr_act, (void *)&print)) {\n            tsk_fprintf(hFile, \"\\nError reading file:  \");\n            tsk_error_print(hFile);\n            tsk_error_reset();\n        }\n        else if (print.idx != 0) {\n            tsk_fprintf(hFile, \"\\n\");\n        }\n    }\n\n    tsk_fs_file_close(fs_file);\n\n    return 0;\n}", "func_src_after": "    yaffsfs_istat(TSK_FS_INFO *fs, TSK_FS_ISTAT_FLAG_ENUM flags, FILE * hFile, TSK_INUM_T inum,\n    TSK_DADDR_T numblock, int32_t sec_skew)\n{\n    TSK_FS_META *fs_meta;\n    TSK_FS_FILE *fs_file;\n    YAFFSFS_INFO *yfs = (YAFFSFS_INFO *)fs;\n    char ls[12];\n    YAFFSFS_PRINT_ADDR print;\n    char timeBuf[128];\n    YaffsCacheObject * obj = NULL;\n    YaffsCacheVersion * version = NULL;\n    YaffsHeader * header = NULL;\n\n    yaffscache_version_find_by_inode(yfs, inum, &version, &obj);\n\n    if ((fs_file = tsk_fs_file_open_meta(fs, NULL, inum)) == NULL) {\n        return 1;\n    }\n    fs_meta = fs_file->meta;\n\n    tsk_fprintf(hFile, \"inode: %\" PRIuINUM \"\\n\", inum);\n    tsk_fprintf(hFile, \"%sAllocated\\n\",\n        (fs_meta->flags & TSK_FS_META_FLAG_ALLOC) ? \"\" : \"Not \");\n\n    if (fs_meta->link)\n        tsk_fprintf(hFile, \"symbolic link to: %s\\n\", fs_meta->link);\n\n    tsk_fprintf(hFile, \"uid / gid: %\" PRIuUID \" / %\" PRIuGID \"\\n\",\n        fs_meta->uid, fs_meta->gid);\n\n    tsk_fs_meta_make_ls(fs_meta, ls, sizeof(ls));\n    tsk_fprintf(hFile, \"mode: %s\\n\", ls);\n\n    tsk_fprintf(hFile, \"size: %\" PRIdOFF \"\\n\", fs_meta->size);\n    tsk_fprintf(hFile, \"num of links: %d\\n\", fs_meta->nlink);\n\n    if(version != NULL){\n        yaffsfs_read_header(yfs, &header, version->ycv_header_chunk->ycc_offset);\n        if(header != NULL){\n            tsk_fprintf(hFile, \"Name: %s\\n\", header->name);\n        }\n    }\n\n    if (sec_skew != 0) {\n        tsk_fprintf(hFile, \"\\nAdjusted Inode Times:\\n\");\n        fs_meta->mtime -= sec_skew;\n        fs_meta->atime -= sec_skew;\n        fs_meta->ctime -= sec_skew;\n\n        tsk_fprintf(hFile, \"Accessed:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->atime, timeBuf));\n        tsk_fprintf(hFile, \"File Modified:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->mtime, timeBuf));\n        tsk_fprintf(hFile, \"Inode Modified:\\t%s\\n\",\n            tsk_fs_time_to_str(fs_meta->ctime, timeBuf));\n\n        fs_meta->mtime += sec_skew;\n        fs_meta->atime += sec_skew;\n        fs_meta->ctime += sec_skew;\n\n        tsk_fprintf(hFile, \"\\nOriginal Inode Times:\\n\");\n    }\n    else {\n        tsk_fprintf(hFile, \"\\nInode Times:\\n\");\n    }\n\n    tsk_fprintf(hFile, \"Accessed:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->atime, timeBuf));\n    tsk_fprintf(hFile, \"File Modified:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->mtime, timeBuf));\n    tsk_fprintf(hFile, \"Inode Modified:\\t%s\\n\",\n        tsk_fs_time_to_str(fs_meta->ctime, timeBuf));\n\n    if(version != NULL){\n        tsk_fprintf(hFile, \"\\nHeader Chunk:\\n\");\n        tsk_fprintf(hFile, \"%\" PRIuDADDR \"\\n\", (version->ycv_header_chunk->ycc_offset / (yfs->page_size + yfs->spare_size)));\n    }\n\n    if (numblock > 0) {\n        TSK_OFF_T lower_size = numblock * fs->block_size;\n        fs_meta->size = (lower_size < fs_meta->size)?(lower_size):(fs_meta->size);\n    }\n    tsk_fprintf(hFile, \"\\nData Chunks:\\n\");\n\n\n    if (flags & TSK_FS_ISTAT_RUNLIST){\n        const TSK_FS_ATTR *fs_attr_default =\n            tsk_fs_file_attr_get_type(fs_file,\n                TSK_FS_ATTR_TYPE_DEFAULT, 0, 0);\n        if (fs_attr_default && (fs_attr_default->flags & TSK_FS_ATTR_NONRES)) {\n            if (tsk_fs_attr_print(fs_attr_default, hFile)) {\n                tsk_fprintf(hFile, \"\\nError creating run lists  \");\n                tsk_error_print(hFile);\n                tsk_error_reset();\n            }\n        }\n    }\n    else {\n        print.idx = 0;\n        print.hFile = hFile;\n\n        if (tsk_fs_file_walk(fs_file, TSK_FS_FILE_WALK_FLAG_AONLY,\n            (TSK_FS_FILE_WALK_CB)print_addr_act, (void *)&print)) {\n            tsk_fprintf(hFile, \"\\nError reading file:  \");\n            tsk_error_print(hFile);\n            tsk_error_reset();\n        }\n        else if (print.idx != 0) {\n            tsk_fprintf(hFile, \"\\n\");\n        }\n    }\n\n    tsk_fs_file_close(fs_file);\n\n    return 0;\n}", "commit_link": "github.com/sleuthkit/sleuthkit/commit/459ae818fc8dae717549810150de4d191ce158f1", "file_name": "tsk/fs/yaffs.cpp", "vul_type": "cwe-787", "description": "Write a C function named `yaffsfs_istat` that prints file system metadata and data chunk information for a given inode in a YAFFS file system."}
{"func_name": "store_versioninfo_gnu_verdef", "func_src_before": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && ((char *)defs + i < end); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tvstart += verdef->vd_aux;\n\t\tif (vstart > end || vstart + sizeof (Elf_(Verdaux)) > end) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || vstart + sizeof(Elf_(Verdaux)) > end) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "func_src_after": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && ((char *)defs + i < end); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || vstart + sizeof (Elf_(Verdaux)) > end) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || vstart + sizeof(Elf_(Verdaux)) > end) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "commit_link": "github.com/radare/radare2/commit/44ded3ff35b8264f54b5a900cab32ec489d9e5b9", "file_name": "libr/bin/format/elf/elf.c", "vul_type": "cwe-125", "description": "Write a C function named `store_versioninfo_gnu_verdef` that processes version definition sections in an ELF binary and stores the information in a database."}
{"func_name": "_map_vol_to_host", "func_src_before": "    def _map_vol_to_host(self, volume_name, host_name):\n        \"\"\"Create a mapping between a volume to a host.\"\"\"\n\n        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '\n                    'host %(host_name)s')\n                  % {'volume_name': volume_name, 'host_name': host_name})\n\n        # Check if this volume is already mapped to this host\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n\n        mapped_flag = False\n        result_lun = '-1'\n        if volume_name in mapping_data:\n            mapped_flag = True\n            result_lun = mapping_data[volume_name]['SCSI_id']\n        else:\n            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]\n            lun_used.sort()\n            # Assume all luns are taken to this point, and then try to find\n            # an unused one\n            result_lun = str(len(lun_used))\n            for index, n in enumerate(lun_used):\n                if n > index:\n                    result_lun = str(index)\n                    break\n\n        # Volume is not mapped to host, create a new LUN\n        if not mapped_flag:\n            ssh_cmd = ('svctask mkvdiskhostmap -host %(host_name)s -scsi '\n                       '%(result_lun)s %(volume_name)s' %\n                       {'host_name': host_name,\n                        'result_lun': result_lun,\n                        'volume_name': volume_name})\n            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)\n            if err and err.startswith('CMMVC6071E'):\n                if not self.configuration.storwize_svc_multihostmap_enabled:\n                    LOG.error(_('storwize_svc_multihostmap_enabled is set '\n                                'to False, Not allow multi host mapping'))\n                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\\\n                                    'was not created because the VDisk is '\\\n                                    'already mapped to a host.\\n\"'\n                    raise exception.CinderException(data=exception_msg)\n                ssh_cmd = ssh_cmd.replace('mkvdiskhostmap',\n                                          'mkvdiskhostmap -force')\n                # try to map one volume to multiple hosts\n                out, err = self._run_ssh(ssh_cmd)\n                LOG.warn(_('volume %s mapping to multi host') % volume_name)\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n            else:\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '\n                    '%(volume_name)s, host %(host_name)s') %\n                  {'result_lun': result_lun,\n                   'volume_name': volume_name,\n                   'host_name': host_name})\n        return result_lun", "func_src_after": "    def _map_vol_to_host(self, volume_name, host_name):\n        \"\"\"Create a mapping between a volume to a host.\"\"\"\n\n        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '\n                    'host %(host_name)s')\n                  % {'volume_name': volume_name, 'host_name': host_name})\n\n        # Check if this volume is already mapped to this host\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n\n        mapped_flag = False\n        result_lun = '-1'\n        if volume_name in mapping_data:\n            mapped_flag = True\n            result_lun = mapping_data[volume_name]['SCSI_id']\n        else:\n            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]\n            lun_used.sort()\n            # Assume all luns are taken to this point, and then try to find\n            # an unused one\n            result_lun = str(len(lun_used))\n            for index, n in enumerate(lun_used):\n                if n > index:\n                    result_lun = str(index)\n                    break\n\n        # Volume is not mapped to host, create a new LUN\n        if not mapped_flag:\n            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,\n                       '-scsi', result_lun, volume_name]\n            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)\n            if err and err.startswith('CMMVC6071E'):\n                if not self.configuration.storwize_svc_multihostmap_enabled:\n                    LOG.error(_('storwize_svc_multihostmap_enabled is set '\n                                'to False, Not allow multi host mapping'))\n                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\\\n                                    'was not created because the VDisk is '\\\n                                    'already mapped to a host.\\n\"'\n                    raise exception.CinderException(data=exception_msg)\n\n                for i in range(len(ssh_cmd)):\n                    if ssh_cmd[i] == 'mkvdiskhostmap':\n                        ssh_cmd.insert(i + 1, '-force')\n\n                # try to map one volume to multiple hosts\n                out, err = self._run_ssh(ssh_cmd)\n                LOG.warn(_('volume %s mapping to multi host') % volume_name)\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n            else:\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '\n                    '%(volume_name)s, host %(host_name)s') %\n                  {'result_lun': result_lun,\n                   'volume_name': volume_name,\n                   'host_name': host_name})\n        return result_lun", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to map a storage volume to a host, handling existing mappings and LUN allocation."}
{"func_name": "TiledInputFile::rawTileData", "func_src_before": "TiledInputFile::rawTileData (int &dx, int &dy,\n\t\t\t     int &lx, int &ly,\n                             const char *&pixelData,\n\t\t\t     int &pixelDataSize)\n{\n    try\n    {\n        Lock lock (*_data->_streamData);\n\n        if (!isValidTile (dx, dy, lx, ly))\n            throw IEX_NAMESPACE::ArgExc (\"Tried to read a tile outside \"\n\t\t\t       \"the image file's data window.\");\n\n        TileBuffer *tileBuffer = _data->getTileBuffer (0);\n\n        //\n        // if file is a multipart file, we have to seek to the required tile\n        // since we don't know where the file pointer is\n        //\n        int old_dx=dx;\n        int old_dy=dy;\n        int old_lx=lx;\n        int old_ly=ly;\n        if(isMultiPart(version()))\n        {\n            _data->_streamData->is->seekg(_data->tileOffsets(dx,dy,lx,ly));\n        }\n        readNextTileData (_data->_streamData, _data, dx, dy, lx, ly,\n\t\t\t  tileBuffer->buffer,\n                          pixelDataSize);\n        if(isMultiPart(version()))\n        {\n            if (old_dx!=dx || old_dy !=dy || old_lx!=lx || old_ly!=ly)\n            {\n                throw IEX_NAMESPACE::ArgExc (\"rawTileData read the wrong tile\");\n            }\n        }\n        pixelData = tileBuffer->buffer;\n    }\n    catch (IEX_NAMESPACE::BaseExc &e)\n    {\n        REPLACE_EXC (e, \"Error reading pixel data from image \"\n                     \"file \\\"\" << fileName() << \"\\\". \" << e.what());\n        throw;\n    }\n}", "func_src_after": "TiledInputFile::rawTileData (int &dx, int &dy,\n\t\t\t     int &lx, int &ly,\n                             const char *&pixelData,\n\t\t\t     int &pixelDataSize)\n{\n    try\n    {\n        Lock lock (*_data->_streamData);\n\n        if (!isValidTile (dx, dy, lx, ly))\n            throw IEX_NAMESPACE::ArgExc (\"Tried to read a tile outside \"\n\t\t\t       \"the image file's data window.\");\n\n        TileBuffer *tileBuffer = _data->getTileBuffer (0);\n\n        //\n        // if file is a multipart file, we have to seek to the required tile\n        // since we don't know where the file pointer is\n        //\n        int old_dx=dx;\n        int old_dy=dy;\n        int old_lx=lx;\n        int old_ly=ly;\n        if(isMultiPart(version()))\n        {\n            _data->_streamData->is->seekg(_data->tileOffsets(dx,dy,lx,ly));\n        }\n        readNextTileData (_data->_streamData, _data, dx, dy, lx, ly,\n\t\t\t  tileBuffer->buffer,\n                          pixelDataSize);\n        if(isMultiPart(version()))\n        {\n            if (old_dx!=dx || old_dy !=dy || old_lx!=lx || old_ly!=ly)\n            {\n                throw IEX_NAMESPACE::ArgExc (\"rawTileData read the wrong tile\");\n            }\n        }\n        else\n        {\n             if(!isValidTile (dx, dy, lx, ly) )\n             {\n                 throw IEX_NAMESPACE::IoExc (\"rawTileData read an invalid tile\");\n             }\n        }\n        pixelData = tileBuffer->buffer;\n    }\n    catch (IEX_NAMESPACE::BaseExc &e)\n    {\n        REPLACE_EXC (e, \"Error reading pixel data from image \"\n                     \"file \\\"\" << fileName() << \"\\\". \" << e.what());\n        throw;\n    }\n}", "commit_link": "github.com/AcademySoftwareFoundation/openexr/commit/6bb36714528a9563dd3b92720c5063a1284b86f8", "file_name": "OpenEXR/IlmImf/ImfTiledInputFile.cpp", "vul_type": "cwe-787", "description": "Provide a C++ function to read raw tile data from an image file, handling multipart files and validating tile coordinates."}
{"func_name": "TNEFParse", "func_src_before": "int TNEFParse(TNEFStruct *TNEF) {\n  WORD key;\n  DWORD type;\n  DWORD size;\n  DWORD signature;\n  BYTE *data;\n  WORD checksum, header_checksum;\n  int i;\n\n  if (TNEF->IO.ReadProc == NULL) {\n    printf(\"ERROR: Setup incorrectly: No ReadProc\\n\");\n    return YTNEF_INCORRECT_SETUP;\n  }\n\n  if (TNEF->IO.InitProc != NULL) {\n    DEBUG(TNEF->Debug, 2, \"About to initialize\");\n    if (TNEF->IO.InitProc(&TNEF->IO) != 0) {\n      return YTNEF_CANNOT_INIT_DATA;\n    }\n    DEBUG(TNEF->Debug, 2, \"Initialization finished\");\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Reading Signature\");\n  if (TNEF->IO.ReadProc(&TNEF->IO, sizeof(DWORD), 1, &signature) < 1) {\n    printf(\"ERROR: Error reading signature\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_ERROR_READING_DATA;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Checking Signature\");\n  if (TNEFCheckForSignature(signature) < 0) {\n    printf(\"ERROR: Signature does not match. Not TNEF.\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_NOT_TNEF_STREAM;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Reading Key.\");\n\n  if (TNEFGetKey(TNEF, &key) < 0) {\n    printf(\"ERROR: Unable to retrieve key.\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_NO_KEY;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Starting Full Processing.\");\n\n  while (TNEFGetHeader(TNEF, &type, &size) == 0) {\n    DEBUG2(TNEF->Debug, 2, \"Header says type=0x%X, size=%u\", type, size);\n    DEBUG2(TNEF->Debug, 2, \"Header says type=%u, size=%u\", type, size);\n    data = calloc(size, sizeof(BYTE));\n    ALLOCCHECK(data);\n    if (TNEFRawRead(TNEF, data, size, &header_checksum) < 0) {\n      printf(\"ERROR: Unable to read data.\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_ERROR_READING_DATA;\n    }\n    if (TNEFRawRead(TNEF, (BYTE *)&checksum, 2, NULL) < 0) {\n      printf(\"ERROR: Unable to read checksum.\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_ERROR_READING_DATA;\n    }\n    checksum = SwapWord((BYTE *)&checksum, sizeof(WORD));\n    if (checksum != header_checksum) {\n      printf(\"ERROR: Checksum mismatch. Data corruption?:\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_BAD_CHECKSUM;\n    }\n    for (i = 0; i < (sizeof(TNEFList) / sizeof(TNEFHandler)); i++) {\n      if (TNEFList[i].id == type) {\n        if (TNEFList[i].handler != NULL) {\n          if (TNEFList[i].handler(TNEF, i, (char*)data, size) < 0) {\n            free(data);\n            if (TNEF->IO.CloseProc != NULL) {\n              TNEF->IO.CloseProc(&TNEF->IO);\n            }\n            return YTNEF_ERROR_IN_HANDLER;\n          } else {\n            //  Found our handler and processed it.  now time to get out\n            break;\n          }\n        } else {\n          DEBUG2(TNEF->Debug, 1, \"No handler for %s: %u bytes\",\n                 TNEFList[i].name, size);\n        }\n      }\n    }\n\n    free(data);\n  }\n\n  if (TNEF->IO.CloseProc != NULL) {\n    TNEF->IO.CloseProc(&TNEF->IO);\n  }\n  return 0;\n\n}", "func_src_after": "int TNEFParse(TNEFStruct *TNEF) {\n  WORD key;\n  DWORD type;\n  DWORD size;\n  DWORD signature;\n  BYTE *data;\n  WORD checksum, header_checksum;\n  int i;\n\n  if (TNEF->IO.ReadProc == NULL) {\n    printf(\"ERROR: Setup incorrectly: No ReadProc\\n\");\n    return YTNEF_INCORRECT_SETUP;\n  }\n\n  if (TNEF->IO.InitProc != NULL) {\n    DEBUG(TNEF->Debug, 2, \"About to initialize\");\n    if (TNEF->IO.InitProc(&TNEF->IO) != 0) {\n      return YTNEF_CANNOT_INIT_DATA;\n    }\n    DEBUG(TNEF->Debug, 2, \"Initialization finished\");\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Reading Signature\");\n  if (TNEF->IO.ReadProc(&TNEF->IO, sizeof(DWORD), 1, &signature) < 1) {\n    printf(\"ERROR: Error reading signature\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_ERROR_READING_DATA;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Checking Signature\");\n  if (TNEFCheckForSignature(signature) < 0) {\n    printf(\"ERROR: Signature does not match. Not TNEF.\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_NOT_TNEF_STREAM;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Reading Key.\");\n\n  if (TNEFGetKey(TNEF, &key) < 0) {\n    printf(\"ERROR: Unable to retrieve key.\\n\");\n    if (TNEF->IO.CloseProc != NULL) {\n      TNEF->IO.CloseProc(&TNEF->IO);\n    }\n    return YTNEF_NO_KEY;\n  }\n\n  DEBUG(TNEF->Debug, 2, \"Starting Full Processing.\");\n\n  while (TNEFGetHeader(TNEF, &type, &size) == 0) {\n    DEBUG2(TNEF->Debug, 2, \"Header says type=0x%X, size=%u\", type, size);\n    DEBUG2(TNEF->Debug, 2, \"Header says type=%u, size=%u\", type, size);\n    if(size == 0) {\n      printf(\"ERROR: Field with size of 0\\n\");\n      return YTNEF_ERROR_READING_DATA;\n    }\n    data = calloc(size, sizeof(BYTE));\n    ALLOCCHECK(data);\n    if (TNEFRawRead(TNEF, data, size, &header_checksum) < 0) {\n      printf(\"ERROR: Unable to read data.\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_ERROR_READING_DATA;\n    }\n    if (TNEFRawRead(TNEF, (BYTE *)&checksum, 2, NULL) < 0) {\n      printf(\"ERROR: Unable to read checksum.\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_ERROR_READING_DATA;\n    }\n    checksum = SwapWord((BYTE *)&checksum, sizeof(WORD));\n    if (checksum != header_checksum) {\n      printf(\"ERROR: Checksum mismatch. Data corruption?:\\n\");\n      if (TNEF->IO.CloseProc != NULL) {\n        TNEF->IO.CloseProc(&TNEF->IO);\n      }\n      free(data);\n      return YTNEF_BAD_CHECKSUM;\n    }\n    for (i = 0; i < (sizeof(TNEFList) / sizeof(TNEFHandler)); i++) {\n      if (TNEFList[i].id == type) {\n        if (TNEFList[i].handler != NULL) {\n          if (TNEFList[i].handler(TNEF, i, (char*)data, size) < 0) {\n            free(data);\n            if (TNEF->IO.CloseProc != NULL) {\n              TNEF->IO.CloseProc(&TNEF->IO);\n            }\n            return YTNEF_ERROR_IN_HANDLER;\n          } else {\n            //  Found our handler and processed it.  now time to get out\n            break;\n          }\n        } else {\n          DEBUG2(TNEF->Debug, 1, \"No handler for %s: %u bytes\",\n                 TNEFList[i].name, size);\n        }\n      }\n    }\n\n    free(data);\n  }\n\n  if (TNEF->IO.CloseProc != NULL) {\n    TNEF->IO.CloseProc(&TNEF->IO);\n  }\n  return 0;\n\n}", "commit_link": "github.com/Yeraze/ytnef/commit/3cb0f914d6427073f262e1b2b5fd973e3043cdf7", "file_name": "lib/ytnef.c", "vul_type": "cwe-125", "description": "Write a C function named `TNEFParse` that processes a TNEF stream using provided I/O procedures."}
{"func_name": "read_mdisk", "func_src_before": "static int read_mdisk(metric_disk *mdisk)\n{\n   mdisk_header md_header;\n   uint32_t busy;\n   uint32_t sig;\n   int fd;\n   char *path;\n\n   DIR* dir;\n   struct dirent* entry;\n\n   dir = opendir(SYS_BLOCK);\n   if (dir == NULL)\n      goto error;\n\n   while((entry = readdir(dir))) {\nretry:\n#ifndef DEBUG_FROM_DOM0\n      if (strcmp(entry->d_name, \".\") == 0 ||\n            strcmp(entry->d_name, \"..\") == 0)\n         continue;\n\n      if (asprintf(&path, \"/dev/%s\", entry->d_name) < 0)\n          goto error;\n#else\n      path = strdup(\"/dev/shm/vhostmd0\");\n#endif\n      /* Open with O_DIRECT to avoid kernel keeping old copies around\n       * in the cache.\n       */\n      fd = open (path, O_RDONLY|O_DIRECT);\n      if (fd == -1) {\n         free (path);\n         continue;\n      }\n      if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n         free (path);\n\t close (fd);\n         continue;\n      }\n\n      if ((sig = ntohl(md_header.sig)) == MDISK_SIGNATURE) {\n         busy = ntohl(md_header.busy);\n         if (busy) {\n\t     close(fd);\n             free(path);\n             sleep(1);\n             goto retry;\n         }\n         mdisk->sum = ntohl(md_header.sum);\n         mdisk->length = ntohl(md_header.length);\n         mdisk->buffer = malloc(mdisk->length);\n         mdisk->disk_name = strdup(path);\n\t /* XXX check return value */\n         odirect_read (fd, mdisk->buffer, sizeof md_header, mdisk->length);\n\t free(path);\n\n         /* Verify data still valid */\n         if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n\t     mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         busy = ntohl(md_header.busy);\n         if (busy || mdisk->sum != ntohl(md_header.sum)) {\n             mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         close (fd);\n\t break;\n      }\n      close (fd);\n   }\n\n   if (mdisk->buffer == NULL)\n      goto error;\n\n   /* Set up a parser context */\n   mdisk->pctxt = xmlNewParserCtxt();\n   if (!mdisk->pctxt || !mdisk->pctxt->sax) {\n      goto error;\n   }\n\n   mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer, \n           mdisk->length, \"mdisk.xml\", NULL, \n           XML_PARSE_NOENT | XML_PARSE_NONET |\n           XML_PARSE_NOWARNING);\n   if (!mdisk->doc) {\n      libmsg(\"%s(): libxml failed to parse mdisk.xml buffer\\n\", __func__);\n      goto error;\n   }\n\n   closedir(dir);\n\n   return 0;\nerror:\n   if (dir)\n       closedir(dir);\n\n   libmsg(\"%s(): Unable to read metrics disk\\n\", __func__);\n\n   return -1;\n}", "func_src_after": "static int read_mdisk(metric_disk *mdisk)\n{\n   mdisk_header md_header;\n   uint32_t busy;\n   uint32_t sig;\n   int fd;\n   char *path;\n\n   DIR* dir;\n   struct dirent* entry;\n\n   dir = opendir(SYS_BLOCK);\n   if (dir == NULL)\n      goto error;\n\n   while((entry = readdir(dir))) {\nretry:\n#ifndef DEBUG_FROM_DOM0\n      if (strcmp(entry->d_name, \".\") == 0 ||\n            strcmp(entry->d_name, \"..\") == 0)\n         continue;\n\n      if (asprintf(&path, \"/dev/%s\", entry->d_name) < 0)\n          goto error;\n#else\n      path = strdup(\"/dev/shm/vhostmd0\");\n#endif\n      /* Open with O_DIRECT to avoid kernel keeping old copies around\n       * in the cache.\n       */\n      fd = open (path, O_RDONLY|O_DIRECT);\n      if (fd == -1) {\n         free (path);\n         continue;\n      }\n      if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n         free (path);\n\t close (fd);\n         continue;\n      }\n\n      if ((sig = ntohl(md_header.sig)) == MDISK_SIGNATURE) {\n         busy = ntohl(md_header.busy);\n         if (busy) {\n\t     close(fd);\n             free(path);\n             sleep(1);\n             goto retry;\n         }\n         mdisk->sum = ntohl(md_header.sum);\n         mdisk->length = ntohl(md_header.length);\n         mdisk->buffer = malloc(mdisk->length);\n         mdisk->disk_name = strdup(path);\n\t /* XXX check return value */\n         odirect_read (fd, mdisk->buffer, sizeof md_header, mdisk->length);\n\t free(path);\n\n         /* Verify data still valid */\n         if (odirect_read (fd, &md_header, 0, sizeof md_header) == -1) {\n\t     mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         busy = ntohl(md_header.busy);\n         if (busy || mdisk->sum != ntohl(md_header.sum)) {\n             mdisk_content_free();\n             close (fd);\n             sleep(1);\n             goto retry;\n         }\n         close (fd);\n\t break;\n      }\n      close (fd);\n   }\n\n   if (mdisk->buffer == NULL)\n      goto error;\n\n   /* Set up a parser context */\n   mdisk->pctxt = xmlNewParserCtxt();\n   if (!mdisk->pctxt || !mdisk->pctxt->sax) {\n      goto error;\n   }\n\n   mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer, \n                                  mdisk->length, \"mdisk.xml\", NULL, \n                                  XML_PARSE_NONET | XML_PARSE_NOWARNING);\n   if (!mdisk->doc) {\n      libmsg(\"%s(): libxml failed to parse mdisk.xml buffer\\n\", __func__);\n      goto error;\n   }\n\n   closedir(dir);\n\n   return 0;\nerror:\n   if (dir)\n       closedir(dir);\n\n   libmsg(\"%s(): Unable to read metrics disk\\n\", __func__);\n\n   return -1;\n}", "line_changes": {"deleted": [{"line_no": 88, "char_start": 2187, "char_end": 2233, "line": "           mdisk->length, \"mdisk.xml\", NULL, \n"}, {"line_no": 89, "char_start": 2233, "char_end": 2280, "line": "           XML_PARSE_NOENT | XML_PARSE_NONET |\n"}, {"line_no": 90, "char_start": 2280, "char_end": 2313, "line": "           XML_PARSE_NOWARNING);\n"}], "added": [{"line_no": 88, "char_start": 2187, "char_end": 2256, "line": "                                  mdisk->length, \"mdisk.xml\", NULL, \n"}, {"line_no": 89, "char_start": 2256, "char_end": 2330, "line": "                                  XML_PARSE_NONET | XML_PARSE_NOWARNING);\n"}]}, "char_changes": {"deleted": [{"char_start": 2244, "char_end": 2261, "chars": "XML_PARSE_NOENT |"}, {"char_start": 2279, "char_end": 2290, "chars": "\n          "}], "added": [{"char_start": 2187, "char_end": 2210, "chars": "                       "}, {"char_start": 2267, "char_end": 2289, "chars": "                      "}]}, "commit_link": "github.com/vhostmd/vhostmd/commit/3d4f3acdfc9f937bea946bb1c7dfad1f3516a6ce", "file_name": "libmetrics.c", "vul_type": "cwe-611", "commit_msg": "libmetrics: Remove unsafe XML_PARSE_NOENT option\n\nFrom coverity scan\n\nError: UNSAFE_XML_PARSE_CONFIG:\nvhostmd-1.1/libmetrics/libmetrics.c:412: unsafe_xml_parse_config: XML parse option should not have flag \"XML_PARSE_NOENT\" set, which is vulnerable to XML external entity attack.\n  410|      mdisk->doc = xmlCtxtReadMemory(mdisk->pctxt, mdisk->buffer,\n  411|              mdisk->length, \"mdisk.xml\", NULL,\n  412|->            XML_PARSE_NOENT | XML_PARSE_NONET |\n  413|              XML_PARSE_NOWARNING);\n  414|      if (!mdisk->doc) {\n\nIt should be safe to remove the option.\n\nSigned-off-by: Jim Fehlig <jfehlig@suse.com>", "parent_commit": "f659ec774221532cc5452a07418e2ab1385f162c", "description": "Write a C function to read and parse data from a metrics disk into a provided structure."}
{"func_name": "al_segment_cwd_prefix", "func_src_before": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 64, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "func_src_after": "int al_segment_cwd_prefix(char** prompt, unsigned int* prompt_len, int* is_first, int* last_bg, int orientation) {\n    char text[16];\n\n    char prefix[16];\n    char home[64];\n    char cwd[512];\n    char *base = cwd;\n\n    if (al_get_home(home, 64) != 0) {\n        return -1;\n    }\n    if (al_get_cwd(cwd, 512) != 0) {\n        return -1;\n    }\n\n    // check if in home directory\n    if (al_string_startswith(cwd, home)) {\n        base = cwd+strlen(home);\n    }\n\n    // copy first path element into dirs buffer\n    if (al_get_dir_count(base) > CWD_LEN) {\n        strcpy(prefix, THREE_DOTS); // deep in hirarchy\n    } else if (al_string_startswith(cwd, home)) {\n        strcpy(prefix, \"~\"); // home directory\n    } else {\n        strcpy(prefix, \"/\"); // everywhere else\n    }\n\n    // add segment to prompt buffer\n    snprintf(text, 16, \" %s \", prefix);\n    al_gen_segment(prompt, prompt_len, COLOR_FG_CWD_PREFIX, COLOR_BG_CWD_PREFIX, FNT_BOLD, text, is_first, last_bg, orientation);\n\n    return 0;\n}", "line_changes": {"deleted": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 64, \" %s \", prefix);\n"}], "added": [{"line_no": 31, "char_start": 809, "char_end": 849, "line": "    snprintf(text, 16, \" %s \", prefix);\n"}]}, "char_changes": {"deleted": [{"char_start": 829, "char_end": 830, "chars": "4"}], "added": [{"char_start": 828, "char_end": 829, "chars": "1"}]}, "commit_link": "github.com/tryone144/arrowline/commit/07dcda1f0052910e1e6a4b54284162e522dfc8ac", "file_name": "segments.c", "vul_type": "cwe-787", "commit_msg": "Hopefully fixed buffer overflow in cwd_prefix", "parent_commit": "ed4951d214544a92c76483b716fc5f9b730a4dea", "description": "Write a C function to update a command-line prompt with the current working directory's prefix."}
{"func_name": "merge", "func_src_before": "function merge(source, target) {\n  for (var key in source) {\n    var value = source[key];\n\n    if (typeof value === 'object' && !Array.isArray(value)) {\n      target[key] = merge(value, target[key] || {});\n    } else {\n      target[key] = key in target ? target[key] : value;\n    }\n  }\n\n  return target;\n}", "func_src_after": "function merge(source, target) {\n  for (var key in source) {\n    if (Object.prototype.hasOwnProperty.call(source, key)) {\n      var value = source[key];\n\n      if (Object.prototype.hasOwnProperty.call(target, key) && typeof value === 'object' && !Array.isArray(value)) {\n        target[key] = merge(value, target[key] || {});\n      } else {\n        target[key] = key in target ? target[key] : value;\n      }\n    }\n  }\n\n  return target;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 61, "char_end": 90, "line": "    var value = source[key];\n"}, {"line_no": 4, "char_start": 90, "char_end": 91, "line": "\n"}, {"line_no": 5, "char_start": 91, "char_end": 153, "line": "    if (typeof value === 'object' && !Array.isArray(value)) {\n"}, {"line_no": 6, "char_start": 153, "char_end": 206, "line": "      target[key] = merge(value, target[key] || {});\n"}, {"line_no": 7, "char_start": 206, "char_end": 219, "line": "    } else {\n"}, {"line_no": 8, "char_start": 219, "char_end": 276, "line": "      target[key] = key in target ? target[key] : value;\n"}], "added": [{"line_no": 3, "char_start": 61, "char_end": 122, "line": "    if (Object.prototype.hasOwnProperty.call(source, key)) {\n"}, {"line_no": 4, "char_start": 122, "char_end": 153, "line": "      var value = source[key];\n"}, {"line_no": 5, "char_start": 153, "char_end": 154, "line": "\n"}, {"line_no": 6, "char_start": 154, "char_end": 271, "line": "      if (Object.prototype.hasOwnProperty.call(target, key) && typeof value === 'object' && !Array.isArray(value)) {\n"}, {"line_no": 7, "char_start": 271, "char_end": 326, "line": "        target[key] = merge(value, target[key] || {});\n"}, {"line_no": 8, "char_start": 326, "char_end": 341, "line": "      } else {\n"}, {"line_no": 9, "char_start": 341, "char_end": 400, "line": "        target[key] = key in target ? target[key] : value;\n"}, {"line_no": 10, "char_start": 400, "char_end": 408, "line": "      }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 61, "char_end": 124, "chars": "    if (Object.prototype.hasOwnProperty.call(source, key)) {\n  "}, {"char_start": 158, "char_end": 160, "chars": "  "}, {"char_start": 164, "char_end": 217, "chars": "Object.prototype.hasOwnProperty.call(target, key) && "}, {"char_start": 271, "char_end": 273, "chars": "  "}, {"char_start": 330, "char_end": 332, "chars": "  "}, {"char_start": 341, "char_end": 343, "chars": "  "}, {"char_start": 400, "char_end": 408, "chars": "      }\n"}]}, "commit_link": "github.com/jakubpawlowicz/clean-css/commit/cfa26b15fa6ff69583c502fa464b0b9b3ec88f2d", "file_name": "compatibility.js", "vul_type": "cwe-915", "commit_msg": "compatibility.js: fix prototype pollution", "parent_commit": "e9c05026d25cdeab2788d3133ca7afe6c0861dff", "description": "Write a JavaScript function named `merge` that recursively merges properties from a source object into a target object, without overwriting existing keys unless the value is an object."}
{"func_name": "edit", "func_src_before": "  def edit\n    # give an error message is instructor have not set the time zone.\n    if current_user.timezonepref.nil?\n      flash.now[:error] = \"You have not specified your preferred timezone yet. Please do this before you set up the deadlines.\"\n    end\n    @topics = SignUpTopic.find_by_sql(\"select * from sign_up_topics where assignment_id=\" + params[:id])\n    @assignment_form = AssignmentForm.create_form_object(params[:id])\n    @user = current_user\n\n    @assignment_questionnaires = AssignmentQuestionnaire.where(assignment_id: params[:id])\n    @due_date_all = DueDate.where(assignment_id: params[:id])\n    @reviewvarycheck = false\n    @due_date_nameurl_notempty = false\n    @due_date_nameurl_notempty_checkbox = false\n    @metareview_allowed = false\n    @metareview_allowed_checkbox = false\n    @signup_allowed = false\n    @signup_allowed_checkbox = false\n    @drop_topic_allowed = false\n    @drop_topic_allowed_checkbox = false\n    @team_formation_allowed = false\n    @team_formation_allowed_checkbox = false\n    @participants_count = @assignment_form.assignment.participants.size\n    @teams_count = @assignment_form.assignment.teams.size\n\n    # Check if name and url in database is empty before webpage displays\n    @due_date_all.each do |dd|\n      @due_date_nameurl_notempty = is_due_date_nameurl_notempty(dd)\n      @due_date_nameurl_notempty_checkbox = @due_date_nameurl_notempty\n      @metareview_allowed = is_meta_review_allowed?(dd)\n      @drop_topic_allowed = is_drop_topic_allowed?(dd)\n      @signup_allowed = is_signup_allowed?(dd)\n      @team_formation_allowed = is_team_formation_allowed?(dd)\n\n      if dd.due_at.present?\n        dd.due_at = dd.due_at.to_s.in_time_zone(current_user.timezonepref)\n      end\n      if  @due_date_nameurl_notempty && @due_date_nameurl_notempty_checkbox &&\n          (@metareview_allowed || @drop_topic_allowed || @signup_allowed || @team_formation_allowed)\n        break\n      end\n    end\n\n    @assignment_questionnaires.each do |aq|\n      unless aq.used_in_round.nil?\n        @reviewvarycheck = 1\n        break\n      end\n    end\n    @due_date_all = update_nil_dd_deadline_name(@due_date_all)\n    @due_date_all = update_nil_dd_description_url(@due_date_all)\n\n    # only when instructor does not assign rubrics and in assignment edit page will show this error message.\n    if !empty_rubrics_list.empty? && request.original_fullpath == \"/assignments/#{@assignment_form.assignment.id}/edit\"\n      rubrics_needed = needed_rubrics(empty_rubrics_list)\n      flash.now[:error] = \"You did not specify all the necessary rubrics. You need \" + rubrics_needed +\n          \" of assignment <b>#{@assignment_form.assignment.name}</b> before saving the assignment. You can assign rubrics <a id='go_to_tabs2' style='color: blue;'>here</a>.\"\n    end\n\n    if @assignment_form.assignment.directory_path.nil? || @assignment_form.assignment.directory_path.empty?\n      flash.now[:error] = \"You did not specify your submission directory.\"\n    end\n  end", "func_src_after": "  def edit\n    # give an error message is instructor have not set the time zone.\n    if current_user.timezonepref.nil?\n      flash.now[:error] = \"You have not specified your preferred timezone yet. Please do this before you set up the deadlines.\"\n    end\n    @topics = SignUpTopic.where(assignment_id: params[:id])\n    @assignment_form = AssignmentForm.create_form_object(params[:id])\n    @user = current_user\n\n    @assignment_questionnaires = AssignmentQuestionnaire.where(assignment_id: params[:id])\n    @due_date_all = DueDate.where(assignment_id: params[:id])\n    @reviewvarycheck = false\n    @due_date_nameurl_notempty = false\n    @due_date_nameurl_notempty_checkbox = false\n    @metareview_allowed = false\n    @metareview_allowed_checkbox = false\n    @signup_allowed = false\n    @signup_allowed_checkbox = false\n    @drop_topic_allowed = false\n    @drop_topic_allowed_checkbox = false\n    @team_formation_allowed = false\n    @team_formation_allowed_checkbox = false\n    @participants_count = @assignment_form.assignment.participants.size\n    @teams_count = @assignment_form.assignment.teams.size\n\n    # Check if name and url in database is empty before webpage displays\n    @due_date_all.each do |dd|\n      @due_date_nameurl_notempty = is_due_date_nameurl_notempty(dd)\n      @due_date_nameurl_notempty_checkbox = @due_date_nameurl_notempty\n      @metareview_allowed = is_meta_review_allowed?(dd)\n      @drop_topic_allowed = is_drop_topic_allowed?(dd)\n      @signup_allowed = is_signup_allowed?(dd)\n      @team_formation_allowed = is_team_formation_allowed?(dd)\n\n      if dd.due_at.present?\n        dd.due_at = dd.due_at.to_s.in_time_zone(current_user.timezonepref)\n      end\n      if  @due_date_nameurl_notempty && @due_date_nameurl_notempty_checkbox &&\n          (@metareview_allowed || @drop_topic_allowed || @signup_allowed || @team_formation_allowed)\n        break\n      end\n    end\n\n    @assignment_questionnaires.each do |aq|\n      unless aq.used_in_round.nil?\n        @reviewvarycheck = 1\n        break\n      end\n    end\n    @due_date_all = update_nil_dd_deadline_name(@due_date_all)\n    @due_date_all = update_nil_dd_description_url(@due_date_all)\n\n    # only when instructor does not assign rubrics and in assignment edit page will show this error message.\n    if !empty_rubrics_list.empty? && request.original_fullpath == \"/assignments/#{@assignment_form.assignment.id}/edit\"\n      rubrics_needed = needed_rubrics(empty_rubrics_list)\n      flash.now[:error] = \"You did not specify all the necessary rubrics. You need \" + rubrics_needed +\n          \" of assignment <b>#{@assignment_form.assignment.name}</b> before saving the assignment. You can assign rubrics <a id='go_to_tabs2' style='color: blue;'>here</a>.\"\n    end\n\n    if @assignment_form.assignment.directory_path.nil? || @assignment_form.assignment.directory_path.empty?\n      flash.now[:error] = \"You did not specify your submission directory.\"\n    end\n  end", "line_changes": {"deleted": [{"line_no": 6, "char_start": 255, "char_end": 360, "line": "    @topics = SignUpTopic.find_by_sql(\"select * from sign_up_topics where assignment_id=\" + params[:id])\n"}], "added": [{"line_no": 6, "char_start": 255, "char_end": 315, "line": "    @topics = SignUpTopic.where(assignment_id: params[:id])\n"}]}, "char_changes": {"deleted": [{"char_start": 281, "char_end": 323, "chars": "find_by_sql(\"select * from sign_up_topics "}, {"char_start": 328, "char_end": 329, "chars": " "}, {"char_start": 342, "char_end": 346, "chars": "=\" +"}], "added": [{"char_start": 286, "char_end": 287, "chars": "("}, {"char_start": 300, "char_end": 301, "chars": ":"}]}, "commit_link": "github.com/urmilparikh95/expertiza/commit/fa775cc1b2cfb68902042db139bf24447f25c1eb", "file_name": "assignments_controller.rb", "vul_type": "cwe-089", "commit_msg": "Handle possible SQL injections.", "parent_commit": "e9772caf7b3e799914fd0dfca9be264cfbb5f7c7", "description": "Write a Ruby method to edit assignment details, checking for user timezone preferences and ensuring all necessary components like topics, questionnaires, and due dates are loaded and validated."}
{"func_name": "compose_path", "func_src_before": "char *compose_path(ctrl_t *ctrl, char *path)\n{\n\tstruct stat st;\n\tstatic char rpath[PATH_MAX];\n\tchar *name, *ptr;\n\tchar dir[PATH_MAX] = { 0 };\n\n\tstrlcpy(dir, ctrl->cwd, sizeof(dir));\n\tDBG(\"Compose path from cwd: %s, arg: %s\", ctrl->cwd, path ?: \"\");\n\tif (!path || !strlen(path))\n\t\tgoto check;\n\n\tif (path) {\n\t\tif (path[0] != '/') {\n\t\t\tif (dir[strlen(dir) - 1] != '/')\n\t\t\t\tstrlcat(dir, \"/\", sizeof(dir));\n\t\t}\n\t\tstrlcat(dir, path, sizeof(dir));\n\t}\n\ncheck:\n\twhile ((ptr = strstr(dir, \"//\")))\n\t\tmemmove(ptr, &ptr[1], strlen(&ptr[1]) + 1);\n\n\tif (!chrooted) {\n\t\tsize_t len = strlen(home);\n\n\t\tDBG(\"Server path from CWD: %s\", dir);\n\t\tif (len > 0 && home[len - 1] == '/')\n\t\t\tlen--;\n\t\tmemmove(dir + len, dir, strlen(dir) + 1);\n\t\tmemcpy(dir, home, len);\n\t\tDBG(\"Resulting non-chroot path: %s\", dir);\n\t}\n\n\t/*\n\t * Handle directories slightly differently, since dirname() on a\n\t * directory returns the parent directory.  So, just squash ..\n\t */\n\tif (!stat(dir, &st) && S_ISDIR(st.st_mode)) {\n\t\tif (!realpath(dir, rpath))\n\t\t\treturn NULL;\n\t} else {\n\t\t/*\n\t\t * Check realpath() of directory containing the file, a\n\t\t * STOR may want to save a new file.  Then append the\n\t\t * file and return it.\n\t\t */\n\t\tname = basename(path);\n\t\tptr = dirname(dir);\n\n\t\tmemset(rpath, 0, sizeof(rpath));\n\t\tif (!realpath(ptr, rpath)) {\n\t\t\tINFO(\"Failed realpath(%s): %m\", ptr);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (rpath[1] != 0)\n\t\t\tstrlcat(rpath, \"/\", sizeof(rpath));\n\t\tstrlcat(rpath, name, sizeof(rpath));\n\t}\n\n\tif (!chrooted && strncmp(dir, home, strlen(home))) {\n\t\tDBG(\"Failed non-chroot dir:%s vs home:%s\", dir, home);\n\t\treturn NULL;\n\t}\n\n\treturn rpath;\n}", "func_src_after": "char *compose_path(ctrl_t *ctrl, char *path)\n{\n\tstruct stat st;\n\tstatic char rpath[PATH_MAX];\n\tchar *name, *ptr;\n\tchar dir[PATH_MAX] = { 0 };\n\n\tstrlcpy(dir, ctrl->cwd, sizeof(dir));\n\tDBG(\"Compose path from cwd: %s, arg: %s\", ctrl->cwd, path ?: \"\");\n\tif (!path || !strlen(path))\n\t\tgoto check;\n\n\tif (path) {\n\t\tif (path[0] != '/') {\n\t\t\tif (dir[strlen(dir) - 1] != '/')\n\t\t\t\tstrlcat(dir, \"/\", sizeof(dir));\n\t\t}\n\t\tstrlcat(dir, path, sizeof(dir));\n\t}\n\ncheck:\n\twhile ((ptr = strstr(dir, \"//\")))\n\t\tmemmove(ptr, &ptr[1], strlen(&ptr[1]) + 1);\n\n\tif (!chrooted) {\n\t\tsize_t len = strlen(home);\n\n\t\tDBG(\"Server path from CWD: %s\", dir);\n\t\tif (len > 0 && home[len - 1] == '/')\n\t\t\tlen--;\n\t\tmemmove(dir + len, dir, strlen(dir) + 1);\n\t\tmemcpy(dir, home, len);\n\t\tDBG(\"Resulting non-chroot path: %s\", dir);\n\t}\n\n\t/*\n\t * Handle directories slightly differently, since dirname() on a\n\t * directory returns the parent directory.  So, just squash ..\n\t */\n\tif (!stat(dir, &st) && S_ISDIR(st.st_mode)) {\n\t\tif (!realpath(dir, rpath))\n\t\t\treturn NULL;\n\t} else {\n\t\t/*\n\t\t * Check realpath() of directory containing the file, a\n\t\t * STOR may want to save a new file.  Then append the\n\t\t * file and return it.\n\t\t */\n\t\tname = basename(path);\n\t\tptr = dirname(dir);\n\n\t\tmemset(rpath, 0, sizeof(rpath));\n\t\tif (!realpath(ptr, rpath)) {\n\t\t\tINFO(\"Failed realpath(%s): %m\", ptr);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (rpath[1] != 0)\n\t\t\tstrlcat(rpath, \"/\", sizeof(rpath));\n\t\tstrlcat(rpath, name, sizeof(rpath));\n\t}\n\n\tif (!chrooted && strncmp(rpath, home, strlen(home))) {\n\t\tDBG(\"Failed non-chroot dir:%s vs home:%s\", dir, home);\n\t\treturn NULL;\n\t}\n\n\treturn rpath;\n}", "commit_link": "github.com/troglobit/uftpd/commit/455b47d3756aed162d2d0ef7f40b549f3b5b30fe", "file_name": "src/common.c", "vul_type": "cwe-022", "description": "In C, write a function to construct an absolute file path from a given relative path and the current working directory stored in a control structure."}
{"func_name": "snd_usb_create_streams", "func_src_before": "static int snd_usb_create_streams(struct snd_usb_audio *chip, int ctrlif)\n{\n\tstruct usb_device *dev = chip->dev;\n\tstruct usb_host_interface *host_iface;\n\tstruct usb_interface_descriptor *altsd;\n\tvoid *control_header;\n\tint i, protocol;\n\n\t/* find audiocontrol interface */\n\thost_iface = &usb_ifnum_to_if(dev, ctrlif)->altsetting[0];\n\tcontrol_header = snd_usb_find_csint_desc(host_iface->extra,\n\t\t\t\t\t\t host_iface->extralen,\n\t\t\t\t\t\t NULL, UAC_HEADER);\n\taltsd = get_iface_desc(host_iface);\n\tprotocol = altsd->bInterfaceProtocol;\n\n\tif (!control_header) {\n\t\tdev_err(&dev->dev, \"cannot find UAC_HEADER\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (protocol) {\n\tdefault:\n\t\tdev_warn(&dev->dev,\n\t\t\t \"unknown interface protocol %#02x, assuming v1\\n\",\n\t\t\t protocol);\n\t\t/* fall through */\n\n\tcase UAC_VERSION_1: {\n\t\tstruct uac1_ac_header_descriptor *h1 = control_header;\n\n\t\tif (!h1->bInCollection) {\n\t\t\tdev_info(&dev->dev, \"skipping empty audio interface (v1)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (h1->bLength < sizeof(*h1) + h1->bInCollection) {\n\t\t\tdev_err(&dev->dev, \"invalid UAC_HEADER (v1)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < h1->bInCollection; i++)\n\t\t\tsnd_usb_create_stream(chip, ctrlif, h1->baInterfaceNr[i]);\n\n\t\tbreak;\n\t}\n\n\tcase UAC_VERSION_2: {\n\t\tstruct usb_interface_assoc_descriptor *assoc =\n\t\t\tusb_ifnum_to_if(dev, ctrlif)->intf_assoc;\n\n\t\tif (!assoc) {\n\t\t\t/*\n\t\t\t * Firmware writers cannot count to three.  So to find\n\t\t\t * the IAD on the NuForce UDH-100, also check the next\n\t\t\t * interface.\n\t\t\t */\n\t\t\tstruct usb_interface *iface =\n\t\t\t\tusb_ifnum_to_if(dev, ctrlif + 1);\n\t\t\tif (iface &&\n\t\t\t    iface->intf_assoc &&\n\t\t\t    iface->intf_assoc->bFunctionClass == USB_CLASS_AUDIO &&\n\t\t\t    iface->intf_assoc->bFunctionProtocol == UAC_VERSION_2)\n\t\t\t\tassoc = iface->intf_assoc;\n\t\t}\n\n\t\tif (!assoc) {\n\t\t\tdev_err(&dev->dev, \"Audio class v2 interfaces need an interface association\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < assoc->bInterfaceCount; i++) {\n\t\t\tint intf = assoc->bFirstInterface + i;\n\n\t\t\tif (intf != ctrlif)\n\t\t\t\tsnd_usb_create_stream(chip, ctrlif, intf);\n\t\t}\n\n\t\tbreak;\n\t}\n\t}\n\n\treturn 0;\n}", "func_src_after": "static int snd_usb_create_streams(struct snd_usb_audio *chip, int ctrlif)\n{\n\tstruct usb_device *dev = chip->dev;\n\tstruct usb_host_interface *host_iface;\n\tstruct usb_interface_descriptor *altsd;\n\tvoid *control_header;\n\tint i, protocol;\n\tint rest_bytes;\n\n\t/* find audiocontrol interface */\n\thost_iface = &usb_ifnum_to_if(dev, ctrlif)->altsetting[0];\n\tcontrol_header = snd_usb_find_csint_desc(host_iface->extra,\n\t\t\t\t\t\t host_iface->extralen,\n\t\t\t\t\t\t NULL, UAC_HEADER);\n\taltsd = get_iface_desc(host_iface);\n\tprotocol = altsd->bInterfaceProtocol;\n\n\tif (!control_header) {\n\t\tdev_err(&dev->dev, \"cannot find UAC_HEADER\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trest_bytes = (void *)(host_iface->extra + host_iface->extralen) -\n\t\tcontrol_header;\n\n\t/* just to be sure -- this shouldn't hit at all */\n\tif (rest_bytes <= 0) {\n\t\tdev_err(&dev->dev, \"invalid control header\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (protocol) {\n\tdefault:\n\t\tdev_warn(&dev->dev,\n\t\t\t \"unknown interface protocol %#02x, assuming v1\\n\",\n\t\t\t protocol);\n\t\t/* fall through */\n\n\tcase UAC_VERSION_1: {\n\t\tstruct uac1_ac_header_descriptor *h1 = control_header;\n\n\t\tif (rest_bytes < sizeof(*h1)) {\n\t\t\tdev_err(&dev->dev, \"too short v1 buffer descriptor\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!h1->bInCollection) {\n\t\t\tdev_info(&dev->dev, \"skipping empty audio interface (v1)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (rest_bytes < h1->bLength) {\n\t\t\tdev_err(&dev->dev, \"invalid buffer length (v1)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (h1->bLength < sizeof(*h1) + h1->bInCollection) {\n\t\t\tdev_err(&dev->dev, \"invalid UAC_HEADER (v1)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < h1->bInCollection; i++)\n\t\t\tsnd_usb_create_stream(chip, ctrlif, h1->baInterfaceNr[i]);\n\n\t\tbreak;\n\t}\n\n\tcase UAC_VERSION_2: {\n\t\tstruct usb_interface_assoc_descriptor *assoc =\n\t\t\tusb_ifnum_to_if(dev, ctrlif)->intf_assoc;\n\n\t\tif (!assoc) {\n\t\t\t/*\n\t\t\t * Firmware writers cannot count to three.  So to find\n\t\t\t * the IAD on the NuForce UDH-100, also check the next\n\t\t\t * interface.\n\t\t\t */\n\t\t\tstruct usb_interface *iface =\n\t\t\t\tusb_ifnum_to_if(dev, ctrlif + 1);\n\t\t\tif (iface &&\n\t\t\t    iface->intf_assoc &&\n\t\t\t    iface->intf_assoc->bFunctionClass == USB_CLASS_AUDIO &&\n\t\t\t    iface->intf_assoc->bFunctionProtocol == UAC_VERSION_2)\n\t\t\t\tassoc = iface->intf_assoc;\n\t\t}\n\n\t\tif (!assoc) {\n\t\t\tdev_err(&dev->dev, \"Audio class v2 interfaces need an interface association\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < assoc->bInterfaceCount; i++) {\n\t\t\tint intf = assoc->bFirstInterface + i;\n\n\t\t\tif (intf != ctrlif)\n\t\t\t\tsnd_usb_create_stream(chip, ctrlif, intf);\n\t\t}\n\n\t\tbreak;\n\t}\n\t}\n\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/bfc81a8bc18e3c4ba0cbaa7666ff76be2f998991", "file_name": "sound/usb/card.c", "vul_type": "cwe-125", "description": "Write a C function named `snd_usb_create_streams` that initializes audio streams for a USB audio device based on its control interface."}
{"func_name": "terminate_connection", "func_src_before": "    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Cleanup after an iSCSI connection has been terminated.\n\n        When we clean up a terminated connection between a given connector\n        and volume, we:\n        1. Translate the given connector to a host name\n        2. Remove the volume-to-host mapping if it exists\n        3. Delete the host if it has no more mappings (hosts are created\n           automatically by this driver when mappings are created)\n        \"\"\"\n        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})\n\n        vol_name = volume['name']\n        host_name = self._get_host_from_connector(connector)\n        # Verify that _get_host_from_connector returned the host.\n        # This should always succeed as we terminate an existing connection.\n        self._driver_assert(\n            host_name is not None,\n            _('_get_host_from_connector failed to return the host name '\n              'for connector'))\n\n        # Check if vdisk-host mapping exists, remove if it does\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n        if vol_name in mapping_data:\n            ssh_cmd = 'svctask rmvdiskhostmap -host %s %s' % \\\n                (host_name, vol_name)\n            out, err = self._run_ssh(ssh_cmd)\n            # Verify CLI behaviour - no output is returned from\n            # rmvdiskhostmap\n            self._assert_ssh_return(len(out.strip()) == 0,\n                                    'terminate_connection', ssh_cmd, out, err)\n            del mapping_data[vol_name]\n        else:\n            LOG.error(_('terminate_connection: No mapping of volume '\n                        '%(vol_name)s to host %(host_name)s found') %\n                      {'vol_name': vol_name, 'host_name': host_name})\n\n        # If this host has no more mappings, delete it\n        if not mapping_data:\n            self._delete_host(host_name)\n\n        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})", "func_src_after": "    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Cleanup after an iSCSI connection has been terminated.\n\n        When we clean up a terminated connection between a given connector\n        and volume, we:\n        1. Translate the given connector to a host name\n        2. Remove the volume-to-host mapping if it exists\n        3. Delete the host if it has no more mappings (hosts are created\n           automatically by this driver when mappings are created)\n        \"\"\"\n        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})\n\n        vol_name = volume['name']\n        host_name = self._get_host_from_connector(connector)\n        # Verify that _get_host_from_connector returned the host.\n        # This should always succeed as we terminate an existing connection.\n        self._driver_assert(\n            host_name is not None,\n            _('_get_host_from_connector failed to return the host name '\n              'for connector'))\n\n        # Check if vdisk-host mapping exists, remove if it does\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n        if vol_name in mapping_data:\n            ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name,\n                       vol_name]\n            out, err = self._run_ssh(ssh_cmd)\n            # Verify CLI behaviour - no output is returned from\n            # rmvdiskhostmap\n            self._assert_ssh_return(len(out.strip()) == 0,\n                                    'terminate_connection', ssh_cmd, out, err)\n            del mapping_data[vol_name]\n        else:\n            LOG.error(_('terminate_connection: No mapping of volume '\n                        '%(vol_name)s to host %(host_name)s found') %\n                      {'vol_name': vol_name, 'host_name': host_name})\n\n        # If this host has no more mappings, delete it\n        if not mapping_data:\n            self._delete_host(host_name)\n\n        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to clean up resources after an iSCSI connection is terminated."}
{"func_name": "ZipUtil::checkDestinationFileForTraversal", "func_src_before": "  private static File checkDestinationFileForTraversal(File outputDir, String name, File destFile) throws IOException {\n    /* If we see the relative traversal string of \"..\" we need to make sure\n     * that the outputdir + name doesn't leave the outputdir. See\n     * DirectoryTraversalMaliciousTest for details.\n     */\n    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalPath().startsWith(outputDir.getCanonicalPath())) {\n      throw new MaliciousZipException(outputDir, name);\n    }\n    return destFile;\n  }", "func_src_after": "  private static File checkDestinationFileForTraversal(File outputDir, String name, File destFile) throws IOException {\n    /* If we see the relative traversal string of \"..\" we need to make sure\n     * that the outputdir + name doesn't leave the outputdir. See\n     * DirectoryTraversalMaliciousTest for details.\n     */\n    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalFile().toPath().startsWith(outputDir.getCanonicalFile().toPath())) {\n      throw new MaliciousZipException(outputDir, name);\n    }\n    return destFile;\n  }", "line_changes": {"deleted": [{"line_no": 6, "char_start": 322, "char_end": 431, "line": "    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalPath().startsWith(outputDir.getCanonicalPath())) {\n"}], "added": [{"line_no": 6, "char_start": 322, "char_end": 449, "line": "    if (name.indexOf(\"..\") != -1 && !destFile.getCanonicalFile().toPath().startsWith(outputDir.getCanonicalFile().toPath())) {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 380, "char_end": 389, "chars": "File().to"}, {"char_start": 429, "char_end": 438, "chars": "File().to"}]}, "commit_link": "github.com/zeroturnaround/zt-zip/commit/627bbc93907ceb69111f86e2edf26375a1abccfa", "file_name": "ZipUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "9b0818802c8fc804d75ef731da538423a7e020fa", "description": "Write a Java function to prevent directory traversal by validating a file's destination path against an intended output directory."}
{"func_name": "asylo::primitives::TrustedPrimitives::UntrustedCall", "func_src_before": "PrimitiveStatus TrustedPrimitives::UntrustedCall(uint64_t untrusted_selector,\n                                                 MessageWriter *input,\n                                                 MessageReader *output) {\n  int ret;\n\n  UntrustedCacheMalloc *untrusted_cache = UntrustedCacheMalloc::Instance();\n\n  SgxParams *const sgx_params =\n      reinterpret_cast<SgxParams *>(untrusted_cache->Malloc(sizeof(SgxParams)));\n  Cleanup clean_up(\n      [sgx_params, untrusted_cache] { untrusted_cache->Free(sgx_params); });\n  sgx_params->input_size = 0;\n  sgx_params->input = nullptr;\n  if (input) {\n    sgx_params->input_size = input->MessageSize();\n    if (sgx_params->input_size > 0) {\n      // Allocate and copy data to |input_buffer|.\n      sgx_params->input = untrusted_cache->Malloc(sgx_params->input_size);\n      input->Serialize(const_cast<void *>(sgx_params->input));\n    }\n  }\n  sgx_params->output_size = 0;\n  sgx_params->output = nullptr;\n  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(sgx_params->output, sgx_params->output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);\n  }\n  return PrimitiveStatus::OkStatus();\n}", "func_src_after": "PrimitiveStatus TrustedPrimitives::UntrustedCall(uint64_t untrusted_selector,\n                                                 MessageWriter *input,\n                                                 MessageReader *output) {\n  int ret;\n\n  UntrustedCacheMalloc *untrusted_cache = UntrustedCacheMalloc::Instance();\n\n  SgxParams *const sgx_params =\n      reinterpret_cast<SgxParams *>(untrusted_cache->Malloc(sizeof(SgxParams)));\n  Cleanup clean_up(\n      [sgx_params, untrusted_cache] { untrusted_cache->Free(sgx_params); });\n  sgx_params->input_size = 0;\n  sgx_params->input = nullptr;\n  if (input) {\n    sgx_params->input_size = input->MessageSize();\n    if (sgx_params->input_size > 0) {\n      // Allocate and copy data to |input_buffer|.\n      sgx_params->input = untrusted_cache->Malloc(sgx_params->input_size);\n      input->Serialize(const_cast<void *>(sgx_params->input));\n    }\n  }\n  sgx_params->output_size = 0;\n  sgx_params->output = nullptr;\n  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  if (!TrustedPrimitives::IsOutsideEnclave(sgx_params->output,\n                                           sgx_params->output_size)) {\n    TrustedPrimitives::BestEffortAbort(\n        \"UntrustedCall: sgx_param output should be in untrusted memory\");\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(sgx_params->output, sgx_params->output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);\n  }\n  return PrimitiveStatus::OkStatus();\n}", "commit_link": "github.com/google/asylo/commit/83036fd841d33baa7e039f842d131aa7881fdcc2", "file_name": "asylo/platform/primitives/sgx/trusted_sgx.cc", "vul_type": "cwe-125", "description": "Write a C++ function for handling an untrusted call with input and output message serialization in a secure enclave environment."}
{"func_name": "(anonymous)", "func_src_before": "\t\texec('node ' + binPath + ' ' + args.join(' '), function () {\n\t\t\tvar actual = fs.statSync('test/minified.png').size;\n\t\t\tvar original = fs.statSync('test/fixtures/test.png').size;\n\t\t\tassert(actual < original);\n\t\t\tcb();\n\t\t});", "func_src_after": "\t\texecFile('node', [binPath].concat(args), function () {\n\t\t\tvar actual = fs.statSync('test/minified.png').size;\n\t\t\tvar original = fs.statSync('test/fixtures/test.png').size;\n\t\t\tassert(actual < original);\n\t\t\tcb();\n\t\t});", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 63, "line": "\t\texec('node ' + binPath + ' ' + args.join(' '), function () {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 57, "line": "\t\texecFile('node', [binPath].concat(args), function () {\n"}]}, "char_changes": {"deleted": [{"char_start": 12, "char_end": 17, "chars": " ' + "}, {"char_start": 24, "char_end": 46, "chars": " + ' ' + args.join(' '"}], "added": [{"char_start": 6, "char_end": 10, "chars": "File"}, {"char_start": 16, "char_end": 20, "chars": "', ["}, {"char_start": 27, "char_end": 40, "chars": "].concat(args"}]}, "commit_link": "github.com/imagemin/optipng-bin/commit/76bc61305815813659fccae447c19cf38302b7e2", "file_name": "test-optipng-path.js", "vul_type": "cwe-078", "commit_msg": "use execFile and arguments array instead of string concat", "parent_commit": "fffe11371341e0360ecde090cf6f2bac5d81505a", "description": "Write a JavaScript function that executes a Node.js script with arguments and checks if the size of a minified image is smaller than the original image."}
{"func_name": "HPHP::WddxPacket::recursiveAddVar", "func_src_before": "bool WddxPacket::recursiveAddVar(const String& varName,\n                                 const Variant& varVariant,\n                                 bool hasVarTag) {\n\n  bool isArray = varVariant.isArray();\n  bool isObject = varVariant.isObject();\n\n  if (isArray || isObject) {\n    if (hasVarTag) {\n      m_packetString += \"<var name='\";\n      m_packetString += varName.data();\n      m_packetString += \"'>\";\n    }\n\n    Array varAsArray;\n    Object varAsObject = varVariant.toObject();\n    if (isArray) varAsArray = varVariant.toArray();\n    if (isObject) varAsArray = varAsObject.toArray();\n\n    int length = varAsArray.length();\n    if (length > 0) {\n      ArrayIter it = ArrayIter(varAsArray);\n      if (it.first().isString()) isObject = true;\n      if (isObject) {\n        m_packetString += \"<struct>\";\n        if (!isArray) {\n          m_packetString += \"<var name='php_class_name'><string>\";\n          m_packetString += varAsObject->o_getClassName().c_str();\n          m_packetString += \"</string></var>\";\n        }\n      } else {\n        m_packetString += \"<array length='\";\n        m_packetString += std::to_string(length);\n        m_packetString += \"'>\";\n      }\n      for (ArrayIter it(varAsArray); it; ++it) {\n        Variant key = it.first();\n        Variant value = it.second();\n        recursiveAddVar(key.toString(), value, isObject);\n      }\n      if (isObject) {\n        m_packetString += \"</struct>\";\n      }\n      else {\n        m_packetString += \"</array>\";\n      }\n    }\n    else {\n      //empty object\n      if (isObject) {\n        m_packetString += \"<struct>\";\n        if (!isArray) {\n          m_packetString += \"<var name='php_class_name'><string>\";\n          m_packetString += varAsObject->o_getClassName().c_str();\n          m_packetString += \"</string></var>\";\n        }\n        m_packetString += \"</struct>\";\n      }\n    }\n    if (hasVarTag) {\n      m_packetString += \"</var>\";\n    }\n    return true;\n  }\n\n  std::string varType = getDataTypeString(varVariant.getType()).data();\n  if (!getWddxEncoded(varType, \"\", varName, false).empty()) {\n    std::string varValue = varVariant.toString().data();\n    if (varType.compare(\"boolean\") == 0) {\n      varValue = varVariant.toBoolean() ? \"true\" : \"false\";\n    }\n    m_packetString += getWddxEncoded(varType, varValue, varName, hasVarTag);\n    return true;\n  }\n\n  return false;\n}", "func_src_after": "bool WddxPacket::recursiveAddVar(const String& varName,\n                                 const Variant& varVariant,\n                                 bool hasVarTag) {\n\n  bool isArray = varVariant.isArray();\n  bool isObject = varVariant.isObject();\n\n  if (isArray || isObject) {\n    if (hasVarTag) {\n      m_packetString += \"<var name='\";\n      m_packetString += varName.data();\n      m_packetString += \"'>\";\n    }\n\n    Array varAsArray;\n    Object varAsObject = varVariant.toObject();\n    if (isArray) varAsArray = varVariant.toArray();\n    if (isObject) varAsArray = varAsObject.toArray();\n\n    int length = varAsArray.length();\n    if (length > 0) {\n      ArrayIter it = ArrayIter(varAsArray);\n      if (it.first().isString()) isObject = true;\n      if (isObject) {\n        m_packetString += \"<struct>\";\n        if (!isArray) {\n          m_packetString += \"<var name='php_class_name'><string>\";\n          m_packetString += varAsObject->o_getClassName().c_str();\n          m_packetString += \"</string></var>\";\n        }\n      } else {\n        m_packetString += \"<array length='\";\n        m_packetString += std::to_string(length);\n        m_packetString += \"'>\";\n      }\n      for (ArrayIter it(varAsArray); it; ++it) {\n        Variant key = it.first();\n        Variant value = it.second();\n        recursiveAddVar(key.toString(), value, isObject);\n      }\n      if (isObject) {\n        m_packetString += \"</struct>\";\n      }\n      else {\n        m_packetString += \"</array>\";\n      }\n    }\n    else {\n      //empty object\n      if (isObject) {\n        m_packetString += \"<struct>\";\n        if (!isArray) {\n          m_packetString += \"<var name='php_class_name'><string>\";\n          m_packetString += varAsObject->o_getClassName().c_str();\n          m_packetString += \"</string></var>\";\n        }\n        m_packetString += \"</struct>\";\n      }\n    }\n    if (hasVarTag) {\n      m_packetString += \"</var>\";\n    }\n    return true;\n  }\n\n  std::string varType = getDataTypeString(varVariant.getType()).data();\n  if (!getWddxEncoded(varType, \"\", varName, false).empty()) {\n    std::string varValue;\n    if (varType.compare(\"boolean\") == 0) {\n      varValue = varVariant.toBoolean() ? \"true\" : \"false\";\n    } else {\n      varValue = StringUtil::HtmlEncode(varVariant.toString(),\n                                        StringUtil::QuoteStyle::Double,\n                                        \"UTF-8\", false, false).toCppString();\n    }\n    m_packetString += getWddxEncoded(varType, varValue, varName, hasVarTag);\n    return true;\n  }\n\n  return false;\n}", "commit_link": "github.com/facebook/hhvm/commit/324701c9fd31beb4f070f1b7ef78b115fbdfec34", "file_name": "hphp/runtime/ext/wddx/ext_wddx.cpp", "vul_type": "cwe-079", "description": "Write a C++ function named `recursiveAddVar` that serializes a variable into a WDDX (Web Distributed Data Exchange) packet string, handling arrays, objects, and primitive data types, and optionally wrapping the serialized data in a `<var>` tag."}
{"func_name": "testNonExistentFile", "func_src_before": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    non_existent = tempfile.mktemp(suffix=\".json\")\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "func_src_after": "  def testNonExistentFile(self):\n    converter = upgrade_schema_lib.Converter()\n    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n    with self.assertRaisesRegex(IOError, \"No such file or directory\"):\n      converter.Convert(non_existent, non_existent)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 80, "char_end": 131, "line": "    non_existent = tempfile.mktemp(suffix=\".json\")\n"}], "added": [{"line_no": 3, "char_start": 80, "char_end": 156, "line": "    _, non_existent = tempfile.mkstemp(suffix=\".json\")  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 83, "char_end": 86, "chars": " _,"}, {"char_start": 113, "char_end": 114, "chars": "s"}, {"char_start": 134, "char_end": 155, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/c63f6333a78ddf89ecd502d926fb877b8dce4f0f", "file_name": "upgrade_schema_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420335872\nChange-Id: I331ec2544a08d3cc3063a74af342cceae655b3dc", "description": "Write a Python unit test that checks if a custom converter raises an IOError when attempting to convert a non-existent JSON file."}
{"func_name": "get_context_data", "func_src_before": "    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['comments'] = self.object.comment_set.all().order_by('-time')\n        context['form'] = self.get_form()\n        context['md'] = markdown(self.object.content,\n                                 extensions=[\n                                     'markdown.extensions.extra',\n                                     'markdown.extensions.codehilite',\n                                     'markdown.extensions.toc',\n                                 ])\n\n        return context", "func_src_after": "    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['comments'] = self.object.comment_set.all().order_by('-time')\n        context['form'] = self.get_form()\n        context['md'] = safe_md(self.object.content)\n\n        return context", "commit_link": "github.com/Cheng-mq1216/production-practice/commit/333dc34f5feada55d1f6ff1255949ca00dec0f9c", "file_name": "app/Index/views.py", "vul_type": "cwe-079", "description": "Write a Python function to enhance the context data with comments, a form, and processed markdown content for a web page."}
{"func_name": "create_playlist", "func_src_before": "def create_playlist(name, db):\n    db.execute(\n        \"INSERT INTO playlist (name, video_position) VALUES('{name}', 0);\".format(name=name))", "func_src_after": "def create_playlist(name, db):\n    db.execute(\n        \"INSERT INTO playlist (name, video_position) VALUES(%s, 0);\", (name,))", "commit_link": "github.com/Madmous/playlist/commit/666e52c5f0b8c1f4296e84471637033d9542a7a6", "file_name": "playlist/playlist_repository.py", "vul_type": "cwe-089", "description": "Write a Python function to add a new playlist with a default video position to a database."}
{"func_name": "X86_insn_reg_intel", "func_src_before": "x86_reg X86_insn_reg_intel(unsigned int id, enum cs_ac_type *access)\n{\n\tunsigned int first = 0;\n\tunsigned int last = ARR_SIZE(insn_regs_intel) - 1;\n\tunsigned int mid = ARR_SIZE(insn_regs_intel) / 2;\n\n\tif (!intel_regs_sorted) {\n\t\tmemcpy(insn_regs_intel_sorted, insn_regs_intel,\n\t\t\t\tsizeof(insn_regs_intel_sorted));\n\t\tqsort(insn_regs_intel_sorted,\n\t\t\t\tARR_SIZE(insn_regs_intel_sorted),\n\t\t\t\tsizeof(struct insn_reg), regs_cmp);\n\t\tintel_regs_sorted = true;\n\t}\n\n\twhile (first <= last) {\n\t\tif (insn_regs_intel_sorted[mid].insn < id) {\n\t\t\tfirst = mid + 1;\n\t\t} else if (insn_regs_intel_sorted[mid].insn == id) {\n\t\t\tif (access) {\n\t\t\t\t*access = insn_regs_intel_sorted[mid].access;\n\t\t\t}\n\t\t\treturn insn_regs_intel_sorted[mid].reg;\n\t\t} else {\n\t\t\tif (mid == 0)\n\t\t\t\tbreak;\n\t\t\tlast = mid - 1;\n\t\t}\n\t\tmid = (first + last) / 2;\n\t}\n\n\t// not found\n\treturn 0;\n}", "func_src_after": "x86_reg X86_insn_reg_intel(unsigned int id, enum cs_ac_type *access)\n{\n\tstatic bool intel_regs_sorted = false;\n\tunsigned int first = 0;\n\tunsigned int last = ARR_SIZE(insn_regs_intel) - 1;\n\tunsigned int mid;\n\n\tif (!intel_regs_sorted) {\n\t\tmemcpy(insn_regs_intel_sorted, insn_regs_intel,\n\t\t\t\tsizeof(insn_regs_intel_sorted));\n\t\tqsort(insn_regs_intel_sorted,\n\t\t\t\tARR_SIZE(insn_regs_intel_sorted),\n\t\t\t\tsizeof(struct insn_reg), regs_cmp);\n\t\tintel_regs_sorted = true;\n\t}\n\n\tif (insn_regs_intel_sorted[0].insn > id ||\n\t\t\tinsn_regs_intel_sorted[last].insn < id) {\n\t\treturn 0;\n\t}\n\n\twhile (first <= last) {\n\t\tmid = (first + last) / 2;\n\t\tif (insn_regs_intel_sorted[mid].insn < id) {\n\t\t\tfirst = mid + 1;\n\t\t} else if (insn_regs_intel_sorted[mid].insn == id) {\n\t\t\tif (access) {\n\t\t\t\t*access = insn_regs_intel_sorted[mid].access;\n\t\t\t}\n\t\t\treturn insn_regs_intel_sorted[mid].reg;\n\t\t} else {\n\t\t\tif (mid == 0)\n\t\t\t\tbreak;\n\t\t\tlast = mid - 1;\n\t\t}\n\t}\n\n\t// not found\n\treturn 0;\n}", "commit_link": "github.com/aquynh/capstone/commit/87a25bb543c8e4c09b48d4b4a6c7db31ce58df06", "file_name": "arch/X86/X86Mapping.c", "vul_type": "cwe-125", "description": "Write a C function named `X86_insn_reg_intel` that performs a binary search on a sorted array to find a register by its ID and optionally returns its access type."}
{"func_name": "percona_command", "func_src_before": "    def percona_command(execute_sql, database_name, table_name, options = {})\n      command = \"pt-online-schema-change --alter '#{execute_sql}' D=#{database_name},t=#{table_name}\"\n\n      # Whitelist\n      options = HashWithIndifferentAccess.new(options)\n      options = options.slice(*self.class.percona_flags.keys)\n\n      # Merge config\n      config = percona_config\n      if config\n        config.slice(*self.class.percona_flags.keys).each do |key, value|\n          options[key] ||= value\n        end\n      end\n\n      # Set defaults\n      self.class.percona_flags.each do |flag, flag_config|\n        options[flag] = flag_config[:default] if flag_config.key?(:default) && !options.key?(flag)\n      end\n\n      \"#{command}#{run_mode_flag(options)}#{command_flags(options)}\"\n    end", "func_src_after": "    def percona_command(execute_sql, database_name, table_name, options = {})\n      command = ['pt-online-schema-change', '--alter', execute_sql || '', \"D=#{database_name},t=#{table_name}\"]\n\n      # Whitelist\n      options = HashWithIndifferentAccess.new(options)\n      options = options.slice(*self.class.percona_flags.keys)\n\n      # Merge config\n      config = percona_config\n      if config\n        config.slice(*self.class.percona_flags.keys).each do |key, value|\n          options[key] ||= value\n        end\n      end\n\n      # Set defaults\n      self.class.percona_flags.each do |flag, flag_config|\n        options[flag] = flag_config[:default] if flag_config.key?(:default) && !options.key?(flag)\n      end\n\n      command_parts = command + [run_mode_flag(options)] + command_flags(options)\n\n      command_parts.shelljoin\n    end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 78, "char_end": 180, "line": "      command = \"pt-online-schema-change --alter '#{execute_sql}' D=#{database_name},t=#{table_name}\"\n"}, {"line_no": 21, "char_start": 704, "char_end": 773, "line": "      \"#{command}#{run_mode_flag(options)}#{command_flags(options)}\"\n"}], "added": [{"line_no": 2, "char_start": 78, "char_end": 190, "line": "      command = ['pt-online-schema-change', '--alter', execute_sql || '', \"D=#{database_name},t=#{table_name}\"]\n"}, {"line_no": 21, "char_start": 714, "char_end": 796, "line": "      command_parts = command + [run_mode_flag(options)] + command_flags(options)\n"}, {"line_no": 22, "char_start": 796, "char_end": 797, "line": "\n"}, {"line_no": 23, "char_start": 797, "char_end": 827, "line": "      command_parts.shelljoin\n"}]}, "char_changes": {"deleted": [{"char_start": 94, "char_end": 95, "chars": "\""}, {"char_start": 118, "char_end": 119, "chars": " "}, {"char_start": 126, "char_end": 130, "chars": " '#{"}, {"char_start": 141, "char_end": 144, "chars": "}' "}, {"char_start": 710, "char_end": 713, "chars": "\"#{"}, {"char_start": 720, "char_end": 723, "chars": "}#{"}, {"char_start": 745, "char_end": 748, "chars": "}#{"}, {"char_start": 770, "char_end": 772, "chars": "}\""}], "added": [{"char_start": 94, "char_end": 96, "chars": "['"}, {"char_start": 119, "char_end": 123, "chars": "', '"}, {"char_start": 130, "char_end": 133, "chars": "', "}, {"char_start": 144, "char_end": 153, "chars": " || '', \""}, {"char_start": 188, "char_end": 189, "chars": "]"}, {"char_start": 727, "char_end": 747, "chars": "_parts = command + ["}, {"char_start": 769, "char_end": 773, "chars": "] + "}, {"char_start": 795, "char_end": 826, "chars": "\n\n      command_parts.shelljoin"}]}, "commit_link": "github.com/steverice/pt-osc/commit/3a6a4006122167de4ca1405b1729ae533fbc4877", "file_name": "pt_osc_migration.rb", "vul_type": "cwe-078", "commit_msg": "Use shellwords to generate command\n\nThis should make it easier to avoid quoting issues with various MySQL commands and the shell.\n\nFixes PagerDuty/pt-osc#12", "description": "Write a Ruby method that constructs a command line for the `pt-online-schema-change` tool, accepting SQL to execute, database and table names, and an optional hash of options."}
{"func_name": "__ns_get_path", "func_src_before": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}", "func_src_after": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_flags |= DCACHE_RCUACCESS;\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}", "commit_link": "github.com/torvalds/linux/commit/073c516ff73557a8f7315066856c04b50383ac34", "file_name": "fs/nsfs.c", "vul_type": "cwe-416", "description": "Write a C function named `__ns_get_path` that retrieves a file path from a namespace object."}
{"func_name": "ac_circ_buf_pushm", "func_src_before": "int ac_circ_buf_pushm(ac_circ_buf_t *cbuf, const void *buf, u32 count)\n{\n\tif (circ_space_to_end(cbuf) < count) {\n\t\tif (circ_count(cbuf) == 0 && count <= cbuf->size)\n\t\t\tcbuf->head = cbuf->tail = 0;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(cbuf->buf.ptr_buf + cbuf->head, buf,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(cbuf->buf.cpy_buf + cbuf->head, buf,\n\t\t       count * cbuf->elem_sz);\n\n\tcbuf->head = (cbuf->head + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "func_src_after": "int ac_circ_buf_pushm(ac_circ_buf_t *cbuf, const void *buf, u32 count)\n{\n\tif (circ_space_to_end(cbuf) < count) {\n\t\tif (circ_count(cbuf) == 0 && count <= cbuf->size)\n\t\t\tcbuf->head = cbuf->tail = 0;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\tif (cbuf->type == PTR_BUF)\n\t\tmemcpy(cbuf->buf.ptr_buf + cbuf->head, buf,\n\t\t       count * sizeof(void *));\n\telse\n\t\tmemcpy(cbuf->buf.cpy_buf + cbuf->head, buf,\n\t\t       (size_t)count * cbuf->elem_sz);\n\n\tcbuf->head = (cbuf->head + (count * cbuf->elem_sz)) &\n\t\t     ((cbuf->size - 1) * cbuf->elem_sz);\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 15, "char_start": 382, "char_end": 415, "line": "\t\t       count * cbuf->elem_sz);\n"}], "added": [{"line_no": 15, "char_start": 382, "char_end": 423, "line": "\t\t       (size_t)count * cbuf->elem_sz);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 391, "char_end": 399, "chars": "(size_t)"}]}, "commit_link": "github.com/ac000/libac/commit/2d6eb46697a185e7ecdbbf0197c405096765cf27", "file_name": "ac_circ_buf.c", "vul_type": "cwe-190", "commit_msg": "ac_circ_buf: Fix some potential unsigned integer overflows\n\nLGTM[0] pointed out some issues in ac_circ_buf.c regarding the\nmalloc(3)'s\n\n  'Multiplication result may overflow 'unsigned int' before it is\n   converted to 'size_t'.'\n\nThis is unlikely to hit in real life, but lets fix it anyway by casting\nthe size/count part of the calculation in the mallocs to size_t.\n\n[0]: https://lgtm.com/\n\nSigned-off-by: Andrew Clayton <andrew@digital-domain.net>", "parent_commit": "536fc22f2c496342e6391b4f81c91ad41816571b", "description": "Write a C function to add multiple elements to a circular buffer, returning 0 on success or -1 if there isn't enough space."}
{"func_name": "decode_frame", "func_src_before": "static int decode_frame(AVCodecContext *avctx,\n                        void *data, int *got_frame,\n                        AVPacket *avpkt)\n{\n    PicContext *s = avctx->priv_data;\n    AVFrame *frame = data;\n    uint32_t *palette;\n    int bits_per_plane, bpp, etype, esize, npal, pos_after_pal;\n    int i, x, y, plane, tmp, ret, val;\n\n    bytestream2_init(&s->g, avpkt->data, avpkt->size);\n\n    if (bytestream2_get_bytes_left(&s->g) < 11)\n        return AVERROR_INVALIDDATA;\n\n    if (bytestream2_get_le16u(&s->g) != 0x1234)\n        return AVERROR_INVALIDDATA;\n\n    s->width       = bytestream2_get_le16u(&s->g);\n    s->height      = bytestream2_get_le16u(&s->g);\n    bytestream2_skip(&s->g, 4);\n    tmp            = bytestream2_get_byteu(&s->g);\n    bits_per_plane = tmp & 0xF;\n    s->nb_planes   = (tmp >> 4) + 1;\n    bpp            = bits_per_plane * s->nb_planes;\n    if (bits_per_plane > 8 || bpp < 1 || bpp > 32) {\n        avpriv_request_sample(avctx, \"Unsupported bit depth\");\n        return AVERROR_PATCHWELCOME;\n    }\n\n    if (bytestream2_peek_byte(&s->g) == 0xFF || bpp == 1 || bpp == 4 || bpp == 8) {\n        bytestream2_skip(&s->g, 2);\n        etype = bytestream2_get_le16(&s->g);\n        esize = bytestream2_get_le16(&s->g);\n        if (bytestream2_get_bytes_left(&s->g) < esize)\n            return AVERROR_INVALIDDATA;\n    } else {\n        etype = -1;\n        esize = 0;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_PAL8;\n\n    if (av_image_check_size(s->width, s->height, 0, avctx) < 0)\n        return -1;\n    if (s->width != avctx->width && s->height != avctx->height) {\n        ret = ff_set_dimensions(avctx, s->width, s->height);\n        if (ret < 0)\n            return ret;\n    }\n\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n    memset(frame->data[0], 0, s->height * frame->linesize[0]);\n    frame->pict_type           = AV_PICTURE_TYPE_I;\n    frame->palette_has_changed = 1;\n\n    pos_after_pal = bytestream2_tell(&s->g) + esize;\n    palette = (uint32_t*)frame->data[1];\n    if (etype == 1 && esize > 1 && bytestream2_peek_byte(&s->g) < 6) {\n        int idx = bytestream2_get_byte(&s->g);\n        npal = 4;\n        for (i = 0; i < npal; i++)\n            palette[i] = ff_cga_palette[ cga_mode45_index[idx][i] ];\n    } else if (etype == 2) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_cga_palette[FFMIN(pal_idx, 15)];\n        }\n    } else if (etype == 3) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_ega_palette[FFMIN(pal_idx, 63)];\n        }\n    } else if (etype == 4 || etype == 5) {\n        npal = FFMIN(esize / 3, 256);\n        for (i = 0; i < npal; i++) {\n            palette[i] = bytestream2_get_be24(&s->g) << 2;\n            palette[i] |= 0xFFU << 24 | palette[i] >> 6 & 0x30303;\n        }\n    } else {\n        if (bpp == 1) {\n            npal = 2;\n            palette[0] = 0xFF000000;\n            palette[1] = 0xFFFFFFFF;\n        } else if (bpp == 2) {\n            npal = 4;\n            for (i = 0; i < npal; i++)\n                palette[i] = ff_cga_palette[ cga_mode45_index[0][i] ];\n        } else {\n            npal = 16;\n            memcpy(palette, ff_cga_palette, npal * 4);\n        }\n    }\n    // fill remaining palette entries\n    memset(palette + npal, 0, AVPALETTE_SIZE - npal * 4);\n    // skip remaining palette bytes\n    bytestream2_seek(&s->g, pos_after_pal, SEEK_SET);\n\n    val = 0;\n    y = s->height - 1;\n    if (bytestream2_get_le16(&s->g)) {\n        x = 0;\n        plane = 0;\n        while (bytestream2_get_bytes_left(&s->g) >= 6) {\n            int stop_size, marker, t1, t2;\n\n            t1        = bytestream2_get_bytes_left(&s->g);\n            t2        = bytestream2_get_le16(&s->g);\n            stop_size = t1 - FFMIN(t1, t2);\n            // ignore uncompressed block size\n            bytestream2_skip(&s->g, 2);\n            marker    = bytestream2_get_byte(&s->g);\n\n            while (plane < s->nb_planes &&\n                   bytestream2_get_bytes_left(&s->g) > stop_size) {\n                int run = 1;\n                val = bytestream2_get_byte(&s->g);\n                if (val == marker) {\n                    run = bytestream2_get_byte(&s->g);\n                    if (run == 0)\n                        run = bytestream2_get_le16(&s->g);\n                    val = bytestream2_get_byte(&s->g);\n                }\n                if (!bytestream2_get_bytes_left(&s->g))\n                    break;\n\n                if (bits_per_plane == 8) {\n                    picmemset_8bpp(s, frame, val, run, &x, &y);\n                    if (y < 0)\n                        goto finish;\n                } else {\n                    picmemset(s, frame, val, run, &x, &y, &plane, bits_per_plane);\n                }\n            }\n        }\n\n        if (x < avctx->width) {\n            int run = (y + 1) * avctx->width - x;\n            if (bits_per_plane == 8)\n                picmemset_8bpp(s, frame, val, run, &x, &y);\n            else\n                picmemset(s, frame, val, run / (8 / bits_per_plane), &x, &y, &plane, bits_per_plane);\n        }\n    } else {\n        while (y >= 0 && bytestream2_get_bytes_left(&s->g) > 0) {\n            memcpy(frame->data[0] + y * frame->linesize[0], s->g.buffer, FFMIN(avctx->width, bytestream2_get_bytes_left(&s->g)));\n            bytestream2_skip(&s->g, avctx->width);\n            y--;\n        }\n    }\nfinish:\n\n    *got_frame      = 1;\n    return avpkt->size;\n}", "func_src_after": "static int decode_frame(AVCodecContext *avctx,\n                        void *data, int *got_frame,\n                        AVPacket *avpkt)\n{\n    PicContext *s = avctx->priv_data;\n    AVFrame *frame = data;\n    uint32_t *palette;\n    int bits_per_plane, bpp, etype, esize, npal, pos_after_pal;\n    int i, x, y, plane, tmp, ret, val;\n\n    bytestream2_init(&s->g, avpkt->data, avpkt->size);\n\n    if (bytestream2_get_bytes_left(&s->g) < 11)\n        return AVERROR_INVALIDDATA;\n\n    if (bytestream2_get_le16u(&s->g) != 0x1234)\n        return AVERROR_INVALIDDATA;\n\n    s->width       = bytestream2_get_le16u(&s->g);\n    s->height      = bytestream2_get_le16u(&s->g);\n    bytestream2_skip(&s->g, 4);\n    tmp            = bytestream2_get_byteu(&s->g);\n    bits_per_plane = tmp & 0xF;\n    s->nb_planes   = (tmp >> 4) + 1;\n    bpp            = bits_per_plane * s->nb_planes;\n    if (bits_per_plane > 8 || bpp < 1 || bpp > 32) {\n        avpriv_request_sample(avctx, \"Unsupported bit depth\");\n        return AVERROR_PATCHWELCOME;\n    }\n\n    if (bytestream2_peek_byte(&s->g) == 0xFF || bpp == 1 || bpp == 4 || bpp == 8) {\n        bytestream2_skip(&s->g, 2);\n        etype = bytestream2_get_le16(&s->g);\n        esize = bytestream2_get_le16(&s->g);\n        if (bytestream2_get_bytes_left(&s->g) < esize)\n            return AVERROR_INVALIDDATA;\n    } else {\n        etype = -1;\n        esize = 0;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_PAL8;\n\n    if (av_image_check_size(s->width, s->height, 0, avctx) < 0)\n        return -1;\n    if (s->width != avctx->width || s->height != avctx->height) {\n        ret = ff_set_dimensions(avctx, s->width, s->height);\n        if (ret < 0)\n            return ret;\n    }\n\n    if ((ret = ff_get_buffer(avctx, frame, 0)) < 0)\n        return ret;\n    memset(frame->data[0], 0, s->height * frame->linesize[0]);\n    frame->pict_type           = AV_PICTURE_TYPE_I;\n    frame->palette_has_changed = 1;\n\n    pos_after_pal = bytestream2_tell(&s->g) + esize;\n    palette = (uint32_t*)frame->data[1];\n    if (etype == 1 && esize > 1 && bytestream2_peek_byte(&s->g) < 6) {\n        int idx = bytestream2_get_byte(&s->g);\n        npal = 4;\n        for (i = 0; i < npal; i++)\n            palette[i] = ff_cga_palette[ cga_mode45_index[idx][i] ];\n    } else if (etype == 2) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_cga_palette[FFMIN(pal_idx, 15)];\n        }\n    } else if (etype == 3) {\n        npal = FFMIN(esize, 16);\n        for (i = 0; i < npal; i++) {\n            int pal_idx = bytestream2_get_byte(&s->g);\n            palette[i]  = ff_ega_palette[FFMIN(pal_idx, 63)];\n        }\n    } else if (etype == 4 || etype == 5) {\n        npal = FFMIN(esize / 3, 256);\n        for (i = 0; i < npal; i++) {\n            palette[i] = bytestream2_get_be24(&s->g) << 2;\n            palette[i] |= 0xFFU << 24 | palette[i] >> 6 & 0x30303;\n        }\n    } else {\n        if (bpp == 1) {\n            npal = 2;\n            palette[0] = 0xFF000000;\n            palette[1] = 0xFFFFFFFF;\n        } else if (bpp == 2) {\n            npal = 4;\n            for (i = 0; i < npal; i++)\n                palette[i] = ff_cga_palette[ cga_mode45_index[0][i] ];\n        } else {\n            npal = 16;\n            memcpy(palette, ff_cga_palette, npal * 4);\n        }\n    }\n    // fill remaining palette entries\n    memset(palette + npal, 0, AVPALETTE_SIZE - npal * 4);\n    // skip remaining palette bytes\n    bytestream2_seek(&s->g, pos_after_pal, SEEK_SET);\n\n    val = 0;\n    y = s->height - 1;\n    if (bytestream2_get_le16(&s->g)) {\n        x = 0;\n        plane = 0;\n        while (bytestream2_get_bytes_left(&s->g) >= 6) {\n            int stop_size, marker, t1, t2;\n\n            t1        = bytestream2_get_bytes_left(&s->g);\n            t2        = bytestream2_get_le16(&s->g);\n            stop_size = t1 - FFMIN(t1, t2);\n            // ignore uncompressed block size\n            bytestream2_skip(&s->g, 2);\n            marker    = bytestream2_get_byte(&s->g);\n\n            while (plane < s->nb_planes &&\n                   bytestream2_get_bytes_left(&s->g) > stop_size) {\n                int run = 1;\n                val = bytestream2_get_byte(&s->g);\n                if (val == marker) {\n                    run = bytestream2_get_byte(&s->g);\n                    if (run == 0)\n                        run = bytestream2_get_le16(&s->g);\n                    val = bytestream2_get_byte(&s->g);\n                }\n                if (!bytestream2_get_bytes_left(&s->g))\n                    break;\n\n                if (bits_per_plane == 8) {\n                    picmemset_8bpp(s, frame, val, run, &x, &y);\n                    if (y < 0)\n                        goto finish;\n                } else {\n                    picmemset(s, frame, val, run, &x, &y, &plane, bits_per_plane);\n                }\n            }\n        }\n\n        if (x < avctx->width) {\n            int run = (y + 1) * avctx->width - x;\n            if (bits_per_plane == 8)\n                picmemset_8bpp(s, frame, val, run, &x, &y);\n            else\n                picmemset(s, frame, val, run / (8 / bits_per_plane), &x, &y, &plane, bits_per_plane);\n        }\n    } else {\n        while (y >= 0 && bytestream2_get_bytes_left(&s->g) > 0) {\n            memcpy(frame->data[0] + y * frame->linesize[0], s->g.buffer, FFMIN(avctx->width, bytestream2_get_bytes_left(&s->g)));\n            bytestream2_skip(&s->g, avctx->width);\n            y--;\n        }\n    }\nfinish:\n\n    *got_frame      = 1;\n    return avpkt->size;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/8c2ea3030af7b40a3c4275696fb5c76cdb80950a", "file_name": "libavcodec/pictordec.c", "vul_type": "cwe-787", "description": "Write a C function for decoding a video frame using the FFmpeg library."}
{"func_name": "String.prototype.strip_tags", "func_src_before": "String.prototype.strip_tags = function(){\n\ttags = this;\n\tstripped = tags.replace(/[\\<\\>]/gi, \"\");\n\treturn stripped;\n};", "func_src_after": "String.prototype.strip_tags = function(){\n\ttags = this;\n\tstripped = tags.replace(/<(.|\\n)*?>/g, '');\n\treturn stripped;\n};", "line_changes": {"deleted": [{"line_no": 3, "char_start": 56, "char_end": 98, "line": "\tstripped = tags.replace(/[\\<\\>]/gi, \"\");\n"}], "added": [{"line_no": 3, "char_start": 56, "char_end": 101, "line": "\tstripped = tags.replace(/<(.|\\n)*?>/g, '');\n"}]}, "char_changes": {"deleted": [{"char_start": 82, "char_end": 88, "chars": "[\\<\\>]"}, {"char_start": 90, "char_end": 91, "chars": "i"}, {"char_start": 93, "char_end": 95, "chars": "\"\""}], "added": [{"char_start": 82, "char_end": 92, "chars": "<(.|\\n)*?>"}, {"char_start": 96, "char_end": 98, "chars": "''"}]}, "commit_link": "github.com/whitekiba/server/commit/cf113409adf82d0834181dbdf4586fd2ad262898", "file_name": "contacts.js", "vul_type": "cwe-079", "commit_msg": "Contacts: Fix XSS.", "description": "Create a JavaScript function that extends the String prototype to remove HTML tags from a string."}
{"func_name": "WriteImageChannels", "func_src_before": "static MagickBooleanType WriteImageChannels(const PSDInfo *psd_info,\n  const ImageInfo *image_info,Image *image,Image *next_image,\n  const MagickBooleanType separate,ExceptionInfo *exception)\n{\n  size_t\n    channels,\n    packet_size;\n\n  unsigned char\n    *compact_pixels;\n\n  /*\n    Write uncompressed pixels as separate planes.\n  */\n  channels=1;\n  packet_size=next_image->depth > 8UL ? 2UL : 1UL;\n  compact_pixels=(unsigned char *) NULL;\n  if (next_image->compression == RLECompression)\n    {\n      compact_pixels=(unsigned char *) AcquireQuantumMemory(2*channels*\n        next_image->columns,packet_size*sizeof(*compact_pixels));\n      if (compact_pixels == (unsigned char *) NULL)\n        ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  if (IsImageGray(next_image) != MagickFalse)\n    {\n      if (next_image->compression == RLECompression)\n        {\n          /*\n            Packbits compression.\n          */\n          (void) WriteBlobMSBShort(image,1);\n          WritePackbitsLength(psd_info,image_info,image,next_image,\n            compact_pixels,GrayQuantum,exception);\n          if (next_image->alpha_trait != UndefinedPixelTrait)\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,AlphaQuantum,exception);\n        }\n      WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n        GrayQuantum,MagickTrue,exception);\n      if (next_image->alpha_trait != UndefinedPixelTrait)\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          AlphaQuantum,separate,exception);\n      (void) SetImageProgress(image,SaveImagesTag,0,1);\n    }\n  else\n    if (next_image->storage_class == PseudoClass)\n      {\n        if (next_image->compression == RLECompression)\n          {\n            /*\n              Packbits compression.\n            */\n            (void) WriteBlobMSBShort(image,1);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,IndexQuantum,exception);\n            if (next_image->alpha_trait != UndefinedPixelTrait)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,AlphaQuantum,exception);\n          }\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          IndexQuantum,MagickTrue,exception);\n        if (next_image->alpha_trait != UndefinedPixelTrait)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            AlphaQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,0,1);\n      }\n    else\n      {\n        if (next_image->colorspace == CMYKColorspace)\n          (void) NegateCMYK(next_image,exception);\n        if (next_image->compression == RLECompression)\n          {\n            /*\n              Packbits compression.\n            */\n            (void) WriteBlobMSBShort(image,1);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,RedQuantum,exception);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,GreenQuantum,exception);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,BlueQuantum,exception);\n            if (next_image->colorspace == CMYKColorspace)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,BlackQuantum,exception);\n            if (next_image->alpha_trait != UndefinedPixelTrait)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,AlphaQuantum,exception);\n          }\n        (void) SetImageProgress(image,SaveImagesTag,0,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          RedQuantum,MagickTrue,exception);\n        (void) SetImageProgress(image,SaveImagesTag,1,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          GreenQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,2,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          BlueQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,3,6);\n        if (next_image->colorspace == CMYKColorspace)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            BlackQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,4,6);\n        if (next_image->alpha_trait != UndefinedPixelTrait)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            AlphaQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,5,6);\n        if (next_image->colorspace == CMYKColorspace)\n          (void) NegateCMYK(next_image,exception);\n      }\n  if (next_image->compression == RLECompression)\n    compact_pixels=(unsigned char *) RelinquishMagickMemory(compact_pixels);\n  return(MagickTrue);\n}", "func_src_after": "static MagickBooleanType WriteImageChannels(const PSDInfo *psd_info,\n  const ImageInfo *image_info,Image *image,Image *next_image,\n  const MagickBooleanType separate,ExceptionInfo *exception)\n{\n  size_t\n    channels,\n    packet_size;\n\n  unsigned char\n    *compact_pixels;\n\n  /*\n    Write uncompressed pixels as separate planes.\n  */\n  channels=1;\n  packet_size=next_image->depth > 8UL ? 2UL : 1UL;\n  compact_pixels=(unsigned char *) NULL;\n  if (next_image->compression == RLECompression)\n    {\n      compact_pixels=(unsigned char *) AcquireQuantumMemory((2*channels*\n        next_image->columns)+1,packet_size*sizeof(*compact_pixels));\n      if (compact_pixels == (unsigned char *) NULL)\n        ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  if (IsImageGray(next_image) != MagickFalse)\n    {\n      if (next_image->compression == RLECompression)\n        {\n          /*\n            Packbits compression.\n          */\n          (void) WriteBlobMSBShort(image,1);\n          WritePackbitsLength(psd_info,image_info,image,next_image,\n            compact_pixels,GrayQuantum,exception);\n          if (next_image->alpha_trait != UndefinedPixelTrait)\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,AlphaQuantum,exception);\n        }\n      WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n        GrayQuantum,MagickTrue,exception);\n      if (next_image->alpha_trait != UndefinedPixelTrait)\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          AlphaQuantum,separate,exception);\n      (void) SetImageProgress(image,SaveImagesTag,0,1);\n    }\n  else\n    if (next_image->storage_class == PseudoClass)\n      {\n        if (next_image->compression == RLECompression)\n          {\n            /*\n              Packbits compression.\n            */\n            (void) WriteBlobMSBShort(image,1);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,IndexQuantum,exception);\n            if (next_image->alpha_trait != UndefinedPixelTrait)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,AlphaQuantum,exception);\n          }\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          IndexQuantum,MagickTrue,exception);\n        if (next_image->alpha_trait != UndefinedPixelTrait)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            AlphaQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,0,1);\n      }\n    else\n      {\n        if (next_image->colorspace == CMYKColorspace)\n          (void) NegateCMYK(next_image,exception);\n        if (next_image->compression == RLECompression)\n          {\n            /*\n              Packbits compression.\n            */\n            (void) WriteBlobMSBShort(image,1);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,RedQuantum,exception);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,GreenQuantum,exception);\n            WritePackbitsLength(psd_info,image_info,image,next_image,\n              compact_pixels,BlueQuantum,exception);\n            if (next_image->colorspace == CMYKColorspace)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,BlackQuantum,exception);\n            if (next_image->alpha_trait != UndefinedPixelTrait)\n              WritePackbitsLength(psd_info,image_info,image,next_image,\n                compact_pixels,AlphaQuantum,exception);\n          }\n        (void) SetImageProgress(image,SaveImagesTag,0,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          RedQuantum,MagickTrue,exception);\n        (void) SetImageProgress(image,SaveImagesTag,1,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          GreenQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,2,6);\n        WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n          BlueQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,3,6);\n        if (next_image->colorspace == CMYKColorspace)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            BlackQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,4,6);\n        if (next_image->alpha_trait != UndefinedPixelTrait)\n          WriteOneChannel(psd_info,image_info,image,next_image,compact_pixels,\n            AlphaQuantum,separate,exception);\n        (void) SetImageProgress(image,SaveImagesTag,5,6);\n        if (next_image->colorspace == CMYKColorspace)\n          (void) NegateCMYK(next_image,exception);\n      }\n  if (next_image->compression == RLECompression)\n    compact_pixels=(unsigned char *) RelinquishMagickMemory(compact_pixels);\n  return(MagickTrue);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/6f1879d498bcc5cce12fe0c5decb8dbc0f608e5d", "file_name": "coders/psd.c", "vul_type": "cwe-125", "description": "In C, write a function to handle writing image channel data with optional RLE compression and color space considerations."}
{"func_name": "userLogin", "func_src_before": "    def userLogin(self):\n\n        sqlName=\"select count(*) from users where name='%s' and \\\n                password='%s';\"%(self.name,self.password)\n        checkName=sql.queryDB(self.conn,sqlName)\n\n        result=checkName[0][0]\n        if result == 0:\n            self.clean()\n            return False\n        else:\n            return True", "func_src_after": "    def userLogin(self):\n\n        sqlName=\"select count(*) from users where name=%s and password=%s;\"\n        params = [self.name,self.password]\n        checkName=sql.queryDB(self.conn,sqlName,params)\n        result=checkName[0][0]\n        if result == 0:\n            self.clean()\n            return False\n        else:\n            return True", "commit_link": "github.com/ShaominLi/Twitter_project/commit/5329d91f9e569c95184053c8e7ef596949c33ce9", "file_name": "modules/users.py", "vul_type": "cwe-089", "description": "Write a Python function named `userLogin` that checks if a user's name and password exist in the database and returns a boolean accordingly."}
{"func_name": "ring_buffer_resize", "func_src_before": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tsize = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tsize *= BUF_PAGE_SIZE;\n\n\t/* we need a minimum of two pages */\n\tif (size < BUF_PAGE_SIZE * 2)\n\t\tsize = BUF_PAGE_SIZE * 2;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}", "func_src_after": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/* we need a minimum of two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tsize = nr_pages * BUF_PAGE_SIZE;\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/59643d1535eb220668692a5359de22545af579f6", "file_name": "kernel/trace/ring_buffer.c", "vul_type": "cwe-190", "description": "Write a C function to resize a ring buffer for a specific CPU or all CPUs, handling memory allocation and synchronization issues."}
{"func_name": "_get_flashcopy_mapping_attributes", "func_src_before": "    def _get_flashcopy_mapping_attributes(self, fc_map_id):\n        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')\n                  % fc_map_id)\n\n        fc_ls_map_cmd = 'svcinfo lsfcmap -filtervalue id=%s -delim !' % \\\n            fc_map_id\n        out, err = self._run_ssh(fc_ls_map_cmd)\n        if not len(out.strip()):\n            return None\n\n        # Get list of FlashCopy mappings\n        # We expect zero or one line if mapping does not exist,\n        # two lines if it does exist, otherwise error\n        lines = out.strip().split('\\n')\n        self._assert_ssh_return(len(lines) <= 2,\n                                '_get_flashcopy_mapping_attributes',\n                                fc_ls_map_cmd, out, err)\n\n        if len(lines) == 2:\n            attributes = self._get_hdr_dic(lines[0], lines[1], '!')\n        else:  # 0 or 1 lines\n            attributes = None\n\n        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '\n                    '%(fc_map_id)s, attributes %(attributes)s') %\n                  {'fc_map_id': fc_map_id, 'attributes': attributes})\n\n        return attributes", "func_src_after": "    def _get_flashcopy_mapping_attributes(self, fc_map_id):\n        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')\n                  % fc_map_id)\n\n        fc_ls_map_cmd = ['svcinfo', 'lsfcmap', '-filtervalue',\n                         'id=%s' % fc_map_id, '-delim', '!']\n        out, err = self._run_ssh(fc_ls_map_cmd)\n        if not len(out.strip()):\n            return None\n\n        # Get list of FlashCopy mappings\n        # We expect zero or one line if mapping does not exist,\n        # two lines if it does exist, otherwise error\n        lines = out.strip().split('\\n')\n        self._assert_ssh_return(len(lines) <= 2,\n                                '_get_flashcopy_mapping_attributes',\n                                fc_ls_map_cmd, out, err)\n\n        if len(lines) == 2:\n            attributes = self._get_hdr_dic(lines[0], lines[1], '!')\n        else:  # 0 or 1 lines\n            attributes = None\n\n        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '\n                    '%(fc_map_id)s, attributes %(attributes)s') %\n                  {'fc_map_id': fc_map_id, 'attributes': attributes})\n\n        return attributes", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to fetch and parse FlashCopy mapping attributes using SSH commands."}
{"func_name": "karma_ask", "func_src_before": "def karma_ask(name):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(\n            ''' SELECT karma FROM people WHERE name='{}' '''.format(name))\n        karma = cursor.fetchone()\n        if karma is None:\n            logger.debug('No karma found for name {}'.format(name))\n            db.close()\n            return karma\n        else:\n            karma = karma[0]\n            logger.debug('karma of {} found for name {}'.format(karma, name))\n            db.close()\n            return karma\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "func_src_after": "def karma_ask(name):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute(''' SELECT karma FROM people WHERE name=%(name)s ''',\n                       (name, ))\n        karma = cursor.fetchone()\n        if karma is None:\n            logger.debug('No karma found for name {}'.format(name))\n            db.close()\n            return karma\n        else:\n            karma = karma[0]\n            logger.debug('karma of {} found for name {}'.format(karma, name))\n            db.close()\n            return karma\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "commit_link": "github.com/tylarb/KarmaBoi-PCF/commit/c1d00a27d7f6b7eb6f15a3dacd4269654a32c10a", "file_name": "KarmaBoi/dbopts.py", "vul_type": "cwe-089", "description": "Write a Python function to query a database for a person's karma by their name and handle the result or potential errors."}
{"func_name": "placeholder", "func_src_before": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "func_src_after": "      def placeholder(filename)\n        css_class = InlineSvg.configuration.svg_not_found_css_class\n        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n\n        if css_class.nil?\n          return \"<svg><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        else\n          return \"<svg class='#{css_class}'><!-- SVG file not found: #{not_found_message}--></svg>\".html_safe\n        end\n      end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 100, "char_end": 172, "line": "        not_found_message = \"'#{filename}' #{extension_hint(filename)}\"\n"}], "added": [{"line_no": 3, "char_start": 100, "char_end": 200, "line": "        not_found_message = \"'#{ERB::Util.html_escape_once(filename)}' #{extension_hint(filename)}\"\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 132, "char_end": 159, "chars": "ERB::Util.html_escape_once("}, {"char_start": 167, "char_end": 168, "chars": ")"}]}, "commit_link": "github.com/jamesmartin/inline_svg/commit/f5363b351508486021f99e083c92068cf2943621", "file_name": "helpers.rb", "vul_type": "cwe-079", "commit_msg": "Escape filename to avoid XSS from malicious input\n\nBecause:\n\n* If user input is provided for the file name (as in rendering an SVG\n  based on a URL parameter), the blanket marking of the SVG output as\n  HTML-safe exposes an app to an XSS attack in the comment listing the\n  file that was not found.\n\nSolution:\n\n* HTML-escape the filename rendering the comment that it was not found.", "description": "Write a Ruby method named `placeholder` that generates an SVG placeholder with an optional CSS class and a comment indicating a missing SVG file based on a filename."}
{"func_name": "delete_data", "func_src_before": "    def delete_data(self, session, id):\n        self._openContainer(session)\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, str(id))\n        query = \"DELETE FROM %s WHERE identifier = '%s';\" % (self.table, sid)\n        self._query(query)\n        return None", "func_src_after": "    def delete_data(self, session, id):\n        self._openContainer(session)\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, str(id))\n        query = \"DELETE FROM %s WHERE identifier = $1;\" % (self.table)\n        self._query(query, sid)\n        return None", "commit_link": "github.com/cheshire3/cheshire3/commit/d350363b4ea10f102c24c8f26d7b76b006323e8e", "file_name": "cheshire3/sql/postgresStore.py", "vul_type": "cwe-089", "description": "Write a Python function to delete a record from a database table by ID, with optional ID normalization."}
{"func_name": "TIFFSeekCustomStream", "func_src_before": "static MagickOffsetType TIFFSeekCustomStream(const MagickOffsetType offset,\n  const int whence,void *user_data)\n{\n  PhotoshopProfile\n    *profile;\n\n  profile=(PhotoshopProfile *) user_data;\n  switch (whence)\n  {\n    case SEEK_SET:\n    default:\n    {\n      if (offset < 0)\n        return(-1);\n      profile->offset=offset;\n      break;\n    }\n    case SEEK_CUR:\n    {\n      if ((profile->offset+offset) < 0)\n        return(-1);\n      profile->offset+=offset;\n      break;\n    }\n    case SEEK_END:\n    {\n      if (((MagickOffsetType) profile->length+offset) < 0)\n        return(-1);\n      profile->offset=profile->length+offset;\n      break;\n    }\n  }\n\n  return(profile->offset);\n}", "func_src_after": "static MagickOffsetType TIFFSeekCustomStream(const MagickOffsetType offset,\n  const int whence,void *user_data)\n{\n  PhotoshopProfile\n    *profile;\n\n  profile=(PhotoshopProfile *) user_data;\n  switch (whence)\n  {\n    case SEEK_SET:\n    default:\n    {\n      if (offset < 0)\n        return(-1);\n      profile->offset=offset;\n      break;\n    }\n    case SEEK_CUR:\n    {\n      if (((offset > 0) && (profile->offset > (SSIZE_MAX-offset))) ||\n          ((offset < 0) && (profile->offset < (-SSIZE_MAX-offset))))\n        {\n          errno=EOVERFLOW;\n          return(-1);\n        }\n      if ((profile->offset+offset) < 0)\n        return(-1);\n      profile->offset+=offset;\n      break;\n    }\n    case SEEK_END:\n    {\n      if (((MagickOffsetType) profile->length+offset) < 0)\n        return(-1);\n      profile->offset=profile->length+offset;\n      break;\n    }\n  }\n\n  return(profile->offset);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/fe5f4b85e6b1b54d3b4588a77133c06ade46d891", "file_name": "coders/tiff.c", "vul_type": "cwe-190", "description": "Write a C function named `TIFFSeekCustomStream` that adjusts the offset within a Photoshop profile structure based on the seek operation specified."}
{"func_name": "extend_volume", "func_src_before": "    def extend_volume(self, volume, new_size):\n        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])\n        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],\n                                                allow_snaps=False)\n        if not ret:\n            exception_message = (_('extend_volume: Extending a volume with '\n                                   'snapshots is not supported.'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        extend_amt = int(new_size) - volume['size']\n        ssh_cmd = ('svctask expandvdisksize -size %(amt)d -unit gb %(name)s'\n                   % {'amt': extend_amt, 'name': volume['name']})\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from expandvdisksize\n        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])", "func_src_after": "    def extend_volume(self, volume, new_size):\n        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])\n        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],\n                                                allow_snaps=False)\n        if not ret:\n            exception_message = (_('extend_volume: Extending a volume with '\n                                   'snapshots is not supported.'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        extend_amt = int(new_size) - volume['size']\n        ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt),\n                    '-unit', 'gb', volume['name']])\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from expandvdisksize\n        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to increase the size of a storage volume using SSH commands, ensuring no snapshots exist."}
{"func_name": "displaySearchHeading", "func_src_before": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.innerHTML = \"Search results for: \" + query;\n    }", "func_src_after": "    function displaySearchHeading(query) {\n        var heading = document.getElementById(\"searchHeading\");\n        heading.textContent = \"Search results for: \" + query;\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 107, "char_end": 167, "line": "        heading.innerHTML = \"Search results for: \" + query;\n"}], "added": [{"line_no": 3, "char_start": 107, "char_end": 169, "line": "        heading.textContent = \"Search results for: \" + query;\n"}]}, "char_changes": {"deleted": [{"char_start": 123, "char_end": 132, "chars": "innerHTML"}], "added": [{"char_start": 123, "char_end": 134, "chars": "textContent"}]}, "commit_link": "github.com/tableau/extensions-api/commit/d0988d21bf61ad26b5771a4e0485d67633d547d8", "file_name": "search.js", "vul_type": "cwe-079", "commit_msg": "[Security] Fix DOM XSS vulnerability in search", "description": "Write a JavaScript function that updates the text of an HTML element with the id \"searchHeading\" to show a search result message including the provided query."}
{"func_name": "copy_over", "func_src_before": "def copy_over(source, dest):\n    \"\"\"\n    Copies from the source to the destination, removing the destination\n    if it exists and is a directory.\n    \"\"\"\n    if os.path.exists(dest) and os.path.isdir(dest):\n        shutil.rmtree(dest)\n    shutil.copytree(source, dest)\n    # mkdtemp will set the directory permissions to 700\n    # for the webserver to read them, we need 755\n    os.chmod(dest, stat.S_IRWXU | stat.S_IRGRP |\n             stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    shutil.rmtree(source)", "func_src_after": "def copy_over(source, dest):\n    \"\"\"\n    Copies from the source to the destination, removing the destination\n    if it exists and is a directory.\n    \"\"\"\n    if os.path.exists(dest) and os.path.isdir(dest):\n        shutil.rmtree(dest)\n    shutil.copytree(force_bytes(source), force_bytes(dest))\n    # mkdtemp will set the directory permissions to 700\n    # for the webserver to read them, we need 755\n    os.chmod(dest, stat.S_IRWXU | stat.S_IRGRP |\n             stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    shutil.rmtree(source)", "line_changes": {"deleted": [{"line_no": 8, "char_start": 235, "char_end": 269, "line": "    shutil.copytree(source, dest)\n"}], "added": [{"line_no": 8, "char_start": 235, "char_end": 295, "line": "    shutil.copytree(force_bytes(source), force_bytes(dest))\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 255, "char_end": 267, "chars": "force_bytes("}, {"char_start": 273, "char_end": 274, "chars": ")"}, {"char_start": 276, "char_end": 288, "chars": "force_bytes("}, {"char_start": 293, "char_end": 294, "chars": ")"}]}, "commit_link": "github.com/mozilla/addons-server/commit/e8731de25ab5319c82efb9efbe0195ba95e5dc1e", "file_name": "utils.py", "vul_type": "cwe-022", "commit_msg": "Fix unicode error on copying over extracted files from zip. (#3874)\n\nFix unicode error on copying over extracted files from zip.\r\n\r\nRefs #3579\r\n\r\nThe problem here is that under very hard unit-testable/reproducable\r\ncircumstances a zip-file with unicode filenames get's extracted to a\r\ntemporary directory and while calling `copy_over` which calls\r\n`shutil.copytree` which fails on our production systems with a\r\nUnicodeDecodeError.\r\n\r\nReproducing this is something along the lines of\r\n\r\n>>> shutil.copytree(u'/tmp/tmp8WG6A8/', u'/tmp/tmp8WG6A8-2/')\r\n# doesn't error...\r\n>>> shutil.copytree('/tmp/tmp8WG6A8/', u'/tmp/tmp8WG6A8-2/')\r\nUnicodeDecodeError: 'ascii' codec can't decode \tbyte 0xd0 in ...\r\n\r\nChecking all relevant environment configs in our production systems, all\r\nLANG, LC_ALL and related configs are perfectly fine. It's not related to\r\nuwsgi, it's not related to EFS so it's something else I have absolutely\r\nno idea about :-/", "parent_commit": "18d3e83979a8c9616f6d1173f82f422da6f2e1c2", "description": "Write a Python function to replace a destination directory with a source directory, adjusting permissions for webserver access."}
{"func_name": "mem_check_range", "func_src_before": "int mem_check_range(struct rxe_mem *mem, u64 iova, size_t length)\n{\n\tswitch (mem->type) {\n\tcase RXE_MEM_TYPE_DMA:\n\t\treturn 0;\n\n\tcase RXE_MEM_TYPE_MR:\n\tcase RXE_MEM_TYPE_FMR:\n\t\treturn ((iova < mem->iova) ||\n\t\t\t((iova + length) > (mem->iova + mem->length))) ?\n\t\t\t-EFAULT : 0;\n\n\tdefault:\n\t\treturn -EFAULT;\n\t}\n}", "func_src_after": "int mem_check_range(struct rxe_mem *mem, u64 iova, size_t length)\n{\n\tswitch (mem->type) {\n\tcase RXE_MEM_TYPE_DMA:\n\t\treturn 0;\n\n\tcase RXE_MEM_TYPE_MR:\n\tcase RXE_MEM_TYPE_FMR:\n\t\tif (iova < mem->iova ||\n\t\t    length > mem->length ||\n\t\t    iova > mem->iova + mem->length - length)\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EFAULT;\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/647bf3d8a8e5777319da92af672289b2a6c4dc66", "file_name": "drivers/infiniband/sw/rxe/rxe_mr.c", "vul_type": "cwe-190", "description": "Write a C function named `mem_check_range` that checks if a memory range specified by `iova` and `length` is valid within a memory region `mem`, returning 0 for valid or `-EFAULT` for invalid."}
{"func_name": "sloka", "func_src_before": "@app.route('/sloka')\ndef sloka():\n\n    sloka_number = request.args.get('sloka_number')\n\n    sloka_number_parts = sloka_number.split('.')\n\n    sloka_number_previous = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)\n    sloka_number_next = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where sloka_number = '%s' order by sloka_line;\" % sloka_number)\n            mula = cur.fetchall();\n\n            cur.execute(\"select * from pada where sloka_number = '%s' order by id;\" % sloka_number)\n            pada = cur.fetchall();\n\n            varga = \"\"\n            if len(pada) > 0:\n                varga = pada[0][\"varga\"]\n\n            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)\n    finally:\n        con.close()", "func_src_after": "@app.route('/sloka')\ndef sloka():\n\n    sloka_number = request.args.get('sloka_number')\n\n    sloka_number_parts = sloka_number.split('.')\n\n    sloka_number_previous = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)\n    sloka_number_next = \"%s.%s.%d\" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)\n\n    try:\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from mula where sloka_number = ? order by sloka_line;\", [sloka_number])\n            mula = cur.fetchall();\n\n            cur.execute(\"select * from pada where sloka_number = ? order by id;\", [sloka_number])\n            pada = cur.fetchall();\n\n            varga = \"\"\n            if len(pada) > 0:\n                varga = pada[0][\"varga\"]\n\n            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)\n    finally:\n        con.close()", "commit_link": "github.com/aupasana/amara-quiz/commit/6ceb5dc8ec38b4a3f1399e578ab970f7e3354922", "file_name": "docker/app.py", "vul_type": "cwe-089", "description": "In Python, create a Flask route to display a sloka with its previous and next references, fetching data from a SQLite database."}
{"func_name": "makeJudge", "func_src_before": "def makeJudge(judge):\n\tdb.execute(\"UPDATE players SET Judge = 1 WHERE Name = '%s' COLLATE NOCASE\" % (judge)) \n\tdatabase.commit()", "func_src_after": "def makeJudge(judge):\n\tdb.execute(\"UPDATE players SET Judge = 1 WHERE Name = ? COLLATE NOCASE\", judge) \n\tdatabase.commit()", "commit_link": "github.com/iScrE4m/XLeague/commit/59cab6e5fd8bd5e47f2418a7c71cb1d4e3cad0d2", "file_name": "plugins/database.py", "vul_type": "cwe-089", "description": "Write a Python function called `makeJudge` that sets a player's 'Judge' status to 1 in a database by their name, case-insensitively."}
{"func_name": "clean_cache", "func_src_before": "    def clean_cache(self, limit):\n        \"\"\"\n        Method that remove several User objects from cache - the least \n        active users\n        :param limit: number of the users that the method should remove\n        from cache\n        :return: None\n        \"\"\"\n\n        log.info('Figuring out the least active users...')\n        # Select users that the least active recently\n        user_ids = tuple(self.users.keys())\n        query = ('SELECT chat_id '\n                 'FROM photo_queries_table2 '\n                 f'WHERE chat_id in {user_ids} '\n                 'GROUP BY chat_id '\n                 'ORDER BY MAX(time) '\n                 f'LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Can't figure out the least active users...\")\n            return\n\n        if not cursor.rowcount:\n            log.warning(\"There are no users in the db\")\n            return\n\n        # Make list out of tuple of tuples that is returned by MySQL\n        least_active_users = [chat_id[0] for chat_id in cursor.fetchall()]\n        log.info('Removing %d least active users from cache...', limit)\n        num_deleted_entries = 0\n        for entry in least_active_users:\n            log.debug('Deleting %s...', entry)\n            deleted_entry = self.users.pop(entry, None)\n            if deleted_entry:\n                num_deleted_entries += 1\n        log.debug(\"%d users were removed from cache.\", num_deleted_entries)", "func_src_after": "    def clean_cache(self, limit):\n        \"\"\"\n        Method that remove several User objects from cache - the least \n        active users\n        :param limit: number of the users that the method should remove\n        from cache\n        :return: None\n        \"\"\"\n\n        log.info('Figuring out the least active users...')\n        # Select users that the least active recently\n        user_ids = tuple(self.users.keys())\n        query = ('SELECT chat_id '\n                 'FROM photo_queries_table2 '\n                 f'WHERE chat_id in {user_ids} '\n                 'GROUP BY chat_id '\n                 'ORDER BY MAX(time) '\n                 f'LIMIT %s')\n\n        parameters = limit,\n\n        try:\n            cursor = db.execute_query(query, parameters)\n        except DatabaseConnectionError:\n            log.error(\"Can't figure out the least active users...\")\n            return\n\n        if not cursor.rowcount:\n            log.warning(\"There are no users in the db\")\n            return\n\n        # Make list out of tuple of tuples that is returned by MySQL\n        least_active_users = [chat_id[0] for chat_id in cursor.fetchall()]\n        log.info('Removing %d least active users from cache...', limit)\n        num_deleted_entries = 0\n        for entry in least_active_users:\n            log.debug('Deleting %s...', entry)\n            deleted_entry = self.users.pop(entry, None)\n            if deleted_entry:\n                num_deleted_entries += 1\n        log.debug(\"%d users were removed from cache.\", num_deleted_entries)", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "Write a Python function to remove the least active users from a cache, limited by a specified number."}
{"func_name": "_get_iscsi_ip_addrs", "func_src_before": "    def _get_iscsi_ip_addrs(self):\n        generator = self._port_conf_generator('svcinfo lsportip')\n        header = next(generator, None)\n        if not header:\n            return\n\n        for port_data in generator:\n            try:\n                port_node_id = port_data['node_id']\n                port_ipv4 = port_data['IP_address']\n                port_ipv6 = port_data['IP_address_6']\n                state = port_data['state']\n            except KeyError:\n                self._handle_keyerror('lsportip', header)\n\n            if port_node_id in self._storage_nodes and (\n                    state == 'configured' or state == 'online'):\n                node = self._storage_nodes[port_node_id]\n                if len(port_ipv4):\n                    node['ipv4'].append(port_ipv4)\n                if len(port_ipv6):\n                    node['ipv6'].append(port_ipv6)", "func_src_after": "    def _get_iscsi_ip_addrs(self):\n        generator = self._port_conf_generator(['svcinfo', 'lsportip'])\n        header = next(generator, None)\n        if not header:\n            return\n\n        for port_data in generator:\n            try:\n                port_node_id = port_data['node_id']\n                port_ipv4 = port_data['IP_address']\n                port_ipv6 = port_data['IP_address_6']\n                state = port_data['state']\n            except KeyError:\n                self._handle_keyerror('lsportip', header)\n\n            if port_node_id in self._storage_nodes and (\n                    state == 'configured' or state == 'online'):\n                node = self._storage_nodes[port_node_id]\n                if len(port_ipv4):\n                    node['ipv4'].append(port_ipv4)\n                if len(port_ipv6):\n                    node['ipv6'].append(port_ipv6)", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to parse iSCSI port IP addresses from a configuration generator and store them in a storage node dictionary."}
{"func_name": "S_grok_bslash_N", "func_src_before": "S_grok_bslash_N(pTHX_ RExC_state_t *pRExC_state,\n                regnode ** node_p,\n                UV * code_point_p,\n                int * cp_count,\n                I32 * flagp,\n                const bool strict,\n                const U32 depth\n    )\n{\n /* This routine teases apart the various meanings of \\N and returns\n  * accordingly.  The input parameters constrain which meaning(s) is/are valid\n  * in the current context.\n  *\n  * Exactly one of <node_p> and <code_point_p> must be non-NULL.\n  *\n  * If <code_point_p> is not NULL, the context is expecting the result to be a\n  * single code point.  If this \\N instance turns out to a single code point,\n  * the function returns TRUE and sets *code_point_p to that code point.\n  *\n  * If <node_p> is not NULL, the context is expecting the result to be one of\n  * the things representable by a regnode.  If this \\N instance turns out to be\n  * one such, the function generates the regnode, returns TRUE and sets *node_p\n  * to point to that regnode.\n  *\n  * If this instance of \\N isn't legal in any context, this function will\n  * generate a fatal error and not return.\n  *\n  * On input, RExC_parse should point to the first char following the \\N at the\n  * time of the call.  On successful return, RExC_parse will have been updated\n  * to point to just after the sequence identified by this routine.  Also\n  * *flagp has been updated as needed.\n  *\n  * When there is some problem with the current context and this \\N instance,\n  * the function returns FALSE, without advancing RExC_parse, nor setting\n  * *node_p, nor *code_point_p, nor *flagp.\n  *\n  * If <cp_count> is not NULL, the caller wants to know the length (in code\n  * points) that this \\N sequence matches.  This is set even if the function\n  * returns FALSE, as detailed below.\n  *\n  * There are 5 possibilities here, as detailed in the next 5 paragraphs.\n  *\n  * Probably the most common case is for the \\N to specify a single code point.\n  * *cp_count will be set to 1, and *code_point_p will be set to that code\n  * point.\n  *\n  * Another possibility is for the input to be an empty \\N{}, which for\n  * backwards compatibility we accept.  *cp_count will be set to 0. *node_p\n  * will be set to a generated NOTHING node.\n  *\n  * Still another possibility is for the \\N to mean [^\\n]. *cp_count will be\n  * set to 0. *node_p will be set to a generated REG_ANY node.\n  *\n  * The fourth possibility is that \\N resolves to a sequence of more than one\n  * code points.  *cp_count will be set to the number of code points in the\n  * sequence. *node_p * will be set to a generated node returned by this\n  * function calling S_reg().\n  *\n  * The final possibility is that it is premature to be calling this function;\n  * that pass1 needs to be restarted.  This can happen when this changes from\n  * /d to /u rules, or when the pattern needs to be upgraded to UTF-8.  The\n  * latter occurs only when the fourth possibility would otherwise be in\n  * effect, and is because one of those code points requires the pattern to be\n  * recompiled as UTF-8.  The function returns FALSE, and sets the\n  * RESTART_PASS1 and NEED_UTF8 flags in *flagp, as appropriate.  When this\n  * happens, the caller needs to desist from continuing parsing, and return\n  * this information to its caller.  This is not set for when there is only one\n  * code point, as this can be called as part of an ANYOF node, and they can\n  * store above-Latin1 code points without the pattern having to be in UTF-8.\n  *\n  * For non-single-quoted regexes, the tokenizer has resolved character and\n  * sequence names inside \\N{...} into their Unicode values, normalizing the\n  * result into what we should see here: '\\N{U+c1.c2...}', where c1... are the\n  * hex-represented code points in the sequence.  This is done there because\n  * the names can vary based on what charnames pragma is in scope at the time,\n  * so we need a way to take a snapshot of what they resolve to at the time of\n  * the original parse. [perl #56444].\n  *\n  * That parsing is skipped for single-quoted regexes, so we may here get\n  * '\\N{NAME}'.  This is a fatal error.  These names have to be resolved by the\n  * parser.  But if the single-quoted regex is something like '\\N{U+41}', that\n  * is legal and handled here.  The code point is Unicode, and has to be\n  * translated into the native character set for non-ASCII platforms.\n  */\n\n    char * endbrace;    /* points to '}' following the name */\n    char *endchar;\t/* Points to '.' or '}' ending cur char in the input\n                           stream */\n    char* p = RExC_parse; /* Temporary */\n\n    GET_RE_DEBUG_FLAGS_DECL;\n\n    PERL_ARGS_ASSERT_GROK_BSLASH_N;\n\n    GET_RE_DEBUG_FLAGS;\n\n    assert(cBOOL(node_p) ^ cBOOL(code_point_p));  /* Exactly one should be set */\n    assert(! (node_p && cp_count));               /* At most 1 should be set */\n\n    if (cp_count) {     /* Initialize return for the most common case */\n        *cp_count = 1;\n    }\n\n    /* The [^\\n] meaning of \\N ignores spaces and comments under the /x\n     * modifier.  The other meanings do not, so use a temporary until we find\n     * out which we are being called with */\n    skip_to_be_ignored_text(pRExC_state, &p,\n                            FALSE /* Don't force to /x */ );\n\n    /* Disambiguate between \\N meaning a named character versus \\N meaning\n     * [^\\n].  The latter is assumed when the {...} following the \\N is a legal\n     * quantifier, or there is no '{' at all */\n    if (*p != '{' || regcurly(p)) {\n\tRExC_parse = p;\n        if (cp_count) {\n            *cp_count = -1;\n        }\n\n\tif (! node_p) {\n            return FALSE;\n        }\n\n\t*node_p = reg_node(pRExC_state, REG_ANY);\n\t*flagp |= HASWIDTH|SIMPLE;\n\tMARK_NAUGHTY(1);\n        Set_Node_Length(*node_p, 1); /* MJD */\n\treturn TRUE;\n    }\n\n    /* Here, we have decided it should be a named character or sequence */\n\n    /* The test above made sure that the next real character is a '{', but\n     * under the /x modifier, it could be separated by space (or a comment and\n     * \\n) and this is not allowed (for consistency with \\x{...} and the\n     * tokenizer handling of \\N{NAME}). */\n    if (*RExC_parse != '{') {\n\tvFAIL(\"Missing braces on \\\\N{}\");\n    }\n\n    RExC_parse++;\t/* Skip past the '{' */\n\n    endbrace = strchr(RExC_parse, '}');\n    if (! endbrace) { /* no trailing brace */\n        vFAIL2(\"Missing right brace on \\\\%c{}\", 'N');\n    }\n    else if (!(   endbrace == RExC_parse\t/* nothing between the {} */\n               || memBEGINs(RExC_parse,   /* U+ (bad hex is checked below\n                                                   for a  better error msg) */\n                                  (STRLEN) (RExC_end - RExC_parse),\n                                 \"U+\")))\n    {\n\tRExC_parse = endbrace;\t/* position msg's '<--HERE' */\n\tvFAIL(\"\\\\N{NAME} must be resolved by the lexer\");\n    }\n\n    REQUIRE_UNI_RULES(flagp, FALSE); /* Unicode named chars imply Unicode\n                                        semantics */\n\n    if (endbrace == RExC_parse) {   /* empty: \\N{} */\n        if (strict) {\n            RExC_parse++;   /* Position after the \"}\" */\n            vFAIL(\"Zero length \\\\N{}\");\n        }\n        if (cp_count) {\n            *cp_count = 0;\n        }\n        nextchar(pRExC_state);\n\tif (! node_p) {\n            return FALSE;\n        }\n\n        *node_p = reg_node(pRExC_state,NOTHING);\n        return TRUE;\n    }\n\n    RExC_parse += 2;\t/* Skip past the 'U+' */\n\n    /* Because toke.c has generated a special construct for us guaranteed not\n     * to have NULs, we can use a str function */\n    endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n\n    /* Code points are separated by dots.  If none, there is only one code\n     * point, and is terminated by the brace */\n\n    if (endchar >= endbrace) {\n\tSTRLEN length_of_hex;\n\tI32 grok_hex_flags;\n\n        /* Here, exactly one code point.  If that isn't what is wanted, fail */\n        if (! code_point_p) {\n            RExC_parse = p;\n            return FALSE;\n        }\n\n        /* Convert code point from hex */\n\tlength_of_hex = (STRLEN)(endchar - RExC_parse);\n\tgrok_hex_flags = PERL_SCAN_ALLOW_UNDERSCORES\n                       | PERL_SCAN_DISALLOW_PREFIX\n\n                           /* No errors in the first pass (See [perl\n                            * #122671].)  We let the code below find the\n                            * errors when there are multiple chars. */\n                       | ((SIZE_ONLY)\n                          ? PERL_SCAN_SILENT_ILLDIGIT\n                          : 0);\n\n        /* This routine is the one place where both single- and double-quotish\n         * \\N{U+xxxx} are evaluated.  The value is a Unicode code point which\n         * must be converted to native. */\n\t*code_point_p = UNI_TO_NATIVE(grok_hex(RExC_parse,\n                                               &length_of_hex,\n                                               &grok_hex_flags,\n                                               NULL));\n\n\t/* The tokenizer should have guaranteed validity, but it's possible to\n         * bypass it by using single quoting, so check.  Don't do the check\n         * here when there are multiple chars; we do it below anyway. */\n        if (length_of_hex == 0\n            || length_of_hex != (STRLEN)(endchar - RExC_parse) )\n        {\n            RExC_parse += length_of_hex;\t/* Includes all the valid */\n            RExC_parse += (RExC_orig_utf8)\t/* point to after 1st invalid */\n                            ? UTF8SKIP(RExC_parse)\n                            : 1;\n            /* Guard against malformed utf8 */\n            if (RExC_parse >= endchar) {\n                RExC_parse = endchar;\n            }\n            vFAIL(\"Invalid hexadecimal number in \\\\N{U+...}\");\n        }\n\n        RExC_parse = endbrace + 1;\n        return TRUE;\n    }\n    else {  /* Is a multiple character sequence */\n\tSV * substitute_parse;\n\tSTRLEN len;\n\tchar *orig_end = RExC_end;\n\tchar *save_start = RExC_start;\n        I32 flags;\n\n        /* Count the code points, if desired, in the sequence */\n        if (cp_count) {\n            *cp_count = 0;\n            while (RExC_parse < endbrace) {\n                /* Point to the beginning of the next character in the sequence. */\n                RExC_parse = endchar + 1;\n                endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n                (*cp_count)++;\n            }\n        }\n\n        /* Fail if caller doesn't want to handle a multi-code-point sequence.\n         * But don't backup up the pointer if the caller wants to know how many\n         * code points there are (they can then handle things) */\n        if (! node_p) {\n            if (! cp_count) {\n                RExC_parse = p;\n            }\n            return FALSE;\n        }\n\n\t/* What is done here is to convert this to a sub-pattern of the form\n         * \\x{char1}\\x{char2}...  and then call reg recursively to parse it\n         * (enclosing in \"(?: ... )\" ).  That way, it retains its atomicness,\n         * while not having to worry about special handling that some code\n         * points may have. */\n\n\tsubstitute_parse = newSVpvs(\"?:\");\n\n\twhile (RExC_parse < endbrace) {\n\n\t    /* Convert to notation the rest of the code understands */\n\t    sv_catpv(substitute_parse, \"\\\\x{\");\n\t    sv_catpvn(substitute_parse, RExC_parse, endchar - RExC_parse);\n\t    sv_catpv(substitute_parse, \"}\");\n\n\t    /* Point to the beginning of the next character in the sequence. */\n\t    RExC_parse = endchar + 1;\n\t    endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n\n\t}\n        sv_catpv(substitute_parse, \")\");\n\n        len = SvCUR(substitute_parse);\n\n\t/* Don't allow empty number */\n\tif (len < (STRLEN) 8) {\n            RExC_parse = endbrace;\n\t    vFAIL(\"Invalid hexadecimal number in \\\\N{U+...}\");\n\t}\n\n        RExC_parse = RExC_start = RExC_adjusted_start\n                                              = SvPV_nolen(substitute_parse);\n\tRExC_end = RExC_parse + len;\n\n        /* The values are Unicode, and therefore not subject to recoding, but\n         * have to be converted to native on a non-Unicode (meaning non-ASCII)\n         * platform. */\n#ifdef EBCDIC\n        RExC_recode_x_to_native = 1;\n#endif\n\n        *node_p = reg(pRExC_state, 1, &flags, depth+1);\n\n        /* Restore the saved values */\n\tRExC_start = RExC_adjusted_start = save_start;\n\tRExC_parse = endbrace;\n\tRExC_end = orig_end;\n#ifdef EBCDIC\n        RExC_recode_x_to_native = 0;\n#endif\n        SvREFCNT_dec_NN(substitute_parse);\n\n        if (! *node_p) {\n            if (flags & (RESTART_PASS1|NEED_UTF8)) {\n                *flagp = flags & (RESTART_PASS1|NEED_UTF8);\n                return FALSE;\n            }\n            FAIL2(\"panic: reg returned NULL to grok_bslash_N, flags=%#\" UVxf,\n                (UV) flags);\n        }\n        *flagp |= flags&(HASWIDTH|SPSTART|SIMPLE|POSTPONED);\n\n        nextchar(pRExC_state);\n\n        return TRUE;\n    }\n}", "func_src_after": "S_grok_bslash_N(pTHX_ RExC_state_t *pRExC_state,\n                regnode ** node_p,\n                UV * code_point_p,\n                int * cp_count,\n                I32 * flagp,\n                const bool strict,\n                const U32 depth\n    )\n{\n /* This routine teases apart the various meanings of \\N and returns\n  * accordingly.  The input parameters constrain which meaning(s) is/are valid\n  * in the current context.\n  *\n  * Exactly one of <node_p> and <code_point_p> must be non-NULL.\n  *\n  * If <code_point_p> is not NULL, the context is expecting the result to be a\n  * single code point.  If this \\N instance turns out to a single code point,\n  * the function returns TRUE and sets *code_point_p to that code point.\n  *\n  * If <node_p> is not NULL, the context is expecting the result to be one of\n  * the things representable by a regnode.  If this \\N instance turns out to be\n  * one such, the function generates the regnode, returns TRUE and sets *node_p\n  * to point to that regnode.\n  *\n  * If this instance of \\N isn't legal in any context, this function will\n  * generate a fatal error and not return.\n  *\n  * On input, RExC_parse should point to the first char following the \\N at the\n  * time of the call.  On successful return, RExC_parse will have been updated\n  * to point to just after the sequence identified by this routine.  Also\n  * *flagp has been updated as needed.\n  *\n  * When there is some problem with the current context and this \\N instance,\n  * the function returns FALSE, without advancing RExC_parse, nor setting\n  * *node_p, nor *code_point_p, nor *flagp.\n  *\n  * If <cp_count> is not NULL, the caller wants to know the length (in code\n  * points) that this \\N sequence matches.  This is set even if the function\n  * returns FALSE, as detailed below.\n  *\n  * There are 5 possibilities here, as detailed in the next 5 paragraphs.\n  *\n  * Probably the most common case is for the \\N to specify a single code point.\n  * *cp_count will be set to 1, and *code_point_p will be set to that code\n  * point.\n  *\n  * Another possibility is for the input to be an empty \\N{}, which for\n  * backwards compatibility we accept.  *cp_count will be set to 0. *node_p\n  * will be set to a generated NOTHING node.\n  *\n  * Still another possibility is for the \\N to mean [^\\n]. *cp_count will be\n  * set to 0. *node_p will be set to a generated REG_ANY node.\n  *\n  * The fourth possibility is that \\N resolves to a sequence of more than one\n  * code points.  *cp_count will be set to the number of code points in the\n  * sequence. *node_p * will be set to a generated node returned by this\n  * function calling S_reg().\n  *\n  * The final possibility is that it is premature to be calling this function;\n  * that pass1 needs to be restarted.  This can happen when this changes from\n  * /d to /u rules, or when the pattern needs to be upgraded to UTF-8.  The\n  * latter occurs only when the fourth possibility would otherwise be in\n  * effect, and is because one of those code points requires the pattern to be\n  * recompiled as UTF-8.  The function returns FALSE, and sets the\n  * RESTART_PASS1 and NEED_UTF8 flags in *flagp, as appropriate.  When this\n  * happens, the caller needs to desist from continuing parsing, and return\n  * this information to its caller.  This is not set for when there is only one\n  * code point, as this can be called as part of an ANYOF node, and they can\n  * store above-Latin1 code points without the pattern having to be in UTF-8.\n  *\n  * For non-single-quoted regexes, the tokenizer has resolved character and\n  * sequence names inside \\N{...} into their Unicode values, normalizing the\n  * result into what we should see here: '\\N{U+c1.c2...}', where c1... are the\n  * hex-represented code points in the sequence.  This is done there because\n  * the names can vary based on what charnames pragma is in scope at the time,\n  * so we need a way to take a snapshot of what they resolve to at the time of\n  * the original parse. [perl #56444].\n  *\n  * That parsing is skipped for single-quoted regexes, so we may here get\n  * '\\N{NAME}'.  This is a fatal error.  These names have to be resolved by the\n  * parser.  But if the single-quoted regex is something like '\\N{U+41}', that\n  * is legal and handled here.  The code point is Unicode, and has to be\n  * translated into the native character set for non-ASCII platforms.\n  */\n\n    char * endbrace;    /* points to '}' following the name */\n    char *endchar;\t/* Points to '.' or '}' ending cur char in the input\n                           stream */\n    char* p = RExC_parse; /* Temporary */\n\n    GET_RE_DEBUG_FLAGS_DECL;\n\n    PERL_ARGS_ASSERT_GROK_BSLASH_N;\n\n    GET_RE_DEBUG_FLAGS;\n\n    assert(cBOOL(node_p) ^ cBOOL(code_point_p));  /* Exactly one should be set */\n    assert(! (node_p && cp_count));               /* At most 1 should be set */\n\n    if (cp_count) {     /* Initialize return for the most common case */\n        *cp_count = 1;\n    }\n\n    /* The [^\\n] meaning of \\N ignores spaces and comments under the /x\n     * modifier.  The other meanings do not, so use a temporary until we find\n     * out which we are being called with */\n    skip_to_be_ignored_text(pRExC_state, &p,\n                            FALSE /* Don't force to /x */ );\n\n    /* Disambiguate between \\N meaning a named character versus \\N meaning\n     * [^\\n].  The latter is assumed when the {...} following the \\N is a legal\n     * quantifier, or there is no '{' at all */\n    if (*p != '{' || regcurly(p)) {\n\tRExC_parse = p;\n        if (cp_count) {\n            *cp_count = -1;\n        }\n\n\tif (! node_p) {\n            return FALSE;\n        }\n\n\t*node_p = reg_node(pRExC_state, REG_ANY);\n\t*flagp |= HASWIDTH|SIMPLE;\n\tMARK_NAUGHTY(1);\n        Set_Node_Length(*node_p, 1); /* MJD */\n\treturn TRUE;\n    }\n\n    /* Here, we have decided it should be a named character or sequence */\n\n    /* The test above made sure that the next real character is a '{', but\n     * under the /x modifier, it could be separated by space (or a comment and\n     * \\n) and this is not allowed (for consistency with \\x{...} and the\n     * tokenizer handling of \\N{NAME}). */\n    if (*RExC_parse != '{') {\n\tvFAIL(\"Missing braces on \\\\N{}\");\n    }\n\n    RExC_parse++;\t/* Skip past the '{' */\n\n    endbrace = (char *) memchr(RExC_parse, '}', RExC_end - RExC_parse);\n    if (! endbrace) { /* no trailing brace */\n        vFAIL2(\"Missing right brace on \\\\%c{}\", 'N');\n    }\n    else if (!(   endbrace == RExC_parse\t/* nothing between the {} */\n               || memBEGINs(RExC_parse,   /* U+ (bad hex is checked below\n                                                   for a  better error msg) */\n                                  (STRLEN) (RExC_end - RExC_parse),\n                                 \"U+\")))\n    {\n\tRExC_parse = endbrace;\t/* position msg's '<--HERE' */\n\tvFAIL(\"\\\\N{NAME} must be resolved by the lexer\");\n    }\n\n    REQUIRE_UNI_RULES(flagp, FALSE); /* Unicode named chars imply Unicode\n                                        semantics */\n\n    if (endbrace == RExC_parse) {   /* empty: \\N{} */\n        if (strict) {\n            RExC_parse++;   /* Position after the \"}\" */\n            vFAIL(\"Zero length \\\\N{}\");\n        }\n        if (cp_count) {\n            *cp_count = 0;\n        }\n        nextchar(pRExC_state);\n\tif (! node_p) {\n            return FALSE;\n        }\n\n        *node_p = reg_node(pRExC_state,NOTHING);\n        return TRUE;\n    }\n\n    RExC_parse += 2;\t/* Skip past the 'U+' */\n\n    /* Because toke.c has generated a special construct for us guaranteed not\n     * to have NULs, we can use a str function */\n    endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n\n    /* Code points are separated by dots.  If none, there is only one code\n     * point, and is terminated by the brace */\n\n    if (endchar >= endbrace) {\n\tSTRLEN length_of_hex;\n\tI32 grok_hex_flags;\n\n        /* Here, exactly one code point.  If that isn't what is wanted, fail */\n        if (! code_point_p) {\n            RExC_parse = p;\n            return FALSE;\n        }\n\n        /* Convert code point from hex */\n\tlength_of_hex = (STRLEN)(endchar - RExC_parse);\n\tgrok_hex_flags = PERL_SCAN_ALLOW_UNDERSCORES\n                       | PERL_SCAN_DISALLOW_PREFIX\n\n                           /* No errors in the first pass (See [perl\n                            * #122671].)  We let the code below find the\n                            * errors when there are multiple chars. */\n                       | ((SIZE_ONLY)\n                          ? PERL_SCAN_SILENT_ILLDIGIT\n                          : 0);\n\n        /* This routine is the one place where both single- and double-quotish\n         * \\N{U+xxxx} are evaluated.  The value is a Unicode code point which\n         * must be converted to native. */\n\t*code_point_p = UNI_TO_NATIVE(grok_hex(RExC_parse,\n                                               &length_of_hex,\n                                               &grok_hex_flags,\n                                               NULL));\n\n\t/* The tokenizer should have guaranteed validity, but it's possible to\n         * bypass it by using single quoting, so check.  Don't do the check\n         * here when there are multiple chars; we do it below anyway. */\n        if (length_of_hex == 0\n            || length_of_hex != (STRLEN)(endchar - RExC_parse) )\n        {\n            RExC_parse += length_of_hex;\t/* Includes all the valid */\n            RExC_parse += (RExC_orig_utf8)\t/* point to after 1st invalid */\n                            ? UTF8SKIP(RExC_parse)\n                            : 1;\n            /* Guard against malformed utf8 */\n            if (RExC_parse >= endchar) {\n                RExC_parse = endchar;\n            }\n            vFAIL(\"Invalid hexadecimal number in \\\\N{U+...}\");\n        }\n\n        RExC_parse = endbrace + 1;\n        return TRUE;\n    }\n    else {  /* Is a multiple character sequence */\n\tSV * substitute_parse;\n\tSTRLEN len;\n\tchar *orig_end = RExC_end;\n\tchar *save_start = RExC_start;\n        I32 flags;\n\n        /* Count the code points, if desired, in the sequence */\n        if (cp_count) {\n            *cp_count = 0;\n            while (RExC_parse < endbrace) {\n                /* Point to the beginning of the next character in the sequence. */\n                RExC_parse = endchar + 1;\n                endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n                (*cp_count)++;\n            }\n        }\n\n        /* Fail if caller doesn't want to handle a multi-code-point sequence.\n         * But don't backup up the pointer if the caller wants to know how many\n         * code points there are (they can then handle things) */\n        if (! node_p) {\n            if (! cp_count) {\n                RExC_parse = p;\n            }\n            return FALSE;\n        }\n\n\t/* What is done here is to convert this to a sub-pattern of the form\n         * \\x{char1}\\x{char2}...  and then call reg recursively to parse it\n         * (enclosing in \"(?: ... )\" ).  That way, it retains its atomicness,\n         * while not having to worry about special handling that some code\n         * points may have. */\n\n\tsubstitute_parse = newSVpvs(\"?:\");\n\n\twhile (RExC_parse < endbrace) {\n\n\t    /* Convert to notation the rest of the code understands */\n\t    sv_catpv(substitute_parse, \"\\\\x{\");\n\t    sv_catpvn(substitute_parse, RExC_parse, endchar - RExC_parse);\n\t    sv_catpv(substitute_parse, \"}\");\n\n\t    /* Point to the beginning of the next character in the sequence. */\n\t    RExC_parse = endchar + 1;\n\t    endchar = RExC_parse + strcspn(RExC_parse, \".}\");\n\n\t}\n        sv_catpv(substitute_parse, \")\");\n\n        len = SvCUR(substitute_parse);\n\n\t/* Don't allow empty number */\n\tif (len < (STRLEN) 8) {\n            RExC_parse = endbrace;\n\t    vFAIL(\"Invalid hexadecimal number in \\\\N{U+...}\");\n\t}\n\n        RExC_parse = RExC_start = RExC_adjusted_start\n                                              = SvPV_nolen(substitute_parse);\n\tRExC_end = RExC_parse + len;\n\n        /* The values are Unicode, and therefore not subject to recoding, but\n         * have to be converted to native on a non-Unicode (meaning non-ASCII)\n         * platform. */\n#ifdef EBCDIC\n        RExC_recode_x_to_native = 1;\n#endif\n\n        *node_p = reg(pRExC_state, 1, &flags, depth+1);\n\n        /* Restore the saved values */\n\tRExC_start = RExC_adjusted_start = save_start;\n\tRExC_parse = endbrace;\n\tRExC_end = orig_end;\n#ifdef EBCDIC\n        RExC_recode_x_to_native = 0;\n#endif\n        SvREFCNT_dec_NN(substitute_parse);\n\n        if (! *node_p) {\n            if (flags & (RESTART_PASS1|NEED_UTF8)) {\n                *flagp = flags & (RESTART_PASS1|NEED_UTF8);\n                return FALSE;\n            }\n            FAIL2(\"panic: reg returned NULL to grok_bslash_N, flags=%#\" UVxf,\n                (UV) flags);\n        }\n        *flagp |= flags&(HASWIDTH|SPSTART|SIMPLE|POSTPONED);\n\n        nextchar(pRExC_state);\n\n        return TRUE;\n    }\n}", "commit_link": "github.com/Perl/perl5/commit/43b2f4ef399e2fd7240b4eeb0658686ad95f8e62", "file_name": "regcomp.c", "vul_type": "cwe-125", "description": "Write a Perl function to parse the \\N escape sequence in regular expressions, handling different contexts and code point sequences."}
{"func_name": "stats_for_realm", "func_src_before": "@require_server_admin\n@has_request_variables\ndef stats_for_realm(request: HttpRequest, realm_str: str) -> HttpResponse:\n    try:\n        realm = get_realm(realm_str)\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n\n    return render_stats(\n        request,\n        f\"/realm/{realm_str}\",\n        realm.name or realm.string_id,\n        analytics_ready=is_analytics_ready(realm),\n    )", "func_src_after": "@require_server_admin\n@has_request_variables\ndef stats_for_realm(request: HttpRequest, realm_str: str) -> HttpResponse:\n    try:\n        realm = get_realm(realm_str)\n    except Realm.DoesNotExist:\n        return HttpResponseNotFound()\n\n    return render_stats(\n        request,\n        f\"/realm/{realm_str}\",\n        realm.name or realm.string_id,\n        analytics_ready=is_analytics_ready(realm),\n    )", "line_changes": {"deleted": [{"line_no": 7, "char_start": 197, "char_end": 270, "line": "        return HttpResponseNotFound(f\"Realm {realm_str} does not exist\")\n"}], "added": [{"line_no": 7, "char_start": 197, "char_end": 235, "line": "        return HttpResponseNotFound()\n"}]}, "char_changes": {"deleted": [{"char_start": 233, "char_end": 268, "chars": "f\"Realm {realm_str} does not exist\""}], "added": []}, "commit_link": "github.com/rht/zulip/commit/0da1bd43e95f78bee0bf3f78f1bcb594e7db66fd", "file_name": "views.py", "vul_type": "cwe-079", "commit_msg": "analytics: Remove buggy HttpResponseNotFound text.\n\nHad this been in normal route, this would have been an XSS bug, as we\nwere passing what the developer clearly believed to be plain text into\nan HTML 404 page.\n\nThe affected routes have @require_server_admin, a permission that we\ndo not expect any self-hosted users to have ever enabled (as it is\nundocumented and doing so is only possible manually via a `manage.py\nshell`, and we believe to only be useful for running a SaaS service\nlike zulip.com).  So the security impact is limited to a handful of\nstaff of zulip.com and this isn't a candidate for a CVE.\n\nThanks to GitHub's CodeQL for finding this.", "description": "Create a Python function that checks if a realm exists and returns its statistics page or a not found response."}
{"func_name": "canonicalize", "func_src_before": "    def canonicalize(self):\n        \"\"\"::\n\n            path = path.canonicalize()\n\n        Canonicalize path. ::\n\n            # \"/foo/baz\"\n            Pyjo.Path.new('/foo/./bar/../baz').canonicalize()\n\n            # \"/../baz\"\n            Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()\n        \"\"\"\n        parts = self.parts\n        i = 0\n        while i < len(parts):\n            if parts[i] == '.' or parts[i] == '':\n                parts.pop(i)\n            elif i < 1 or parts[i] != '..' or parts[i - 1] == '..':\n                i += 1\n            else:\n                i -= 1\n                parts.pop(i)\n                parts.pop(i)\n\n        if not parts:\n            self.trailing_slash = False\n\n        return self", "func_src_after": "    def canonicalize(self):\n        \"\"\"::\n\n            path = path.canonicalize()\n\n        Canonicalize path by resolving ``.`` and ``..``, in addition ``...`` will be\n        treated as ``.`` to protect from path traversal attacks.\n\n            # \"/foo/baz\"\n            Pyjo.Path.new('/foo/./bar/../baz').canonicalize()\n\n            # \"/../baz\"\n            Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()\n\n            # \"/foo/bar\"\n            Pyjo.Path.new('/foo/.../bar').canonicalize()\n        \"\"\"\n        parts = self.parts\n        i = 0\n        while i < len(parts):\n            if parts[i] == '' or parts[i] == '.' or parts[i] == '...':\n                parts.pop(i)\n            elif i < 1 or parts[i] != '..' or parts[i - 1] == '..':\n                i += 1\n            else:\n                i -= 1\n                parts.pop(i)\n                parts.pop(i)\n\n        if not parts:\n            self.trailing_slash = False\n\n        return self", "commit_link": "github.com/dex4er/Pyjoyment/commit/e4b115bc80c41615b2133091af3a74ee5d995c2e", "file_name": "Pyjo/Path.py", "vul_type": "cwe-022", "description": "Write a Python function named `canonicalize` that simplifies file paths by resolving \".\", \"..\", and optionally \"...\" in a path object."}
{"func_name": "ubpf_load_elf", "func_src_before": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "func_src_after": "ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)\n{\n    struct bounds b = { .base=elf, .size=elf_size };\n    void *text_copy = NULL;\n    int i;\n\n    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));\n    if (!ehdr) {\n        *errmsg = ubpf_error(\"not enough data for ELF header\");\n        goto error;\n    }\n\n    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {\n        *errmsg = ubpf_error(\"wrong magic\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {\n        *errmsg = ubpf_error(\"wrong class\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {\n        *errmsg = ubpf_error(\"wrong byte order\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_VERSION] != 1) {\n        *errmsg = ubpf_error(\"wrong version\");\n        goto error;\n    }\n\n    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {\n        *errmsg = ubpf_error(\"wrong OS ABI\");\n        goto error;\n    }\n\n    if (ehdr->e_type != ET_REL) {\n        *errmsg = ubpf_error(\"wrong type, expected relocatable\");\n        goto error;\n    }\n\n    if (ehdr->e_machine != EM_NONE && ehdr->e_machine != EM_BPF) {\n        *errmsg = ubpf_error(\"wrong machine, expected none or BPF, got %d\",\n                             ehdr->e_machine);\n        goto error;\n    }\n\n    if (ehdr->e_shnum > MAX_SECTIONS) {\n        *errmsg = ubpf_error(\"too many sections\");\n        goto error;\n    }\n\n    /* Parse section headers into an array */\n    struct section sections[MAX_SECTIONS];\n    uint64_t shoff = ehdr->e_shoff;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n        shoff += ehdr->e_shentsize;\n        if (!shdr) {\n            *errmsg = ubpf_error(\"bad section header offset or size\");\n            goto error;\n        }\n\n        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);\n        if (!data) {\n            *errmsg = ubpf_error(\"bad section offset or size\");\n            goto error;\n        }\n\n        sections[i].shdr = shdr;\n        sections[i].data = data;\n        sections[i].size = shdr->sh_size;\n    }\n\n    /* Find first text section */\n    int text_shndx = 0;\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        const Elf64_Shdr *shdr = sections[i].shdr;\n        if (shdr->sh_type == SHT_PROGBITS &&\n                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {\n            text_shndx = i;\n            break;\n        }\n    }\n\n    if (!text_shndx) {\n        *errmsg = ubpf_error(\"text section not found\");\n        goto error;\n    }\n\n    struct section *text = &sections[text_shndx];\n\n    /* May need to modify text for relocations, so make a copy */\n    text_copy = malloc(text->size);\n    if (!text_copy) {\n        *errmsg = ubpf_error(\"failed to allocate memory\");\n        goto error;\n    }\n    memcpy(text_copy, text->data, text->size);\n\n    /* Process each relocation section */\n    for (i = 0; i < ehdr->e_shnum; i++) {\n        struct section *rel = &sections[i];\n        if (rel->shdr->sh_type != SHT_REL) {\n            continue;\n        } else if (rel->shdr->sh_info != text_shndx) {\n            continue;\n        }\n\n        const Elf64_Rel *rs = rel->data;\n\n        if (rel->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad symbol table section index\");\n            goto error;\n        }\n\n        struct section *symtab = &sections[rel->shdr->sh_link];\n        const Elf64_Sym *syms = symtab->data;\n        uint32_t num_syms = symtab->size/sizeof(syms[0]);\n\n        if (symtab->shdr->sh_link >= ehdr->e_shnum) {\n            *errmsg = ubpf_error(\"bad string table section index\");\n            goto error;\n        }\n\n        struct section *strtab = &sections[symtab->shdr->sh_link];\n        const char *strings = strtab->data;\n\n        int j;\n        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {\n            const Elf64_Rel *r = &rs[j];\n\n            if (ELF64_R_TYPE(r->r_info) != 2) {\n                *errmsg = ubpf_error(\"bad relocation type %u\", ELF64_R_TYPE(r->r_info));\n                goto error;\n            }\n\n            uint32_t sym_idx = ELF64_R_SYM(r->r_info);\n            if (sym_idx >= num_syms) {\n                *errmsg = ubpf_error(\"bad symbol index\");\n                goto error;\n            }\n\n            const Elf64_Sym *sym = &syms[sym_idx];\n\n            if (sym->st_name >= strtab->size) {\n                *errmsg = ubpf_error(\"bad symbol name\");\n                goto error;\n            }\n\n            const char *sym_name = strings + sym->st_name;\n\n            if (r->r_offset + 8 > text->size) {\n                *errmsg = ubpf_error(\"bad relocation offset\");\n                goto error;\n            }\n\n            unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);\n            if (imm == -1) {\n                *errmsg = ubpf_error(\"function '%s' not found\", sym_name);\n                goto error;\n            }\n\n            *(uint32_t *)(text_copy + r->r_offset + 4) = imm;\n        }\n    }\n\n    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);\n    free(text_copy);\n    return rv;\n\nerror:\n    free(text_copy);\n    return -1;\n}", "line_changes": {"deleted": [{"line_no": 57, "char_start": 1554, "char_end": 1657, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));\n"}], "added": [{"line_no": 56, "char_start": 1512, "char_end": 1548, "line": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"line_no": 58, "char_start": 1590, "char_end": 1663, "line": "        const Elf64_Shdr *shdr = bounds_check(&b, shoff, sizeof(*shdr));\n"}, {"line_no": 59, "char_start": 1663, "char_end": 1699, "line": "        shoff += ehdr->e_shentsize;\n"}]}, "char_changes": {"deleted": [{"char_start": 1604, "char_end": 1612, "chars": "ehdr->e_"}, {"char_start": 1620, "char_end": 1622, "chars": "i*"}, {"char_start": 1639, "char_end": 1655, "chars": ", sizeof(*shdr))"}], "added": [{"char_start": 1512, "char_end": 1548, "chars": "    uint64_t shoff = ehdr->e_shoff;\n"}, {"char_start": 1640, "char_end": 1671, "chars": "shoff, sizeof(*shdr));\n        "}, {"char_start": 1678, "char_end": 1679, "chars": "="}]}, "commit_link": "github.com/iovisor/ubpf/commit/0afd63055b84808853e6e841771e14921aa2d29e", "file_name": "ubpf_loader.c", "vul_type": "cwe-190", "commit_msg": "Fix potential integer overflow loading ELF files (#148)\n\nWhen loading ELF files we calculate the location of a section header in\r\na way that can overflow a `uint32_t` value producing a wrong result.\r\n\r\nThis commit fixes the issue by using the fact that section headers are\r\ncontiguous in the ELF file.\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>\r\n\r\nSigned-off-by: Matthew Gretton-Dann <matthew.gretton-dann@arm.com>", "parent_commit": "92f58039c1a57e25875a265ed91db6ca213f71f0", "description": "Write a C function named `ubpf_load_elf` that loads an ELF file into a uBPF virtual machine and handles errors."}
{"func_name": "__get_data_block", "func_src_before": "static int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}", "func_src_after": "static int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = (u64)map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/b86e33075ed1909d8002745b56ecf73b833db143", "file_name": "fs/f2fs/data.c", "vul_type": "cwe-190", "description": "Write a C function named `__get_data_block` that maps blocks for a given inode in a filesystem, handling buffer head and block mapping with error checking."}
{"func_name": "writeToDb", "func_src_before": "    def writeToDb(self, url):\n        try:\n            self.cursor.execute(\"INSERT INTO queue (url, visited) VALUES ('{}', '0');\".format(url))\n            self.db.commit()\n        except Exception as e:\n            print(e)", "func_src_after": "    def writeToDb(self, url):\n        try:\n            self.cursor.execute(\"INSERT INTO queue (url, visited) VALUES (?, '0');\", url)\n            self.db.commit()\n        except Exception as e:\n            print(e)", "commit_link": "github.com/jappe999/WebScraper/commit/46a4e0843aa44d903293637afad53dfcbc37b480", "file_name": "beta/database.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a URL into a database table with a visited flag set to '0', handling exceptions."}
{"func_name": "HPHP::exif_process_APP12", "func_src_before": "static void exif_process_APP12(image_info_type *ImageInfo,\n                               char *buffer, size_t length) {\n  size_t l1, l2=0;\n  if ((l1 = php_strnlen(buffer+2, length-2)) > 0) {\n    exif_iif_add_tag(ImageInfo, SECTION_APP12, \"Company\",\n                     TAG_NONE, TAG_FMT_STRING, l1, buffer+2);\n    if (length > 2+l1+1) {\n      l2 = php_strnlen(buffer+2+l1+1, length-2-l1+1);\n      exif_iif_add_tag(ImageInfo, SECTION_APP12, \"Info\",\n                       TAG_NONE, TAG_FMT_STRING, l2, buffer+2+l1+1);\n    }\n  }\n}", "func_src_after": "static void exif_process_APP12(image_info_type *ImageInfo,\n                               char *buffer, size_t length) {\n  size_t l1, l2=0;\n  if ((l1 = php_strnlen(buffer+2, length-2)) > 0) {\n    exif_iif_add_tag(ImageInfo, SECTION_APP12, \"Company\",\n                     TAG_NONE, TAG_FMT_STRING, l1, buffer+2);\n    if (length > 2+l1+1) {\n      l2 = php_strnlen(buffer+2+l1+1, length-2-l1-1);\n      exif_iif_add_tag(ImageInfo, SECTION_APP12, \"Info\",\n                       TAG_NONE, TAG_FMT_STRING, l2, buffer+2+l1+1);\n    }\n  }\n}", "commit_link": "github.com/facebook/hhvm/commit/f1cd34e63c2a0d9702be3d41462db7bfd0ae7da3", "file_name": "hphp/runtime/ext/gd/ext_gd.cpp", "vul_type": "cwe-125", "description": "Write a C function named `exif_process_APP12` that processes APP12 EXIF tags from a buffer and adds them to an image information structure."}
{"func_name": "is_cgi", "func_src_before": "    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script,\n        and return a boolean.\n\n        This function sets self.cgi_info to a tuple (dir, rest)\n        when it returns True, where dir is the directory part before\n        the CGI script name.  Note that rest begins with a\n        slash if it is not empty.\n\n        The default implementation tests whether the path\n        begins with one of the strings in the list\n        self.cgi_directories (and the next character is a '/'\n        or the end of the string).\n        \"\"\"\n\n        path = self.path\n\n        for x in self.cgi_directories:\n            i = len(x)\n            if path[:i] == x and (not path[i:] or path[i] == '/'):\n                self.cgi_info = path[:i], path[i+1:]\n                return True\n        return False", "func_src_after": "    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script.\n\n        Returns True and updates the cgi_info attribute to the tuple\n        (dir, rest) if self.path requires running a CGI script.\n        Returns False otherwise.\n\n        The default implementation tests whether the normalized url\n        path begins with one of the strings in self.cgi_directories\n        (and the next character is a '/' or the end of the string).\n        \"\"\"\n        splitpath = _url_collapse_path_split(self.path)\n        if splitpath[0] in self.cgi_directories:\n            self.cgi_info = splitpath\n            return True\n        return False", "commit_link": "github.com/Ricky-Wilson/Python/commit/c5abced949e6a4b001d1dee321593e74ecadecfe", "file_name": "Lib/CGIHTTPServer.py", "vul_type": "cwe-022", "description": "Create a Python function that checks if a given path is a CGI script and updates an attribute with the script's directory information."}
{"func_name": "new_goal", "func_src_before": "def new_goal():\n    \"\"\"\n    new goal\n    \"\"\"\n\n    goals_dir_check()\n\n    click.echo(chalk.blue('Input a single-word name of the goal:'))\n    goal_name = input().strip()\n\n    if goal_name_exists(goal_name):\n        click.echo(chalk.red(\n            'A goal with this name already exists. Please type \"yoda goals view\" to see a list of existing goals'))\n    else:\n        click.echo(chalk.blue('Input description of the goal:'))\n        text = input().strip()\n\n        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))\n        deadline = input().strip()\n\n        if os.path.isfile(GOALS_CONFIG_FILE_PATH):\n            setup_data = dict(\n                name=goal_name,\n                text=text,\n                deadline=deadline,\n                status=0\n            )\n            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)\n        else:\n            setup_data = dict(\n                entries=[\n                    dict(\n                        name=goal_name,\n                        text=text,\n                        deadline=deadline,\n                        status=0\n                    )\n                ]\n            )\n            input_data(setup_data, GOALS_CONFIG_FILE_PATH)\n\n        input_data(dict(entries=[]), get_goal_file_path(goal_name))", "func_src_after": "def new_goal():\n    \"\"\"\n    new goal\n    \"\"\"\n\n    goals_dir_check()\n\n    goal_name_not_ok = True\n\n    click.echo(chalk.blue('Input a single-word name of the goal:'))\n    while goal_name_not_ok:\n        goal_name = input().strip()\n        if goal_name.isalnum():\n            goal_name_not_ok = False\n        else:\n            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))\n\n    if goal_name_exists(goal_name):\n        click.echo(chalk.red(\n            'A goal with this name already exists. Please type \"yoda goals view\" to see a list of existing goals'))\n    else:\n        click.echo(chalk.blue('Input description of the goal:'))\n        text = input().strip()\n\n        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))\n        incorrect_date_format = True\n        while incorrect_date_format:\n            deadline = input().strip()\n            try:\n                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')\n                if date_str != deadline:\n                    raise ValueError\n                incorrect_date_format = False\n            except ValueError:\n                click.echo(chalk.red(\"Incorrect data format, should be YYYY-MM-DD. Please repeat:\"))\n\n        if os.path.isfile(GOALS_CONFIG_FILE_PATH):\n            setup_data = dict(\n                name=goal_name,\n                text=text,\n                deadline=deadline,\n                status=0\n            )\n            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)\n        else:\n            setup_data = dict(\n                entries=[\n                    dict(\n                        name=goal_name,\n                        text=text,\n                        deadline=deadline,\n                        status=0\n                    )\n                ]\n            )\n            input_data(setup_data, GOALS_CONFIG_FILE_PATH)\n\n        input_data(dict(entries=[]), get_goal_file_path(goal_name))", "commit_link": "github.com/yoda-pa/yoda/commit/263946316041601de75638ee303a892f2652cf40", "file_name": "modules/goals.py", "vul_type": "cwe-022", "description": "Write a Python function to create a new goal with validation for name uniqueness and proper date format."}
{"func_name": "rwpng_read_image24_libpng", "func_src_before": "static pngquant_error rwpng_read_image24_libpng(FILE *infile, png24_image *mainprog_ptr, int verbose)\n{\n    png_structp  png_ptr = NULL;\n    png_infop    info_ptr = NULL;\n    png_size_t   rowbytes;\n    int          color_type, bit_depth;\n\n    png_ptr = png_create_read_struct(PNG_LIBPNG_VER_STRING, mainprog_ptr,\n      rwpng_error_handler, verbose ? rwpng_warning_stderr_handler : rwpng_warning_silent_handler);\n    if (!png_ptr) {\n        return PNG_OUT_OF_MEMORY_ERROR;   /* out of memory */\n    }\n\n    info_ptr = png_create_info_struct(png_ptr);\n    if (!info_ptr) {\n        png_destroy_read_struct(&png_ptr, NULL, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;   /* out of memory */\n    }\n\n    /* setjmp() must be called in every function that calls a non-trivial\n     * libpng function */\n\n    if (setjmp(mainprog_ptr->jmpbuf)) {\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return LIBPNG_FATAL_ERROR;   /* fatal libpng error (via longjmp()) */\n    }\n\n#if defined(PNG_SKIP_sRGB_CHECK_PROFILE) && defined(PNG_SET_OPTION_SUPPORTED)\n    png_set_option(png_ptr, PNG_SKIP_sRGB_CHECK_PROFILE, PNG_OPTION_ON);\n#endif\n\n#if PNG_LIBPNG_VER >= 10500 && defined(PNG_UNKNOWN_CHUNKS_SUPPORTED)\n    /* copy standard chunks too */\n    png_set_keep_unknown_chunks(png_ptr, PNG_HANDLE_CHUNK_IF_SAFE, (png_const_bytep)\"pHYs\\0iTXt\\0tEXt\\0zTXt\", 4);\n#endif\n    png_set_read_user_chunk_fn(png_ptr, &mainprog_ptr->chunks, read_chunk_callback);\n\n    struct rwpng_read_data read_data = {infile, 0};\n    png_set_read_fn(png_ptr, &read_data, user_read_data);\n\n    png_read_info(png_ptr, info_ptr);  /* read all PNG info up to image data */\n\n    /* alternatively, could make separate calls to png_get_image_width(),\n     * etc., but want bit_depth and color_type for later [don't care about\n     * compression_type and filter_type => NULLs] */\n\n    png_get_IHDR(png_ptr, info_ptr, &mainprog_ptr->width, &mainprog_ptr->height,\n                 &bit_depth, &color_type, NULL, NULL, NULL);\n\n    // For overflow safety reject images that won't fit in 32-bit\n    if (mainprog_ptr->width > INT_MAX/mainprog_ptr->height) {\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;  /* not quite true, but whatever */\n    }\n\n    /* expand palette images to RGB, low-bit-depth grayscale images to 8 bits,\n     * transparency chunks to full alpha channel; strip 16-bit-per-sample\n     * images to 8 bits per sample; and convert grayscale to RGB[A] */\n\n    /* GRR TO DO:  preserve all safe-to-copy ancillary PNG chunks */\n\n    if (!(color_type & PNG_COLOR_MASK_ALPHA)) {\n#ifdef PNG_READ_FILLER_SUPPORTED\n        png_set_expand(png_ptr);\n        png_set_filler(png_ptr, 65535L, PNG_FILLER_AFTER);\n#else\n        fprintf(stderr, \"pngquant readpng:  image is neither RGBA nor GA\\n\");\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        mainprog_ptr->retval = WRONG_INPUT_COLOR_TYPE;\n        return mainprog_ptr->retval;\n#endif\n    }\n\n    if (bit_depth == 16) {\n        png_set_strip_16(png_ptr);\n    }\n\n    if (!(color_type & PNG_COLOR_MASK_COLOR)) {\n        png_set_gray_to_rgb(png_ptr);\n    }\n\n    /* get source gamma for gamma correction, or use sRGB default */\n    double gamma = 0.45455;\n    if (png_get_valid(png_ptr, info_ptr, PNG_INFO_sRGB)) {\n        mainprog_ptr->input_color = RWPNG_SRGB;\n        mainprog_ptr->output_color = RWPNG_SRGB;\n    } else {\n        png_get_gAMA(png_ptr, info_ptr, &gamma);\n        if (gamma > 0 && gamma <= 1.0) {\n            mainprog_ptr->input_color = RWPNG_GAMA_ONLY;\n            mainprog_ptr->output_color = RWPNG_GAMA_ONLY;\n        } else {\n            fprintf(stderr, \"pngquant readpng:  ignored out-of-range gamma %f\\n\", gamma);\n            mainprog_ptr->input_color = RWPNG_NONE;\n            mainprog_ptr->output_color = RWPNG_NONE;\n            gamma = 0.45455;\n        }\n    }\n    mainprog_ptr->gamma = gamma;\n\n    png_set_interlace_handling(png_ptr);\n\n    /* all transformations have been registered; now update info_ptr data,\n     * get rowbytes and channels, and allocate image memory */\n\n    png_read_update_info(png_ptr, info_ptr);\n\n    rowbytes = png_get_rowbytes(png_ptr, info_ptr);\n\n    if ((mainprog_ptr->rgba_data = malloc(rowbytes * mainprog_ptr->height)) == NULL) {\n        fprintf(stderr, \"pngquant readpng:  unable to allocate image data\\n\");\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;\n    }\n\n    png_bytepp row_pointers = rwpng_create_row_pointers(info_ptr, png_ptr, mainprog_ptr->rgba_data, mainprog_ptr->height, 0);\n\n    /* now we can go ahead and just read the whole image */\n\n    png_read_image(png_ptr, row_pointers);\n\n    /* and we're done!  (png_read_end() can be omitted if no processing of\n     * post-IDAT text/time/etc. is desired) */\n\n    png_read_end(png_ptr, NULL);\n\n#if USE_LCMS\n#if PNG_LIBPNG_VER < 10500\n    png_charp ProfileData;\n#else\n    png_bytep ProfileData;\n#endif\n    png_uint_32 ProfileLen;\n\n    cmsHPROFILE hInProfile = NULL;\n\n    /* color_type is read from the image before conversion to RGBA */\n    int COLOR_PNG = color_type & PNG_COLOR_MASK_COLOR;\n\n    /* embedded ICC profile */\n    if (png_get_iCCP(png_ptr, info_ptr, &(png_charp){0}, &(int){0}, &ProfileData, &ProfileLen)) {\n\n        hInProfile = cmsOpenProfileFromMem(ProfileData, ProfileLen);\n        cmsColorSpaceSignature colorspace = cmsGetColorSpace(hInProfile);\n\n        /* only RGB (and GRAY) valid for PNGs */\n        if (colorspace == cmsSigRgbData && COLOR_PNG) {\n            mainprog_ptr->input_color = RWPNG_ICCP;\n            mainprog_ptr->output_color = RWPNG_SRGB;\n        } else {\n            if (colorspace == cmsSigGrayData && !COLOR_PNG) {\n                mainprog_ptr->input_color = RWPNG_ICCP_WARN_GRAY;\n                mainprog_ptr->output_color = RWPNG_SRGB;\n            }\n            cmsCloseProfile(hInProfile);\n            hInProfile = NULL;\n        }\n    }\n\n    /* build RGB profile from cHRM and gAMA */\n    if (hInProfile == NULL && COLOR_PNG &&\n        !png_get_valid(png_ptr, info_ptr, PNG_INFO_sRGB) &&\n        png_get_valid(png_ptr, info_ptr, PNG_INFO_gAMA) &&\n        png_get_valid(png_ptr, info_ptr, PNG_INFO_cHRM)) {\n\n        cmsCIExyY WhitePoint;\n        cmsCIExyYTRIPLE Primaries;\n\n        png_get_cHRM(png_ptr, info_ptr, &WhitePoint.x, &WhitePoint.y,\n                     &Primaries.Red.x, &Primaries.Red.y,\n                     &Primaries.Green.x, &Primaries.Green.y,\n                     &Primaries.Blue.x, &Primaries.Blue.y);\n\n        WhitePoint.Y = Primaries.Red.Y = Primaries.Green.Y = Primaries.Blue.Y = 1.0;\n\n        cmsToneCurve *GammaTable[3];\n        GammaTable[0] = GammaTable[1] = GammaTable[2] = cmsBuildGamma(NULL, 1/gamma);\n\n        hInProfile = cmsCreateRGBProfile(&WhitePoint, &Primaries, GammaTable);\n\n        cmsFreeToneCurve(GammaTable[0]);\n\n        mainprog_ptr->input_color = RWPNG_GAMA_CHRM;\n        mainprog_ptr->output_color = RWPNG_SRGB;\n    }\n\n    /* transform image to sRGB colorspace */\n    if (hInProfile != NULL) {\n\n        cmsHPROFILE hOutProfile = cmsCreate_sRGBProfile();\n        cmsHTRANSFORM hTransform = cmsCreateTransform(hInProfile, TYPE_RGBA_8,\n                                                      hOutProfile, TYPE_RGBA_8,\n                                                      INTENT_PERCEPTUAL,\n                                                      omp_get_max_threads() > 1 ? cmsFLAGS_NOCACHE : 0);\n\n        #pragma omp parallel for \\\n            if (mainprog_ptr->height*mainprog_ptr->width > 8000) \\\n            schedule(static)\n        for (unsigned int i = 0; i < mainprog_ptr->height; i++) {\n            /* It is safe to use the same block for input and output,\n               when both are of the same TYPE. */\n            cmsDoTransform(hTransform, row_pointers[i],\n                                       row_pointers[i],\n                                       mainprog_ptr->width);\n        }\n\n        cmsDeleteTransform(hTransform);\n        cmsCloseProfile(hOutProfile);\n        cmsCloseProfile(hInProfile);\n\n        mainprog_ptr->gamma = 0.45455;\n    }\n#endif\n\n    png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n\n    mainprog_ptr->file_size = read_data.bytes_read;\n    mainprog_ptr->row_pointers = (unsigned char **)row_pointers;\n\n    return SUCCESS;\n}", "func_src_after": "static pngquant_error rwpng_read_image24_libpng(FILE *infile, png24_image *mainprog_ptr, int verbose)\n{\n    png_structp  png_ptr = NULL;\n    png_infop    info_ptr = NULL;\n    png_size_t   rowbytes;\n    int          color_type, bit_depth;\n\n    png_ptr = png_create_read_struct(PNG_LIBPNG_VER_STRING, mainprog_ptr,\n      rwpng_error_handler, verbose ? rwpng_warning_stderr_handler : rwpng_warning_silent_handler);\n    if (!png_ptr) {\n        return PNG_OUT_OF_MEMORY_ERROR;   /* out of memory */\n    }\n\n    info_ptr = png_create_info_struct(png_ptr);\n    if (!info_ptr) {\n        png_destroy_read_struct(&png_ptr, NULL, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;   /* out of memory */\n    }\n\n    /* setjmp() must be called in every function that calls a non-trivial\n     * libpng function */\n\n    if (setjmp(mainprog_ptr->jmpbuf)) {\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return LIBPNG_FATAL_ERROR;   /* fatal libpng error (via longjmp()) */\n    }\n\n#if defined(PNG_SKIP_sRGB_CHECK_PROFILE) && defined(PNG_SET_OPTION_SUPPORTED)\n    png_set_option(png_ptr, PNG_SKIP_sRGB_CHECK_PROFILE, PNG_OPTION_ON);\n#endif\n\n#if PNG_LIBPNG_VER >= 10500 && defined(PNG_UNKNOWN_CHUNKS_SUPPORTED)\n    /* copy standard chunks too */\n    png_set_keep_unknown_chunks(png_ptr, PNG_HANDLE_CHUNK_IF_SAFE, (png_const_bytep)\"pHYs\\0iTXt\\0tEXt\\0zTXt\", 4);\n#endif\n    png_set_read_user_chunk_fn(png_ptr, &mainprog_ptr->chunks, read_chunk_callback);\n\n    struct rwpng_read_data read_data = {infile, 0};\n    png_set_read_fn(png_ptr, &read_data, user_read_data);\n\n    png_read_info(png_ptr, info_ptr);  /* read all PNG info up to image data */\n\n    /* alternatively, could make separate calls to png_get_image_width(),\n     * etc., but want bit_depth and color_type for later [don't care about\n     * compression_type and filter_type => NULLs] */\n\n    png_get_IHDR(png_ptr, info_ptr, &mainprog_ptr->width, &mainprog_ptr->height,\n                 &bit_depth, &color_type, NULL, NULL, NULL);\n\n    /* expand palette images to RGB, low-bit-depth grayscale images to 8 bits,\n     * transparency chunks to full alpha channel; strip 16-bit-per-sample\n     * images to 8 bits per sample; and convert grayscale to RGB[A] */\n\n    /* GRR TO DO:  preserve all safe-to-copy ancillary PNG chunks */\n\n    if (!(color_type & PNG_COLOR_MASK_ALPHA)) {\n#ifdef PNG_READ_FILLER_SUPPORTED\n        png_set_expand(png_ptr);\n        png_set_filler(png_ptr, 65535L, PNG_FILLER_AFTER);\n#else\n        fprintf(stderr, \"pngquant readpng:  image is neither RGBA nor GA\\n\");\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        mainprog_ptr->retval = WRONG_INPUT_COLOR_TYPE;\n        return mainprog_ptr->retval;\n#endif\n    }\n\n    if (bit_depth == 16) {\n        png_set_strip_16(png_ptr);\n    }\n\n    if (!(color_type & PNG_COLOR_MASK_COLOR)) {\n        png_set_gray_to_rgb(png_ptr);\n    }\n\n    /* get source gamma for gamma correction, or use sRGB default */\n    double gamma = 0.45455;\n    if (png_get_valid(png_ptr, info_ptr, PNG_INFO_sRGB)) {\n        mainprog_ptr->input_color = RWPNG_SRGB;\n        mainprog_ptr->output_color = RWPNG_SRGB;\n    } else {\n        png_get_gAMA(png_ptr, info_ptr, &gamma);\n        if (gamma > 0 && gamma <= 1.0) {\n            mainprog_ptr->input_color = RWPNG_GAMA_ONLY;\n            mainprog_ptr->output_color = RWPNG_GAMA_ONLY;\n        } else {\n            fprintf(stderr, \"pngquant readpng:  ignored out-of-range gamma %f\\n\", gamma);\n            mainprog_ptr->input_color = RWPNG_NONE;\n            mainprog_ptr->output_color = RWPNG_NONE;\n            gamma = 0.45455;\n        }\n    }\n    mainprog_ptr->gamma = gamma;\n\n    png_set_interlace_handling(png_ptr);\n\n    /* all transformations have been registered; now update info_ptr data,\n     * get rowbytes and channels, and allocate image memory */\n\n    png_read_update_info(png_ptr, info_ptr);\n\n    rowbytes = png_get_rowbytes(png_ptr, info_ptr);\n\n    // For overflow safety reject images that won't fit in 32-bit\n    if (rowbytes > INT_MAX/mainprog_ptr->height) {\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;\n    }\n\n    if ((mainprog_ptr->rgba_data = malloc(rowbytes * mainprog_ptr->height)) == NULL) {\n        fprintf(stderr, \"pngquant readpng:  unable to allocate image data\\n\");\n        png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n        return PNG_OUT_OF_MEMORY_ERROR;\n    }\n\n    png_bytepp row_pointers = rwpng_create_row_pointers(info_ptr, png_ptr, mainprog_ptr->rgba_data, mainprog_ptr->height, 0);\n\n    /* now we can go ahead and just read the whole image */\n\n    png_read_image(png_ptr, row_pointers);\n\n    /* and we're done!  (png_read_end() can be omitted if no processing of\n     * post-IDAT text/time/etc. is desired) */\n\n    png_read_end(png_ptr, NULL);\n\n#if USE_LCMS\n#if PNG_LIBPNG_VER < 10500\n    png_charp ProfileData;\n#else\n    png_bytep ProfileData;\n#endif\n    png_uint_32 ProfileLen;\n\n    cmsHPROFILE hInProfile = NULL;\n\n    /* color_type is read from the image before conversion to RGBA */\n    int COLOR_PNG = color_type & PNG_COLOR_MASK_COLOR;\n\n    /* embedded ICC profile */\n    if (png_get_iCCP(png_ptr, info_ptr, &(png_charp){0}, &(int){0}, &ProfileData, &ProfileLen)) {\n\n        hInProfile = cmsOpenProfileFromMem(ProfileData, ProfileLen);\n        cmsColorSpaceSignature colorspace = cmsGetColorSpace(hInProfile);\n\n        /* only RGB (and GRAY) valid for PNGs */\n        if (colorspace == cmsSigRgbData && COLOR_PNG) {\n            mainprog_ptr->input_color = RWPNG_ICCP;\n            mainprog_ptr->output_color = RWPNG_SRGB;\n        } else {\n            if (colorspace == cmsSigGrayData && !COLOR_PNG) {\n                mainprog_ptr->input_color = RWPNG_ICCP_WARN_GRAY;\n                mainprog_ptr->output_color = RWPNG_SRGB;\n            }\n            cmsCloseProfile(hInProfile);\n            hInProfile = NULL;\n        }\n    }\n\n    /* build RGB profile from cHRM and gAMA */\n    if (hInProfile == NULL && COLOR_PNG &&\n        !png_get_valid(png_ptr, info_ptr, PNG_INFO_sRGB) &&\n        png_get_valid(png_ptr, info_ptr, PNG_INFO_gAMA) &&\n        png_get_valid(png_ptr, info_ptr, PNG_INFO_cHRM)) {\n\n        cmsCIExyY WhitePoint;\n        cmsCIExyYTRIPLE Primaries;\n\n        png_get_cHRM(png_ptr, info_ptr, &WhitePoint.x, &WhitePoint.y,\n                     &Primaries.Red.x, &Primaries.Red.y,\n                     &Primaries.Green.x, &Primaries.Green.y,\n                     &Primaries.Blue.x, &Primaries.Blue.y);\n\n        WhitePoint.Y = Primaries.Red.Y = Primaries.Green.Y = Primaries.Blue.Y = 1.0;\n\n        cmsToneCurve *GammaTable[3];\n        GammaTable[0] = GammaTable[1] = GammaTable[2] = cmsBuildGamma(NULL, 1/gamma);\n\n        hInProfile = cmsCreateRGBProfile(&WhitePoint, &Primaries, GammaTable);\n\n        cmsFreeToneCurve(GammaTable[0]);\n\n        mainprog_ptr->input_color = RWPNG_GAMA_CHRM;\n        mainprog_ptr->output_color = RWPNG_SRGB;\n    }\n\n    /* transform image to sRGB colorspace */\n    if (hInProfile != NULL) {\n\n        cmsHPROFILE hOutProfile = cmsCreate_sRGBProfile();\n        cmsHTRANSFORM hTransform = cmsCreateTransform(hInProfile, TYPE_RGBA_8,\n                                                      hOutProfile, TYPE_RGBA_8,\n                                                      INTENT_PERCEPTUAL,\n                                                      omp_get_max_threads() > 1 ? cmsFLAGS_NOCACHE : 0);\n\n        #pragma omp parallel for \\\n            if (mainprog_ptr->height*mainprog_ptr->width > 8000) \\\n            schedule(static)\n        for (unsigned int i = 0; i < mainprog_ptr->height; i++) {\n            /* It is safe to use the same block for input and output,\n               when both are of the same TYPE. */\n            cmsDoTransform(hTransform, row_pointers[i],\n                                       row_pointers[i],\n                                       mainprog_ptr->width);\n        }\n\n        cmsDeleteTransform(hTransform);\n        cmsCloseProfile(hOutProfile);\n        cmsCloseProfile(hInProfile);\n\n        mainprog_ptr->gamma = 0.45455;\n    }\n#endif\n\n    png_destroy_read_struct(&png_ptr, &info_ptr, NULL);\n\n    mainprog_ptr->file_size = read_data.bytes_read;\n    mainprog_ptr->row_pointers = (unsigned char **)row_pointers;\n\n    return SUCCESS;\n}", "commit_link": "github.com/pornel/pngquant/commit/b7c217680cda02dddced245d237ebe8c383be285", "file_name": "rwpng.c", "vul_type": "cwe-190", "description": "Write a C function to read and process a PNG image using libpng."}
{"func_name": "sctp_sf_ootb", "func_src_before": "sctp_disposition_t sctp_sf_ootb(struct net *net,\n\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\tconst sctp_subtype_t type,\n\t\t\t\tvoid *arg,\n\t\t\t\tsctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sk_buff *skb = chunk->skb;\n\tsctp_chunkhdr_t *ch;\n\tsctp_errhdr_t *err;\n\t__u8 *ch_end;\n\tint ootb_shut_ack = 0;\n\tint ootb_cookie_ack = 0;\n\n\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\n\tch = (sctp_chunkhdr_t *) chunk->chunk_hdr;\n\tdo {\n\t\t/* Report violation if the chunk is less then minimal */\n\t\tif (ntohs(ch->length) < sizeof(sctp_chunkhdr_t))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Now that we know we at least have a chunk header,\n\t\t * do things that are type appropriate.\n\t\t */\n\t\tif (SCTP_CID_SHUTDOWN_ACK == ch->type)\n\t\t\tootb_shut_ack = 1;\n\n\t\t/* RFC 2960, Section 3.3.7\n\t\t *   Moreover, under any circumstances, an endpoint that\n\t\t *   receives an ABORT  MUST NOT respond to that ABORT by\n\t\t *   sending an ABORT of its own.\n\t\t */\n\t\tif (SCTP_CID_ABORT == ch->type)\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\t/* RFC 8.4, 7) If the packet contains a \"Stale cookie\" ERROR\n\t\t * or a COOKIE ACK the SCTP Packet should be silently\n\t\t * discarded.\n\t\t */\n\n\t\tif (SCTP_CID_COOKIE_ACK == ch->type)\n\t\t\tootb_cookie_ack = 1;\n\n\t\tif (SCTP_CID_ERROR == ch->type) {\n\t\t\tsctp_walk_errors(err, ch) {\n\t\t\t\tif (SCTP_ERROR_STALE_COOKIE == err->cause) {\n\t\t\t\t\tootb_cookie_ack = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* Report violation if chunk len overflows */\n\t\tch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));\n\t\tif (ch_end > skb_tail_pointer(skb))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\tch = (sctp_chunkhdr_t *) ch_end;\n\t} while (ch_end < skb_tail_pointer(skb));\n\n\tif (ootb_shut_ack)\n\t\treturn sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands);\n\telse if (ootb_cookie_ack)\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\telse\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n}", "func_src_after": "sctp_disposition_t sctp_sf_ootb(struct net *net,\n\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\tconst sctp_subtype_t type,\n\t\t\t\tvoid *arg,\n\t\t\t\tsctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sk_buff *skb = chunk->skb;\n\tsctp_chunkhdr_t *ch;\n\tsctp_errhdr_t *err;\n\t__u8 *ch_end;\n\tint ootb_shut_ack = 0;\n\tint ootb_cookie_ack = 0;\n\n\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\n\tch = (sctp_chunkhdr_t *) chunk->chunk_hdr;\n\tdo {\n\t\t/* Report violation if the chunk is less then minimal */\n\t\tif (ntohs(ch->length) < sizeof(sctp_chunkhdr_t))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Report violation if chunk len overflows */\n\t\tch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));\n\t\tif (ch_end > skb_tail_pointer(skb))\n\t\t\treturn sctp_sf_violation_chunklen(net, ep, asoc, type, arg,\n\t\t\t\t\t\t  commands);\n\n\t\t/* Now that we know we at least have a chunk header,\n\t\t * do things that are type appropriate.\n\t\t */\n\t\tif (SCTP_CID_SHUTDOWN_ACK == ch->type)\n\t\t\tootb_shut_ack = 1;\n\n\t\t/* RFC 2960, Section 3.3.7\n\t\t *   Moreover, under any circumstances, an endpoint that\n\t\t *   receives an ABORT  MUST NOT respond to that ABORT by\n\t\t *   sending an ABORT of its own.\n\t\t */\n\t\tif (SCTP_CID_ABORT == ch->type)\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\t/* RFC 8.4, 7) If the packet contains a \"Stale cookie\" ERROR\n\t\t * or a COOKIE ACK the SCTP Packet should be silently\n\t\t * discarded.\n\t\t */\n\n\t\tif (SCTP_CID_COOKIE_ACK == ch->type)\n\t\t\tootb_cookie_ack = 1;\n\n\t\tif (SCTP_CID_ERROR == ch->type) {\n\t\t\tsctp_walk_errors(err, ch) {\n\t\t\t\tif (SCTP_ERROR_STALE_COOKIE == err->cause) {\n\t\t\t\t\tootb_cookie_ack = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tch = (sctp_chunkhdr_t *) ch_end;\n\t} while (ch_end < skb_tail_pointer(skb));\n\n\tif (ootb_shut_ack)\n\t\treturn sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands);\n\telse if (ootb_cookie_ack)\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\telse\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n}", "commit_link": "github.com/torvalds/linux/commit/bf911e985d6bbaa328c20c3e05f4eb03de11fdd6", "file_name": "net/sctp/sm_statefuns.c", "vul_type": "cwe-125", "description": "Write a C function named `sctp_sf_ootb` that handles out-of-the-blue SCTP packets by checking for protocol violations and responding appropriately."}
{"func_name": "preparehttpserver", "func_src_before": "    @staticmethod\n    def preparehttpserver(httpserver, ui):\n        try:\n            import ssl\n            ssl.wrap_socket\n        except ImportError:\n            raise error.Abort(_(\"SSL support is unavailable\"))\n\n        certfile = ui.config('web', 'certificate')\n        httpserver.socket = ssl.wrap_socket(\n            httpserver.socket, server_side=True,\n            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1)", "func_src_after": "    @staticmethod\n    def preparehttpserver(httpserver, ui):\n        try:\n            from .. import sslutil\n            sslutil.modernssl\n        except ImportError:\n            raise error.Abort(_(\"SSL support is unavailable\"))\n\n        certfile = ui.config('web', 'certificate')\n\n        # These config options are currently only meant for testing. Use\n        # at your own risk.\n        cafile = ui.config('devel', 'servercafile')\n        reqcert = ui.configbool('devel', 'serverrequirecert')\n\n        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n                                                     ui,\n                                                     certfile=certfile,\n                                                     cafile=cafile,\n                                                     requireclientcert=reqcert)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 74, "char_end": 97, "line": "            import ssl\n"}, {"line_no": 5, "char_start": 97, "char_end": 125, "line": "            ssl.wrap_socket\n"}, {"line_no": 10, "char_start": 268, "char_end": 313, "line": "        httpserver.socket = ssl.wrap_socket(\n"}, {"line_no": 11, "char_start": 313, "char_end": 362, "line": "            httpserver.socket, server_side=True,\n"}, {"line_no": 12, "char_start": 362, "char_end": 424, "line": "            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1)\n"}], "added": [{"line_no": 4, "char_start": 74, "char_end": 109, "line": "            from .. import sslutil\n"}, {"line_no": 5, "char_start": 109, "char_end": 139, "line": "            sslutil.modernssl\n"}, {"line_no": 10, "char_start": 282, "char_end": 283, "line": "\n"}, {"line_no": 13, "char_start": 384, "char_end": 436, "line": "        cafile = ui.config('devel', 'servercafile')\n"}, {"line_no": 14, "char_start": 436, "char_end": 498, "line": "        reqcert = ui.configbool('devel', 'serverrequirecert')\n"}, {"line_no": 15, "char_start": 498, "char_end": 499, "line": "\n"}, {"line_no": 16, "char_start": 499, "char_end": 571, "line": "        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n"}, {"line_no": 17, "char_start": 571, "char_end": 628, "line": "                                                     ui,\n"}, {"line_no": 18, "char_start": 628, "char_end": 700, "line": "                                                     certfile=certfile,\n"}, {"line_no": 19, "char_start": 700, "char_end": 768, "line": "                                                     cafile=cafile,\n"}, {"line_no": 20, "char_start": 768, "char_end": 847, "line": "                                                     requireclientcert=reqcert)\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 124, "chars": ".wrap_socket"}, {"char_start": 276, "char_end": 423, "chars": "httpserver.socket = ssl.wrap_socket(\n            httpserver.socket, server_side=True,\n            certfile=certfile, ssl_version=ssl.PROTOCOL_TLSv1"}], "added": [{"char_start": 85, "char_end": 93, "chars": " from .."}, {"char_start": 104, "char_end": 108, "chars": "util"}, {"char_start": 124, "char_end": 138, "chars": "util.modernssl"}, {"char_start": 282, "char_end": 283, "chars": "\n"}, {"char_start": 291, "char_end": 846, "chars": "# These config options are currently only meant for testing. Use\n        # at your own risk.\n        cafile = ui.config('devel', 'servercafile')\n        reqcert = ui.configbool('devel', 'serverrequirecert')\n\n        httpserver.socket = sslutil.wrapserversocket(httpserver.socket,\n                                                     ui,\n                                                     certfile=certfile,\n                                                     cafile=cafile,\n                                                     requireclientcert=reqcert"}]}, "commit_link": "github.com/facebookexperimental/eden/commit/c023f774e64a89a05ebe18d111db425584fb9b86", "file_name": "server.py", "vul_type": "cwe-327", "commit_msg": "hgweb: use sslutil.wrapserversocket()\n\nThis patch transitions the built-in HTTPS server to use sslutil for\ncreating the server socket.\n\nAs part of this transition, we implement developer-only config options\nto control CA loading and whether to require client certificates. This\neliminates the need for the custom extension in test-https.t to define\nthese.\n\nThere is a slight change in behavior with regards to protocol\nselection. Before, we would always use the TLS 1.0 constant to define\nthe protocol version. This would *only* use TLS 1.0. sslutil defaults\nto TLS 1.0+. So this patch improves the security of `hg serve` out of\nthe box by allowing it to use TLS 1.1 and 1.2 (if available).", "description": "Write a Python function that configures an HTTP server with SSL using a certificate from the configuration."}
{"func_name": "getGameCountInSeriesSoFar", "func_src_before": "def getGameCountInSeriesSoFar(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT COUNT(*) FROM ChallengeRankings WHERE SeriesTitle = '\" + getTitle(submission) + \"' AND Date <= '\" + getSubmissionDateFromDatabase(submission) + \"'\").fetchone()[0]\n    database.close()", "func_src_after": "def getGameCountInSeriesSoFar(submission):\n    database = sqlite3.connect('database.db')\n    cursor = database.cursor()\n    return cursor.execute(\"SELECT COUNT(*) FROM ChallengeRankings WHERE SeriesTitle = ? AND Date <= ?\", [getTitle(submission), getSubmissionDateFromDatabase(submission)]).fetchone()[0]\n    database.close()", "commit_link": "github.com/LiquidFun/Reddit-GeoGuessr-Tracking-Bot/commit/0cad2d52e24b05da32789fbc8face7a9999a71f9", "file_name": "CheckAndPostForSeriesSubmissions.py", "vul_type": "cwe-089", "description": "Write a Python function to count the number of games in a series up to the date of a given submission using SQLite."}
{"func_name": "verify_rno", "func_src_before": "    def verify_rno(self, rno):\n        query = \"SELECT COUNT(rno) FROM rides WHERE rno = {rno}\".format(rno = rno)\n        self.cursor.execute(query)\n        result = self.cursor.fetchone()\n        if (int(result[0]) > 0):\n            return True \n        else:\n            return False", "func_src_after": "    def verify_rno(self, rno):\n        self.cursor.execute(\"SELECT COUNT(rno) FROM rides WHERE rno = :rno\", {'rno': rno})\n        result = self.cursor.fetchone()\n        if (int(result[0]) > 0):\n            return True \n        else:\n            return False", "commit_link": "github.com/kenboo98/291-Mini-Project-I/commit/3080ccb687c79c83954ce703faee8fcceec8c9eb", "file_name": "book_rides/book_rides.py", "vul_type": "cwe-089", "description": "Write a Python function to check if a ride number exists in a database using SQL queries."}
{"func_name": "dmi_memory_device_size_str", "func_src_before": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[8];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "func_src_after": "static char *dmi_memory_device_size_str(u16 code)\n{\n\tstatic char size[16];\n\n\tif (code == 0)\n\t\tstrcpy(size, \"Empty\");\n\telse if (code == 0xFFFF)\n\t\tstrcpy(size, \"Unknown\");\n\telse\n\t{\n\t\tif (code & 0x8000)\n\t\t\tsprintf(size, \"%u kB\", code & 0x7FFF);\n\t\telse\n\t\t\tsprintf(size, \"%u MB\", code);\n\t}\n\n\treturn size;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 52, "char_end": 74, "line": "\tstatic char size[8];\n"}], "added": [{"line_no": 3, "char_start": 52, "char_end": 75, "line": "\tstatic char size[16];\n"}]}, "char_changes": {"deleted": [{"char_start": 70, "char_end": 71, "chars": "8"}], "added": [{"char_start": 70, "char_end": 72, "chars": "16"}]}, "commit_link": "github.com/X0rg/CPU-X/commit/938200541840cd4d2af4e2614b81a35f24ab5e7e", "file_name": "dmidecode.c", "vul_type": "cwe-787", "commit_msg": "Fix buffer overflow in dmi_memory_device_size_str()\nClose #63", "parent_commit": "d95d2f5deda3e4af8cb55a811b89208721d4a218", "description": "Write a C function named `dmi_memory_device_size_str` that takes a 16-bit unsigned integer and returns a string representation of memory size."}
{"func_name": "PHP_FUNCTION", "func_src_before": "PHP_FUNCTION(unserialize)\n{\n\tchar *buf = NULL;\n\tsize_t buf_len;\n\tconst unsigned char *p;\n\tphp_unserialize_data_t var_hash;\n\tzval *options = NULL, *classes = NULL;\n\tHashTable *class_hash = NULL;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS(), \"s|a\", &buf, &buf_len, &options) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tif (buf_len == 0) {\n\t\tRETURN_FALSE;\n\t}\n\n\tp = (const unsigned char*) buf;\n\tPHP_VAR_UNSERIALIZE_INIT(var_hash);\n\tif(options != NULL) {\n\t\tclasses = zend_hash_str_find(Z_ARRVAL_P(options), \"allowed_classes\", sizeof(\"allowed_classes\")-1);\n\t\tif(classes && (Z_TYPE_P(classes) == IS_ARRAY || !zend_is_true(classes))) {\n\t\t\tALLOC_HASHTABLE(class_hash);\n\t\t\tzend_hash_init(class_hash, (Z_TYPE_P(classes) == IS_ARRAY)?zend_hash_num_elements(Z_ARRVAL_P(classes)):0, NULL, NULL, 0);\n\t\t}\n\t\tif(class_hash && Z_TYPE_P(classes) == IS_ARRAY) {\n\t\t\tzval *entry;\n\t\t\tzend_string *lcname;\n\n\t\t\tZEND_HASH_FOREACH_VAL(Z_ARRVAL_P(classes), entry) {\n\t\t\t\tconvert_to_string_ex(entry);\n\t\t\t\tlcname = zend_string_tolower(Z_STR_P(entry));\n\t\t\t\tzend_hash_add_empty_element(class_hash, lcname);\n\t\t        zend_string_release(lcname);\n\t\t\t} ZEND_HASH_FOREACH_END();\n\t\t}\n\t}\n\n\tif (!php_var_unserialize_ex(return_value, &p, p + buf_len, &var_hash, class_hash)) {\n\t\tPHP_VAR_UNSERIALIZE_DESTROY(var_hash);\n\t\tif (class_hash) {\n\t\t\tzend_hash_destroy(class_hash);\n\t\t\tFREE_HASHTABLE(class_hash);\n\t\t}\n\t\tzval_ptr_dtor(return_value);\n\t\tif (!EG(exception)) {\n\t\t\tphp_error_docref(NULL, E_NOTICE, \"Error at offset \" ZEND_LONG_FMT \" of %zd bytes\",\n\t\t\t\t(zend_long)((char*)p - buf), buf_len);\n\t\t}\n\t\tRETURN_FALSE;\n\t}\n\t/* We should keep an reference to return_value to prevent it from being dtor\n\t   in case nesting calls to unserialize */\n\tvar_push_dtor(&var_hash, return_value);\n\n\tPHP_VAR_UNSERIALIZE_DESTROY(var_hash);\n\tif (class_hash) {\n\t\tzend_hash_destroy(class_hash);\n\t\tFREE_HASHTABLE(class_hash);\n\t}\n}", "func_src_after": "PHP_FUNCTION(unserialize)\n{\n\tchar *buf = NULL;\n\tsize_t buf_len;\n\tconst unsigned char *p;\n\tphp_unserialize_data_t var_hash;\n\tzval *options = NULL, *classes = NULL;\n\tzval *retval;\n\tHashTable *class_hash = NULL;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS(), \"s|a\", &buf, &buf_len, &options) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tif (buf_len == 0) {\n\t\tRETURN_FALSE;\n\t}\n\n\tp = (const unsigned char*) buf;\n\tPHP_VAR_UNSERIALIZE_INIT(var_hash);\n\tif(options != NULL) {\n\t\tclasses = zend_hash_str_find(Z_ARRVAL_P(options), \"allowed_classes\", sizeof(\"allowed_classes\")-1);\n\t\tif(classes && (Z_TYPE_P(classes) == IS_ARRAY || !zend_is_true(classes))) {\n\t\t\tALLOC_HASHTABLE(class_hash);\n\t\t\tzend_hash_init(class_hash, (Z_TYPE_P(classes) == IS_ARRAY)?zend_hash_num_elements(Z_ARRVAL_P(classes)):0, NULL, NULL, 0);\n\t\t}\n\t\tif(class_hash && Z_TYPE_P(classes) == IS_ARRAY) {\n\t\t\tzval *entry;\n\t\t\tzend_string *lcname;\n\n\t\t\tZEND_HASH_FOREACH_VAL(Z_ARRVAL_P(classes), entry) {\n\t\t\t\tconvert_to_string_ex(entry);\n\t\t\t\tlcname = zend_string_tolower(Z_STR_P(entry));\n\t\t\t\tzend_hash_add_empty_element(class_hash, lcname);\n\t\t        zend_string_release(lcname);\n\t\t\t} ZEND_HASH_FOREACH_END();\n\t\t}\n\t}\n\n\tretval = var_tmp_var(&var_hash);\n\tif (!php_var_unserialize_ex(retval, &p, p + buf_len, &var_hash, class_hash)) {\n\t\tPHP_VAR_UNSERIALIZE_DESTROY(var_hash);\n\t\tif (class_hash) {\n\t\t\tzend_hash_destroy(class_hash);\n\t\t\tFREE_HASHTABLE(class_hash);\n\t\t}\n\t\tif (!EG(exception)) {\n\t\t\tphp_error_docref(NULL, E_NOTICE, \"Error at offset \" ZEND_LONG_FMT \" of %zd bytes\",\n\t\t\t\t(zend_long)((char*)p - buf), buf_len);\n\t\t}\n\t\tRETURN_FALSE;\n\t}\n\n\tZVAL_COPY(return_value, retval);\n\n\tPHP_VAR_UNSERIALIZE_DESTROY(var_hash);\n\tif (class_hash) {\n\t\tzend_hash_destroy(class_hash);\n\t\tFREE_HASHTABLE(class_hash);\n\t}\n}", "commit_link": "github.com/php/php-src/commit/b2af4e8868726a040234de113436c6e4f6372d17", "file_name": "ext/standard/var.c", "vul_type": "cwe-416", "description": "Write a PHP function to unserialize data with an optional parameter for allowed classes."}
{"func_name": "track_header", "func_src_before": "static int track_header(VividasDemuxContext *viv, AVFormatContext *s,  uint8_t *buf, int size)\n{\n    int i, j, ret;\n    int64_t off;\n    int val_1;\n    int num_video;\n    AVIOContext pb0, *pb = &pb0;\n\n    ffio_init_context(pb, buf, size, 0, NULL, NULL, NULL, NULL);\n\n    ffio_read_varlen(pb); // track_header_len\n    avio_r8(pb); // '1'\n\n    val_1 = ffio_read_varlen(pb);\n\n    for (i=0;i<val_1;i++) {\n        int c = avio_r8(pb);\n        if (avio_feof(pb))\n            return AVERROR_EOF;\n        for (j=0;j<c;j++) {\n            if (avio_feof(pb))\n                return AVERROR_EOF;\n            avio_r8(pb); // val_3\n            avio_r8(pb); // val_4\n        }\n    }\n\n    avio_r8(pb); // num_streams\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_5\n\n    avio_r8(pb); // '2'\n    num_video = avio_r8(pb);\n\n    avio_seek(pb, off, SEEK_SET);\n    if (num_video != 1) {\n        av_log(s, AV_LOG_ERROR, \"number of video tracks %d is not 1\\n\", num_video);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    for (i = 0; i < num_video; i++) {\n        AVStream *st = avformat_new_stream(s, NULL);\n        int num, den;\n\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;\n        st->codecpar->codec_id = AV_CODEC_ID_VP6;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb);\n        avio_r8(pb); // '3'\n        avio_r8(pb); // val_7\n        num = avio_rl32(pb); // frame_time\n        den = avio_rl32(pb); // time_base\n        avpriv_set_pts_info(st, 64, num, den);\n        st->nb_frames = avio_rl32(pb); // n frames\n        st->codecpar->width = avio_rl16(pb); // width\n        st->codecpar->height = avio_rl16(pb); // height\n        avio_r8(pb); // val_8\n        avio_rl32(pb); // val_9\n\n        avio_seek(pb, off, SEEK_SET);\n    }\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_10\n    avio_r8(pb); // '4'\n    viv->num_audio = avio_r8(pb);\n    avio_seek(pb, off, SEEK_SET);\n\n    if (viv->num_audio != 1)\n        av_log(s, AV_LOG_WARNING, \"number of audio tracks %d is not 1\\n\", viv->num_audio);\n\n    for(i=0;i<viv->num_audio;i++) {\n        int q;\n        AVStream *st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = num_video + i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_AUDIO;\n        st->codecpar->codec_id = AV_CODEC_ID_VORBIS;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb); // length\n        avio_r8(pb); // '5'\n        avio_r8(pb); //codec_id\n        avio_rl16(pb); //codec_subid\n        st->codecpar->channels = avio_rl16(pb); // channels\n        st->codecpar->sample_rate = avio_rl32(pb); // sample_rate\n        avio_seek(pb, 10, SEEK_CUR); // data_1\n        q = avio_r8(pb);\n        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 0;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n            ffio_read_varlen(pb); // len_3\n            num_data = avio_r8(pb);\n            for (j = 0; j < num_data; j++) {\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, 64 + xd_size + xd_size / 255);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                if (delta > data_len[j]) {\n                    return AVERROR_INVALIDDATA;\n                }\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;\n        }\n    }\n\n    return 0;\n}", "func_src_after": "static int track_header(VividasDemuxContext *viv, AVFormatContext *s,  uint8_t *buf, int size)\n{\n    int i, j, ret;\n    int64_t off;\n    int val_1;\n    int num_video;\n    AVIOContext pb0, *pb = &pb0;\n\n    ffio_init_context(pb, buf, size, 0, NULL, NULL, NULL, NULL);\n\n    ffio_read_varlen(pb); // track_header_len\n    avio_r8(pb); // '1'\n\n    val_1 = ffio_read_varlen(pb);\n\n    for (i=0;i<val_1;i++) {\n        int c = avio_r8(pb);\n        if (avio_feof(pb))\n            return AVERROR_EOF;\n        for (j=0;j<c;j++) {\n            if (avio_feof(pb))\n                return AVERROR_EOF;\n            avio_r8(pb); // val_3\n            avio_r8(pb); // val_4\n        }\n    }\n\n    avio_r8(pb); // num_streams\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_5\n\n    avio_r8(pb); // '2'\n    num_video = avio_r8(pb);\n\n    avio_seek(pb, off, SEEK_SET);\n    if (num_video != 1) {\n        av_log(s, AV_LOG_ERROR, \"number of video tracks %d is not 1\\n\", num_video);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    for (i = 0; i < num_video; i++) {\n        AVStream *st = avformat_new_stream(s, NULL);\n        int num, den;\n\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;\n        st->codecpar->codec_id = AV_CODEC_ID_VP6;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb);\n        avio_r8(pb); // '3'\n        avio_r8(pb); // val_7\n        num = avio_rl32(pb); // frame_time\n        den = avio_rl32(pb); // time_base\n        avpriv_set_pts_info(st, 64, num, den);\n        st->nb_frames = avio_rl32(pb); // n frames\n        st->codecpar->width = avio_rl16(pb); // width\n        st->codecpar->height = avio_rl16(pb); // height\n        avio_r8(pb); // val_8\n        avio_rl32(pb); // val_9\n\n        avio_seek(pb, off, SEEK_SET);\n    }\n\n    off = avio_tell(pb);\n    off += ffio_read_varlen(pb); // val_10\n    avio_r8(pb); // '4'\n    viv->num_audio = avio_r8(pb);\n    avio_seek(pb, off, SEEK_SET);\n\n    if (viv->num_audio != 1)\n        av_log(s, AV_LOG_WARNING, \"number of audio tracks %d is not 1\\n\", viv->num_audio);\n\n    for(i=0;i<viv->num_audio;i++) {\n        int q;\n        AVStream *st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR(ENOMEM);\n\n        st->id = num_video + i;\n\n        st->codecpar->codec_type = AVMEDIA_TYPE_AUDIO;\n        st->codecpar->codec_id = AV_CODEC_ID_VORBIS;\n\n        off = avio_tell(pb);\n        off += ffio_read_varlen(pb); // length\n        avio_r8(pb); // '5'\n        avio_r8(pb); //codec_id\n        avio_rl16(pb); //codec_subid\n        st->codecpar->channels = avio_rl16(pb); // channels\n        st->codecpar->sample_rate = avio_rl32(pb); // sample_rate\n        avio_seek(pb, 10, SEEK_CUR); // data_1\n        q = avio_r8(pb);\n        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 1;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n            ffio_read_varlen(pb); // len_3\n            num_data = avio_r8(pb);\n            for (j = 0; j < num_data; j++) {\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len + 1 + len/255;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, xd_size);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                av_assert0(delta <= xd_size - offset);\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                av_assert0(data_len[j] <= xd_size - offset);\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;\n        }\n    }\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/27a99e2c7d450fef15594671eef4465c8a166bd7", "file_name": "libavformat/vividas.c", "vul_type": "cwe-787", "description": "Write a C function to parse video and audio track headers from a buffer in an AVFormatContext using FFmpeg libraries."}
{"func_name": "save_failure_transaction", "func_src_before": "    def save_failure_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )\" % (project_id, user_id, money))\n        self.db.commit()", "func_src_after": "    def save_failure_transaction(self, user_id, project_id, money):\n        self.cursor.execute(\"insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, \"\n                            \"%s, now(), 'failed' )\", (project_id, user_id, money))\n        self.db.commit()", "commit_link": "github.com/JLucka/kickstarter-dev/commit/e2ffa062697e060fdfbd2eccbb89a8c53a569e0b", "file_name": "backend/transactions/TransactionConnector.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a failed transaction record into a database."}
{"func_name": "self.status", "func_src_before": "    def self.status(repo_path)\n      @git_status = {}\n\n      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-uall', '--ignored']) do |output|\n        output.read.split(\"\\x0\").map { |x| x.split(' ', 2) }.each do |mode, file|\n          @git_status[file] = mode\n        end\n      end\n      warn \"git status failed in #{repo_path}\" unless $CHILD_STATUS.success?\n\n      @git_status\n    end", "func_src_after": "    def self.status(repo_path)\n      @git_status = {}\n\n      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-unormal', '--ignored']) do |output|\n        output.read.split(\"\\x0\").map { |x| x.split(' ', 2) }.each do |mode, file|\n          @git_status[file] = mode\n        end\n      end\n      warn \"git status failed in #{repo_path}\" unless $CHILD_STATUS.success?\n\n      @git_status\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 55, "char_end": 161, "line": "      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-uall', '--ignored']) do |output|\n"}], "added": [{"line_no": 4, "char_start": 55, "char_end": 164, "line": "      IO.popen(['git', '-C', repo_path, 'status', '--porcelain', '-z', '-unormal', '--ignored']) do |output|\n"}]}, "char_changes": {"deleted": [{"char_start": 130, "char_end": 131, "chars": "l"}], "added": [{"char_start": 129, "char_end": 133, "chars": "norm"}]}, "commit_link": "github.com/athityakumar/colorls/commit/b362fa1eb81e7e6fa208cc8cab51f110db20057b", "file_name": "git.rb", "vul_type": "cwe-022", "commit_msg": "Improve git-status processing\n\n* no longer traverse complete directory trees to determine git status for\n  directories\n\n* properly report status for folders with changed files\n\n* skip the parent folder since we do not have git status about it", "parent_commit": "bb270b319a68adb96bf91a311250481020bdde81", "description": "Write a Ruby method that captures the status of files in a git repository at a given path and returns a hash with file names and their corresponding status codes."}
{"func_name": "store_order", "func_src_before": "    async def store_order(self, uid, tree_id, order, ordered_at):\n        \"\"\" store remote order \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.orders(manager, tree, order_data, ordered_at)\n                VALUES ('{uid}', '{tree_id}', '{order_data}', '{ordered_at}') \"\"\"\n                        return await cur.scalar(query)\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def store_order(self, uid, tree_id, order, ordered_at):\n        \"\"\" store remote order \"\"\"\n\n        try:\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.orders(manager, tree, order_data, ordered_at)\n                VALUES ('{uid}', '{tree_id}', '{order_data}', '{ordered_at}') \"\"\"\n                        return await cur.scalar(query)\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 14, "char_start": 587, "char_end": 620, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 14, "char_start": 587, "char_end": 624, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 605, "char_end": 609, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously insert an order record into a PostgreSQL database."}
{"func_name": "rds_rdma_extra_size", "func_src_before": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\n\tstruct rds_iovec vec;\n\tstruct rds_iovec __user *local_vec;\n\tint tot_pages = 0;\n\tunsigned int nr_pages;\n\tunsigned int i;\n\n\tlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\n\n\t/* figure out the number of pages in the vector */\n\tfor (i = 0; i < args->nr_local; i++) {\n\t\tif (copy_from_user(&vec, &local_vec[i],\n\t\t\t\t   sizeof(struct rds_iovec)))\n\t\t\treturn -EFAULT;\n\n\t\tnr_pages = rds_pages_in_vec(&vec);\n\t\tif (nr_pages == 0)\n\t\t\treturn -EINVAL;\n\n\t\ttot_pages += nr_pages;\n\n\t\t/*\n\t\t * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,\n\t\t * so tot_pages cannot overflow without first going negative.\n\t\t */\n\t\tif (tot_pages < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn tot_pages * sizeof(struct scatterlist);\n}", "func_src_after": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\n\tstruct rds_iovec vec;\n\tstruct rds_iovec __user *local_vec;\n\tint tot_pages = 0;\n\tunsigned int nr_pages;\n\tunsigned int i;\n\n\tlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\n\n\tif (args->nr_local == 0)\n\t\treturn -EINVAL;\n\n\t/* figure out the number of pages in the vector */\n\tfor (i = 0; i < args->nr_local; i++) {\n\t\tif (copy_from_user(&vec, &local_vec[i],\n\t\t\t\t   sizeof(struct rds_iovec)))\n\t\t\treturn -EFAULT;\n\n\t\tnr_pages = rds_pages_in_vec(&vec);\n\t\tif (nr_pages == 0)\n\t\t\treturn -EINVAL;\n\n\t\ttot_pages += nr_pages;\n\n\t\t/*\n\t\t * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,\n\t\t * so tot_pages cannot overflow without first going negative.\n\t\t */\n\t\tif (tot_pages < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn tot_pages * sizeof(struct scatterlist);\n}", "commit_link": "github.com/torvalds/linux/commit/c095508770aebf1b9218e77026e48345d719b17c", "file_name": "net/rds/rdma.c", "vul_type": "cwe-787", "description": "Write a C function named `rds_rdma_extra_size` that calculates the total size of memory pages described by an array of `rds_iovec` structures, handling user-space memory copying and validation."}
{"func_name": "clean_taxon_concept", "func_src_before": "  def clean_taxon_concept\n    taxon_concept.gsub(\"\\n\",\" \").gsub(\"\\'\", \"\\\\\\'\")\n  end", "func_src_after": "  def clean_taxon_concept\n    taxon_concept.gsub(\"\\n\", ' ').gsub(\"\\'\", \"\\\\\\'\")\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 78, "line": "    taxon_concept.gsub(\"\\n\",\" \").gsub(\"\\'\", \"\\\\\\'\")\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 79, "line": "    taxon_concept.gsub(\"\\n\", ' ').gsub(\"\\'\", \"\\\\\\'\")\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 57, "chars": "\" \""}], "added": [{"char_start": 54, "char_end": 58, "chars": " ' '"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby function named `clean_taxon_concept` that replaces newline characters with spaces and escapes single quotes in a string."}
{"func_name": "whitelist", "func_src_before": "def whitelist(users: str):\n    for user in users.split():\n        call(WHITELIST_COMMAND_TEMPLATE.format(user))", "func_src_after": "def whitelist(channel: discord.TextChannel, users: str):\n    for user in users.split():\n        if not re.match(r'^[A-Za-z0-9_]{3,16}$', user):  # as per https://help.mojang.com/customer/en/portal/articles/928638-minecraft-usernames?b_id=5408\n            await channel.send('\\'{}\\' is not a valid Minecraft username'.format(user))\n        else:\n            call(WHITELIST_COMMAND_TEMPLATE.format(user))", "commit_link": "github.com/thomotron/Gatekeep/commit/955660f9b3dc336ab0d5dfb4392b3ab6deac6b25", "file_name": "bot.py", "vul_type": "cwe-078", "description": "Write a Python function that processes a string of usernames and executes a command for each valid username."}
{"func_name": "jbig2_image_compose", "func_src_before": "jbig2_image_compose(Jbig2Ctx *ctx, Jbig2Image *dst, Jbig2Image *src, int x, int y, Jbig2ComposeOp op)\n{\n    uint32_t w, h;\n    uint32_t shift;\n    uint32_t leftbyte;\n    uint8_t *ss;\n    uint8_t *dd;\n    uint8_t leftmask, rightmask;\n    int early = x >= 0;\n    int late;\n    uint32_t bytewidth;\n    uint32_t syoffset = 0;\n\n    if (src == NULL)\n        return 0;\n\n    /* This code takes a src image and combines it onto dst at offset (x,y), with operation op. */\n\n    /* Data is packed msb first within a byte, so with bits numbered: 01234567.\n     * Second byte is: 89abcdef. So to combine into a run, we use:\n     *       (s[0]<<8) | s[1] == 0123456789abcdef.\n     * To read from src into dst at offset 3, we need to read:\n     *    read:      0123456789abcdef...\n     *    write:  0123456798abcdef...\n     * In general, to read from src and write into dst at offset x, we need to shift\n     * down by (x&7) bits to allow for bit alignment. So shift = x&7.\n     * So the 'central' part of our runs will see us doing:\n     *   *d++ op= ((s[0]<<8)|s[1])>>shift;\n     * with special cases on the left and right edges of the run to mask.\n     * With the left hand edge, we have to be careful not to 'underread' the start of\n     * the src image; this is what the early flag is about. Similarly we have to be\n     * careful not to read off the right hand edge; this is what the late flag is for.\n     */\n\n    /* clip */\n    w = src->width;\n    h = src->height;\n    shift = (x & 7);\n    ss = src->data - early;\n\n    if (x < 0) {\n        if (w < (uint32_t) -x)\n            w = 0;\n        else\n            w += x;\n        ss += (-x-1)>>3;\n        x = 0;\n    }\n    if (y < 0) {\n        if (h < (uint32_t) -y)\n            h = 0;\n        else\n            h += y;\n        syoffset = -y * src->stride;\n        y = 0;\n    }\n    if ((uint32_t)x + w > dst->width)\n    {\n        if (dst->width < (uint32_t)x)\n            w = 0;\n        else\n            w = dst->width - x;\n    }\n    if ((uint32_t)y + h > dst->height)\n    {\n        if (dst->height < (uint32_t)y)\n            h = 0;\n        else\n            h = dst->height - y;\n    }\n#ifdef JBIG2_DEBUG\n    jbig2_error(ctx, JBIG2_SEVERITY_DEBUG, -1, \"compositing %dx%d at (%d, %d) after clipping\", w, h, x, y);\n#endif\n\n    /* check for zero clipping region */\n    if ((w <= 0) || (h <= 0)) {\n#ifdef JBIG2_DEBUG\n        jbig2_error(ctx, JBIG2_SEVERITY_DEBUG, -1, \"zero clipping region\");\n#endif\n        return 0;\n    }\n\n    leftbyte = (uint32_t) x >> 3;\n    dd = dst->data + y * dst->stride + leftbyte;\n    bytewidth = (((uint32_t) x + w - 1) >> 3) - leftbyte + 1;\n    leftmask = 255>>(x&7);\n    rightmask = (((x+w)&7) == 0) ? 255 : ~(255>>((x+w)&7));\n    if (bytewidth == 1)\n        leftmask &= rightmask;\n    late = (ss + bytewidth >= src->data + ((src->width+7)>>3));\n    ss += syoffset;\n\n    switch(op)\n    {\n    case JBIG2_COMPOSE_OR:\n        jbig2_image_compose_opt_OR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_AND:\n        jbig2_image_compose_opt_AND(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_XOR:\n        jbig2_image_compose_opt_XOR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_XNOR:\n        jbig2_image_compose_opt_XNOR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_REPLACE:\n        jbig2_image_compose_opt_REPLACE(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    }\n\n    return 0;\n}", "func_src_after": "jbig2_image_compose(Jbig2Ctx *ctx, Jbig2Image *dst, Jbig2Image *src, int x, int y, Jbig2ComposeOp op)\n{\n    uint32_t w, h;\n    uint32_t shift;\n    uint32_t leftbyte;\n    uint8_t *ss;\n    uint8_t *dd;\n    uint8_t leftmask, rightmask;\n    int early = x >= 0;\n    int late;\n    uint32_t bytewidth;\n    uint32_t syoffset = 0;\n\n    if (src == NULL)\n        return 0;\n\n    if ((UINT32_MAX - src->width  < (x > 0 ? x : -x)) ||\n        (UINT32_MAX - src->height < (y > 0 ? y : -y)))\n    {\n#ifdef JBIG2_DEBUG\n        jbig2_error(ctx, JBIG2_SEVERITY_DEBUG, -1, \"overflow in compose_image\");\n#endif\n        return 0;\n    }\n\n    /* This code takes a src image and combines it onto dst at offset (x,y), with operation op. */\n\n    /* Data is packed msb first within a byte, so with bits numbered: 01234567.\n     * Second byte is: 89abcdef. So to combine into a run, we use:\n     *       (s[0]<<8) | s[1] == 0123456789abcdef.\n     * To read from src into dst at offset 3, we need to read:\n     *    read:      0123456789abcdef...\n     *    write:  0123456798abcdef...\n     * In general, to read from src and write into dst at offset x, we need to shift\n     * down by (x&7) bits to allow for bit alignment. So shift = x&7.\n     * So the 'central' part of our runs will see us doing:\n     *   *d++ op= ((s[0]<<8)|s[1])>>shift;\n     * with special cases on the left and right edges of the run to mask.\n     * With the left hand edge, we have to be careful not to 'underread' the start of\n     * the src image; this is what the early flag is about. Similarly we have to be\n     * careful not to read off the right hand edge; this is what the late flag is for.\n     */\n\n    /* clip */\n    w = src->width;\n    h = src->height;\n    shift = (x & 7);\n    ss = src->data - early;\n\n    if (x < 0) {\n        if (w < (uint32_t) -x)\n            w = 0;\n        else\n            w += x;\n        ss += (-x-1)>>3;\n        x = 0;\n    }\n    if (y < 0) {\n        if (h < (uint32_t) -y)\n            h = 0;\n        else\n            h += y;\n        syoffset = -y * src->stride;\n        y = 0;\n    }\n    if ((uint32_t)x + w > dst->width)\n    {\n        if (dst->width < (uint32_t)x)\n            w = 0;\n        else\n            w = dst->width - x;\n    }\n    if ((uint32_t)y + h > dst->height)\n    {\n        if (dst->height < (uint32_t)y)\n            h = 0;\n        else\n            h = dst->height - y;\n    }\n#ifdef JBIG2_DEBUG\n    jbig2_error(ctx, JBIG2_SEVERITY_DEBUG, -1, \"compositing %dx%d at (%d, %d) after clipping\", w, h, x, y);\n#endif\n\n    /* check for zero clipping region */\n    if ((w <= 0) || (h <= 0)) {\n#ifdef JBIG2_DEBUG\n        jbig2_error(ctx, JBIG2_SEVERITY_DEBUG, -1, \"zero clipping region\");\n#endif\n        return 0;\n    }\n\n    leftbyte = (uint32_t) x >> 3;\n    dd = dst->data + y * dst->stride + leftbyte;\n    bytewidth = (((uint32_t) x + w - 1) >> 3) - leftbyte + 1;\n    leftmask = 255>>(x&7);\n    rightmask = (((x+w)&7) == 0) ? 255 : ~(255>>((x+w)&7));\n    if (bytewidth == 1)\n        leftmask &= rightmask;\n    late = (ss + bytewidth >= src->data + ((src->width+7)>>3));\n    ss += syoffset;\n\n    switch(op)\n    {\n    case JBIG2_COMPOSE_OR:\n        jbig2_image_compose_opt_OR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_AND:\n        jbig2_image_compose_opt_AND(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_XOR:\n        jbig2_image_compose_opt_XOR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_XNOR:\n        jbig2_image_compose_opt_XNOR(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    case JBIG2_COMPOSE_REPLACE:\n        jbig2_image_compose_opt_REPLACE(ss, dd, early, late, leftmask, rightmask, bytewidth, h, shift, dst->stride, src->stride);\n        break;\n    }\n\n    return 0;\n}", "commit_link": "github.com/ArtifexSoftware/jbig2dec/commit/0726320a4b55078e9d8deb590e477d598b3da66e", "file_name": "jbig2_image.c", "vul_type": "cwe-787", "description": "Write a C function to overlay one image onto another at a specified position using a given composition operation."}
{"func_name": "delete_playlists_videos", "func_src_before": "def delete_playlists_videos(playlist_id, db):\n    db.execute(\"DELETE FROM video where playlist_id={playlist_id};\".format(\n        playlist_id=playlist_id))", "func_src_after": "def delete_playlists_videos(playlist_id, db):\n    db.execute(\"DELETE FROM video where playlist_id=%s;\", (playlist_id,))", "commit_link": "github.com/Madmous/playlist/commit/666e52c5f0b8c1f4296e84471637033d9542a7a6", "file_name": "video/video_repository.py", "vul_type": "cwe-089", "description": "Write a Python function to remove all videos from a specific playlist in a database using the playlist ID."}
{"func_name": "self.find_siblings", "func_src_before": "  def self.find_siblings(hierarchy_id, parent_id)\n    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n                        order by string;\")\n          end", "func_src_after": "  def self.find_siblings(hierarchy_id, parent_id)\n    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n                        (select count(*) from hierarchy_entries as h2 where h2.parent_id=h1.id)\n                          as siblings_count,\n                          h1.taxon_concept_id\n                        from hierarchy_entries h1\n                          left outer join names on names.id=name_id\n                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n                        order by string;\")\n\n    else\n      return []\n    end\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 8, "char_start": 430, "char_end": 532, "line": "                        where hierarchy_id=#{hierarchy_id} and parent_id=#{parent_id} and published=1\n"}, {"line_no": 10, "char_start": 575, "char_end": 588, "line": "          end\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 125, "line": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n"}, {"line_no": 3, "char_start": 125, "char_end": 202, "line": "      self.find_by_sql(\"select string as taxon_concept, h1.id, h1.parent_id,\n"}, {"line_no": 9, "char_start": 507, "char_end": 619, "line": "                        where hierarchy_id=#{hierarchy_id.to_i} and parent_id=#{parent_id.to_i} and published=1\n"}, {"line_no": 11, "char_start": 662, "char_end": 663, "line": "\n"}, {"line_no": 12, "char_start": 663, "char_end": 672, "line": "    else\n"}, {"line_no": 13, "char_start": 672, "char_end": 688, "line": "      return []\n"}, {"line_no": 14, "char_start": 688, "char_end": 696, "line": "    end\n"}, {"line_no": 15, "char_start": 696, "char_end": 701, "line": "  end\n"}]}, "char_changes": {"deleted": [{"char_start": 579, "char_end": 583, "chars": "    "}], "added": [{"char_start": 50, "char_end": 127, "chars": "    if (hierarchy_id.to_i.is_a? Integer) && (parent_id.to_i.is_a? Integer)\n  "}, {"char_start": 564, "char_end": 569, "chars": ".to_i"}, {"char_start": 596, "char_end": 601, "chars": ".to_i"}, {"char_start": 662, "char_end": 663, "chars": "\n"}, {"char_start": 667, "char_end": 696, "chars": "else\n      return []\n    end\n"}]}, "commit_link": "github.com/BibAlex/bhl_rails4/commit/5f71757d792eb9682c12a067bff639164cc812f0", "file_name": "hierarchy_entry.rb", "vul_type": "cwe-089", "commit_msg": "Fixing SQL Injection possibility", "description": "Write a Ruby method to query a database for sibling entries based on a hierarchy ID and a parent ID."}
{"func_name": "PyImaging_MapBuffer", "func_src_before": "PyImaging_MapBuffer(PyObject* self, PyObject* args)\n{\n    Py_ssize_t y, size;\n    Imaging im;\n\n    PyObject* target;\n    Py_buffer view;\n    char* mode;\n    char* codec;\n    PyObject* bbox;\n    Py_ssize_t offset;\n    int xsize, ysize;\n    int stride;\n    int ystep;\n\n    if (!PyArg_ParseTuple(args, \"O(ii)sOn(sii)\", &target, &xsize, &ysize,\n                          &codec, &bbox, &offset, &mode, &stride, &ystep))\n        return NULL;\n\n    if (!PyImaging_CheckBuffer(target)) {\n        PyErr_SetString(PyExc_TypeError, \"expected string or buffer\");\n        return NULL;\n    }\n\n    if (stride <= 0) {\n        if (!strcmp(mode, \"L\") || !strcmp(mode, \"P\"))\n            stride = xsize;\n        else if (!strncmp(mode, \"I;16\", 4))\n            stride = xsize * 2;\n        else\n            stride = xsize * 4;\n    }\n\n    size = (Py_ssize_t) ysize * stride;\n\n    /* check buffer size */\n    if (PyImaging_GetBuffer(target, &view) < 0)\n        return NULL;\n\n    if (view.len < 0) {\n        PyErr_SetString(PyExc_ValueError, \"buffer has negative size\");\n        return NULL;\n    }\n    if (offset + size > view.len) {\n        PyErr_SetString(PyExc_ValueError, \"buffer is not large enough\");\n        return NULL;\n    }\n\n    im = ImagingNewPrologueSubtype(\n        mode, xsize, ysize, sizeof(ImagingBufferInstance)\n        );\n    if (!im)\n        return NULL;\n\n    /* setup file pointers */\n    if (ystep > 0)\n        for (y = 0; y < ysize; y++)\n            im->image[y] = (char*)view.buf + offset + y * stride;\n    else\n        for (y = 0; y < ysize; y++)\n            im->image[ysize-y-1] = (char*)view.buf + offset + y * stride;\n\n    im->destroy = mapping_destroy_buffer;\n\n    Py_INCREF(target);\n    ((ImagingBufferInstance*) im)->target = target;\n    ((ImagingBufferInstance*) im)->view = view;\n\n    if (!ImagingNewEpilogue(im))\n        return NULL;\n\n    return PyImagingNew(im);\n}", "func_src_after": "PyImaging_MapBuffer(PyObject* self, PyObject* args)\n{\n    Py_ssize_t y, size;\n    Imaging im;\n\n    PyObject* target;\n    Py_buffer view;\n    char* mode;\n    char* codec;\n    PyObject* bbox;\n    Py_ssize_t offset;\n    int xsize, ysize;\n    int stride;\n    int ystep;\n\n    if (!PyArg_ParseTuple(args, \"O(ii)sOn(sii)\", &target, &xsize, &ysize,\n                          &codec, &bbox, &offset, &mode, &stride, &ystep))\n        return NULL;\n\n    if (!PyImaging_CheckBuffer(target)) {\n        PyErr_SetString(PyExc_TypeError, \"expected string or buffer\");\n        return NULL;\n    }\n\n    if (stride <= 0) {\n        if (!strcmp(mode, \"L\") || !strcmp(mode, \"P\"))\n            stride = xsize;\n        else if (!strncmp(mode, \"I;16\", 4))\n            stride = xsize * 2;\n        else\n            stride = xsize * 4;\n    }\n\n    if (ysize > INT_MAX / stride) {\n        PyErr_SetString(PyExc_MemoryError, \"Integer overflow in ysize\");\n        return NULL;\n    }\n\n    size = (Py_ssize_t) ysize * stride;\n\n    if (offset > SIZE_MAX - size) {\n        PyErr_SetString(PyExc_MemoryError, \"Integer overflow in offset\");\n        return NULL;\n    }        \n\n    /* check buffer size */\n    if (PyImaging_GetBuffer(target, &view) < 0)\n        return NULL;\n\n    if (view.len < 0) {\n        PyErr_SetString(PyExc_ValueError, \"buffer has negative size\");\n        return NULL;\n    }\n    if (offset + size > view.len) {\n        PyErr_SetString(PyExc_ValueError, \"buffer is not large enough\");\n        return NULL;\n    }\n\n    im = ImagingNewPrologueSubtype(\n        mode, xsize, ysize, sizeof(ImagingBufferInstance)\n        );\n    if (!im)\n        return NULL;\n\n    /* setup file pointers */\n    if (ystep > 0)\n        for (y = 0; y < ysize; y++)\n            im->image[y] = (char*)view.buf + offset + y * stride;\n    else\n        for (y = 0; y < ysize; y++)\n            im->image[ysize-y-1] = (char*)view.buf + offset + y * stride;\n\n    im->destroy = mapping_destroy_buffer;\n\n    Py_INCREF(target);\n    ((ImagingBufferInstance*) im)->target = target;\n    ((ImagingBufferInstance*) im)->view = view;\n\n    if (!ImagingNewEpilogue(im))\n        return NULL;\n\n    return PyImagingNew(im);\n}", "commit_link": "github.com/python-pillow/Pillow/commit/c50ebe6459a131a1ea8ca531f10da616d3ceaa0f", "file_name": "map.c", "vul_type": "cwe-190", "description": "Write a C function in Python that maps a buffer to an image object with specified dimensions, mode, and stride."}
{"func_name": "get_previous_yields", "func_src_before": "    def get_previous_yields(self, inverter_serial):\n        query = '''\n           SELECT TimeStamp, EToday, ETotal\n           FROM Inverters\n           WHERE Serial = '%s'\n        ''' % (inverter_serial)\n        self.c.execute(query)\n        data = self.c.fetchone()\n        return data[0], data[1], data[2]", "func_src_after": "    def get_previous_yields(self, inverter_serial):\n        query = '''\n           SELECT TimeStamp, EToday, ETotal\n           FROM Inverters\n           WHERE Serial=?\n        '''\n        self.c.execute(query, (inverter_serial,))\n        data = self.c.fetchone()\n        return data[0], data[1], data[2]", "commit_link": "github.com/philipptrenz/s0-bridge/commit/269b48caa05377b7c58c3e6d1622a4429cb5ba65", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch the timestamp and energy yields of today and total from a database for a given inverter serial number."}
{"func_name": "lha_read_file_header_1", "func_src_before": "lha_read_file_header_1(struct archive_read *a, struct lha *lha)\n{\n\tconst unsigned char *p;\n\tsize_t extdsize;\n\tint i, err, err2;\n\tint namelen, padding;\n\tunsigned char headersum, sum_calculated;\n\n\terr = ARCHIVE_OK;\n\n\tif ((p = __archive_read_ahead(a, H1_FIXED_SIZE, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tlha->header_size = p[H1_HEADER_SIZE_OFFSET] + 2;\n\theadersum = p[H1_HEADER_SUM_OFFSET];\n\t/* Note: An extended header size is included in a compsize. */\n\tlha->compsize = archive_le32dec(p + H1_COMP_SIZE_OFFSET);\n\tlha->origsize = archive_le32dec(p + H1_ORIG_SIZE_OFFSET);\n\tlha->mtime = lha_dos_time(p + H1_DOS_TIME_OFFSET);\n\tnamelen = p[H1_NAME_LEN_OFFSET];\n\t/* Calculate a padding size. The result will be normally 0 only(?) */\n\tpadding = ((int)lha->header_size) - H1_FIXED_SIZE - namelen;\n\n\tif (namelen > 230 || padding < 0)\n\t\tgoto invalid;\n\n\tif ((p = __archive_read_ahead(a, lha->header_size, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tfor (i = 0; i < namelen; i++) {\n\t\tif (p[i + H1_FILE_NAME_OFFSET] == 0xff)\n\t\t\tgoto invalid;/* Invalid filename. */\n\t}\n\tarchive_strncpy(&lha->filename, p + H1_FILE_NAME_OFFSET, namelen);\n\tlha->crc = archive_le16dec(p + H1_FILE_NAME_OFFSET + namelen);\n\tlha->setflag |= CRC_IS_SET;\n\n\tsum_calculated = lha_calcsum(0, p, 2, lha->header_size - 2);\n\t/* Consume used bytes but not include `next header size' data\n\t * since it will be consumed in lha_read_file_extended_header(). */\n\t__archive_read_consume(a, lha->header_size - 2);\n\n\t/* Read extended headers */\n\terr2 = lha_read_file_extended_header(a, lha, NULL, 2,\n\t    (size_t)(lha->compsize + 2), &extdsize);\n\tif (err2 < ARCHIVE_WARN)\n\t\treturn (err2);\n\tif (err2 < err)\n\t\terr = err2;\n\t/* Get a real compressed file size. */\n\tlha->compsize -= extdsize - 2;\n\n\tif (sum_calculated != headersum) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_MISC,\n\t\t    \"LHa header sum error\");\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\treturn (err);\ninvalid:\n\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t    \"Invalid LHa header\");\n\treturn (ARCHIVE_FATAL);\n}", "func_src_after": "lha_read_file_header_1(struct archive_read *a, struct lha *lha)\n{\n\tconst unsigned char *p;\n\tsize_t extdsize;\n\tint i, err, err2;\n\tint namelen, padding;\n\tunsigned char headersum, sum_calculated;\n\n\terr = ARCHIVE_OK;\n\n\tif ((p = __archive_read_ahead(a, H1_FIXED_SIZE, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tlha->header_size = p[H1_HEADER_SIZE_OFFSET] + 2;\n\theadersum = p[H1_HEADER_SUM_OFFSET];\n\t/* Note: An extended header size is included in a compsize. */\n\tlha->compsize = archive_le32dec(p + H1_COMP_SIZE_OFFSET);\n\tlha->origsize = archive_le32dec(p + H1_ORIG_SIZE_OFFSET);\n\tlha->mtime = lha_dos_time(p + H1_DOS_TIME_OFFSET);\n\tnamelen = p[H1_NAME_LEN_OFFSET];\n\t/* Calculate a padding size. The result will be normally 0 only(?) */\n\tpadding = ((int)lha->header_size) - H1_FIXED_SIZE - namelen;\n\n\tif (namelen > 230 || padding < 0)\n\t\tgoto invalid;\n\n\tif ((p = __archive_read_ahead(a, lha->header_size, NULL)) == NULL)\n\t\treturn (truncated_error(a));\n\n\tfor (i = 0; i < namelen; i++) {\n\t\tif (p[i + H1_FILE_NAME_OFFSET] == 0xff)\n\t\t\tgoto invalid;/* Invalid filename. */\n\t}\n\tarchive_strncpy(&lha->filename, p + H1_FILE_NAME_OFFSET, namelen);\n\tlha->crc = archive_le16dec(p + H1_FILE_NAME_OFFSET + namelen);\n\tlha->setflag |= CRC_IS_SET;\n\n\tsum_calculated = lha_calcsum(0, p, 2, lha->header_size - 2);\n\t/* Consume used bytes but not include `next header size' data\n\t * since it will be consumed in lha_read_file_extended_header(). */\n\t__archive_read_consume(a, lha->header_size - 2);\n\n\t/* Read extended headers */\n\terr2 = lha_read_file_extended_header(a, lha, NULL, 2,\n\t    (size_t)(lha->compsize + 2), &extdsize);\n\tif (err2 < ARCHIVE_WARN)\n\t\treturn (err2);\n\tif (err2 < err)\n\t\terr = err2;\n\t/* Get a real compressed file size. */\n\tlha->compsize -= extdsize - 2;\n\n\tif (lha->compsize < 0)\n\t\tgoto invalid;\t/* Invalid compressed file size */\n\n\tif (sum_calculated != headersum) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_MISC,\n\t\t    \"LHa header sum error\");\n\t\treturn (ARCHIVE_FATAL);\n\t}\n\treturn (err);\ninvalid:\n\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t    \"Invalid LHa header\");\n\treturn (ARCHIVE_FATAL);\n}", "commit_link": "github.com/libarchive/libarchive/commit/98dcbbf0bf4854bf987557e55e55fff7abbf3ea9", "file_name": "libarchive/archive_read_support_format_lha.c", "vul_type": "cwe-125", "description": "Write a C function `lha_read_file_header_1` to read and validate the header of an LHA file archive."}
{"func_name": "tag_num_to_tag", "func_src_before": "    def tag_num_to_tag(self, tag_num):\n        ''' Returns tag given tag_num. '''\n\n        q = \"SELECT tag FROM tags WHERE rowid = '\" + str(tag_num) + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]", "func_src_after": "    def tag_num_to_tag(self, tag_num):\n        ''' Returns tag given tag_num. '''\n\n        q = \"SELECT tag FROM tags WHERE rowid = ?\"\n        self.query(q, tag_num)\n        return self.c.fetchone()[0]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves a tag from a database using a tag number as input."}
{"func_name": "read_primary_locale_file", "func_src_before": "      def read_primary_locale_file\n        primary_file = \"#{self.locales_config_path}/#{self.primary_locale_name}.yml\"\n        File.exists?(primary_file) ? flat_hash(YAML::load(IO.read(primary_file))[self.primary_locale_name]) : {}\n      end", "func_src_after": "      def read_primary_locale_file\n        primary_file = \"#{self.locales_config_path}/#{self.primary_locale_name}.yml\"\n        File.exists?(primary_file) ? flat_hash(YAML::safe_load(IO.read(primary_file))[self.primary_locale_name]) : {}\n      end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 120, "char_end": 233, "line": "        File.exists?(primary_file) ? flat_hash(YAML::load(IO.read(primary_file))[self.primary_locale_name]) : {}\n"}], "added": [{"line_no": 3, "char_start": 120, "char_end": 238, "line": "        File.exists?(primary_file) ? flat_hash(YAML::safe_load(IO.read(primary_file))[self.primary_locale_name]) : {}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 173, "char_end": 178, "chars": "safe_"}]}, "commit_link": "github.com/MyMedsAndMe/tolk/commit/625237a3df7a64b496374f38f27c774b56e2a582", "file_name": "sync.rb", "vul_type": "cwe-502", "commit_msg": "Use YAML's `safe_load` instead of `load`.", "parent_commit": "b5b66f923436a1c0fce47ae68d5da05323039d61", "description": "Write a Ruby method to read and flatten the contents of a primary locale YAML file if it exists, returning an empty hash otherwise."}
{"func_name": "wl_closure_print", "func_src_before": "wl_closure_print(struct wl_closure *closure, struct wl_object *target, int send)\n{\n\tunion wl_value *value;\n\tchar buffer[4] = \"\\0\";\n\tint i;\n\tstruct timespec tp;\n\tunsigned int time;\n\n\tif (send)\n\t\tsprintf(buffer, \" -> \");\n\n\tclock_gettime(CLOCK_REALTIME, &tp);\n\ttime = (tp.tv_sec * 1000000L) + (tp.tv_nsec / 1000);\n\n\tfprintf(stderr, \"[%10.3f] %s%s@%d.%s(\",\n\t\ttime / 1000.0,\n\t\tbuffer,\n\t\ttarget->interface->name, target->id,\n\t\tclosure->message->name);\n\n\tfor (i = 2; i < closure->count; i++) {\n\t\tif (i > 2)\n\t\t\tfprintf(stderr, \", \");\n\n\t\tvalue = closure->args[i];\n\t\tswitch (closure->message->signature[i - 2]) {\n\t\tcase 'u':\n\t\t\tfprintf(stderr, \"%u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfprintf(stderr, \"%d\", value->uint32);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tfprintf(stderr, \"\\\"%s\\\"\", value->string);\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tif (value->object)\n\t\t\t\tfprintf(stderr, \"%s@%u\",\n\t\t\t\t\tvalue->object->interface->name,\n\t\t\t\t\tvalue->object->id);\n\t\t\telse\n\t\t\t\tfprintf(stderr, \"nil\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tfprintf(stderr, \"new id %u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tfprintf(stderr, \"array\");\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\tfprintf(stderr, \"fd %d\", value->uint32);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfprintf(stderr, \")\\n\");\n}", "func_src_after": "wl_closure_print(struct wl_closure *closure, struct wl_object *target, int send)\n{\n\tunion wl_value *value;\n\tint i;\n\tstruct timespec tp;\n\tunsigned int time;\n\n\tclock_gettime(CLOCK_REALTIME, &tp);\n\ttime = (tp.tv_sec * 1000000L) + (tp.tv_nsec / 1000);\n\n\tfprintf(stderr, \"[%10.3f] %s%s@%d.%s(\",\n\t\ttime / 1000.0,\n\t\tsend ? \" -> \" : \"\",\n\t\ttarget->interface->name, target->id,\n\t\tclosure->message->name);\n\n\tfor (i = 2; i < closure->count; i++) {\n\t\tif (i > 2)\n\t\t\tfprintf(stderr, \", \");\n\n\t\tvalue = closure->args[i];\n\t\tswitch (closure->message->signature[i - 2]) {\n\t\tcase 'u':\n\t\t\tfprintf(stderr, \"%u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfprintf(stderr, \"%d\", value->uint32);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tfprintf(stderr, \"\\\"%s\\\"\", value->string);\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tif (value->object)\n\t\t\t\tfprintf(stderr, \"%s@%u\",\n\t\t\t\t\tvalue->object->interface->name,\n\t\t\t\t\tvalue->object->id);\n\t\t\telse\n\t\t\t\tfprintf(stderr, \"nil\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tfprintf(stderr, \"new id %u\", value->uint32);\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tfprintf(stderr, \"array\");\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\tfprintf(stderr, \"fd %d\", value->uint32);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfprintf(stderr, \")\\n\");\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 107, "char_end": 131, "line": "\tchar buffer[4] = \"\\0\";\n"}, {"line_no": 9, "char_start": 181, "char_end": 192, "line": "\tif (send)\n"}, {"line_no": 10, "char_start": 192, "char_end": 219, "line": "\t\tsprintf(buffer, \" -> \");\n"}, {"line_no": 11, "char_start": 219, "char_end": 220, "line": "\n"}, {"line_no": 17, "char_start": 370, "char_end": 380, "line": "\t\tbuffer,\n"}], "added": [{"line_no": 13, "char_start": 307, "char_end": 329, "line": "\t\tsend ? \" -> \" : \"\",\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 131, "chars": "\tchar buffer[4] = \"\\0\";\n"}, {"char_start": 181, "char_end": 220, "chars": "\tif (send)\n\t\tsprintf(buffer, \" -> \");\n\n"}, {"char_start": 372, "char_end": 378, "chars": "buffer"}], "added": [{"char_start": 309, "char_end": 327, "chars": "send ? \" -> \" : \"\""}]}, "commit_link": "github.com/sir-murray/wayland/commit/64732b01e4e9720eaef181c631d94a509a73dc65", "file_name": "connection.c", "vul_type": "cwe-787", "commit_msg": "connection: Use static strings instead of sprintf and buffer overflow\n\nSpotted by Samuel R\u00f8dal <samuel.rodal@nokia.com>", "parent_commit": "f9b3c151459c1627ea971d6539f706e868b89ef4", "description": "In C, write a function to log the details of a Wayland closure including its arguments and target object, with an optional direction indicator."}
{"func_name": "igraph_read_graph_graphml", "func_src_before": "int igraph_read_graph_graphml(igraph_t *graph, FILE *instream, int index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < sizeof(buffer) && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data\", IGRAPH_PARSEERROR);\n    }\n    ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                   &state,\n                                   buffer,\n                                   res,\n                                   NULL);\n    /*   ctxt=xmlCreateIOParserCtxt(&igraph_i_graphml_sax_handler, &state, */\n    /*               igraph_i_libxml2_read_callback, */\n    /*               igraph_i_libxml2_close_callback, */\n    /*               instream, XML_CHAR_ENCODING_NONE); */\n    if (ctxt == NULL) {\n        IGRAPH_ERROR(\"Can't create progressive parser context\", IGRAPH_PARSEERROR);\n    }\n\n    /* Set parsing options */\n    if (xmlCtxtUseOptions(ctxt,\n                          XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n                          XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                          XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                         )) {\n        xmlFreeParserCtxt(ctxt);\n        IGRAPH_ERROR(\"Cannot set options for the parser context\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we need to pop off\n     * igraph_i_graphml_parser_state_destroy() from the stack and temporarily\n     * assume responsibility for calling it ourselves until we are back from the\n     * parser */\n    IGRAPH_FINALLY_CLEAN(1);\n\n    /* Do the parsing */\n    while ((res = (int) fread(buffer, 1, sizeof(buffer), instream)) > 0) {\n        xmlParseChunk(ctxt, buffer, res, 0);\n        if (!state.successful) {\n            break;\n        }\n    }\n    xmlParseChunk(ctxt, buffer, res, 1);\n\n    /* Free the context */\n    doc = ctxt->myDoc;\n    xmlFreeParserCtxt(ctxt);\n    if (doc) {\n        /* In theory this should not be necessary, but it looks like certain malformed\n         * GraphML files leave a partially-parsed doc in memory */\n        xmlFreeDoc(doc);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* Now that we have lifted error_message out of the parser state, we can\n     * put the destructor of the parser state back on the FINALLY stack */\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "func_src_after": "int igraph_read_graph_graphml(igraph_t *graph, FILE *instream, int index) {\n\n#if HAVE_LIBXML == 1\n    xmlParserCtxtPtr ctxt;\n    xmlDocPtr doc;\n\n    struct igraph_i_graphml_parser_state state;\n    int res;\n    char buffer[4096];\n    igraph_bool_t parsing_successful;\n    char* error_message;\n\n    if (index < 0) {\n        IGRAPH_ERROR(\"Graph index must be non-negative\", IGRAPH_EINVAL);\n    }\n\n    xmlInitParser();\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_init(&state, graph, index));\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* Create a progressive parser context */\n    res = (int) fread(buffer, 1, sizeof(buffer), instream);\n    if (res < sizeof(buffer) && !feof(instream)) {\n        IGRAPH_ERROR(\"IO error while reading GraphML data\", IGRAPH_PARSEERROR);\n    }\n    ctxt = xmlCreatePushParserCtxt(&igraph_i_graphml_sax_handler,\n                                   &state,\n                                   buffer,\n                                   res,\n                                   NULL);\n    /*   ctxt=xmlCreateIOParserCtxt(&igraph_i_graphml_sax_handler, &state, */\n    /*               igraph_i_libxml2_read_callback, */\n    /*               igraph_i_libxml2_close_callback, */\n    /*               instream, XML_CHAR_ENCODING_NONE); */\n    if (ctxt == NULL) {\n        IGRAPH_ERROR(\"Can't create progressive parser context\", IGRAPH_PARSEERROR);\n    }\n\n    /* Set parsing options */\n    if (xmlCtxtUseOptions(ctxt,\n                          XML_PARSE_NOBLANKS |\n                          XML_PARSE_NONET | XML_PARSE_NSCLEAN |\n                          XML_PARSE_NOCDATA | XML_PARSE_HUGE\n                         )) {\n        xmlFreeParserCtxt(ctxt);\n        IGRAPH_ERROR(\"Cannot set options for the parser context\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, parsing will start now. The parser might do things that eventually\n     * trigger the igraph error handler, but we want the parser state to\n     * survive whatever happens here. So, we need to pop off\n     * igraph_i_graphml_parser_state_destroy() from the stack and temporarily\n     * assume responsibility for calling it ourselves until we are back from the\n     * parser */\n    IGRAPH_FINALLY_CLEAN(1);\n\n    /* Do the parsing */\n    while ((res = (int) fread(buffer, 1, sizeof(buffer), instream)) > 0) {\n        xmlParseChunk(ctxt, buffer, res, 0);\n        if (!state.successful) {\n            break;\n        }\n    }\n    xmlParseChunk(ctxt, buffer, res, 1);\n\n    /* Free the context */\n    doc = ctxt->myDoc;\n    xmlFreeParserCtxt(ctxt);\n    if (doc) {\n        /* In theory this should not be necessary, but it looks like certain malformed\n         * GraphML files leave a partially-parsed doc in memory */\n        xmlFreeDoc(doc);\n    }\n\n    /* Extract the error message from the parser state (if any), and make a\n     * copy so we can safely destroy the parser state before triggering the\n     * error */\n    parsing_successful = state.successful;\n    error_message = parsing_successful || state.error_message == NULL ? NULL : strdup(state.error_message);\n\n    /* Now that we have lifted error_message out of the parser state, we can\n     * put the destructor of the parser state back on the FINALLY stack */\n    IGRAPH_FINALLY(igraph_i_graphml_parser_state_destroy, &state);\n\n    /* ...and we can also put the error message pointer on the FINALLY stack */\n    if (error_message != NULL) {\n        IGRAPH_FINALLY(free, error_message);\n    }\n\n    /* Trigger the stored error if needed */\n    if (!parsing_successful) {\n        if (error_message != NULL) {\n            IGRAPH_ERROR(error_message, IGRAPH_PARSEERROR);\n        } else {\n            IGRAPH_ERROR(\"Malformed GraphML file\", IGRAPH_PARSEERROR);\n        }\n    }\n\n    /* Did we actually manage to reach the graph to be parsed, given its index?\n     * If not, that's an error as well. */\n    if (state.index >= 0) {\n        IGRAPH_ERROR(\"Graph index was too large\", IGRAPH_EINVAL);\n    }\n\n    /* Okay, everything seems good. We can now take the parser state and\n     * construct our graph from the data gathered during the parsing */\n    IGRAPH_CHECK(igraph_i_graphml_parser_state_finish_parsing(&state));\n\n    igraph_i_graphml_parser_state_destroy(&state);\n    IGRAPH_FINALLY_CLEAN(1);\n\n    return IGRAPH_SUCCESS;\n#else\n    IGRAPH_UNUSED(graph);\n    IGRAPH_UNUSED(instream);\n    IGRAPH_UNUSED(index);\n\n    IGRAPH_ERROR(\"GraphML support is disabled\", IGRAPH_UNIMPLEMENTED);\n#endif\n}", "line_changes": {"deleted": [{"line_no": 41, "char_start": 1463, "char_end": 1528, "line": "                          XML_PARSE_NOENT | XML_PARSE_NOBLANKS |\n"}], "added": [{"line_no": 41, "char_start": 1463, "char_end": 1510, "line": "                          XML_PARSE_NOBLANKS |\n"}]}, "char_changes": {"deleted": [{"char_start": 1488, "char_end": 1506, "chars": " XML_PARSE_NOENT |"}], "added": []}, "commit_link": "github.com/igraph/igraph/commit/6ce2353fa77a891d1b556b8908ca9e4c227c3619", "file_name": "graphml.c", "vul_type": "cwe-611", "commit_msg": "fix: disable external XML entity resolution", "parent_commit": "cb8f28bae4c5ab92e5b628d9b4f827d3c61ba6ce", "description": "Write a C function to parse a GraphML file into an igraph_t structure using libxml2."}
{"func_name": "rf_host_ver", "func_src_before": "static void rf_host_ver(void)\n{\n  char *buf = (char *)cur_slot->readbuf;\n  int major = 3, minor = 3;\n  int remote_major, remote_minor;\n  char ver_msg[12];\n\n  if ( strncmp(buf, \"RFB \", 4) != 0 || !isdigit(buf[4]) ||\n       !isdigit(buf[4]) || !isdigit(buf[5]) || !isdigit(buf[6]) ||\n       buf[7] != '.' || !isdigit(buf[8]) || !isdigit(buf[9]) ||\n       !isdigit(buf[10]) || buf[11] != '\\n' ) {\n    log_write(LL_ERROR, \"Wrong greeting data received from host\");\n    aio_close(0);\n    return;\n  }\n\n  remote_major = atoi(&buf[4]);\n  remote_minor = atoi(&buf[8]);\n  log_write(LL_INFO, \"Remote RFB Protocol version is %d.%d\",\n            remote_major, remote_minor);\n\n  if (remote_major != major) {\n    log_write(LL_ERROR, \"Wrong protocol version, expected %d.x\", major);\n    aio_close(0);\n    return;\n  } else if (remote_minor != minor) {\n    log_write(LL_WARN, \"Protocol sub-version does not match (ignoring)\");\n  }\n\n  sprintf(ver_msg, \"RFB %03d.%03d\\n\", abs(major) % 999, abs(minor) % 999);\n  aio_write(NULL, ver_msg, 12);\n  aio_setread(rf_host_auth, NULL, 4);\n}", "func_src_after": "static void rf_host_ver(void)\n{\n  char *buf = (char *)cur_slot->readbuf;\n  int major = 3, minor = 3;\n  int remote_major, remote_minor;\n  char ver_msg[16];\n\n  if ( strncmp(buf, \"RFB \", 4) != 0 || !isdigit(buf[4]) ||\n       !isdigit(buf[4]) || !isdigit(buf[5]) || !isdigit(buf[6]) ||\n       buf[7] != '.' || !isdigit(buf[8]) || !isdigit(buf[9]) ||\n       !isdigit(buf[10]) || buf[11] != '\\n' ) {\n    log_write(LL_ERROR, \"Wrong greeting data received from host\");\n    aio_close(0);\n    return;\n  }\n\n  remote_major = atoi(&buf[4]);\n  remote_minor = atoi(&buf[8]);\n  log_write(LL_INFO, \"Remote RFB Protocol version is %d.%d\",\n            remote_major, remote_minor);\n\n  if (remote_major != major) {\n    log_write(LL_ERROR, \"Wrong protocol version, expected %d.x\", major);\n    aio_close(0);\n    return;\n  } else if (remote_minor != minor) {\n    log_write(LL_WARN, \"Protocol sub-version does not match (ignoring)\");\n  }\n\n  sprintf(ver_msg, \"RFB %03d.%03d\\n\", abs(major) % 999, abs(minor) % 999);\n  aio_write(NULL, ver_msg, 12);\n  aio_setread(rf_host_auth, NULL, 4);\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 135, "char_end": 155, "line": "  char ver_msg[12];\n"}], "added": [{"line_no": 6, "char_start": 135, "char_end": 155, "line": "  char ver_msg[16];\n"}]}, "char_changes": {"deleted": [{"char_start": 151, "char_end": 152, "chars": "2"}], "added": [{"char_start": 151, "char_end": 152, "chars": "6"}]}, "commit_link": "github.com/inovex/vnc-reflector/commit/50a7d41c56a3f169296349853d6149183c32d6bb", "file_name": "host_connect.c", "vul_type": "cwe-787", "commit_msg": "A one-byte buffer overflow has been fixed.", "parent_commit": "40f1587ac621b9fbebee3525397ac0cdbb211344", "description": "In C, write a function to validate and log the remote host's RFB protocol version, then respond with the server's version."}
{"func_name": "unimac_mdio_probe", "func_src_before": "static int unimac_mdio_probe(struct platform_device *pdev)\n{\n\tstruct unimac_mdio_pdata *pdata = pdev->dev.platform_data;\n\tstruct unimac_mdio_priv *priv;\n\tstruct device_node *np;\n\tstruct mii_bus *bus;\n\tstruct resource *r;\n\tint ret;\n\n\tnp = pdev->dev.of_node;\n\n\tpriv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tr = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\n\t/* Just ioremap, as this MDIO block is usually integrated into an\n\t * Ethernet MAC controller register range\n\t */\n\tpriv->base = devm_ioremap(&pdev->dev, r->start, resource_size(r));\n\tif (!priv->base) {\n\t\tdev_err(&pdev->dev, \"failed to remap register\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv->mii_bus = mdiobus_alloc();\n\tif (!priv->mii_bus)\n\t\treturn -ENOMEM;\n\n\tbus = priv->mii_bus;\n\tbus->priv = priv;\n\tif (pdata) {\n\t\tbus->name = pdata->bus_name;\n\t\tpriv->wait_func = pdata->wait_func;\n\t\tpriv->wait_func_data = pdata->wait_func_data;\n\t\tbus->phy_mask = ~pdata->phy_mask;\n\t} else {\n\t\tbus->name = \"unimac MII bus\";\n\t\tpriv->wait_func_data = priv;\n\t\tpriv->wait_func = unimac_mdio_poll;\n\t}\n\tbus->parent = &pdev->dev;\n\tbus->read = unimac_mdio_read;\n\tbus->write = unimac_mdio_write;\n\tbus->reset = unimac_mdio_reset;\n\tsnprintf(bus->id, MII_BUS_ID_SIZE, \"%s-%d\", pdev->name, pdev->id);\n\n\tret = of_mdiobus_register(bus, np);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"MDIO bus registration failed\\n\");\n\t\tgoto out_mdio_free;\n\t}\n\n\tplatform_set_drvdata(pdev, priv);\n\n\tdev_info(&pdev->dev, \"Broadcom UniMAC MDIO bus at 0x%p\\n\", priv->base);\n\n\treturn 0;\n\nout_mdio_free:\n\tmdiobus_free(bus);\n\treturn ret;\n}", "func_src_after": "static int unimac_mdio_probe(struct platform_device *pdev)\n{\n\tstruct unimac_mdio_pdata *pdata = pdev->dev.platform_data;\n\tstruct unimac_mdio_priv *priv;\n\tstruct device_node *np;\n\tstruct mii_bus *bus;\n\tstruct resource *r;\n\tint ret;\n\n\tnp = pdev->dev.of_node;\n\n\tpriv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tr = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!r)\n\t\treturn -EINVAL;\n\n\t/* Just ioremap, as this MDIO block is usually integrated into an\n\t * Ethernet MAC controller register range\n\t */\n\tpriv->base = devm_ioremap(&pdev->dev, r->start, resource_size(r));\n\tif (!priv->base) {\n\t\tdev_err(&pdev->dev, \"failed to remap register\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv->mii_bus = mdiobus_alloc();\n\tif (!priv->mii_bus)\n\t\treturn -ENOMEM;\n\n\tbus = priv->mii_bus;\n\tbus->priv = priv;\n\tif (pdata) {\n\t\tbus->name = pdata->bus_name;\n\t\tpriv->wait_func = pdata->wait_func;\n\t\tpriv->wait_func_data = pdata->wait_func_data;\n\t\tbus->phy_mask = ~pdata->phy_mask;\n\t} else {\n\t\tbus->name = \"unimac MII bus\";\n\t\tpriv->wait_func_data = priv;\n\t\tpriv->wait_func = unimac_mdio_poll;\n\t}\n\tbus->parent = &pdev->dev;\n\tbus->read = unimac_mdio_read;\n\tbus->write = unimac_mdio_write;\n\tbus->reset = unimac_mdio_reset;\n\tsnprintf(bus->id, MII_BUS_ID_SIZE, \"%s-%d\", pdev->name, pdev->id);\n\n\tret = of_mdiobus_register(bus, np);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"MDIO bus registration failed\\n\");\n\t\tgoto out_mdio_free;\n\t}\n\n\tplatform_set_drvdata(pdev, priv);\n\n\tdev_info(&pdev->dev, \"Broadcom UniMAC MDIO bus at 0x%p\\n\", priv->base);\n\n\treturn 0;\n\nout_mdio_free:\n\tmdiobus_free(bus);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/297a6961ffb8ff4dc66c9fbf53b924bd1dda05d5", "file_name": "drivers/net/phy/mdio-bcm-unimac.c", "vul_type": "cwe-476", "description": "Write a C function for probing a Broadcom UniMAC MDIO bus in a Linux kernel driver."}
{"func_name": "update_title", "func_src_before": "  def update_title(self, title = None):\n    if (not self.title):\n      self.title = title\n\n    # This will fall to a sql injection \n    sql = \"UPDATE jdk_entries SET title = '\" + self.title + \"'\" + \\\n          \"WHERE jdk_entries.id = '\" + self.entry_id + \"';\" \n\n    db_execute(sql)\n    \n    self.update_date_modified()\n\n    return None", "func_src_after": "  def update_title(self, title = None):\n    if (not self.title):\n      self.title = title\n\n    quote_tuple = self.title, self.entry_id\n\n    # This will fall to a sql injection \n    sql = \"UPDATE jdk_entries SET title = ?\" + \\\n          \"WHERE jdk_entries.id = ?;\" \n\n    db_execute(sql, quote_tuple)\n    \n    self.update_date_modified()\n\n    return None", "commit_link": "github.com/peterlebrun/jdk/commit/000238566fbe55ba09676c3d57af04ae207235ae", "file_name": "entry.py", "vul_type": "cwe-089", "description": "Write a Python function to update the title of a database entry with basic SQL injection vulnerability and a version with parameterized query protection."}
{"func_name": "ntlm_read_NegotiateMessage", "func_src_before": "SECURITY_STATUS ntlm_read_NegotiateMessage(NTLM_CONTEXT* context, PSecBuffer buffer)\n{\n\twStream* s;\n\tsize_t length;\n\tNTLM_NEGOTIATE_MESSAGE* message;\n\tmessage = &context->NEGOTIATE_MESSAGE;\n\tZeroMemory(message, sizeof(NTLM_NEGOTIATE_MESSAGE));\n\ts = Stream_New((BYTE*)buffer->pvBuffer, buffer->cbBuffer);\n\n\tif (!s)\n\t\treturn SEC_E_INTERNAL_ERROR;\n\n\tif (ntlm_read_message_header(s, (NTLM_MESSAGE_HEADER*)message) < 0)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->MessageType != MESSAGE_TYPE_NEGOTIATE)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tStream_Read_UINT32(s, message->NegotiateFlags); /* NegotiateFlags (4 bytes) */\n\n\tif (!((message->NegotiateFlags & NTLMSSP_REQUEST_TARGET) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_NTLM) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_UNICODE)))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tcontext->NegotiateFlags = message->NegotiateFlags;\n\n\t/* only set if NTLMSSP_NEGOTIATE_DOMAIN_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->DomainName)) < 0) /* DomainNameFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\t/* only set if NTLMSSP_NEGOTIATE_WORKSTATION_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->Workstation)) < 0) /* WorkstationFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t{\n\t\tif (ntlm_read_version_info(s, &(message->Version)) < 0) /* Version (8 bytes) */\n\t\t{\n\t\t\tStream_Free(s, FALSE);\n\t\t\treturn SEC_E_INVALID_TOKEN;\n\t\t}\n\t}\n\n\tlength = Stream_GetPosition(s);\n\tbuffer->cbBuffer = length;\n\n\tif (!sspi_SecBufferAlloc(&context->NegotiateMessage, length))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INTERNAL_ERROR;\n\t}\n\n\tCopyMemory(context->NegotiateMessage.pvBuffer, buffer->pvBuffer, buffer->cbBuffer);\n\tcontext->NegotiateMessage.BufferType = buffer->BufferType;\n#ifdef WITH_DEBUG_NTLM\n\tWLog_DBG(TAG, \"NEGOTIATE_MESSAGE (length = %\" PRIu32 \")\", context->NegotiateMessage.cbBuffer);\n\twinpr_HexDump(TAG, WLOG_DEBUG, context->NegotiateMessage.pvBuffer,\n\t              context->NegotiateMessage.cbBuffer);\n\tntlm_print_negotiate_flags(message->NegotiateFlags);\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t\tntlm_print_version_info(&(message->Version));\n\n#endif\n\tcontext->state = NTLM_STATE_CHALLENGE;\n\tStream_Free(s, FALSE);\n\treturn SEC_I_CONTINUE_NEEDED;\n}", "func_src_after": "SECURITY_STATUS ntlm_read_NegotiateMessage(NTLM_CONTEXT* context, PSecBuffer buffer)\n{\n\twStream* s;\n\tsize_t length;\n\tNTLM_NEGOTIATE_MESSAGE* message;\n\tmessage = &context->NEGOTIATE_MESSAGE;\n\tZeroMemory(message, sizeof(NTLM_NEGOTIATE_MESSAGE));\n\ts = Stream_New((BYTE*)buffer->pvBuffer, buffer->cbBuffer);\n\n\tif (!s)\n\t\treturn SEC_E_INTERNAL_ERROR;\n\n\tif (ntlm_read_message_header(s, (NTLM_MESSAGE_HEADER*)message) < 0)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->MessageType != MESSAGE_TYPE_NEGOTIATE)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 4)\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\tStream_Read_UINT32(s, message->NegotiateFlags); /* NegotiateFlags (4 bytes) */\n\n\tif (!((message->NegotiateFlags & NTLMSSP_REQUEST_TARGET) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_NTLM) &&\n\t      (message->NegotiateFlags & NTLMSSP_NEGOTIATE_UNICODE)))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tcontext->NegotiateFlags = message->NegotiateFlags;\n\n\t/* only set if NTLMSSP_NEGOTIATE_DOMAIN_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->DomainName)) < 0) /* DomainNameFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\t/* only set if NTLMSSP_NEGOTIATE_WORKSTATION_SUPPLIED is set */\n\n\tif (ntlm_read_message_fields(s, &(message->Workstation)) < 0) /* WorkstationFields (8 bytes) */\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INVALID_TOKEN;\n\t}\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t{\n\t\tif (ntlm_read_version_info(s, &(message->Version)) < 0) /* Version (8 bytes) */\n\t\t{\n\t\t\tStream_Free(s, FALSE);\n\t\t\treturn SEC_E_INVALID_TOKEN;\n\t\t}\n\t}\n\n\tlength = Stream_GetPosition(s);\n\tbuffer->cbBuffer = length;\n\n\tif (!sspi_SecBufferAlloc(&context->NegotiateMessage, length))\n\t{\n\t\tStream_Free(s, FALSE);\n\t\treturn SEC_E_INTERNAL_ERROR;\n\t}\n\n\tCopyMemory(context->NegotiateMessage.pvBuffer, buffer->pvBuffer, buffer->cbBuffer);\n\tcontext->NegotiateMessage.BufferType = buffer->BufferType;\n#ifdef WITH_DEBUG_NTLM\n\tWLog_DBG(TAG, \"NEGOTIATE_MESSAGE (length = %\" PRIu32 \")\", context->NegotiateMessage.cbBuffer);\n\twinpr_HexDump(TAG, WLOG_DEBUG, context->NegotiateMessage.pvBuffer,\n\t              context->NegotiateMessage.cbBuffer);\n\tntlm_print_negotiate_flags(message->NegotiateFlags);\n\n\tif (message->NegotiateFlags & NTLMSSP_NEGOTIATE_VERSION)\n\t\tntlm_print_version_info(&(message->Version));\n\n#endif\n\tcontext->state = NTLM_STATE_CHALLENGE;\n\tStream_Free(s, FALSE);\n\treturn SEC_I_CONTINUE_NEEDED;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/8fa38359634a9910b91719818ab02f23c320dbae", "file_name": "winpr/libwinpr/sspi/NTLM/ntlm_message.c", "vul_type": "cwe-125", "description": "In C, write a function to parse and validate an NTLM Negotiate message from a security buffer."}
{"func_name": "LookupModMask", "func_src_before": "LookupModMask(struct xkb_context *ctx, const void *priv, xkb_atom_t field,\n              enum expr_value_type type, xkb_mod_mask_t *val_rtrn)\n{\n    const char *str;\n    xkb_mod_index_t ndx;\n    const LookupModMaskPriv *arg = priv;\n    const struct xkb_mod_set *mods = arg->mods;\n    enum mod_type mod_type = arg->mod_type;\n\n    if (type != EXPR_TYPE_INT)\n        return false;\n\n    str = xkb_atom_text(ctx, field);\n\n    if (istreq(str, \"all\")) {\n        *val_rtrn  = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    if (istreq(str, \"none\")) {\n        *val_rtrn = 0;\n        return true;\n    }\n\n    ndx = XkbModNameToIndex(mods, field, mod_type);\n    if (ndx == XKB_MOD_INVALID)\n        return false;\n\n    *val_rtrn = (1u << ndx);\n    return true;\n}", "func_src_after": "LookupModMask(struct xkb_context *ctx, const void *priv, xkb_atom_t field,\n              enum expr_value_type type, xkb_mod_mask_t *val_rtrn)\n{\n    const char *str;\n    xkb_mod_index_t ndx;\n    const LookupModMaskPriv *arg = priv;\n    const struct xkb_mod_set *mods = arg->mods;\n    enum mod_type mod_type = arg->mod_type;\n\n    if (type != EXPR_TYPE_INT)\n        return false;\n\n    str = xkb_atom_text(ctx, field);\n    if (!str)\n        return false;\n\n    if (istreq(str, \"all\")) {\n        *val_rtrn  = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    if (istreq(str, \"none\")) {\n        *val_rtrn = 0;\n        return true;\n    }\n\n    ndx = XkbModNameToIndex(mods, field, mod_type);\n    if (ndx == XKB_MOD_INVALID)\n        return false;\n\n    *val_rtrn = (1u << ndx);\n    return true;\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/4e2ee9c3f6050d773f8bbe05bc0edb17f1ff8371", "file_name": "src/xkbcomp/expr.c", "vul_type": "cwe-476", "description": "Write a C function named `LookupModMask` that determines the modifier mask for a given string representation of a keyboard modifier."}
{"func_name": "getAuthenticatedBasket", "func_src_before": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "func_src_after": "func getAuthenticatedBasket(w http.ResponseWriter, r *http.Request, ps httprouter.Params) (string, Basket) {\n\tname := ps.ByName(\"basket\")\n\tif !validBasketName.MatchString(name) {\n\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n\t} else if basket := basketsDb.Get(name); basket != nil {\n\t\t// maybe custom header, e.g. basket_key, basket_token\n\t\tif token := r.Header.Get(\"Authorization\"); basket.Authorize(token) || token == serverConfig.MasterToken {\n\t\t\treturn name, basket\n\t\t}\n\t\tw.WriteHeader(http.StatusUnauthorized)\n\t} else {\n\t\tw.WriteHeader(http.StatusNotFound)\n\t}\n\n\treturn \"\", nil\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 179, "char_end": 303, "line": "\t\thttp.Error(w, \"invalid basket name; [\"+name+\"] does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}], "added": [{"line_no": 4, "char_start": 179, "char_end": 301, "line": "\t\thttp.Error(w, \"invalid basket name; the name does not match pattern: \"+validBasketName.String(), http.StatusBadRequest)\n"}]}, "char_changes": {"deleted": [{"char_start": 217, "char_end": 220, "chars": "[\"+"}, {"char_start": 224, "char_end": 227, "chars": "+\"]"}], "added": [{"char_start": 217, "char_end": 221, "chars": "the "}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go function to authenticate and retrieve a basket by name from a database, handling HTTP requests and responses."}
{"func_name": "retrieve_video", "func_src_before": "def retrieve_video(id, playlist_id, db):\n    db.execute(\"SELECT id, position from video WHERE id={id} and playlist_id={playlist_id};\".format(\n        id=id, playlist_id=playlist_id))\n    row = db.fetchone()\n    return row", "func_src_after": "def retrieve_video(id, playlist_id, db):\n    db.execute(\n        \"SELECT id, position from video WHERE id=%s and playlist_id=%s;\", (id, playlist_id))\n    row = db.fetchone()\n    return row", "commit_link": "github.com/Madmous/playlist/commit/666e52c5f0b8c1f4296e84471637033d9542a7a6", "file_name": "video/video_repository.py", "vul_type": "cwe-089", "description": "Write a Python function named `retrieve_video` that fetches a video's ID and position from a database given a video ID and playlist ID."}
{"func_name": "h2h", "func_src_before": "@endpoints.route(\"/h2h\")\ndef h2h():\n    if db == None:\n        init()\n\n    player1 = request.args.get('tag1', default=\"christmasmike\")\n    player2 = request.args.get('tag2', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '\"+str(player1)+\"' OR \"\\\n            +\"player2 = '\"+str(player1)+\"') AND (player1 = '\"+str(player2)+\"' OR \"\\\n            +\"player2 = '\"+str(player2)+\"') ORDER BY date DESC;\"\n    result = db.exec(sql)\n    return json.dumps(result)", "func_src_after": "@endpoints.route(\"/h2h\")\ndef h2h():\n    if db == None:\n        init()\n\n    player1 = request.args.get('tag1', default=\"christmasmike\")\n    player2 = request.args.get('tag2', default=\"christmasmike\")\n    sql = \"SELECT * FROM matches WHERE (player1 = '{player1}' OR \"\\\n            +\"player2 = '{player1}') AND (player1 = '{player2}' OR \"\\\n            +\"player2 = '{player2}') ORDER BY date DESC;\"\n    args = {'player1': player1, 'player2': player2}\n    result = db.exec(sql, args)\n    return json.dumps(result)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "endpoints.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint function named 'h2h' that retrieves head-to-head match records between two players from a database and returns the result as JSON, with default player names as 'christmasmike'."}
{"func_name": "dd_get_item_size", "func_src_before": "long dd_get_item_size(struct dump_dir *dd, const char *name)\n{\n    long size = -1;\n    char *iname = concat_path_file(dd->dd_dirname, name);\n    struct stat statbuf;\n\n    if (lstat(iname, &statbuf) == 0 && S_ISREG(statbuf.st_mode))\n        size = statbuf.st_size;\n    else\n    {\n        if (errno == ENOENT)\n            size = 0;\n        else\n            perror_msg(\"Can't get size of file '%s'\", iname);\n    }\n\n    free(iname);\n\n    return size;\n}", "func_src_after": "long dd_get_item_size(struct dump_dir *dd, const char *name)\n{\n    if (!str_is_correct_filename(name))\n        error_msg_and_die(\"Cannot get item size. '%s' is not a valid file name\", name);\n\n    long size = -1;\n    char *iname = concat_path_file(dd->dd_dirname, name);\n    struct stat statbuf;\n\n    if (lstat(iname, &statbuf) == 0 && S_ISREG(statbuf.st_mode))\n        size = statbuf.st_size;\n    else\n    {\n        if (errno == ENOENT)\n            size = 0;\n        else\n            perror_msg(\"Can't get size of file '%s'\", iname);\n    }\n\n    free(iname);\n\n    return size;\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "In C, write a function to get the size of a file within a directory structure, handling errors appropriately."}
{"func_name": "add_language", "func_src_before": "def add_language(lang):\n    try:\n        cur.execute(f\"INSERT INTO language (name) VALUES ('{lang}')\")\n    except Exception as e:\n        pass\n    cur.execute(f\"SELECT language_id FROM language where name='{lang}'\")\n    lang_id = cur.fetchone()[0]\n    if conn.commit():\n        return lang_id\n    return lang_id", "func_src_after": "def add_language(lang):\n    try:\n        cur.execute(\"INSERT INTO language (name) VALUES (%s)\", (lang, ))\n    except Exception as e:\n        pass\n    cur.execute(\"SELECT language_id FROM language where name=%s\", (lang, ))\n    lang_id = cur.fetchone()[0]\n    if conn.commit():\n        return lang_id\n    return lang_id", "commit_link": "github.com/Elbertbiggs360/dvdrental/commit/ad144ae2a08a332498d0831bc255170d57ba754b", "file_name": "app.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new language into a database and return its ID, handling exceptions silently."}
{"func_name": "buildHTML", "func_src_before": "  function buildHTML(results) {\n    var html = [];\n    for (var i=0; i<results.length; i++) {\n      html.push([\n        '<li class=\"module-item\">',\n          '<p class=\"module-item-title\">',\n              'File: <a href=\"', results[i].absolute_url, \n                    '?highlight=', $(\"#id_site_search_2\").val(), '\">', \n                    results[i].project.name,\n                    \" - \", results[i].name, \"</a>\",\n          \"</p>\",\n          \"<p>\", results[i].text, \"</p>\",\n        \"</li>\"].join('')\n      );\n    }   \n    return html.join('');\n  }", "func_src_after": "  function buildHTML(results) {\n    var html = [];\n    for (var i=0; i<results.length; i++) {\n      html.push([\n        '<li class=\"module-item\">',\n          '<p class=\"module-item-title\">',\n              'File: <a href=\"', results[i].absolute_url, \n                    '?highlight=', encodeURIComponent($(\"#id_site_search_2\").val()), '\">', \n                    _(results[i].project.name),\n                    \" - \", _(results[i].name), \"</a>\",\n          \"</p>\",\n          \"<p>\", _(results[i].text), \"</p>\",\n        \"</li>\"].join('')\n      );\n    }   \n    return html.join('');\n  }", "line_changes": {"deleted": [{"line_no": 8, "char_start": 250, "char_end": 322, "line": "                    '?highlight=', $(\"#id_site_search_2\").val(), '\">', \n"}, {"line_no": 9, "char_start": 322, "char_end": 367, "line": "                    results[i].project.name,\n"}, {"line_no": 10, "char_start": 367, "char_end": 419, "line": "                    \" - \", results[i].name, \"</a>\",\n"}, {"line_no": 12, "char_start": 437, "char_end": 479, "line": "          \"<p>\", results[i].text, \"</p>\",\n"}], "added": [{"line_no": 8, "char_start": 250, "char_end": 342, "line": "                    '?highlight=', encodeURIComponent($(\"#id_site_search_2\").val()), '\">', \n"}, {"line_no": 9, "char_start": 342, "char_end": 390, "line": "                    _(results[i].project.name),\n"}, {"line_no": 10, "char_start": 390, "char_end": 445, "line": "                    \" - \", _(results[i].name), \"</a>\",\n"}, {"line_no": 12, "char_start": 463, "char_end": 508, "line": "          \"<p>\", _(results[i].text), \"</p>\",\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 285, "char_end": 304, "chars": "encodeURIComponent("}, {"char_start": 332, "char_end": 333, "chars": ")"}, {"char_start": 362, "char_end": 364, "chars": "_("}, {"char_start": 387, "char_end": 388, "chars": ")"}, {"char_start": 417, "char_end": 419, "chars": "_("}, {"char_start": 434, "char_end": 435, "chars": ")"}, {"char_start": 480, "char_end": 482, "chars": "_("}, {"char_start": 497, "char_end": 498, "chars": ")"}]}, "commit_link": "github.com/CedarLogic/readthedocs.org/commit/a54cabb2f8e649973e0324f2c7dfac0e7efc25a2", "file_name": "instantsearch.js", "vul_type": "cwe-079", "commit_msg": "Fix potential XSS in instantsearch.js", "description": "Write a JavaScript function that generates an HTML list from an array of results, including links and text content."}
{"func_name": "post", "func_src_before": "    def post(self):\n        \"\"\" Returns JWT upon login verification \"\"\"\n        json_data = request.get_json()\n        if not json_data['email']:\n            return jsonify({\"msg\": \"Missing email\"}), 400\n\n        data = database_utilities.execute_query(\n            f\"\"\"select * from admins where email = '{json_data['email']}'\"\"\")\n        if data:\n            email = data[0]['email']\n            access_token = create_access_token(identity=email)\n            refresh_token = create_refresh_token(identity=email)\n\n            resp = jsonify({\"login\": True})\n            set_access_cookies(resp, access_token)\n            set_refresh_cookies(resp, refresh_token)\n            return resp\n        else:\n            return jsonify({\"msg\": \"User is not an admin\"})", "func_src_after": "    def post(self):\n        \"\"\" Returns JWT upon login verification \"\"\"\n        json_data = request.get_json()\n        if not json_data['email']:\n            return jsonify({\"msg\": \"Missing email\"}), 400\n\n        data = database_utilities.execute_query(\n            f\"\"\"select * from admins where email = %s\"\"\", (json_data['email'], ))\n        if data:\n            email = data[0]['email']\n            access_token = create_access_token(identity=email)\n            refresh_token = create_refresh_token(identity=email)\n\n            resp = jsonify({\"login\": True})\n            set_access_cookies(resp, access_token)\n            set_refresh_cookies(resp, refresh_token)\n            return resp\n        else:\n            return jsonify({\"msg\": \"User is not an admin\"})", "commit_link": "github.com/sgosal2/tiger-boards-backend/commit/4670109dd613df2f2fe7e8403ebd149df2b55485", "file_name": "apis/login.py", "vul_type": "cwe-089", "description": "Write a Python function that handles admin login by verifying email and issuing JWT tokens."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/micha-p/sqlgopher/commit/6037d5be1da195100db4328e9745b842a29a389c", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "3b4884a57e465df2433dde79a7e7a31c92a15099", "description": "Write a Go function to display the contents of a specified database table in a web page."}
{"func_name": "mem_iconveha", "func_src_before": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "func_src_after": "mem_iconveha (const char *src, size_t srclen,\n              const char *from_codeset, const char *to_codeset,\n              bool transliterate,\n              enum iconv_ilseq_handler handler,\n              size_t *offsets,\n              char **resultp, size_t *lengthp)\n{\n  if (srclen == 0)\n    {\n      /* Nothing to convert.  */\n      *lengthp = 0;\n      return 0;\n    }\n\n  /* When using GNU libc >= 2.2 or GNU libiconv >= 1.5,\n     we want to use transliteration.  */\n#if (((__GLIBC__ == 2 && __GLIBC_MINOR__ >= 2) || __GLIBC__ > 2) \\\n     && !defined __UCLIBC__) \\\n    || _LIBICONV_VERSION >= 0x0105\n  if (transliterate)\n    {\n      int retval;\n      size_t len = strlen (to_codeset);\n      char *to_codeset_suffixed = (char *) malloca (len + 10 + 1);\n      if (to_codeset_suffixed == NULL)\n        {\n          errno = ENOMEM;\n          return -1;\n        }\n      memcpy (to_codeset_suffixed, to_codeset, len);\n      memcpy (to_codeset_suffixed + len, \"//TRANSLIT\", 10 + 1);\n\n      retval = mem_iconveha_notranslit (src, srclen,\n                                        from_codeset, to_codeset_suffixed,\n                                        handler, offsets, resultp, lengthp);\n\n      freea (to_codeset_suffixed);\n\n      return retval;\n    }\n  else\n#endif\n    return mem_iconveha_notranslit (src, srclen,\n                                    from_codeset, to_codeset,\n                                    handler, offsets, resultp, lengthp);\n}", "commit_link": "github.com/coreutils/gnulib/commit/fce9817d48c97339c3f66a92e72faba8e69d405c", "file_name": "lib/striconveha.c", "vul_type": "cwe-476", "description": "Write a C function `mem_iconveha` that converts a string from one character encoding to another, with an option for transliteration."}
{"func_name": "make_eb_config", "func_src_before": "def make_eb_config(application_name, default_region):\n    # Capture our current directory\n    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n    # Create the jinja2 environment.\n    # Notice the use of trim_blocks, which greatly helps control whitespace.\n    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))\n    return j2_env.get_template('templates/eb/config.yml').render(\n        APPLICATION_NAME=application_name,\n        DEFAULT_REGION=default_region\n    )", "func_src_after": "def make_eb_config(application_name, default_region):\n    # Capture our current directory\n    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n    # Create the jinja2 environment.\n    # Notice the use of trim_blocks, which greatly helps control whitespace.\n    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)\n    return j2_env.get_template('templates/eb/config.yml').render(\n        APPLICATION_NAME=application_name,\n        DEFAULT_REGION=default_region\n    )", "commit_link": "github.com/OkunaOrg/okuna-www-api/commit/8c40c66ea7c483a0cbda4c21940180af909aab99", "file_name": "utils/make_eb_config.py", "vul_type": "cwe-079", "description": "Write a Python function to generate an Elastic Beanstalk configuration file from a template with application name and default region variables."}
{"func_name": "imap_hcache_open", "func_src_before": "header_cache_t *imap_hcache_open(struct ImapData *idata, const char *path)\n{\n  struct ImapMbox mx;\n  struct Url url;\n  char cachepath[PATH_MAX];\n  char mbox[PATH_MAX];\n\n  if (path)\n    imap_cachepath(idata, path, mbox, sizeof(mbox));\n  else\n  {\n    if (!idata->ctx || imap_parse_path(idata->ctx->path, &mx) < 0)\n      return NULL;\n\n    imap_cachepath(idata, mx.mbox, mbox, sizeof(mbox));\n    FREE(&mx.mbox);\n  }\n\n  mutt_account_tourl(&idata->conn->account, &url);\n  url.path = mbox;\n  url_tostring(&url, cachepath, sizeof(cachepath), U_PATH);\n\n  return mutt_hcache_open(HeaderCache, cachepath, imap_hcache_namer);\n}", "func_src_after": "header_cache_t *imap_hcache_open(struct ImapData *idata, const char *path)\n{\n  struct ImapMbox mx;\n  struct Url url;\n  char cachepath[PATH_MAX];\n  char mbox[PATH_MAX];\n\n  if (path)\n    imap_cachepath(idata, path, mbox, sizeof(mbox));\n  else\n  {\n    if (!idata->ctx || imap_parse_path(idata->ctx->path, &mx) < 0)\n      return NULL;\n\n    imap_cachepath(idata, mx.mbox, mbox, sizeof(mbox));\n    FREE(&mx.mbox);\n  }\n\n  if (strstr(mbox, \"/../\") || (strcmp(mbox, \"..\") == 0) || (strncmp(mbox, \"../\", 3) == 0))\n    return NULL;\n  size_t len = strlen(mbox);\n  if ((len > 3) && (strcmp(mbox + len - 3, \"/..\") == 0))\n    return NULL;\n\n  mutt_account_tourl(&idata->conn->account, &url);\n  url.path = mbox;\n  url_tostring(&url, cachepath, sizeof(cachepath), U_PATH);\n\n  return mutt_hcache_open(HeaderCache, cachepath, imap_hcache_namer);\n}", "commit_link": "github.com/neomutt/neomutt/commit/57971dba06346b2d7179294f4528b8d4427a7c5d", "file_name": "imap/util.c", "vul_type": "cwe-022", "description": "Write a C function named `imap_hcache_open` that opens an IMAP header cache, optionally using a provided path."}
{"func_name": "get_net_ns_by_id", "func_src_before": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tget_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}", "func_src_after": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}", "commit_link": "github.com/torvalds/linux/commit/21b5944350052d2583e82dd59b19a9ba94a007f0", "file_name": "net/core/net_namespace.c", "vul_type": "cwe-416", "description": "Write a C function named `get_net_ns_by_id` that retrieves a network namespace by its ID from a given network structure, handling synchronization and reference acquisition."}
{"func_name": "xsltKeyFunction", "func_src_before": "xsltKeyFunction(xmlXPathParserContextPtr ctxt, int nargs){\n    xmlXPathObjectPtr obj1, obj2;\n\n    if (nargs != 2) {\n\txsltTransformError(xsltXPathGetTransformContext(ctxt), NULL, NULL,\n\t\t\"key() : expects two arguments\\n\");\n\tctxt->error = XPATH_INVALID_ARITY;\n\treturn;\n    }\n\n    /*\n    * Get the key's value.\n    */\n    obj2 = valuePop(ctxt);\n    xmlXPathStringFunction(ctxt, 1);\n    if ((obj2 == NULL) ||\n\t(ctxt->value == NULL) || (ctxt->value->type != XPATH_STRING)) {\n\txsltTransformError(xsltXPathGetTransformContext(ctxt), NULL, NULL,\n\t    \"key() : invalid arg expecting a string\\n\");\n\tctxt->error = XPATH_INVALID_TYPE;\n\txmlXPathFreeObject(obj2);\n\n\treturn;\n    }\n    /*\n    * Get the key's name.\n    */\n    obj1 = valuePop(ctxt);\n\n    if ((obj2->type == XPATH_NODESET) || (obj2->type == XPATH_XSLT_TREE)) {\n\tint i;\n\txmlXPathObjectPtr newobj, ret;\n\n\tret = xmlXPathNewNodeSet(NULL);\n\n\tif (obj2->nodesetval != NULL) {\n\t    for (i = 0; i < obj2->nodesetval->nodeNr; i++) {\n\t\tvaluePush(ctxt, xmlXPathObjectCopy(obj1));\n\t\tvaluePush(ctxt,\n\t\t\t  xmlXPathNewNodeSet(obj2->nodesetval->nodeTab[i]));\n\t\txmlXPathStringFunction(ctxt, 1);\n\t\txsltKeyFunction(ctxt, 2);\n\t\tnewobj = valuePop(ctxt);\n                if (newobj != NULL)\n\t\t    ret->nodesetval = xmlXPathNodeSetMerge(ret->nodesetval,\n\t\t\t\t\t\t           newobj->nodesetval);\n\t\txmlXPathFreeObject(newobj);\n\t    }\n\t}\n\tvaluePush(ctxt, ret);\n    } else {\n\txmlNodeSetPtr nodelist = NULL;\n\txmlChar *key = NULL, *value;\n\tconst xmlChar *keyURI;\n\txsltTransformContextPtr tctxt;\n\txmlChar *qname, *prefix;\n\txmlXPathContextPtr xpctxt = ctxt->context;\n\txmlNodePtr tmpNode = NULL;\n\txsltDocumentPtr oldDocInfo;\n\n\ttctxt = xsltXPathGetTransformContext(ctxt);\n\n\toldDocInfo = tctxt->document;\n\n\tif (xpctxt->node == NULL) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"Internal error in xsltKeyFunction(): \"\n\t\t\"The context node is not set on the XPath context.\\n\");\n\t    tctxt->state = XSLT_STATE_STOPPED;\n\t    goto error;\n\t}\n\t/*\n\t * Get the associated namespace URI if qualified name\n\t */\n\tqname = obj1->stringval;\n\tkey = xmlSplitQName2(qname, &prefix);\n\tif (key == NULL) {\n\t    key = xmlStrdup(obj1->stringval);\n\t    keyURI = NULL;\n\t    if (prefix != NULL)\n\t\txmlFree(prefix);\n\t} else {\n\t    if (prefix != NULL) {\n\t\tkeyURI = xmlXPathNsLookup(xpctxt, prefix);\n\t\tif (keyURI == NULL) {\n\t\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\t\"key() : prefix %s is not bound\\n\", prefix);\n\t\t    /*\n\t\t    * TODO: Shouldn't we stop here?\n\t\t    */\n\t\t}\n\t\txmlFree(prefix);\n\t    } else {\n\t\tkeyURI = NULL;\n\t    }\n\t}\n\n\t/*\n\t * Force conversion of first arg to string\n\t */\n\tvaluePush(ctxt, obj2);\n\txmlXPathStringFunction(ctxt, 1);\n\tobj2 = valuePop(ctxt);\n\tif ((obj2 == NULL) || (obj2->type != XPATH_STRING)) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"key() : invalid arg expecting a string\\n\");\n\t    ctxt->error = XPATH_INVALID_TYPE;\n\t    goto error;\n\t}\n\tvalue = obj2->stringval;\n\n\t/*\n\t* We need to ensure that ctxt->document is available for\n\t* xsltGetKey().\n\t* First find the relevant doc, which is the context node's\n\t* owner doc; using context->doc is not safe, since\n\t* the doc could have been acquired via the document() function,\n\t* or the doc might be a Result Tree Fragment.\n\t* FUTURE INFO: In XSLT 2.0 the key() function takes an additional\n\t* argument indicating the doc to use.\n\t*/\n\tif (xpctxt->node->type == XML_NAMESPACE_DECL) {\n\t    /*\n\t    * REVISIT: This is a libxml hack! Check xpath.c for details.\n\t    * The XPath module sets the owner element of a ns-node on\n\t    * the ns->next field.\n\t    */\n\t    if ((((xmlNsPtr) xpctxt->node)->next != NULL) &&\n\t\t(((xmlNsPtr) xpctxt->node)->next->type == XML_ELEMENT_NODE))\n\t    {\n\t\ttmpNode = (xmlNodePtr) ((xmlNsPtr) xpctxt->node)->next;\n\t    }\n\t} else\n\t    tmpNode = xpctxt->node;\n\n\tif ((tmpNode == NULL) || (tmpNode->doc == NULL)) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"Internal error in xsltKeyFunction(): \"\n\t\t\"Couldn't get the doc of the XPath context node.\\n\");\n\t    goto error;\n\t}\n\n\tif ((tctxt->document == NULL) ||\n\t    (tctxt->document->doc != tmpNode->doc))\n\t{\n\t    if (tmpNode->doc->name && (tmpNode->doc->name[0] == ' ')) {\n\t\t/*\n\t\t* This is a Result Tree Fragment.\n\t\t*/\n\t\tif (tmpNode->doc->_private == NULL) {\n\t\t    tmpNode->doc->_private = xsltNewDocument(tctxt, tmpNode->doc);\n\t\t    if (tmpNode->doc->_private == NULL)\n\t\t\tgoto error;\n\t\t}\n\t\ttctxt->document = (xsltDocumentPtr) tmpNode->doc->_private;\n\t    } else {\n\t\t/*\n\t\t* May be the initial source doc or a doc acquired via the\n\t\t* document() function.\n\t\t*/\n\t\ttctxt->document = xsltFindDocument(tctxt, tmpNode->doc);\n\t    }\n\t    if (tctxt->document == NULL) {\n\t\txsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t    \"Internal error in xsltKeyFunction(): \"\n\t\t    \"Could not get the document info of a context doc.\\n\");\n\t\ttctxt->state = XSLT_STATE_STOPPED;\n\t\tgoto error;\n\t    }\n\t}\n\t/*\n\t* Get/compute the key value.\n\t*/\n\tnodelist = xsltGetKey(tctxt, key, keyURI, value);\n\nerror:\n\ttctxt->document = oldDocInfo;\n\tvaluePush(ctxt, xmlXPathWrapNodeSet(\n\t    xmlXPathNodeSetMerge(NULL, nodelist)));\n\tif (key != NULL)\n\t    xmlFree(key);\n    }\n\n    if (obj1 != NULL)\n\txmlXPathFreeObject(obj1);\n    if (obj2 != NULL)\n\txmlXPathFreeObject(obj2);\n}", "func_src_after": "xsltKeyFunction(xmlXPathParserContextPtr ctxt, int nargs){\n    xmlXPathObjectPtr obj1, obj2;\n\n    if (nargs != 2) {\n\txsltTransformError(xsltXPathGetTransformContext(ctxt), NULL, NULL,\n\t\t\"key() : expects two arguments\\n\");\n\tctxt->error = XPATH_INVALID_ARITY;\n\treturn;\n    }\n\n    /*\n    * Get the key's value.\n    */\n    obj2 = valuePop(ctxt);\n    xmlXPathStringFunction(ctxt, 1);\n    if ((obj2 == NULL) ||\n\t(ctxt->value == NULL) || (ctxt->value->type != XPATH_STRING)) {\n\txsltTransformError(xsltXPathGetTransformContext(ctxt), NULL, NULL,\n\t    \"key() : invalid arg expecting a string\\n\");\n\tctxt->error = XPATH_INVALID_TYPE;\n\txmlXPathFreeObject(obj2);\n\n\treturn;\n    }\n    /*\n    * Get the key's name.\n    */\n    obj1 = valuePop(ctxt);\n\n    if ((obj2->type == XPATH_NODESET) || (obj2->type == XPATH_XSLT_TREE)) {\n\tint i;\n\txmlXPathObjectPtr newobj, ret;\n\n\tret = xmlXPathNewNodeSet(NULL);\n        if (ret == NULL) {\n            ctxt->error = XPATH_MEMORY_ERROR;\n            xmlXPathFreeObject(obj1);\n            xmlXPathFreeObject(obj2);\n            return;\n        }\n\n\tif (obj2->nodesetval != NULL) {\n\t    for (i = 0; i < obj2->nodesetval->nodeNr; i++) {\n\t\tvaluePush(ctxt, xmlXPathObjectCopy(obj1));\n\t\tvaluePush(ctxt,\n\t\t\t  xmlXPathNewNodeSet(obj2->nodesetval->nodeTab[i]));\n\t\txmlXPathStringFunction(ctxt, 1);\n\t\txsltKeyFunction(ctxt, 2);\n\t\tnewobj = valuePop(ctxt);\n                if (newobj != NULL)\n\t\t    ret->nodesetval = xmlXPathNodeSetMerge(ret->nodesetval,\n\t\t\t\t\t\t           newobj->nodesetval);\n\t\txmlXPathFreeObject(newobj);\n\t    }\n\t}\n\tvaluePush(ctxt, ret);\n    } else {\n\txmlNodeSetPtr nodelist = NULL;\n\txmlChar *key = NULL, *value;\n\tconst xmlChar *keyURI;\n\txsltTransformContextPtr tctxt;\n\txmlChar *qname, *prefix;\n\txmlXPathContextPtr xpctxt = ctxt->context;\n\txmlNodePtr tmpNode = NULL;\n\txsltDocumentPtr oldDocInfo;\n\n\ttctxt = xsltXPathGetTransformContext(ctxt);\n\n\toldDocInfo = tctxt->document;\n\n\tif (xpctxt->node == NULL) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"Internal error in xsltKeyFunction(): \"\n\t\t\"The context node is not set on the XPath context.\\n\");\n\t    tctxt->state = XSLT_STATE_STOPPED;\n\t    goto error;\n\t}\n\t/*\n\t * Get the associated namespace URI if qualified name\n\t */\n\tqname = obj1->stringval;\n\tkey = xmlSplitQName2(qname, &prefix);\n\tif (key == NULL) {\n\t    key = xmlStrdup(obj1->stringval);\n\t    keyURI = NULL;\n\t    if (prefix != NULL)\n\t\txmlFree(prefix);\n\t} else {\n\t    if (prefix != NULL) {\n\t\tkeyURI = xmlXPathNsLookup(xpctxt, prefix);\n\t\tif (keyURI == NULL) {\n\t\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\t\"key() : prefix %s is not bound\\n\", prefix);\n\t\t    /*\n\t\t    * TODO: Shouldn't we stop here?\n\t\t    */\n\t\t}\n\t\txmlFree(prefix);\n\t    } else {\n\t\tkeyURI = NULL;\n\t    }\n\t}\n\n\t/*\n\t * Force conversion of first arg to string\n\t */\n\tvaluePush(ctxt, obj2);\n\txmlXPathStringFunction(ctxt, 1);\n\tobj2 = valuePop(ctxt);\n\tif ((obj2 == NULL) || (obj2->type != XPATH_STRING)) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"key() : invalid arg expecting a string\\n\");\n\t    ctxt->error = XPATH_INVALID_TYPE;\n\t    goto error;\n\t}\n\tvalue = obj2->stringval;\n\n\t/*\n\t* We need to ensure that ctxt->document is available for\n\t* xsltGetKey().\n\t* First find the relevant doc, which is the context node's\n\t* owner doc; using context->doc is not safe, since\n\t* the doc could have been acquired via the document() function,\n\t* or the doc might be a Result Tree Fragment.\n\t* FUTURE INFO: In XSLT 2.0 the key() function takes an additional\n\t* argument indicating the doc to use.\n\t*/\n\tif (xpctxt->node->type == XML_NAMESPACE_DECL) {\n\t    /*\n\t    * REVISIT: This is a libxml hack! Check xpath.c for details.\n\t    * The XPath module sets the owner element of a ns-node on\n\t    * the ns->next field.\n\t    */\n\t    if ((((xmlNsPtr) xpctxt->node)->next != NULL) &&\n\t\t(((xmlNsPtr) xpctxt->node)->next->type == XML_ELEMENT_NODE))\n\t    {\n\t\ttmpNode = (xmlNodePtr) ((xmlNsPtr) xpctxt->node)->next;\n\t    }\n\t} else\n\t    tmpNode = xpctxt->node;\n\n\tif ((tmpNode == NULL) || (tmpNode->doc == NULL)) {\n\t    xsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t\"Internal error in xsltKeyFunction(): \"\n\t\t\"Couldn't get the doc of the XPath context node.\\n\");\n\t    goto error;\n\t}\n\n\tif ((tctxt->document == NULL) ||\n\t    (tctxt->document->doc != tmpNode->doc))\n\t{\n\t    if (tmpNode->doc->name && (tmpNode->doc->name[0] == ' ')) {\n\t\t/*\n\t\t* This is a Result Tree Fragment.\n\t\t*/\n\t\tif (tmpNode->doc->_private == NULL) {\n\t\t    tmpNode->doc->_private = xsltNewDocument(tctxt, tmpNode->doc);\n\t\t    if (tmpNode->doc->_private == NULL)\n\t\t\tgoto error;\n\t\t}\n\t\ttctxt->document = (xsltDocumentPtr) tmpNode->doc->_private;\n\t    } else {\n\t\t/*\n\t\t* May be the initial source doc or a doc acquired via the\n\t\t* document() function.\n\t\t*/\n\t\ttctxt->document = xsltFindDocument(tctxt, tmpNode->doc);\n\t    }\n\t    if (tctxt->document == NULL) {\n\t\txsltTransformError(tctxt, NULL, tctxt->inst,\n\t\t    \"Internal error in xsltKeyFunction(): \"\n\t\t    \"Could not get the document info of a context doc.\\n\");\n\t\ttctxt->state = XSLT_STATE_STOPPED;\n\t\tgoto error;\n\t    }\n\t}\n\t/*\n\t* Get/compute the key value.\n\t*/\n\tnodelist = xsltGetKey(tctxt, key, keyURI, value);\n\nerror:\n\ttctxt->document = oldDocInfo;\n\tvaluePush(ctxt, xmlXPathWrapNodeSet(\n\t    xmlXPathNodeSetMerge(NULL, nodelist)));\n\tif (key != NULL)\n\t    xmlFree(key);\n    }\n\n    if (obj1 != NULL)\n\txmlXPathFreeObject(obj1);\n    if (obj2 != NULL)\n\txmlXPathFreeObject(obj2);\n}", "commit_link": "github.com/GNOME/libxslt/commit/aab7eedca3c2dcaa1795d6acba38a4c9811d2a75", "file_name": "libxslt/functions.c", "vul_type": "cwe-476", "description": "Write a C function named `xsltKeyFunction` that implements the XSLT key function in libxml2."}
{"func_name": "(anonymous)", "func_src_before": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = '\" + uid + \"';\", function(err, row) {\n        event.uid = uid;\n        obj.keyval_parse(event, row.key, row.value, row.payload);\n        if (typeof event.times === 'string') {\n          event.times = event.times.split(',').map(time=>+time)\n        }\n    }, function(err, rows) {", "func_src_after": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = ? ;\", [uid], function(err, row) {\n        event.uid = uid;\n        obj.keyval_parse(event, row.key, row.value, row.payload);\n        if (typeof event.times === 'string') {\n          event.times = event.times.split(',').map(time=>+time)\n        }\n    }, function(err, rows) {", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 92, "line": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = '\" + uid + \"';\", function(err, row) {\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 88, "line": "    this.db.each(\"SELECT * FROM tb_events WHERE uid = ? ;\", [uid], function(err, row) {\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 69, "chars": "'\" + uid + \"';\""}], "added": [{"char_start": 54, "char_end": 65, "chars": "? ;\", [uid]"}]}, "commit_link": "github.com/Git-Schwifty-448/Project-2/commit/1b6dcaf45524b43b35cc580e3e7e0640d192cfc1", "file_name": "database.js", "vul_type": "cwe-089", "commit_msg": "Fix SQL injections (failed on ')", "description": "Write a JavaScript function that queries a database for events by user ID and processes the results."}
{"func_name": "upsert_mapped_projects", "func_src_before": "    @staticmethod\n    def upsert_mapped_projects(user_id: int, project_id: int):\n        \"\"\" Adds projects to mapped_projects if it doesn't exist \"\"\"\n        sql = \"select * from users where id = {0} and projects_mapped @> '{{{1}}}'\".format(user_id, project_id)\n        result = db.engine.execute(sql)\n\n        if result.rowcount > 0:\n            return  # User has previously mapped this project so return\n\n        sql = '''update users\n                    set projects_mapped = array_append(projects_mapped, {0})\n                  where id = {1}'''.format(project_id, user_id)\n\n        db.engine.execute(sql)", "func_src_after": "    @staticmethod\n    def upsert_mapped_projects(user_id: int, project_id: int):\n        \"\"\" Adds projects to mapped_projects if it doesn't exist \"\"\"\n        sql = \"select * from users where id = :user_id and projects_mapped @> '{{:project_id}}'\"\n        result = db.engine.execute(text(sql), user_id=user_id, project_id=project_id)\n\n        if result.rowcount > 0:\n            return  # User has previously mapped this project so return\n\n        sql = '''update users\n                    set projects_mapped = array_append(projects_mapped, :project_id)\n                  where id = :user_id'''\n\n        db.engine.execute(text(sql), project_id=project_id, user_id=user_id)", "commit_link": "github.com/hotosm/tasking-manager/commit/dee040a2d22b3c4d5e38e2dbf8c6b651ad4c241a", "file_name": "server/models/postgis/user.py", "vul_type": "cwe-089", "description": "Write a Python function that checks if a project is already mapped to a user and if not, appends it to their mapped projects in the database."}
{"func_name": "libsoc_pwm_get_polarity", "func_src_before": "int libsoc_pwm_get_polarity(pwm *pwm)\n{\n  int polarity;\n  char path[STR_BUF];\n  char tmp_str[1];\n\n  if (pwm == NULL)\n  {\n    libsoc_pwm_debug(__func__, -1, -1, \"invalid pwm pointer\");\n    return EXIT_FAILURE;\n  }\n\n  sprintf(path, \"/sys/class/pwm/pwmchip%d/pwm%d/polarity\", pwm->chip, pwm->pwm);\n\n  if (file_read_str(path, tmp_str, 1) == EXIT_FAILURE)\n  {\n    return EXIT_FAILURE;\n  }\n\n  tmp_str[1] = NULL;\n\n  if (strncmp(tmp_str, \"i\", 1) == 0)\n  {\n    polarity = INVERSED;\n  }\n  else if (strncmp(tmp_str, \"n\", 1) == 0)\n  {\n    polarity = NORMAL;\n  }\n  else\n  {\n    polarity = POLARITY_ERROR;\n    return EXIT_FAILURE;\n  }\n\n  libsoc_pwm_debug(__func__, pwm->chip, pwm->pwm, \"got polarity as %s\", pwm_polarity_strings[polarity]);\n\n  return polarity;\n}", "func_src_after": "int libsoc_pwm_get_polarity(pwm *pwm)\n{\n  int polarity;\n  char path[STR_BUF];\n  char tmp_str[1];\n\n  if (pwm == NULL)\n  {\n    libsoc_pwm_debug(__func__, -1, -1, \"invalid pwm pointer\");\n    return EXIT_FAILURE;\n  }\n\n  sprintf(path, \"/sys/class/pwm/pwmchip%d/pwm%d/polarity\", pwm->chip, pwm->pwm);\n\n  if (file_read_str(path, tmp_str, 1) == EXIT_FAILURE)\n  {\n    return EXIT_FAILURE;\n  }\n\n  if (strncmp(tmp_str, \"i\", 1) == 0)\n  {\n    polarity = INVERSED;\n  }\n  else if (strncmp(tmp_str, \"n\", 1) == 0)\n  {\n    polarity = NORMAL;\n  }\n  else\n  {\n    polarity = POLARITY_ERROR;\n    return EXIT_FAILURE;\n  }\n\n  libsoc_pwm_debug(__func__, pwm->chip, pwm->pwm, \"got polarity as %s\", pwm_polarity_strings[polarity]);\n\n  return polarity;\n}", "line_changes": {"deleted": [{"line_no": 20, "char_start": 385, "char_end": 406, "line": "  tmp_str[1] = NULL;\n"}, {"line_no": 21, "char_start": 406, "char_end": 407, "line": "\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 385, "char_end": 407, "chars": "  tmp_str[1] = NULL;\n\n"}], "added": []}, "commit_link": "github.com/jackmitch/libsoc/commit/f4e262dd1de930388f22b779b8ab13d5b89db31e", "file_name": "pwm.c", "vul_type": "cwe-119", "commit_msg": "pwm: fix subscription error\n\ntmp_str array has a size of 1, so tmp_str[1] access is out of range.\nBesides file_read_str() reads only one byte and strncmp() compares only\none byte too.\n\nSigned-off-by: Yegor Yefremov <yegorslists@googlemail.com>", "parent_commit": "1722feb7591284a22fbf03e4c5cf5b2a2776ab7f", "description": "Write a C function to read and return the polarity of a PWM signal, handling invalid inputs and errors."}
{"func_name": "candidate_paths_for_url", "func_src_before": "    def candidate_paths_for_url(self, url):\n        for root, prefix in self.directories:\n            if url.startswith(prefix):\n                yield os.path.join(root, url[len(prefix):])", "func_src_after": "    def candidate_paths_for_url(self, url):\n        for root, prefix in self.directories:\n            if url.startswith(prefix):\n                path = os.path.join(root, url[len(prefix):])\n                if os.path.commonprefix((root, path)) == root:\n                    yield path", "commit_link": "github.com/evansd/whitenoise/commit/4d8a3ab1e97d7ddb18b3fa8b4909c92bad5529c6", "file_name": "whitenoise/base.py", "vul_type": "cwe-022", "description": "Write a Python function that yields file system paths corresponding to a given URL based on predefined directory mappings."}
{"func_name": "download", "func_src_before": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           open io_or_filename, 'wb'\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "func_src_after": "  def download uri, io_or_filename, parameters = [], referer = nil, headers = {}\n    page = transact do\n      get uri, parameters, referer, headers\n    end\n\n    io = if io_or_filename.respond_to? :write then\n           io_or_filename\n         else\n           ::File.open(io_or_filename, 'wb')\n         end\n\n    case page\n    when Mechanize::File then\n      io.write page.body\n    else\n      body_io = page.body_io\n\n      until body_io.eof? do\n        io.write body_io.read 16384\n      end\n    end", "line_changes": {"deleted": [{"line_no": 9, "char_start": 248, "char_end": 285, "line": "           open io_or_filename, 'wb'\n"}], "added": [{"line_no": 9, "char_start": 248, "char_end": 293, "line": "           ::File.open(io_or_filename, 'wb')\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 264, "chars": " "}], "added": [{"char_start": 259, "char_end": 266, "chars": "::File."}, {"char_start": 270, "char_end": 271, "chars": "("}, {"char_start": 291, "char_end": 292, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/2ac906b26f4a565a0af92df5fb9c8a36c2b75375", "file_name": "mechanize.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in Mechanize#download\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `download` that retrieves content from a URL and saves it to a file or an IO object."}
{"func_name": "xc2028_set_config", "func_src_before": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\trc = -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}", "func_src_after": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tpriv->ctrl.fname = NULL;\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}", "commit_link": "github.com/torvalds/linux/commit/8dfbcc4351a0b6d2f2d77f367552f48ffefafe18", "file_name": "drivers/media/tuners/tuner-xc2028.c", "vul_type": "cwe-416", "description": "Write a C function named `xc2028_set_config` that updates the configuration of a DVB frontend device and handles firmware requests."}
{"func_name": "set_geometry", "func_src_before": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif (g->sect <= 0 ||\n\t    g->head <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "func_src_after": "static int set_geometry(unsigned int cmd, struct floppy_struct *g,\n\t\t\t       int drive, int type, struct block_device *bdev)\n{\n\tint cnt;\n\n\t/* sanity checking for parameters. */\n\tif ((int)g->sect <= 0 ||\n\t    (int)g->head <= 0 ||\n\t    /* check for overflow in max_sector */\n\t    (int)(g->sect * g->head) <= 0 ||\n\t    /* check for zero in F_SECT_PER_TRACK */\n\t    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||\n\t    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||\n\t    /* check if reserved bits are set */\n\t    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)\n\t\treturn -EINVAL;\n\tif (type) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmutex_lock(&open_lock);\n\t\tif (lock_fdc(drive)) {\n\t\t\tmutex_unlock(&open_lock);\n\t\t\treturn -EINTR;\n\t\t}\n\t\tfloppy_type[type] = *g;\n\t\tfloppy_type[type].name = \"user format\";\n\t\tfor (cnt = type << 2; cnt < (type << 2) + 4; cnt++)\n\t\t\tfloppy_sizes[cnt] = floppy_sizes[cnt + 0x80] =\n\t\t\t    floppy_type[type].size + 1;\n\t\tprocess_fd_request();\n\t\tfor (cnt = 0; cnt < N_DRIVE; cnt++) {\n\t\t\tstruct block_device *bdev = opened_bdev[cnt];\n\t\t\tif (!bdev || ITYPE(drive_state[cnt].fd_device) != type)\n\t\t\t\tcontinue;\n\t\t\t__invalidate_device(bdev, true);\n\t\t}\n\t\tmutex_unlock(&open_lock);\n\t} else {\n\t\tint oldStretch;\n\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (cmd != FDDEFPRM) {\n\t\t\t/* notice a disk change immediately, else\n\t\t\t * we lose our settings immediately*/\n\t\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\toldStretch = g->stretch;\n\t\tuser_params[drive] = *g;\n\t\tif (buffer_drive == drive)\n\t\t\tSUPBOUND(buffer_max, user_params[drive].sect);\n\t\tcurrent_type[drive] = &user_params[drive];\n\t\tfloppy_sizes[drive] = user_params[drive].size;\n\t\tif (cmd == FDDEFPRM)\n\t\t\tDRS->keep_data = -1;\n\t\telse\n\t\t\tDRS->keep_data = 1;\n\t\t/* invalidation. Invalidate only when needed, i.e.\n\t\t * when there are already sectors in the buffer cache\n\t\t * whose number will change. This is useful, because\n\t\t * mtools often changes the geometry of the disk after\n\t\t * looking at the boot block */\n\t\tif (DRS->maxblock > user_params[drive].sect ||\n\t\t    DRS->maxtrack ||\n\t\t    ((user_params[drive].sect ^ oldStretch) &\n\t\t     (FD_SWAPSIDES | FD_SECTBASEMASK)))\n\t\t\tinvalidate_drive(bdev);\n\t\telse\n\t\t\tprocess_fd_request();\n\t}\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/da99466ac243f15fbba65bd261bfc75ffa1532b6", "file_name": "drivers/block/floppy.c", "vul_type": "cwe-190", "description": "Write a C function named `set_geometry` that configures the disk geometry for a floppy drive."}
{"func_name": "gdi_Bitmap_Decompress", "func_src_before": "static BOOL gdi_Bitmap_Decompress(rdpContext* context, rdpBitmap* bitmap,\n                                  const BYTE* pSrcData, UINT32 DstWidth, UINT32 DstHeight,\n                                  UINT32 bpp, UINT32 length, BOOL compressed,\n                                  UINT32 codecId)\n{\n\tUINT32 SrcSize = length;\n\trdpGdi* gdi = context->gdi;\n\tbitmap->compressed = FALSE;\n\tbitmap->format = gdi->dstFormat;\n\tbitmap->length = DstWidth * DstHeight * GetBytesPerPixel(bitmap->format);\n\tbitmap->data = (BYTE*) _aligned_malloc(bitmap->length, 16);\n\n\tif (!bitmap->data)\n\t\treturn FALSE;\n\n\tif (compressed)\n\t{\n\t\tif (bpp < 32)\n\t\t{\n\t\t\tif (!interleaved_decompress(context->codecs->interleaved,\n\t\t\t                            pSrcData, SrcSize,\n\t\t\t                            DstWidth, DstHeight,\n\t\t\t                            bpp,\n\t\t\t                            bitmap->data, bitmap->format,\n\t\t\t                            0, 0, 0, DstWidth, DstHeight,\n\t\t\t                            &gdi->palette))\n\t\t\t\treturn FALSE;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (!planar_decompress(context->codecs->planar, pSrcData, SrcSize,\n\t\t\t                       DstWidth, DstHeight,\n\t\t\t                       bitmap->data, bitmap->format, 0, 0, 0,\n\t\t\t                       DstWidth, DstHeight, TRUE))\n\t\t\t\treturn FALSE;\n\t\t}\n\t}\n\telse\n\t{\n\t\tconst UINT32 SrcFormat = gdi_get_pixel_format(bpp);\n\t\tconst size_t sbpp = GetBytesPerPixel(SrcFormat);\n\t\tconst size_t dbpp = GetBytesPerPixel(bitmap->format);\n\n\t\tif ((sbpp == 0) || (dbpp == 0))\n\t\t\treturn FALSE;\n\t\telse\n\t\t{\n\t\t\tconst size_t dstSize = SrcSize * dbpp / sbpp;\n\n\t\t\tif (dstSize  < bitmap->length)\n\t\t\t\treturn FALSE;\n\t\t}\n\n\t\tif (!freerdp_image_copy(bitmap->data, bitmap->format, 0, 0, 0,\n\t\t                        DstWidth, DstHeight, pSrcData, SrcFormat,\n\t\t                        0, 0, 0, &gdi->palette, FREERDP_FLIP_VERTICAL))\n\t\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}", "func_src_after": "static BOOL gdi_Bitmap_Decompress(rdpContext* context, rdpBitmap* bitmap,\n                                  const BYTE* pSrcData, UINT32 DstWidth, UINT32 DstHeight,\n                                  UINT32 bpp, UINT32 length, BOOL compressed,\n                                  UINT32 codecId)\n{\n\tUINT32 SrcSize = length;\n\trdpGdi* gdi = context->gdi;\n\tUINT32 size = DstWidth * DstHeight;\n\tbitmap->compressed = FALSE;\n\tbitmap->format = gdi->dstFormat;\n\n\tif ((GetBytesPerPixel(bitmap->format) == 0) ||\n\t    (DstWidth == 0) || (DstHeight == 0) || (DstWidth > UINT32_MAX / DstHeight) ||\n\t    (size > (UINT32_MAX / GetBytesPerPixel(bitmap->format))))\n\t\treturn FALSE;\n\n\tsize *= GetBytesPerPixel(bitmap->format);\n\tbitmap->length = size;\n\tbitmap->data = (BYTE*) _aligned_malloc(bitmap->length, 16);\n\n\tif (!bitmap->data)\n\t\treturn FALSE;\n\n\tif (compressed)\n\t{\n\t\tif (bpp < 32)\n\t\t{\n\t\t\tif (!interleaved_decompress(context->codecs->interleaved,\n\t\t\t                            pSrcData, SrcSize,\n\t\t\t                            DstWidth, DstHeight,\n\t\t\t                            bpp,\n\t\t\t                            bitmap->data, bitmap->format,\n\t\t\t                            0, 0, 0, DstWidth, DstHeight,\n\t\t\t                            &gdi->palette))\n\t\t\t\treturn FALSE;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (!planar_decompress(context->codecs->planar, pSrcData, SrcSize,\n\t\t\t                       DstWidth, DstHeight,\n\t\t\t                       bitmap->data, bitmap->format, 0, 0, 0,\n\t\t\t                       DstWidth, DstHeight, TRUE))\n\t\t\t\treturn FALSE;\n\t\t}\n\t}\n\telse\n\t{\n\t\tconst UINT32 SrcFormat = gdi_get_pixel_format(bpp);\n\t\tconst size_t sbpp = GetBytesPerPixel(SrcFormat);\n\t\tconst size_t dbpp = GetBytesPerPixel(bitmap->format);\n\n\t\tif ((sbpp == 0) || (dbpp == 0))\n\t\t\treturn FALSE;\n\t\telse\n\t\t{\n\t\t\tconst size_t dstSize = SrcSize * dbpp / sbpp;\n\n\t\t\tif (dstSize  < bitmap->length)\n\t\t\t\treturn FALSE;\n\t\t}\n\n\t\tif (!freerdp_image_copy(bitmap->data, bitmap->format, 0, 0, 0,\n\t\t                        DstWidth, DstHeight, pSrcData, SrcFormat,\n\t\t                        0, 0, 0, &gdi->palette, FREERDP_FLIP_VERTICAL))\n\t\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/09b9d4f1994a674c4ec85b4947aa656eda1aed8a", "file_name": "libfreerdp/gdi/graphics.c", "vul_type": "cwe-190", "description": "Write a C function named `gdi_Bitmap_Decompress` that decompresses a bitmap image in a remote desktop protocol context."}
{"func_name": "delete_crawl", "func_src_before": "@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"DELETE FROM Crawls WHERE cid = %s\" % cid)\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))", "func_src_after": "@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"\"\"DELETE FROM Crawls WHERE cid = %s\"\"\" (cid,))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))", "commit_link": "github.com/yannvon/table-detection/commit/4bad3673debf0b9491b520f0e22e9186af78c375", "file_name": "bar.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to delete an entry from a MySQL database using a POST request and flash a success message."}
{"func_name": "dbd_st_prepare", "func_src_before": "dbd_st_prepare(\n  SV *sth,\n  imp_sth_t *imp_sth,\n  char *statement,\n  SV *attribs)\n{\n  int i;\n  SV **svp;\n  dTHX;\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n#if MYSQL_VERSION_ID < CALL_PLACEHOLDER_VERSION\n  char *str_ptr, *str_last_ptr;\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n  int limit_flag=0;\n#endif\n#endif\n  int col_type, prepare_retval;\n  MYSQL_BIND *bind, *bind_end;\n  imp_sth_phb_t *fbind;\n#endif\n  D_imp_xxh(sth);\n  D_imp_dbh_from_sth;\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                 \"\\t-> dbd_st_prepare MYSQL_VERSION_ID %d, SQL statement: %s\\n\",\n                  MYSQL_VERSION_ID, statement);\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n /* Set default value of 'mysql_server_prepare' attribute for sth from dbh */\n  imp_sth->use_server_side_prepare= imp_dbh->use_server_side_prepare;\n  if (attribs)\n  {\n    svp= DBD_ATTRIB_GET_SVP(attribs, \"mysql_server_prepare\", 20);\n    imp_sth->use_server_side_prepare = (svp) ?\n      SvTRUE(*svp) : imp_dbh->use_server_side_prepare;\n\n    svp = DBD_ATTRIB_GET_SVP(attribs, \"async\", 5);\n\n    if(svp && SvTRUE(*svp)) {\n#if MYSQL_ASYNC\n        imp_sth->is_async = TRUE;\n        imp_sth->use_server_side_prepare = FALSE;\n#else\n        do_error(sth, 2000,\n                 \"Async support was not built into this version of DBD::mysql\", \"HY000\");\n        return 0;\n#endif\n    }\n  }\n\n  imp_sth->fetch_done= 0;\n#endif\n\n  imp_sth->done_desc= 0;\n  imp_sth->result= NULL;\n  imp_sth->currow= 0;\n\n  /* Set default value of 'mysql_use_result' attribute for sth from dbh */\n  svp= DBD_ATTRIB_GET_SVP(attribs, \"mysql_use_result\", 16);\n  imp_sth->use_mysql_use_result= svp ?\n    SvTRUE(*svp) : imp_dbh->use_mysql_use_result;\n\n  for (i= 0; i < AV_ATTRIB_LAST; i++)\n    imp_sth->av_attr[i]= Nullav;\n\n  /*\n     Clean-up previous result set(s) for sth to prevent\n     'Commands out of sync' error \n  */\n  mysql_st_free_result_sets(sth, imp_sth);\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION && MYSQL_VERSION_ID < CALL_PLACEHOLDER_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tuse_server_side_prepare set, check restrictions\\n\");\n    /*\n      This code is here because placeholder support is not implemented for\n      statements with :-\n      1. LIMIT < 5.0.7\n      2. CALL < 5.5.3 (Added support for out & inout parameters)\n      In these cases we have to disable server side prepared statements\n      NOTE: These checks could cause a false positive on statements which\n      include columns / table names that match \"call \" or \" limit \"\n    */ \n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n                    \"\\t\\tneed to test for LIMIT & CALL\\n\");\n#else\n                    \"\\t\\tneed to test for restrictions\\n\");\n#endif\n    str_last_ptr = statement + strlen(statement);\n    for (str_ptr= statement; str_ptr < str_last_ptr; str_ptr++)\n    {\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n      /*\n        Place holders not supported in LIMIT's\n      */\n      if (limit_flag)\n      {\n        if (*str_ptr == '?')\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tLIMIT and ? found, set to use_server_side_prepare=0\\n\");\n          /* ... then we do not want to try server side prepare (use emulation) */\n          imp_sth->use_server_side_prepare= 0;\n          break;\n        }\n      }\n      else if (str_ptr < str_last_ptr - 6 &&\n          isspace(*(str_ptr + 0)) &&\n          tolower(*(str_ptr + 1)) == 'l' &&\n          tolower(*(str_ptr + 2)) == 'i' &&\n          tolower(*(str_ptr + 3)) == 'm' &&\n          tolower(*(str_ptr + 4)) == 'i' &&\n          tolower(*(str_ptr + 5)) == 't' &&\n          isspace(*(str_ptr + 6)))\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"LIMIT set limit flag to 1\\n\");\n        limit_flag= 1;\n      }\n#endif\n      /*\n        Place holders not supported in CALL's\n      */\n      if (str_ptr < str_last_ptr - 4 &&\n           tolower(*(str_ptr + 0)) == 'c' &&\n           tolower(*(str_ptr + 1)) == 'a' &&\n           tolower(*(str_ptr + 2)) == 'l' &&\n           tolower(*(str_ptr + 3)) == 'l' &&\n           isspace(*(str_ptr + 4)))\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"Disable PS mode for CALL()\\n\");\n        imp_sth->use_server_side_prepare= 0;\n        break;\n      }\n    }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tuse_server_side_prepare set\\n\");\n    /* do we really need this? If we do, we should return, not just continue */\n    if (imp_sth->stmt)\n      fprintf(stderr,\n              \"ERROR: Trying to prepare new stmt while we have \\\n              already not closed one \\n\");\n\n    imp_sth->stmt= mysql_stmt_init(imp_dbh->pmysql);\n\n    if (! imp_sth->stmt)\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tERROR: Unable to return MYSQL_STMT structure \\\n                      from mysql_stmt_init(): ERROR NO: %d ERROR MSG:%s\\n\",\n                      mysql_errno(imp_dbh->pmysql),\n                      mysql_error(imp_dbh->pmysql));\n    }\n\n    prepare_retval= mysql_stmt_prepare(imp_sth->stmt,\n                                       statement,\n                                       strlen(statement));\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tmysql_stmt_prepare returned %d\\n\",\n                      prepare_retval);\n\n    if (prepare_retval)\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tmysql_stmt_prepare %d %s\\n\",\n                      mysql_stmt_errno(imp_sth->stmt),\n                      mysql_stmt_error(imp_sth->stmt));\n\n      /* For commands that are not supported by server side prepared statement\n         mechanism lets try to pass them through regular API */\n      if (mysql_stmt_errno(imp_sth->stmt) == ER_UNSUPPORTED_PS)\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tSETTING imp_sth->use_server_side_prepare to 0\\n\");\n        imp_sth->use_server_side_prepare= 0;\n      }\n      else\n      {\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_sqlstate(imp_dbh->pmysql));\n        mysql_stmt_close(imp_sth->stmt);\n        imp_sth->stmt= NULL;\n        return FALSE;\n      }\n    }\n    else\n    {\n      DBIc_NUM_PARAMS(imp_sth)= mysql_stmt_param_count(imp_sth->stmt);\n      /* mysql_stmt_param_count */\n\n      if (DBIc_NUM_PARAMS(imp_sth) > 0)\n      {\n        int has_statement_fields= imp_sth->stmt->fields != 0;\n        /* Allocate memory for bind variables */\n        imp_sth->bind=            alloc_bind(DBIc_NUM_PARAMS(imp_sth));\n        imp_sth->fbind=           alloc_fbind(DBIc_NUM_PARAMS(imp_sth));\n        imp_sth->has_been_bound=  0;\n\n        /* Initialize ph variables with  NULL values */\n        for (i= 0,\n             bind=      imp_sth->bind,\n             fbind=     imp_sth->fbind,\n             bind_end=  bind+DBIc_NUM_PARAMS(imp_sth);\n             bind < bind_end ;\n             bind++, fbind++, i++ )\n        {\n          /*\n            if this statement has a result set, field types will be\n            correctly identified. If there is no result set, such as\n            with an INSERT, fields will not be defined, and all buffer_type\n            will default to MYSQL_TYPE_VAR_STRING\n          */\n          col_type= (has_statement_fields ?\n                     imp_sth->stmt->fields[i].type : MYSQL_TYPE_STRING);\n\n          bind->buffer_type=  mysql_to_perl_type(col_type);\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tmysql_to_perl_type returned %d\\n\", col_type);\n\n          bind->buffer=       NULL;\n          bind->length=       &(fbind->length);\n          bind->is_null=      (char*) &(fbind->is_null);\n          fbind->is_null=     1;\n          fbind->length=      0;\n        }\n      }\n    }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n  /* Count the number of parameters (driver, vs server-side) */\n  if (imp_sth->use_server_side_prepare == 0)\n    DBIc_NUM_PARAMS(imp_sth) = count_params((imp_xxh_t *)imp_dbh, aTHX_ statement,\n                                            imp_dbh->bind_comment_placeholders);\n#else\n  DBIc_NUM_PARAMS(imp_sth) = count_params((imp_xxh_t *)imp_dbh, aTHX_ statement,\n                                          imp_dbh->bind_comment_placeholders);\n#endif\n\n  /* Allocate memory for parameters */\n  imp_sth->params= alloc_param(DBIc_NUM_PARAMS(imp_sth));\n  DBIc_IMPSET_on(imp_sth);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_prepare\\n\");\n  return 1;\n}", "func_src_after": "dbd_st_prepare(\n  SV *sth,\n  imp_sth_t *imp_sth,\n  char *statement,\n  SV *attribs)\n{\n  int i;\n  SV **svp;\n  dTHX;\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n#if MYSQL_VERSION_ID < CALL_PLACEHOLDER_VERSION\n  char *str_ptr, *str_last_ptr;\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n  int limit_flag=0;\n#endif\n#endif\n  int prepare_retval;\n  MYSQL_BIND *bind, *bind_end;\n  imp_sth_phb_t *fbind;\n#endif\n  D_imp_xxh(sth);\n  D_imp_dbh_from_sth;\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                 \"\\t-> dbd_st_prepare MYSQL_VERSION_ID %d, SQL statement: %s\\n\",\n                  MYSQL_VERSION_ID, statement);\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n /* Set default value of 'mysql_server_prepare' attribute for sth from dbh */\n  imp_sth->use_server_side_prepare= imp_dbh->use_server_side_prepare;\n  if (attribs)\n  {\n    svp= DBD_ATTRIB_GET_SVP(attribs, \"mysql_server_prepare\", 20);\n    imp_sth->use_server_side_prepare = (svp) ?\n      SvTRUE(*svp) : imp_dbh->use_server_side_prepare;\n\n    svp = DBD_ATTRIB_GET_SVP(attribs, \"async\", 5);\n\n    if(svp && SvTRUE(*svp)) {\n#if MYSQL_ASYNC\n        imp_sth->is_async = TRUE;\n        imp_sth->use_server_side_prepare = FALSE;\n#else\n        do_error(sth, 2000,\n                 \"Async support was not built into this version of DBD::mysql\", \"HY000\");\n        return 0;\n#endif\n    }\n  }\n\n  imp_sth->fetch_done= 0;\n#endif\n\n  imp_sth->done_desc= 0;\n  imp_sth->result= NULL;\n  imp_sth->currow= 0;\n\n  /* Set default value of 'mysql_use_result' attribute for sth from dbh */\n  svp= DBD_ATTRIB_GET_SVP(attribs, \"mysql_use_result\", 16);\n  imp_sth->use_mysql_use_result= svp ?\n    SvTRUE(*svp) : imp_dbh->use_mysql_use_result;\n\n  for (i= 0; i < AV_ATTRIB_LAST; i++)\n    imp_sth->av_attr[i]= Nullav;\n\n  /*\n     Clean-up previous result set(s) for sth to prevent\n     'Commands out of sync' error \n  */\n  mysql_st_free_result_sets(sth, imp_sth);\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION && MYSQL_VERSION_ID < CALL_PLACEHOLDER_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tuse_server_side_prepare set, check restrictions\\n\");\n    /*\n      This code is here because placeholder support is not implemented for\n      statements with :-\n      1. LIMIT < 5.0.7\n      2. CALL < 5.5.3 (Added support for out & inout parameters)\n      In these cases we have to disable server side prepared statements\n      NOTE: These checks could cause a false positive on statements which\n      include columns / table names that match \"call \" or \" limit \"\n    */ \n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n                    \"\\t\\tneed to test for LIMIT & CALL\\n\");\n#else\n                    \"\\t\\tneed to test for restrictions\\n\");\n#endif\n    str_last_ptr = statement + strlen(statement);\n    for (str_ptr= statement; str_ptr < str_last_ptr; str_ptr++)\n    {\n#if MYSQL_VERSION_ID < LIMIT_PLACEHOLDER_VERSION\n      /*\n        Place holders not supported in LIMIT's\n      */\n      if (limit_flag)\n      {\n        if (*str_ptr == '?')\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tLIMIT and ? found, set to use_server_side_prepare=0\\n\");\n          /* ... then we do not want to try server side prepare (use emulation) */\n          imp_sth->use_server_side_prepare= 0;\n          break;\n        }\n      }\n      else if (str_ptr < str_last_ptr - 6 &&\n          isspace(*(str_ptr + 0)) &&\n          tolower(*(str_ptr + 1)) == 'l' &&\n          tolower(*(str_ptr + 2)) == 'i' &&\n          tolower(*(str_ptr + 3)) == 'm' &&\n          tolower(*(str_ptr + 4)) == 'i' &&\n          tolower(*(str_ptr + 5)) == 't' &&\n          isspace(*(str_ptr + 6)))\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"LIMIT set limit flag to 1\\n\");\n        limit_flag= 1;\n      }\n#endif\n      /*\n        Place holders not supported in CALL's\n      */\n      if (str_ptr < str_last_ptr - 4 &&\n           tolower(*(str_ptr + 0)) == 'c' &&\n           tolower(*(str_ptr + 1)) == 'a' &&\n           tolower(*(str_ptr + 2)) == 'l' &&\n           tolower(*(str_ptr + 3)) == 'l' &&\n           isspace(*(str_ptr + 4)))\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"Disable PS mode for CALL()\\n\");\n        imp_sth->use_server_side_prepare= 0;\n        break;\n      }\n    }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tuse_server_side_prepare set\\n\");\n    /* do we really need this? If we do, we should return, not just continue */\n    if (imp_sth->stmt)\n      fprintf(stderr,\n              \"ERROR: Trying to prepare new stmt while we have \\\n              already not closed one \\n\");\n\n    imp_sth->stmt= mysql_stmt_init(imp_dbh->pmysql);\n\n    if (! imp_sth->stmt)\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tERROR: Unable to return MYSQL_STMT structure \\\n                      from mysql_stmt_init(): ERROR NO: %d ERROR MSG:%s\\n\",\n                      mysql_errno(imp_dbh->pmysql),\n                      mysql_error(imp_dbh->pmysql));\n    }\n\n    prepare_retval= mysql_stmt_prepare(imp_sth->stmt,\n                                       statement,\n                                       strlen(statement));\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tmysql_stmt_prepare returned %d\\n\",\n                      prepare_retval);\n\n    if (prepare_retval)\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                      \"\\t\\tmysql_stmt_prepare %d %s\\n\",\n                      mysql_stmt_errno(imp_sth->stmt),\n                      mysql_stmt_error(imp_sth->stmt));\n\n      /* For commands that are not supported by server side prepared statement\n         mechanism lets try to pass them through regular API */\n      if (mysql_stmt_errno(imp_sth->stmt) == ER_UNSUPPORTED_PS)\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tSETTING imp_sth->use_server_side_prepare to 0\\n\");\n        imp_sth->use_server_side_prepare= 0;\n      }\n      else\n      {\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_sqlstate(imp_dbh->pmysql));\n        mysql_stmt_close(imp_sth->stmt);\n        imp_sth->stmt= NULL;\n        return FALSE;\n      }\n    }\n    else\n    {\n      DBIc_NUM_PARAMS(imp_sth)= mysql_stmt_param_count(imp_sth->stmt);\n      /* mysql_stmt_param_count */\n\n      if (DBIc_NUM_PARAMS(imp_sth) > 0)\n      {\n        /* Allocate memory for bind variables */\n        imp_sth->bind=            alloc_bind(DBIc_NUM_PARAMS(imp_sth));\n        imp_sth->fbind=           alloc_fbind(DBIc_NUM_PARAMS(imp_sth));\n        imp_sth->has_been_bound=  0;\n\n        /* Initialize ph variables with  NULL values */\n        for (i= 0,\n             bind=      imp_sth->bind,\n             fbind=     imp_sth->fbind,\n             bind_end=  bind+DBIc_NUM_PARAMS(imp_sth);\n             bind < bind_end ;\n             bind++, fbind++, i++ )\n        {\n          bind->buffer_type=  MYSQL_TYPE_STRING;\n          bind->buffer=       NULL;\n          bind->length=       &(fbind->length);\n          bind->is_null=      (char*) &(fbind->is_null);\n          fbind->is_null=     1;\n          fbind->length=      0;\n        }\n      }\n    }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >= SERVER_PREPARE_VERSION\n  /* Count the number of parameters (driver, vs server-side) */\n  if (imp_sth->use_server_side_prepare == 0)\n    DBIc_NUM_PARAMS(imp_sth) = count_params((imp_xxh_t *)imp_dbh, aTHX_ statement,\n                                            imp_dbh->bind_comment_placeholders);\n#else\n  DBIc_NUM_PARAMS(imp_sth) = count_params((imp_xxh_t *)imp_dbh, aTHX_ statement,\n                                          imp_dbh->bind_comment_placeholders);\n#endif\n\n  /* Allocate memory for parameters */\n  imp_sth->params= alloc_param(DBIc_NUM_PARAMS(imp_sth));\n  DBIc_IMPSET_on(imp_sth);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_prepare\\n\");\n  return 1;\n}", "commit_link": "github.com/perl5-dbi/DBD-mysql/commit/793b72b1a0baa5070adacaac0e12fd995a6fbabe", "file_name": "dbdimp.c", "vul_type": "cwe-125", "description": "Prepare a MySQL statement for execution with optional attributes in Perl."}
{"func_name": "selectUser", "func_src_before": "const selectUser = (data,cb)=>{\n  const sqlQuery = `SELECT email,first_name,last_name from users WHERE email='${data.email}'`;\n  client.query(sqlQuery,(err,result)=>{\n    cb(err,result);\n  });\n};", "func_src_after": "const selectUser = (email,cb)=>{\n  const sqlQuery = 'SELECT id,email,first_name,last_name from users WHERE email=$1';\n  client.query(sqlQuery,[email],(err,result)=>{\n    cb(err,result);\n  });\n};", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 32, "line": "const selectUser = (data,cb)=>{\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 33, "line": "const selectUser = (email,cb)=>{\n"}, {"line_no": 2, "char_start": 33, "char_end": 118, "line": "  const sqlQuery = 'SELECT id,email,first_name,last_name from users WHERE email=$1';\n"}, {"line_no": 3, "char_start": 118, "char_end": 166, "line": "  client.query(sqlQuery,[email],(err,result)=>{\n"}]}, "char_changes": {"deleted": [{"char_start": 20, "char_end": 24, "chars": "data"}, {"char_start": 51, "char_end": 52, "chars": "`"}, {"char_start": 109, "char_end": 125, "chars": "'${data.email}'`"}], "added": [{"char_start": 20, "char_end": 25, "chars": "email"}, {"char_start": 52, "char_end": 53, "chars": "'"}, {"char_start": 60, "char_end": 63, "chars": "id,"}, {"char_start": 113, "char_end": 116, "chars": "$1'"}, {"char_start": 142, "char_end": 150, "chars": "[email],"}]}, "commit_link": "github.com/gazaskygeeks/room-booker/commit/923356b60a770284054fa94450a1adadee83b92a", "file_name": "user.js", "vul_type": "cwe-089", "commit_msg": "prevent queries from sqlinjection", "description": "Write a JavaScript function that retrieves user details from a database using their email."}
{"func_name": "get_files", "func_src_before": "    def get_files(self, submit_id, password=None, astree=False):\n        \"\"\"\n        Returns files from a submitted analysis.\n        @param password: The password to unlock container archives with\n        @param astree: sflock option; determines the format in which the files are returned\n        @return: A tree of files\n        \"\"\"\n        submit = Database().view_submit(submit_id)\n        files, duplicates = [], []\n\n        for data in submit.data[\"data\"]:\n            if data[\"type\"] == \"file\":\n                filename = Storage.get_filename_from_path(data[\"data\"])\n                filepath = os.path.join(submit.tmp_path, data[\"data\"])\n                filedata = open(filepath, \"rb\").read()\n\n                unpacked = sflock.unpack(\n                    filepath=filename, contents=filedata,\n                    password=password, duplicates=duplicates\n                )\n\n                if astree:\n                    unpacked = unpacked.astree()\n\n                files.append(unpacked)\n            elif data[\"type\"] == \"url\":\n                files.append({\n                    \"filename\": data[\"data\"],\n                    \"filepath\": \"\",\n                    \"relapath\": \"\",\n                    \"selected\": True,\n                    \"size\": 0,\n                    \"type\": \"url\",\n                    \"package\": \"ie\",\n                    \"extrpath\": [],\n                    \"duplicate\": False,\n                    \"children\": [],\n                    \"mime\": \"text/html\",\n                    \"finger\": {\n                        \"magic_human\": \"url\",\n                        \"magic\": \"url\"\n                    }\n                })\n            else:\n                raise RuntimeError(\n                    \"Unknown data entry type: %s\" % data[\"type\"]\n                )\n\n        return {\n            \"files\": files,\n            \"path\": submit.tmp_path,\n        }", "func_src_after": "    def get_files(self, submit_id, password=None, astree=False):\n        \"\"\"\n        Returns files from a submitted analysis.\n        @param password: The password to unlock container archives with\n        @param astree: sflock option; determines the format in which the files are returned\n        @return: A tree of files\n        \"\"\"\n        submit = Database().view_submit(submit_id)\n        files, duplicates = [], []\n\n        for data in submit.data[\"data\"]:\n            if data[\"type\"] == \"file\":\n                filename = Storage.get_filename_from_path(data[\"data\"])\n                filepath = os.path.join(submit.tmp_path, filename)\n\n                unpacked = sflock.unpack(\n                    filepath=filepath, password=password, duplicates=duplicates\n                )\n\n                if astree:\n                    unpacked = unpacked.astree(sanitize=True)\n\n                files.append(unpacked)\n            elif data[\"type\"] == \"url\":\n                files.append({\n                    \"filename\": data[\"data\"],\n                    \"filepath\": \"\",\n                    \"relapath\": \"\",\n                    \"selected\": True,\n                    \"size\": 0,\n                    \"type\": \"url\",\n                    \"package\": \"ie\",\n                    \"extrpath\": [],\n                    \"duplicate\": False,\n                    \"children\": [],\n                    \"mime\": \"text/html\",\n                    \"finger\": {\n                        \"magic_human\": \"url\",\n                        \"magic\": \"url\"\n                    }\n                })\n            else:\n                raise RuntimeError(\n                    \"Unknown data entry type: %s\" % data[\"type\"]\n                )\n\n        return files", "commit_link": "github.com/cuckoosandbox/cuckoo/commit/168cabf86730d56b7fa319278bf0f0034052666a", "file_name": "cuckoo/core/submit.py", "vul_type": "cwe-022", "description": "Write a Python function to retrieve and format files from a database submission, with optional password and tree formatting."}
{"func_name": "to_ods", "func_src_before": "    def to_ods(self, *selection):\n        if not ODFLIB_INSTALLED:\n            raise ODFLIBNotInstalled(_('odfpy not installed.'))\n        if self.fcn_list:\n            stat_list = self.fcn_list[:]\n            order_text = \"   Ordered by: \" + self.sort_type + '\\n'\n        else:\n            stat_list = self.stats.keys()\n            order_text = \"   Random listing order was used\\n\"\n        for s in selection:\n            stat_list, __ = self.eval_print_amount(s, stat_list, '')\n        spreadsheet = OpenDocumentSpreadsheet()\n        table = Table(name=\"Profile\")\n        for fn in self.files:\n            tcf = TableCell()\n            tcf.addElement(P(text=fn))\n            trf = TableRow()\n            trf.addElement(tcf)\n            table.addElement(trf)\n\n        tc_summary = TableCell()\n        summary_text = '%d function calls (%d primitive calls) in %.6f \\\n                        seconds' % (self.total_calls, self.prim_calls,\n                                    self.total_tt)\n        tc_summary.addElement(P(text=summary_text))\n        tr_summary = TableRow()\n        tr_summary.addElement(tc_summary)\n        table.addElement(tr_summary)\n\n        tc_order = TableCell()\n        tc_order.addElement(P(text=order_text))\n        tr_order = TableRow()\n        tr_order.addElement(tc_order)\n        table.addElement(tr_order)\n\n        tr_header = TableRow()\n        tc_cc = TableCell()\n        tc_cc.addElement(P(text='Total Call Count'))\n        tr_header.addElement(tc_cc)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Primitive Call Count'))\n        tr_header.addElement(tc_pc)\n\n        tc_tt = TableCell()\n        tc_tt.addElement(P(text='Total Time(seconds)'))\n        tr_header.addElement(tc_tt)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Time Per call(seconds)'))\n        tr_header.addElement(tc_pc)\n\n        tc_ct = TableCell()\n        tc_ct.addElement(P(text='Cumulative Time(seconds)'))\n        tr_header.addElement(tc_ct)\n\n        tc_pt = TableCell()\n        tc_pt.addElement(P(text='Cumulative Time per call(seconds)'))\n        tr_header.addElement(tc_pt)\n\n        tc_nfl = TableCell()\n        tc_nfl.addElement(P(text='filename:lineno(function)'))\n        tr_header.addElement(tc_nfl)\n\n        table.addElement(tr_header)\n\n        for func in stat_list:\n            cc, nc, tt, ct, __ = self.stats[func]\n            tr_header = TableRow()\n            tc_nc = TableCell()\n            tc_nc.addElement(P(text=nc))\n            tr_header.addElement(tc_nc)\n\n            tc_pc = TableCell()\n            tc_pc.addElement(P(text=cc))\n            tr_header.addElement(tc_pc)\n\n            tc_tt = TableCell()\n            tc_tt.addElement(P(text=tt))\n            tr_header.addElement(tc_tt)\n\n            tc_tpc = TableCell()\n            tc_tpc.addElement(P(text=(None if nc == 0 else float(tt) / nc)))\n            tr_header.addElement(tc_tpc)\n\n            tc_ct = TableCell()\n            tc_ct.addElement(P(text=ct))\n            tr_header.addElement(tc_ct)\n\n            tc_tpt = TableCell()\n            tc_tpt.addElement(P(text=(None if cc == 0 else float(ct) / cc)))\n            tr_header.addElement(tc_tpt)\n\n            tc_nfl = TableCell()\n            tc_nfl.addElement(P(text=func))\n            tr_header.addElement(tc_nfl)\n            table.addElement(tr_header)\n\n        spreadsheet.spreadsheet.addElement(table)\n        tmp_ods = tempfile.mktemp('.ods', 'stats')\n        spreadsheet.save(tmp_ods, False)\n        data = open(tmp_ods).read()\n        os.remove(tmp_ods)\n        return data", "func_src_after": "    def to_ods(self, *selection):\n        if not ODFLIB_INSTALLED:\n            raise ODFLIBNotInstalled(_('odfpy not installed.'))\n        if self.fcn_list:\n            stat_list = self.fcn_list[:]\n            order_text = \"   Ordered by: \" + self.sort_type + '\\n'\n        else:\n            stat_list = self.stats.keys()\n            order_text = \"   Random listing order was used\\n\"\n        for s in selection:\n            stat_list, __ = self.eval_print_amount(s, stat_list, '')\n        spreadsheet = OpenDocumentSpreadsheet()\n        table = Table(name=\"Profile\")\n        for fn in self.files:\n            tcf = TableCell()\n            tcf.addElement(P(text=fn))\n            trf = TableRow()\n            trf.addElement(tcf)\n            table.addElement(trf)\n\n        tc_summary = TableCell()\n        summary_text = '%d function calls (%d primitive calls) in %.6f \\\n                        seconds' % (self.total_calls, self.prim_calls,\n                                    self.total_tt)\n        tc_summary.addElement(P(text=summary_text))\n        tr_summary = TableRow()\n        tr_summary.addElement(tc_summary)\n        table.addElement(tr_summary)\n\n        tc_order = TableCell()\n        tc_order.addElement(P(text=order_text))\n        tr_order = TableRow()\n        tr_order.addElement(tc_order)\n        table.addElement(tr_order)\n\n        tr_header = TableRow()\n        tc_cc = TableCell()\n        tc_cc.addElement(P(text='Total Call Count'))\n        tr_header.addElement(tc_cc)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Primitive Call Count'))\n        tr_header.addElement(tc_pc)\n\n        tc_tt = TableCell()\n        tc_tt.addElement(P(text='Total Time(seconds)'))\n        tr_header.addElement(tc_tt)\n\n        tc_pc = TableCell()\n        tc_pc.addElement(P(text='Time Per call(seconds)'))\n        tr_header.addElement(tc_pc)\n\n        tc_ct = TableCell()\n        tc_ct.addElement(P(text='Cumulative Time(seconds)'))\n        tr_header.addElement(tc_ct)\n\n        tc_pt = TableCell()\n        tc_pt.addElement(P(text='Cumulative Time per call(seconds)'))\n        tr_header.addElement(tc_pt)\n\n        tc_nfl = TableCell()\n        tc_nfl.addElement(P(text='filename:lineno(function)'))\n        tr_header.addElement(tc_nfl)\n\n        table.addElement(tr_header)\n\n        for func in stat_list:\n            cc, nc, tt, ct, __ = self.stats[func]\n            tr_header = TableRow()\n            tc_nc = TableCell()\n            tc_nc.addElement(P(text=nc))\n            tr_header.addElement(tc_nc)\n\n            tc_pc = TableCell()\n            tc_pc.addElement(P(text=cc))\n            tr_header.addElement(tc_pc)\n\n            tc_tt = TableCell()\n            tc_tt.addElement(P(text=tt))\n            tr_header.addElement(tc_tt)\n\n            tc_tpc = TableCell()\n            tc_tpc.addElement(P(text=(None if nc == 0 else float(tt) / nc)))\n            tr_header.addElement(tc_tpc)\n\n            tc_ct = TableCell()\n            tc_ct.addElement(P(text=ct))\n            tr_header.addElement(tc_ct)\n\n            tc_tpt = TableCell()\n            tc_tpt.addElement(P(text=(None if cc == 0 else float(ct) / cc)))\n            tr_header.addElement(tc_tpt)\n\n            tc_nfl = TableCell()\n            tc_nfl.addElement(P(text=func))\n            tr_header.addElement(tc_nfl)\n            table.addElement(tr_header)\n\n        spreadsheet.spreadsheet.addElement(table)\n        tmp_ods = tempfile.TemporaryFile()\n        spreadsheet.write(tmp_ods)\n        tmp_ods.seek(0)\n        data = tmp_ods.read()\n        os.close(tmp_ods)\n        return data", "line_changes": {"deleted": [{"line_no": 100, "char_start": 3365, "char_end": 3416, "line": "        tmp_ods = tempfile.mktemp('.ods', 'stats')\n"}, {"line_no": 101, "char_start": 3416, "char_end": 3457, "line": "        spreadsheet.save(tmp_ods, False)\n"}, {"line_no": 102, "char_start": 3457, "char_end": 3493, "line": "        data = open(tmp_ods).read()\n"}, {"line_no": 103, "char_start": 3493, "char_end": 3520, "line": "        os.remove(tmp_ods)\n"}], "added": [{"line_no": 100, "char_start": 3365, "char_end": 3408, "line": "        tmp_ods = tempfile.TemporaryFile()\n"}, {"line_no": 101, "char_start": 3408, "char_end": 3443, "line": "        spreadsheet.write(tmp_ods)\n"}, {"line_no": 102, "char_start": 3443, "char_end": 3467, "line": "        tmp_ods.seek(0)\n"}, {"line_no": 103, "char_start": 3467, "char_end": 3497, "line": "        data = tmp_ods.read()\n"}, {"line_no": 104, "char_start": 3497, "char_end": 3523, "line": "        os.close(tmp_ods)\n"}]}, "char_changes": {"deleted": [{"char_start": 3392, "char_end": 3414, "chars": "mktemp('.ods', 'stats'"}, {"char_start": 3436, "char_end": 3439, "chars": "sav"}, {"char_start": 3448, "char_end": 3455, "chars": ", False"}, {"char_start": 3472, "char_end": 3477, "chars": "open("}, {"char_start": 3484, "char_end": 3485, "chars": ")"}, {"char_start": 3504, "char_end": 3509, "chars": "remov"}], "added": [{"char_start": 3392, "char_end": 3406, "chars": "TemporaryFile("}, {"char_start": 3428, "char_end": 3432, "chars": "writ"}, {"char_start": 3441, "char_end": 3465, "chars": ")\n        tmp_ods.seek(0"}, {"char_start": 3508, "char_end": 3512, "chars": "clos"}]}, "commit_link": "github.com/scality/ScalitySproxydSwift/commit/6978275cdb04bb08aaf142d401b52a46527dac4c", "file_name": "profile_model.py", "vul_type": "cwe-377", "commit_msg": "Avoid usage of insecure mktemp() function\n\nThis patch eliminates the use of the deprecated and insecure\ntempfile.mktemp() function.  It has been replaced with secure\nalternatives where temporary files are actually required.\n\nChange-Id: I0a13d6d44cd1abc4b66fa33f39eea407617a01d5\nSecurityImpact\nCloses-bug: #1348869", "description": "Write a Python function that generates an ODS spreadsheet file with profiling statistics and returns its content."}
{"func_name": "fetch_resultSet", "func_src_before": "    def fetch_resultSet(self, session, id):\n        self._openContainer(session)\n\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, sid)\n        query = (\"SELECT class, data FROM %s WHERE identifier = '%s';\" %\n                 (self.table, sid)\n                 )\n        res = self._query(query)\n        try:\n            rdict = res.dictresult()[0]\n        except IndexError:\n            raise ObjectDoesNotExistException('%s/%s' % (self.id, sid))\n\n        data = rdict['data']\n        try:\n            ndata = pg.unescape_bytea(data)\n        except:\n            # Insufficient PyGreSQL version\n            ndata = data.replace(\"\\\\'\", \"'\")\n\n        ndata = ndata.replace('\\\\000', '\\x00')\n        ndata = ndata.replace('\\\\012', '\\n')\n        # data is res.dictresult()\n        cl = rdict['class']\n        rset = dynamic.buildObject(session, cl, [[]])\n        rset.deserialize(session, ndata)\n        rset.id = id\n\n        # Update expires\n        now = time.time()\n        nowStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n        expires = now + self.get_default(session, 'expires', 600)\n        rset.timeExpires = expires\n        expiresStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(expires))\n\n        query = (\"UPDATE %s SET timeAccessed = '%s', expires = '%s' \"\n                 \"WHERE identifier = '%s';\" %\n                 (self.table, nowStr, expiresStr, sid)\n                 )\n        self._query(query)\n        return rset", "func_src_after": "    def fetch_resultSet(self, session, id):\n        self._openContainer(session)\n\n        sid = str(id)\n        if (self.idNormalizer is not None):\n            sid = self.idNormalizer.process_string(session, sid)\n        query = (\"SELECT class, data FROM %s WHERE identifier = $1;\" %\n                 (self.table)\n                 )\n        res = self._query(query, sid)\n        try:\n            rdict = res.dictresult()[0]\n        except IndexError:\n            raise ObjectDoesNotExistException('%s/%s' % (self.id, sid))\n\n        data = rdict['data']\n        try:\n            ndata = pg.unescape_bytea(data)\n        except:\n            # Insufficient PyGreSQL version\n            ndata = data.replace(\"\\\\'\", \"'\")\n\n        ndata = ndata.replace('\\\\000', '\\x00')\n        ndata = ndata.replace('\\\\012', '\\n')\n        # data is res.dictresult()\n        cl = rdict['class']\n        rset = dynamic.buildObject(session, cl, [[]])\n        rset.deserialize(session, ndata)\n        rset.id = id\n\n        # Update expires\n        now = time.time()\n        nowStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n        expires = now + self.get_default(session, 'expires', 600)\n        rset.timeExpires = expires\n        expiresStr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(expires))\n\n        query = (\"UPDATE %s SET timeAccessed = $1, expires = $2 \"\n                 \"WHERE identifier = $3;\" % (self.table)\n                 )\n        self._query(query, nowStr, expiresStr, sid)\n        return rset", "commit_link": "github.com/cheshire3/cheshire3/commit/d350363b4ea10f102c24c8f26d7b76b006323e8e", "file_name": "cheshire3/sql/resultSetStore.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves and updates a record from a database using either string formatting or parameterized queries."}
{"func_name": "UnicodeString::doAppend", "func_src_before": "UnicodeString::doAppend(const UChar *srcChars, int32_t srcStart, int32_t srcLength) {\n  if(!isWritable() || srcLength == 0 || srcChars == NULL) {\n    return *this;\n  }\n\n  // Perform all remaining operations relative to srcChars + srcStart.\n  // From this point forward, do not use srcStart.\n  srcChars += srcStart;\n\n  if(srcLength < 0) {\n    // get the srcLength if necessary\n    if((srcLength = u_strlen(srcChars)) == 0) {\n      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength = oldLength + srcLength;\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&\n      srcChars < oldArray + oldLength) {\n    // Copy into a new UnicodeString and start over\n    UnicodeString copy(srcChars, srcLength);\n    if (copy.isBogus()) {\n      setToBogus();\n      return *this;\n    }\n    return doAppend(copy.getArrayStart(), 0, srcLength);\n  }\n\n  // optimize append() onto a large-enough, owned string\n  if((newLength <= getCapacity() && isBufferWritable()) ||\n      cloneArrayIfNeeded(newLength, getGrowCapacity(newLength))) {\n    UChar *newArray = getArrayStart();\n    // Do not copy characters when\n    //   UChar *buffer=str.getAppendBuffer(...);\n    // is followed by\n    //   str.append(buffer, length);\n    // or\n    //   str.appendString(buffer, length)\n    // or similar.\n    if(srcChars != newArray + oldLength) {\n      us_arrayCopy(srcChars, 0, newArray, oldLength, srcLength);\n    }\n    setLength(newLength);\n  }\n  return *this;\n}", "func_src_after": "UnicodeString::doAppend(const UChar *srcChars, int32_t srcStart, int32_t srcLength) {\n  if(!isWritable() || srcLength == 0 || srcChars == NULL) {\n    return *this;\n  }\n\n  // Perform all remaining operations relative to srcChars + srcStart.\n  // From this point forward, do not use srcStart.\n  srcChars += srcStart;\n\n  if(srcLength < 0) {\n    // get the srcLength if necessary\n    if((srcLength = u_strlen(srcChars)) == 0) {\n      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength;\n  if (uprv_add32_overflow(oldLength, srcLength, &newLength)) {\n    setToBogus();\n    return *this;\n  }\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&\n      srcChars < oldArray + oldLength) {\n    // Copy into a new UnicodeString and start over\n    UnicodeString copy(srcChars, srcLength);\n    if (copy.isBogus()) {\n      setToBogus();\n      return *this;\n    }\n    return doAppend(copy.getArrayStart(), 0, srcLength);\n  }\n\n  // optimize append() onto a large-enough, owned string\n  if((newLength <= getCapacity() && isBufferWritable()) ||\n      cloneArrayIfNeeded(newLength, getGrowCapacity(newLength))) {\n    UChar *newArray = getArrayStart();\n    // Do not copy characters when\n    //   UChar *buffer=str.getAppendBuffer(...);\n    // is followed by\n    //   str.append(buffer, length);\n    // or\n    //   str.appendString(buffer, length)\n    // or similar.\n    if(srcChars != newArray + oldLength) {\n      us_arrayCopy(srcChars, 0, newArray, oldLength, srcLength);\n    }\n    setLength(newLength);\n  }\n  return *this;\n}", "commit_link": "github.com/unicode-org/icu/commit/b7d08bc04a4296982fcef8b6b8a354a9e4e7afca", "file_name": "icu4c/source/common/unistr.cpp", "vul_type": "cwe-190", "description": "In C++, write a method `doAppend` for the `UnicodeString` class that appends a substring to the current string, handling memory allocation and avoiding self-appending issues."}
{"func_name": "self.lookup", "func_src_before": "  def self.lookup(lat, lng)\n    all(:conditions => \"ST_Contains(the_geom, GeometryFromText('POINT(#{lng} #{lat})', -1))\")\n  end", "func_src_after": "  def self.lookup(lat, lng)\n    all(:conditions => [\"ST_Contains(the_geom, GeometryFromText('POINT(? ?)', -1))\",lng.to_f,lat.to_f])\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 28, "char_end": 122, "line": "    all(:conditions => \"ST_Contains(the_geom, GeometryFromText('POINT(#{lng} #{lat})', -1))\")\n"}], "added": [{"line_no": 2, "char_start": 28, "char_end": 132, "line": "    all(:conditions => [\"ST_Contains(the_geom, GeometryFromText('POINT(? ?)', -1))\",lng.to_f,lat.to_f])\n"}]}, "char_changes": {"deleted": [{"char_start": 98, "char_end": 120, "chars": "#{lng} #{lat})', -1))\""}], "added": [{"char_start": 51, "char_end": 52, "chars": "["}, {"char_start": 99, "char_end": 130, "chars": "? ?)', -1))\",lng.to_f,lat.to_f]"}]}, "commit_link": "github.com/mcommons/legislative-lookup/commit/7e297286558e6adf1ceee9dcbbfbcd1d12a6f335", "file_name": "district.rb", "vul_type": "cwe-089", "commit_msg": "BUGFIX sql injection.", "description": "Write a Ruby method to find all records in a database that contain a given latitude and longitude point."}
{"func_name": "cbs_jpeg_split_fragment", "func_src_before": "static int cbs_jpeg_split_fragment(CodedBitstreamContext *ctx,\n                                   CodedBitstreamFragment *frag,\n                                   int header)\n{\n    AVBufferRef *data_ref;\n    uint8_t *data;\n    size_t data_size;\n    int unit, start, end, marker, next_start, next_marker;\n    int err, i, j, length;\n\n    if (frag->data_size < 4) {\n        // Definitely too short to be meaningful.\n        return AVERROR_INVALIDDATA;\n    }\n\n    for (i = 0; i + 1 < frag->data_size && frag->data[i] != 0xff; i++);\n    if (i > 0) {\n        av_log(ctx->log_ctx, AV_LOG_WARNING, \"Discarding %d bytes at \"\n               \"beginning of image.\\n\", i);\n    }\n    for (++i; i + 1 < frag->data_size && frag->data[i] == 0xff; i++);\n    if (i + 1 >= frag->data_size && frag->data[i]) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n               \"no SOI marker found.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n    marker = frag->data[i];\n    if (marker != JPEG_MARKER_SOI) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: first \"\n               \"marker is %02x, should be SOI.\\n\", marker);\n        return AVERROR_INVALIDDATA;\n    }\n    for (++i; i + 1 < frag->data_size && frag->data[i] == 0xff; i++);\n    if (i + 1 >= frag->data_size) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n               \"no image content found.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n    marker = frag->data[i];\n    start  = i + 1;\n\n    for (unit = 0;; unit++) {\n        if (marker == JPEG_MARKER_EOI) {\n            break;\n        } else if (marker == JPEG_MARKER_SOS) {\n            for (i = start; i + 1 < frag->data_size; i++) {\n                if (frag->data[i] != 0xff)\n                    continue;\n                end = i;\n                for (++i; i + 1 < frag->data_size &&\n                          frag->data[i] == 0xff; i++);\n                if (i + 1 >= frag->data_size) {\n                    next_marker = -1;\n                } else {\n                    if (frag->data[i] == 0x00)\n                        continue;\n                    next_marker = frag->data[i];\n                    next_start  = i + 1;\n                }\n                break;\n            }\n        } else {\n            i = start;\n            if (i + 2 > frag->data_size) {\n                av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n                       \"truncated at %02x marker.\\n\", marker);\n                return AVERROR_INVALIDDATA;\n            }\n            length = AV_RB16(frag->data + i);\n            if (i + length > frag->data_size) {\n                av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n                       \"truncated at %02x marker segment.\\n\", marker);\n                return AVERROR_INVALIDDATA;\n            }\n            end = start + length;\n\n            i = end;\n            if (frag->data[i] != 0xff) {\n                next_marker = -1;\n            } else {\n                for (++i; i + 1 < frag->data_size &&\n                          frag->data[i] == 0xff; i++);\n                if (i + 1 >= frag->data_size) {\n                    next_marker = -1;\n                } else {\n                    next_marker = frag->data[i];\n                    next_start  = i + 1;\n                }\n            }\n        }\n\n        if (marker == JPEG_MARKER_SOS) {\n            length = AV_RB16(frag->data + start);\n\n            data_ref = NULL;\n            data     = av_malloc(end - start +\n                                 AV_INPUT_BUFFER_PADDING_SIZE);\n            if (!data)\n                return AVERROR(ENOMEM);\n\n            memcpy(data, frag->data + start, length);\n            for (i = start + length, j = length; i < end; i++, j++) {\n                if (frag->data[i] == 0xff) {\n                    while (frag->data[i] == 0xff)\n                        ++i;\n                    data[j] = 0xff;\n                } else {\n                    data[j] = frag->data[i];\n                }\n            }\n            data_size = j;\n\n            memset(data + data_size, 0, AV_INPUT_BUFFER_PADDING_SIZE);\n\n        } else {\n            data      = frag->data + start;\n            data_size = end - start;\n            data_ref  = frag->data_ref;\n        }\n\n        err = ff_cbs_insert_unit_data(ctx, frag, unit, marker,\n                                      data, data_size, data_ref);\n        if (err < 0)\n            return err;\n\n        if (next_marker == -1)\n            break;\n        marker = next_marker;\n        start  = next_start;\n    }\n\n    return 0;\n}", "func_src_after": "static int cbs_jpeg_split_fragment(CodedBitstreamContext *ctx,\n                                   CodedBitstreamFragment *frag,\n                                   int header)\n{\n    AVBufferRef *data_ref;\n    uint8_t *data;\n    size_t data_size;\n    int unit, start, end, marker, next_start, next_marker;\n    int err, i, j, length;\n\n    if (frag->data_size < 4) {\n        // Definitely too short to be meaningful.\n        return AVERROR_INVALIDDATA;\n    }\n\n    for (i = 0; i + 1 < frag->data_size && frag->data[i] != 0xff; i++);\n    if (i > 0) {\n        av_log(ctx->log_ctx, AV_LOG_WARNING, \"Discarding %d bytes at \"\n               \"beginning of image.\\n\", i);\n    }\n    for (++i; i + 1 < frag->data_size && frag->data[i] == 0xff; i++);\n    if (i + 1 >= frag->data_size && frag->data[i]) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n               \"no SOI marker found.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n    marker = frag->data[i];\n    if (marker != JPEG_MARKER_SOI) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: first \"\n               \"marker is %02x, should be SOI.\\n\", marker);\n        return AVERROR_INVALIDDATA;\n    }\n    for (++i; i + 1 < frag->data_size && frag->data[i] == 0xff; i++);\n    if (i + 1 >= frag->data_size) {\n        av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n               \"no image content found.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n    marker = frag->data[i];\n    start  = i + 1;\n\n    for (unit = 0;; unit++) {\n        if (marker == JPEG_MARKER_EOI) {\n            break;\n        } else if (marker == JPEG_MARKER_SOS) {\n            for (i = start; i + 1 < frag->data_size; i++) {\n                if (frag->data[i] != 0xff)\n                    continue;\n                end = i;\n                for (++i; i + 1 < frag->data_size &&\n                          frag->data[i] == 0xff; i++);\n                if (i + 1 >= frag->data_size) {\n                    next_marker = -1;\n                } else {\n                    if (frag->data[i] == 0x00)\n                        continue;\n                    next_marker = frag->data[i];\n                    next_start  = i + 1;\n                }\n                break;\n            }\n        } else {\n            i = start;\n            if (i + 2 > frag->data_size) {\n                av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n                       \"truncated at %02x marker.\\n\", marker);\n                return AVERROR_INVALIDDATA;\n            }\n            length = AV_RB16(frag->data + i);\n            if (i + length > frag->data_size) {\n                av_log(ctx->log_ctx, AV_LOG_ERROR, \"Invalid JPEG image: \"\n                       \"truncated at %02x marker segment.\\n\", marker);\n                return AVERROR_INVALIDDATA;\n            }\n            end = start + length;\n\n            i = end;\n            if (frag->data[i] != 0xff) {\n                next_marker = -1;\n            } else {\n                for (++i; i + 1 < frag->data_size &&\n                          frag->data[i] == 0xff; i++);\n                if (i + 1 >= frag->data_size) {\n                    next_marker = -1;\n                } else {\n                    next_marker = frag->data[i];\n                    next_start  = i + 1;\n                }\n            }\n        }\n\n        if (marker == JPEG_MARKER_SOS) {\n            length = AV_RB16(frag->data + start);\n\n            if (length > end - start)\n                return AVERROR_INVALIDDATA;\n\n            data_ref = NULL;\n            data     = av_malloc(end - start +\n                                 AV_INPUT_BUFFER_PADDING_SIZE);\n            if (!data)\n                return AVERROR(ENOMEM);\n\n            memcpy(data, frag->data + start, length);\n            for (i = start + length, j = length; i < end; i++, j++) {\n                if (frag->data[i] == 0xff) {\n                    while (frag->data[i] == 0xff)\n                        ++i;\n                    data[j] = 0xff;\n                } else {\n                    data[j] = frag->data[i];\n                }\n            }\n            data_size = j;\n\n            memset(data + data_size, 0, AV_INPUT_BUFFER_PADDING_SIZE);\n\n        } else {\n            data      = frag->data + start;\n            data_size = end - start;\n            data_ref  = frag->data_ref;\n        }\n\n        err = ff_cbs_insert_unit_data(ctx, frag, unit, marker,\n                                      data, data_size, data_ref);\n        if (err < 0)\n            return err;\n\n        if (next_marker == -1)\n            break;\n        marker = next_marker;\n        start  = next_start;\n    }\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/1812352d767ccf5431aa440123e2e260a4db2726", "file_name": "libavcodec/cbs_jpeg.c", "vul_type": "cwe-787", "description": "Write a C function to parse and validate a JPEG image fragment for errors and markers."}
{"func_name": "respond_error", "func_src_before": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % stack", "func_src_after": "    def respond_error(self, context, exception):\n        context.respond_server_error()\n        stack = traceback.format_exc()\n        return \"\"\"\n        <html>\n            <body>\n\n                <style>\n                    body {\n                        font-family: sans-serif;\n                        color: #888;\n                        text-align: center;\n                    }\n\n                    body pre {\n                        width: 600px;\n                        text-align: left;\n                        margin: auto;\n                        font-family: monospace;\n                    }\n                </style>\n\n                <img src=\"/ajenti:static/main/error.jpeg\" />\n                <br/>\n                <p>\n                    Server error\n                </p>\n                <pre>\n%s\n                </pre>\n            </body>\n        </html>\n        \"\"\" % cgi.escape(stack)", "commit_link": "github.com/Eugeny/ajenti/commit/d3fc5eb142ff16d55d158afb050af18d5ff09120", "file_name": "ajenti/routing.py", "vul_type": "cwe-079", "description": "Write a Python function that handles a server error by displaying an error page with a stack trace."}
{"func_name": "extend_volume", "func_src_before": "    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(\"growvv -f %s %sg\" % (volume_name, growth_size),\n                          None)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)", "func_src_after": "    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to resize a storage volume by increasing its size and log the operation."}
{"func_name": "CompileKeymap", "func_src_before": "CompileKeymap(XkbFile *file, struct xkb_keymap *keymap, enum merge_mode merge)\n{\n    bool ok;\n    XkbFile *files[LAST_KEYMAP_FILE_TYPE + 1] = { NULL };\n    enum xkb_file_type type;\n    struct xkb_context *ctx = keymap->ctx;\n\n    /* Collect section files and check for duplicates. */\n    for (file = (XkbFile *) file->defs; file;\n         file = (XkbFile *) file->common.next) {\n        if (file->file_type < FIRST_KEYMAP_FILE_TYPE ||\n            file->file_type > LAST_KEYMAP_FILE_TYPE) {\n            log_err(ctx, \"Cannot define %s in a keymap file\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        if (files[file->file_type]) {\n            log_err(ctx,\n                    \"More than one %s section in keymap file; \"\n                    \"All sections after the first ignored\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        files[file->file_type] = file;\n    }\n\n    /*\n     * Check that all required section were provided.\n     * Report everything before failing.\n     */\n    ok = true;\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        if (files[type] == NULL) {\n            log_err(ctx, \"Required section %s missing from keymap\\n\",\n                    xkb_file_type_to_string(type));\n            ok = false;\n        }\n    }\n    if (!ok)\n        return false;\n\n    /* Compile sections. */\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        log_dbg(ctx, \"Compiling %s \\\"%s\\\"\\n\",\n                xkb_file_type_to_string(type), files[type]->name);\n\n        ok = compile_file_fns[type](files[type], keymap, merge);\n        if (!ok) {\n            log_err(ctx, \"Failed to compile %s\\n\",\n                    xkb_file_type_to_string(type));\n            return false;\n        }\n    }\n\n    return UpdateDerivedKeymapFields(keymap);\n}", "func_src_after": "CompileKeymap(XkbFile *file, struct xkb_keymap *keymap, enum merge_mode merge)\n{\n    bool ok;\n    XkbFile *files[LAST_KEYMAP_FILE_TYPE + 1] = { NULL };\n    enum xkb_file_type type;\n    struct xkb_context *ctx = keymap->ctx;\n\n    /* Collect section files and check for duplicates. */\n    for (file = (XkbFile *) file->defs; file;\n         file = (XkbFile *) file->common.next) {\n        if (file->file_type < FIRST_KEYMAP_FILE_TYPE ||\n            file->file_type > LAST_KEYMAP_FILE_TYPE) {\n            if (file->file_type == FILE_TYPE_GEOMETRY) {\n                log_vrb(ctx, 1,\n                        \"Geometry sections are not supported; ignoring\\n\");\n            } else {\n                log_err(ctx, \"Cannot define %s in a keymap file\\n\",\n                        xkb_file_type_to_string(file->file_type));\n            }\n            continue;\n        }\n\n        if (files[file->file_type]) {\n            log_err(ctx,\n                    \"More than one %s section in keymap file; \"\n                    \"All sections after the first ignored\\n\",\n                    xkb_file_type_to_string(file->file_type));\n            continue;\n        }\n\n        files[file->file_type] = file;\n    }\n\n    /*\n     * Check that all required section were provided.\n     * Report everything before failing.\n     */\n    ok = true;\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        if (files[type] == NULL) {\n            log_err(ctx, \"Required section %s missing from keymap\\n\",\n                    xkb_file_type_to_string(type));\n            ok = false;\n        }\n    }\n    if (!ok)\n        return false;\n\n    /* Compile sections. */\n    for (type = FIRST_KEYMAP_FILE_TYPE;\n         type <= LAST_KEYMAP_FILE_TYPE;\n         type++) {\n        log_dbg(ctx, \"Compiling %s \\\"%s\\\"\\n\",\n                xkb_file_type_to_string(type), files[type]->name);\n\n        ok = compile_file_fns[type](files[type], keymap, merge);\n        if (!ok) {\n            log_err(ctx, \"Failed to compile %s\\n\",\n                    xkb_file_type_to_string(type));\n            return false;\n        }\n    }\n\n    return UpdateDerivedKeymapFields(keymap);\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/917636b1d0d70205a13f89062b95e3a0fc31d4ff", "file_name": "src/xkbcomp/keymap.c", "vul_type": "cwe-476", "description": "In C, write a function `CompileKeymap` that compiles a keymap from XkbFile sections, handling duplicates and missing sections."}
{"func_name": "dd_exist", "func_src_before": "int dd_exist(const struct dump_dir *dd, const char *path)\n{\n    char *full_path = concat_path_file(dd->dd_dirname, path);\n    int ret = exist_file_dir(full_path);\n    free(full_path);\n    return ret;\n}", "func_src_after": "int dd_exist(const struct dump_dir *dd, const char *path)\n{\n    if (!str_is_correct_filename(path))\n        error_msg_and_die(\"Cannot test existence. '%s' is not a valid file name\", path);\n\n    char *full_path = concat_path_file(dd->dd_dirname, path);\n    int ret = exist_file_dir(full_path);\n    free(full_path);\n    return ret;\n}", "commit_link": "github.com/abrt/libreport/commit/239c4f7d1f47265526b39ad70106767d00805277", "file_name": "src/lib/dump_dir.c", "vul_type": "cwe-022", "description": "Write a C function named `dd_exist` that checks if a file exists within a directory structure represented by a `dump_dir` structure, and handles invalid filenames with an error message."}
{"func_name": "decode_studio_vop_header", "func_src_before": "static int decode_studio_vop_header(Mpeg4DecContext *ctx, GetBitContext *gb)\n{\n    MpegEncContext *s = &ctx->m;\n\n    if (get_bits_left(gb) <= 32)\n        return 0;\n\n    s->partitioned_frame = 0;\n    s->decode_mb = mpeg4_decode_studio_mb;\n\n    decode_smpte_tc(ctx, gb);\n\n    skip_bits(gb, 10); /* temporal_reference */\n    skip_bits(gb, 2); /* vop_structure */\n    s->pict_type = get_bits(gb, 2) + AV_PICTURE_TYPE_I; /* vop_coding_type */\n    if (get_bits1(gb)) { /* vop_coded */\n        skip_bits1(gb); /* top_field_first */\n        skip_bits1(gb); /* repeat_first_field */\n        s->progressive_frame = get_bits1(gb) ^ 1; /* progressive_frame */\n    }\n\n    if (s->pict_type == AV_PICTURE_TYPE_I) {\n        if (get_bits1(gb))\n            reset_studio_dc_predictors(s);\n    }\n\n    if (ctx->shape != BIN_ONLY_SHAPE) {\n        s->alternate_scan = get_bits1(gb);\n        s->frame_pred_frame_dct = get_bits1(gb);\n        s->dct_precision = get_bits(gb, 2);\n        s->intra_dc_precision = get_bits(gb, 2);\n        s->q_scale_type = get_bits1(gb);\n    }\n\n    if (s->alternate_scan) {\n        ff_init_scantable(s->idsp.idct_permutation, &s->inter_scantable,   ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_scantable,   ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_h_scantable, ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_v_scantable, ff_alternate_vertical_scan);\n    } else {\n        ff_init_scantable(s->idsp.idct_permutation, &s->inter_scantable,   ff_zigzag_direct);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_scantable,   ff_zigzag_direct);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_h_scantable, ff_alternate_horizontal_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_v_scantable, ff_alternate_vertical_scan);\n    }\n\n    mpeg4_load_default_matrices(s);\n\n    next_start_code_studio(gb);\n    extension_and_user_data(s, gb, 4);\n\n    return 0;\n}", "func_src_after": "static int decode_studio_vop_header(Mpeg4DecContext *ctx, GetBitContext *gb)\n{\n    MpegEncContext *s = &ctx->m;\n\n    if (get_bits_left(gb) <= 32)\n        return 0;\n\n    s->partitioned_frame = 0;\n    s->interlaced_dct = 0;\n    s->decode_mb = mpeg4_decode_studio_mb;\n\n    decode_smpte_tc(ctx, gb);\n\n    skip_bits(gb, 10); /* temporal_reference */\n    skip_bits(gb, 2); /* vop_structure */\n    s->pict_type = get_bits(gb, 2) + AV_PICTURE_TYPE_I; /* vop_coding_type */\n    if (get_bits1(gb)) { /* vop_coded */\n        skip_bits1(gb); /* top_field_first */\n        skip_bits1(gb); /* repeat_first_field */\n        s->progressive_frame = get_bits1(gb) ^ 1; /* progressive_frame */\n    }\n\n    if (s->pict_type == AV_PICTURE_TYPE_I) {\n        if (get_bits1(gb))\n            reset_studio_dc_predictors(s);\n    }\n\n    if (ctx->shape != BIN_ONLY_SHAPE) {\n        s->alternate_scan = get_bits1(gb);\n        s->frame_pred_frame_dct = get_bits1(gb);\n        s->dct_precision = get_bits(gb, 2);\n        s->intra_dc_precision = get_bits(gb, 2);\n        s->q_scale_type = get_bits1(gb);\n    }\n\n    if (s->alternate_scan) {\n        ff_init_scantable(s->idsp.idct_permutation, &s->inter_scantable,   ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_scantable,   ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_h_scantable, ff_alternate_vertical_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_v_scantable, ff_alternate_vertical_scan);\n    } else {\n        ff_init_scantable(s->idsp.idct_permutation, &s->inter_scantable,   ff_zigzag_direct);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_scantable,   ff_zigzag_direct);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_h_scantable, ff_alternate_horizontal_scan);\n        ff_init_scantable(s->idsp.idct_permutation, &s->intra_v_scantable, ff_alternate_vertical_scan);\n    }\n\n    mpeg4_load_default_matrices(s);\n\n    next_start_code_studio(gb);\n    extension_and_user_data(s, gb, 4);\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/1f686d023b95219db933394a7704ad9aa5f01cbb", "file_name": "libavcodec/mpeg4videodec.c", "vul_type": "cwe-125", "description": "Write a C function to decode the header of a studio video object plane (VOP) for an MPEG4 decoder context."}
{"func_name": "take_bug_report", "func_src_before": "    def take_bug_report(self, test_name, begin_time):\n        \"\"\"Takes a bug report on the device and stores it in a file.\n\n        Args:\n            test_name: Name of the test case that triggered this bug report.\n            begin_time: Logline format timestamp taken when the test started.\n        \"\"\"\n        new_br = True\n        try:\n            stdout = self.adb.shell('bugreportz -v').decode('utf-8')\n            # This check is necessary for builds before N, where adb shell's ret\n            # code and stderr are not propagated properly.\n            if 'not found' in stdout:\n                new_br = False\n        except adb.AdbError:\n            new_br = False\n        br_path = os.path.join(self.log_path, 'BugReports')\n        utils.create_dir(br_path)\n        base_name = ',%s,%s.txt' % (begin_time, self.serial)\n        if new_br:\n            base_name = base_name.replace('.txt', '.zip')\n        test_name_len = utils.MAX_FILENAME_LEN - len(base_name)\n        out_name = test_name[:test_name_len] + base_name\n        full_out_path = os.path.join(br_path, out_name.replace(' ', r'\\ '))\n        # in case device restarted, wait for adb interface to return\n        self.wait_for_boot_completion()\n        self.log.info('Taking bugreport for %s.', test_name)\n        if new_br:\n            out = self.adb.shell('bugreportz').decode('utf-8')\n            if not out.startswith('OK'):\n                raise DeviceError(self, 'Failed to take bugreport: %s' % out)\n            br_out_path = out.split(':')[1].strip()\n            self.adb.pull('%s %s' % (br_out_path, full_out_path))\n        else:\n            self.adb.bugreport(' > %s' % full_out_path)\n        self.log.info('Bugreport for %s taken at %s.', test_name,\n                      full_out_path)", "func_src_after": "    def take_bug_report(self, test_name, begin_time):\n        \"\"\"Takes a bug report on the device and stores it in a file.\n\n        Args:\n            test_name: Name of the test case that triggered this bug report.\n            begin_time: Logline format timestamp taken when the test started.\n        \"\"\"\n        new_br = True\n        try:\n            stdout = self.adb.shell('bugreportz -v').decode('utf-8')\n            # This check is necessary for builds before N, where adb shell's ret\n            # code and stderr are not propagated properly.\n            if 'not found' in stdout:\n                new_br = False\n        except adb.AdbError:\n            new_br = False\n        br_path = os.path.join(self.log_path, 'BugReports')\n        utils.create_dir(br_path)\n        base_name = ',%s,%s.txt' % (begin_time, self.serial)\n        if new_br:\n            base_name = base_name.replace('.txt', '.zip')\n        test_name_len = utils.MAX_FILENAME_LEN - len(base_name)\n        out_name = test_name[:test_name_len] + base_name\n        full_out_path = os.path.join(br_path, out_name.replace(' ', r'\\ '))\n        # in case device restarted, wait for adb interface to return\n        self.wait_for_boot_completion()\n        self.log.info('Taking bugreport for %s.', test_name)\n        if new_br:\n            out = self.adb.shell('bugreportz').decode('utf-8')\n            if not out.startswith('OK'):\n                raise DeviceError(self, 'Failed to take bugreport: %s' % out)\n            br_out_path = out.split(':')[1].strip()\n            self.adb.pull([br_out_path, full_out_path])\n        else:\n            # shell=True as this command redirects the stdout to a local file\n            # using shell redirection.\n            self.adb.bugreport(' > %s' % full_out_path, shell=True)\n        self.log.info('Bugreport for %s taken at %s.', test_name,\n                      full_out_path)", "commit_link": "github.com/google/mobly/commit/3862e8ba359040fbdd6e1a6d36e51d07cda8e1ee", "file_name": "mobly/controllers/android_device.py", "vul_type": "cwe-078", "description": "Write a Python function to capture a bug report from a device, save it to a file, and handle different versions of the bug reporting tool."}
{"func_name": "_installDependencies", "func_src_before": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && '));\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "func_src_after": "function _installDependencies(src) {\n    gutil.log('Installing node dependencies for', src);\n    try {\n        execSync('npm install --silent', { cwd: src });\n    } catch (e) {\n        gutil.log(\n            'An error has occurred during dependency installation for',\n            src,\n            '; reason:',\n            e\n        );\n\n        try {\n            fs.rmdirSync(src + '/node_modules');\n        } catch (ignored) {}\n\n        throw 'Unable to install dependencies for module ' + src;\n    }\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 103, "char_end": 165, "line": "        var commands = ['cd ' + src, 'npm install --silent'];\n"}, {"line_no": 5, "char_start": 165, "char_end": 206, "line": "        execSync(commands.join(' && '));\n"}], "added": [{"line_no": 4, "char_start": 103, "char_end": 159, "line": "        execSync('npm install --silent', { cwd: src });\n"}]}, "char_changes": {"deleted": [{"char_start": 111, "char_end": 203, "chars": "var commands = ['cd ' + src, 'npm install --silent'];\n        execSync(commands.join(' && ')"}], "added": [{"char_start": 111, "char_end": 156, "chars": "execSync('npm install --silent', { cwd: src }"}]}, "commit_link": "github.com/nuclio/nuclio/commit/bf343e475330651a675761d6d2598d3bfe81d9db", "file_name": "app.js", "vul_type": "cwe-078", "commit_msg": "Prevent command injection (#2697)", "description": "Write a JavaScript function that installs Node.js dependencies for a given source directory and logs the process."}
{"func_name": "job_browse", "func_src_before": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = os.path.join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "func_src_after": "@gui.route(\"/job/<int:job_id>/browse\", defaults={\"path\": \"\"})\n@gui.route(\"/job/<int:job_id>/browse/<path:path>\")\n@login_required\ndef job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = safe_join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)", "line_changes": {"deleted": [{"line_no": 24, "char_start": 691, "char_end": 739, "line": "    abs_path = os.path.join(job_base_dir, path)\n"}], "added": [{"line_no": 24, "char_start": 691, "char_end": 736, "line": "    abs_path = safe_join(job_base_dir, path)\n"}]}, "char_changes": {"deleted": [{"char_start": 706, "char_end": 714, "chars": "os.path."}], "added": [{"char_start": 706, "char_end": 711, "chars": "safe_"}]}, "commit_link": "github.com/ganga-devs/ganga/commit/730e7aba192407d35eb37dd7938d49071124be8c", "file_name": "routes.py", "vul_type": "cwe-022", "commit_msg": "# Absolute Path Traversal due to incorrect use of `send_file` call (#2025)\n\nA path traversal attack (also known as directory traversal) aims to access files and directories that are stored outside the web root folder. By manipulating variables that reference files with \u201cdot-dot-slash (../)\u201d sequences and its variations or by using absolute file paths, it may be possible to access arbitrary files and directories stored on file system including application source code or configuration and critical system files. This attack is also known as \u201cdot-dot-slash\u201d, \u201cdirectory traversal\u201d, \u201cdirectory climbing\u201d and \u201cbacktracking\u201d.\r\n\r\n## Common Weakness Enumeration category\r\nCWE - 36\r\n\r\n## Root Cause Analysis\r\n\r\nThe `os.path.join` call is unsafe for use with untrusted input. When the `os.path.join` call encounters an absolute path, it ignores all the parameters it has encountered till that point and starts working with the new absolute path.  Please see the example below.\r\n```\r\n>>> import os.path\r\n>>> static = \"path/to/mySafeStaticDir\"\r\n>>> malicious = \"/../../../../../etc/passwd\"\r\n>>> os.path.join(t,malicious)\r\n'/../../../../../etc/passwd'\r\n```\r\nSince the \"malicious\" parameter represents an absolute path, the result of `os.path.join` ignores the static directory completely. Hence, untrusted input is passed via the `os.path.join` call to `flask.send_file` can lead to path traversal attacks.\r\n\r\nIn this case, the problems occurs due to the following code :\r\nhttps://github.com/ganga-devs/ganga/blob/0c0f9e33b36ee7ead0855f1464f8d4efad26bdbc/ganga/GangaGUI/gui/routes.py#L671\r\n\r\nHere, the `path` parameter is attacker controlled. This parameter passes through the unsafe `os.path.join` call making the effective directory and filename passed to the `send_file` call attacker controlled. This leads to a path traversal attack.\r\n\r\n## Proof of Concept\r\n\r\nThe bug can be verified using a proof of concept similar to the one shown below.\r\n\r\n```\r\ncurl --path-as-is 'http://<domain>/job/<int:job_id>/browse///../../../../etc/passwd\"'\r\n```\r\n## Remediation\r\n\r\nThis can be fixed by preventing flow of untrusted data to the vulnerable `send_file` function. In case the application logic necessiates this behaviour, one can either use the `werkzeug.utils.safe_join` to join untrusted paths or replace `flask.send_file` calls with `flask.send_from_directory` calls.\r\n\r\n## Common Vulnerability Scoring System Vector\r\n\r\nThe attack can be carried over the network. A complex non-standard configuration or a specialized condition is not required for the attack to be successfully conducted. There is no user interaction required for successful execution. The attack can affect components outside the scope of the target module. The attack can be used to gain access to confidential files like passwords, login credentials and other secrets. It cannot be directly used to affect a change on a system resource. Hence has limited to no impact on integrity. Using this attack vector a attacker may make multiple requests for accessing huge files such as a database. This can lead to a partial system denial service. However, the impact on availability is quite low in this case. Taking this account an appropriate CVSS v3.1 vector would be\r\n\r\n(AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L)[https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:L&version=3.1]\r\n\r\nThis gives it a base score of 9.3/10 and a severity rating of critical.\r\n\r\n## References\r\n* [OWASP Path Traversal](https://owasp.org/www-community/attacks/Path_Traversal)\r\n* github/securitylab#669\r\n\r\n### This bug was found using *[CodeQL by Github](https://codeql.github.com/)*\r\n\r\nCo-authored-by: Porcupiney Hairs <porucpiney.hairs@protonmail.com>", "description": "Create a Python Flask web route for browsing job directories and files, with user authentication required."}
{"func_name": "Perl_re_op_compile", "func_src_before": "REGEXP *\nPerl_re_op_compile(pTHX_ SV ** const patternp, int pat_count,\n\t\t    OP *expr, const regexp_engine* eng, REGEXP *old_re,\n\t\t     bool *is_bare_re, const U32 orig_rx_flags, const U32 pm_flags)\n{\n    dVAR;\n    REGEXP *Rx;         /* Capital 'R' means points to a REGEXP */\n    STRLEN plen;\n    char *exp;\n    regnode *scan;\n    I32 flags;\n    SSize_t minlen = 0;\n    U32 rx_flags;\n    SV *pat;\n    SV** new_patternp = patternp;\n\n    /* these are all flags - maybe they should be turned\n     * into a single int with different bit masks */\n    I32 sawlookahead = 0;\n    I32 sawplus = 0;\n    I32 sawopen = 0;\n    I32 sawminmod = 0;\n\n    regex_charset initial_charset = get_regex_charset(orig_rx_flags);\n    bool recompile = 0;\n    bool runtime_code = 0;\n    scan_data_t data;\n    RExC_state_t RExC_state;\n    RExC_state_t * const pRExC_state = &RExC_state;\n#ifdef TRIE_STUDY_OPT\n    int restudied = 0;\n    RExC_state_t copyRExC_state;\n#endif\n    GET_RE_DEBUG_FLAGS_DECL;\n\n    PERL_ARGS_ASSERT_RE_OP_COMPILE;\n\n    DEBUG_r(if (!PL_colorset) reginitcolors());\n\n    /* Initialize these here instead of as-needed, as is quick and avoids\n     * having to test them each time otherwise */\n    if (! PL_InBitmap) {\n#ifdef DEBUGGING\n        char * dump_len_string;\n#endif\n\n        /* This is calculated here, because the Perl program that generates the\n         * static global ones doesn't currently have access to\n         * NUM_ANYOF_CODE_POINTS */\n\tPL_InBitmap = _new_invlist(2);\n\tPL_InBitmap = _add_range_to_invlist(PL_InBitmap, 0,\n                                                    NUM_ANYOF_CODE_POINTS - 1);\n#ifdef DEBUGGING\n        dump_len_string = PerlEnv_getenv(\"PERL_DUMP_RE_MAX_LEN\");\n        if (   ! dump_len_string\n            || ! grok_atoUV(dump_len_string, (UV *)&PL_dump_re_max_len, NULL))\n        {\n            PL_dump_re_max_len = 60;    /* A reasonable default */\n        }\n#endif\n    }\n\n    pRExC_state->warn_text = NULL;\n    pRExC_state->unlexed_names = NULL;\n    pRExC_state->code_blocks = NULL;\n\n    if (is_bare_re)\n\t*is_bare_re = FALSE;\n\n    if (expr && (expr->op_type == OP_LIST ||\n\t\t(expr->op_type == OP_NULL && expr->op_targ == OP_LIST))) {\n\t/* allocate code_blocks if needed */\n\tOP *o;\n\tint ncode = 0;\n\n\tfor (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o))\n\t    if (o->op_type == OP_NULL && (o->op_flags & OPf_SPECIAL))\n\t\tncode++; /* count of DO blocks */\n\n\tif (ncode)\n            pRExC_state->code_blocks = S_alloc_code_blocks(aTHX_ ncode);\n    }\n\n    if (!pat_count) {\n        /* compile-time pattern with just OP_CONSTs and DO blocks */\n\n        int n;\n        OP *o;\n\n        /* find how many CONSTs there are */\n        assert(expr);\n        n = 0;\n        if (expr->op_type == OP_CONST)\n            n = 1;\n        else\n            for (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o)) {\n                if (o->op_type == OP_CONST)\n                    n++;\n            }\n\n        /* fake up an SV array */\n\n        assert(!new_patternp);\n        Newx(new_patternp, n, SV*);\n        SAVEFREEPV(new_patternp);\n        pat_count = n;\n\n        n = 0;\n        if (expr->op_type == OP_CONST)\n            new_patternp[n] = cSVOPx_sv(expr);\n        else\n            for (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o)) {\n                if (o->op_type == OP_CONST)\n                    new_patternp[n++] = cSVOPo_sv;\n            }\n\n    }\n\n    DEBUG_PARSE_r(Perl_re_printf( aTHX_\n        \"Assembling pattern from %d elements%s\\n\", pat_count,\n            orig_rx_flags & RXf_SPLIT ? \" for split\" : \"\"));\n\n    /* set expr to the first arg op */\n\n    if (pRExC_state->code_blocks && pRExC_state->code_blocks->count\n         && expr->op_type != OP_CONST)\n    {\n            expr = cLISTOPx(expr)->op_first;\n            assert(   expr->op_type == OP_PUSHMARK\n                   || (expr->op_type == OP_NULL && expr->op_targ == OP_PUSHMARK)\n                   || expr->op_type == OP_PADRANGE);\n            expr = OpSIBLING(expr);\n    }\n\n    pat = S_concat_pat(aTHX_ pRExC_state, NULL, new_patternp, pat_count,\n                        expr, &recompile, NULL);\n\n    /* handle bare (possibly after overloading) regex: foo =~ $re */\n    {\n        SV *re = pat;\n        if (SvROK(re))\n            re = SvRV(re);\n        if (SvTYPE(re) == SVt_REGEXP) {\n            if (is_bare_re)\n                *is_bare_re = TRUE;\n            SvREFCNT_inc(re);\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_\n                \"Precompiled pattern%s\\n\",\n                    orig_rx_flags & RXf_SPLIT ? \" for split\" : \"\"));\n\n            return (REGEXP*)re;\n        }\n    }\n\n    exp = SvPV_nomg(pat, plen);\n\n    if (!eng->op_comp) {\n\tif ((SvUTF8(pat) && IN_BYTES)\n\t\t|| SvGMAGICAL(pat) || SvAMAGIC(pat))\n\t{\n\t    /* make a temporary copy; either to convert to bytes,\n\t     * or to avoid repeating get-magic / overloaded stringify */\n\t    pat = newSVpvn_flags(exp, plen, SVs_TEMP |\n\t\t\t\t\t(IN_BYTES ? 0 : SvUTF8(pat)));\n\t}\n\treturn CALLREGCOMP_ENG(eng, pat, orig_rx_flags);\n    }\n\n    /* ignore the utf8ness if the pattern is 0 length */\n    RExC_utf8 = RExC_orig_utf8 = (plen == 0 || IN_BYTES) ? 0 : SvUTF8(pat);\n    RExC_uni_semantics = 0;\n    RExC_contains_locale = 0;\n    RExC_strict = cBOOL(pm_flags & RXf_PMf_STRICT);\n    RExC_in_script_run = 0;\n    RExC_study_started = 0;\n    pRExC_state->runtime_code_qr = NULL;\n    RExC_frame_head= NULL;\n    RExC_frame_last= NULL;\n    RExC_frame_count= 0;\n    RExC_latest_warn_offset = 0;\n    RExC_use_BRANCHJ = 0;\n    RExC_total_parens = 0;\n    RExC_open_parens = NULL;\n    RExC_close_parens = NULL;\n    RExC_paren_names = NULL;\n    RExC_size = 0;\n    RExC_seen_d_op = FALSE;\n#ifdef DEBUGGING\n    RExC_paren_name_list = NULL;\n#endif\n\n    DEBUG_r({\n        RExC_mysv1= sv_newmortal();\n        RExC_mysv2= sv_newmortal();\n    });\n\n    DEBUG_COMPILE_r({\n            SV *dsv= sv_newmortal();\n            RE_PV_QUOTED_DECL(s, RExC_utf8, dsv, exp, plen, PL_dump_re_max_len);\n            Perl_re_printf( aTHX_  \"%sCompiling REx%s %s\\n\",\n                          PL_colors[4], PL_colors[5], s);\n        });\n\n    /* we jump here if we have to recompile, e.g., from upgrading the pattern\n     * to utf8 */\n\n    if ((pm_flags & PMf_USE_RE_EVAL)\n\t\t/* this second condition covers the non-regex literal case,\n\t\t * i.e.  $foo =~ '(?{})'. */\n\t\t|| (IN_PERL_COMPILETIME && (PL_hints & HINT_RE_EVAL))\n    )\n\truntime_code = S_has_runtime_code(aTHX_ pRExC_state, exp, plen);\n\n  redo_parse:\n    /* return old regex if pattern hasn't changed */\n    /* XXX: note in the below we have to check the flags as well as the\n     * pattern.\n     *\n     * Things get a touch tricky as we have to compare the utf8 flag\n     * independently from the compile flags.  */\n\n    if (   old_re\n        && !recompile\n        && !!RX_UTF8(old_re) == !!RExC_utf8\n        && ( RX_COMPFLAGS(old_re) == ( orig_rx_flags & RXf_PMf_FLAGCOPYMASK ) )\n\t&& RX_PRECOMP(old_re)\n\t&& RX_PRELEN(old_re) == plen\n        && memEQ(RX_PRECOMP(old_re), exp, plen)\n\t&& !runtime_code /* with runtime code, always recompile */ )\n    {\n        return old_re;\n    }\n\n    /* Allocate the pattern's SV */\n    RExC_rx_sv = Rx = (REGEXP*) newSV_type(SVt_REGEXP);\n    RExC_rx = ReANY(Rx);\n    if ( RExC_rx == NULL )\n        FAIL(\"Regexp out of space\");\n\n    rx_flags = orig_rx_flags;\n\n    if (   (UTF || RExC_uni_semantics)\n        && initial_charset == REGEX_DEPENDS_CHARSET)\n    {\n\n\t/* Set to use unicode semantics if the pattern is in utf8 and has the\n\t * 'depends' charset specified, as it means unicode when utf8  */\n\tset_regex_charset(&rx_flags, REGEX_UNICODE_CHARSET);\n        RExC_uni_semantics = 1;\n    }\n\n    RExC_pm_flags = pm_flags;\n\n    if (runtime_code) {\n        assert(TAINTING_get || !TAINT_get);\n\tif (TAINT_get)\n\t    Perl_croak(aTHX_ \"Eval-group in insecure regular expression\");\n\n\tif (!S_compile_runtime_code(aTHX_ pRExC_state, exp, plen)) {\n\t    /* whoops, we have a non-utf8 pattern, whilst run-time code\n\t     * got compiled as utf8. Try again with a utf8 pattern */\n            S_pat_upgrade_to_utf8(aTHX_ pRExC_state, &exp, &plen,\n                pRExC_state->code_blocks ? pRExC_state->code_blocks->count : 0);\n            goto redo_parse;\n\t}\n    }\n    assert(!pRExC_state->runtime_code_qr);\n\n    RExC_sawback = 0;\n\n    RExC_seen = 0;\n    RExC_maxlen = 0;\n    RExC_in_lookbehind = 0;\n    RExC_seen_zerolen = *exp == '^' ? -1 : 0;\n#ifdef EBCDIC\n    RExC_recode_x_to_native = 0;\n#endif\n    RExC_in_multi_char_class = 0;\n\n    RExC_start = RExC_copy_start_in_constructed = RExC_copy_start_in_input = RExC_precomp = exp;\n    RExC_precomp_end = RExC_end = exp + plen;\n    RExC_nestroot = 0;\n    RExC_whilem_seen = 0;\n    RExC_end_op = NULL;\n    RExC_recurse = NULL;\n    RExC_study_chunk_recursed = NULL;\n    RExC_study_chunk_recursed_bytes= 0;\n    RExC_recurse_count = 0;\n    pRExC_state->code_index = 0;\n\n    /* Initialize the string in the compiled pattern.  This is so that there is\n     * something to output if necessary */\n    set_regex_pv(pRExC_state, Rx);\n\n    DEBUG_PARSE_r({\n        Perl_re_printf( aTHX_\n            \"Starting parse and generation\\n\");\n        RExC_lastnum=0;\n        RExC_lastparse=NULL;\n    });\n\n    /* Allocate space and zero-initialize. Note, the two step process\n       of zeroing when in debug mode, thus anything assigned has to\n       happen after that */\n    if (!  RExC_size) {\n\n        /* On the first pass of the parse, we guess how big this will be.  Then\n         * we grow in one operation to that amount and then give it back.  As\n         * we go along, we re-allocate what we need.\n         *\n         * XXX Currently the guess is essentially that the pattern will be an\n         * EXACT node with one byte input, one byte output.  This is crude, and\n         * better heuristics are welcome.\n         *\n         * On any subsequent passes, we guess what we actually computed in the\n         * latest earlier pass.  Such a pass probably didn't complete so is\n         * missing stuff.  We could improve those guesses by knowing where the\n         * parse stopped, and use the length so far plus apply the above\n         * assumption to what's left. */\n        RExC_size = STR_SZ(RExC_end - RExC_start);\n    }\n\n    Newxc(RExC_rxi, sizeof(regexp_internal) + RExC_size, char, regexp_internal);\n    if ( RExC_rxi == NULL )\n        FAIL(\"Regexp out of space\");\n\n    Zero(RExC_rxi, sizeof(regexp_internal) + RExC_size, char);\n    RXi_SET( RExC_rx, RExC_rxi );\n\n    /* We start from 0 (over from 0 in the case this is a reparse.  The first\n     * node parsed will give back any excess memory we have allocated so far).\n     * */\n    RExC_size = 0;\n\n    /* non-zero initialization begins here */\n    RExC_rx->engine= eng;\n    RExC_rx->extflags = rx_flags;\n    RXp_COMPFLAGS(RExC_rx) = orig_rx_flags & RXf_PMf_FLAGCOPYMASK;\n\n    if (pm_flags & PMf_IS_QR) {\n\tRExC_rxi->code_blocks = pRExC_state->code_blocks;\n        if (RExC_rxi->code_blocks) {\n            RExC_rxi->code_blocks->refcnt++;\n        }\n    }\n\n    RExC_rx->intflags = 0;\n\n    RExC_flags = rx_flags;\t/* don't let top level (?i) bleed */\n    RExC_parse = exp;\n\n    /* This NUL is guaranteed because the pattern comes from an SV*, and the sv\n     * code makes sure the final byte is an uncounted NUL.  But should this\n     * ever not be the case, lots of things could read beyond the end of the\n     * buffer: loops like\n     *      while(isFOO(*RExC_parse)) RExC_parse++;\n     *      strchr(RExC_parse, \"foo\");\n     * etc.  So it is worth noting. */\n    assert(*RExC_end == '\\0');\n\n    RExC_naughty = 0;\n    RExC_npar = 1;\n    RExC_parens_buf_size = 0;\n    RExC_emit_start = RExC_rxi->program;\n    pRExC_state->code_index = 0;\n\n    *((char*) RExC_emit_start) = (char) REG_MAGIC;\n    RExC_emit = 1;\n\n    /* Do the parse */\n    if (reg(pRExC_state, 0, &flags, 1)) {\n\n        /* Success!, But we may need to redo the parse knowing how many parens\n         * there actually are */\n        if (IN_PARENS_PASS) {\n            flags |= RESTART_PARSE;\n        }\n\n        /* We have that number in RExC_npar */\n        RExC_total_parens = RExC_npar;\n    }\n    else if (! MUST_RESTART(flags)) {\n\tReREFCNT_dec(Rx);\n        Perl_croak(aTHX_ \"panic: reg returned failure to re_op_compile, flags=%#\" UVxf, (UV) flags);\n    }\n\n    /* Here, we either have success, or we have to redo the parse for some reason */\n    if (MUST_RESTART(flags)) {\n\n        /* It's possible to write a regexp in ascii that represents Unicode\n        codepoints outside of the byte range, such as via \\x{100}. If we\n        detect such a sequence we have to convert the entire pattern to utf8\n        and then recompile, as our sizing calculation will have been based\n        on 1 byte == 1 character, but we will need to use utf8 to encode\n        at least some part of the pattern, and therefore must convert the whole\n        thing.\n        -- dmq */\n        if (flags & NEED_UTF8) {\n\n            /* We have stored the offset of the final warning output so far.\n             * That must be adjusted.  Any variant characters between the start\n             * of the pattern and this warning count for 2 bytes in the final,\n             * so just add them again */\n            if (UNLIKELY(RExC_latest_warn_offset > 0)) {\n                RExC_latest_warn_offset +=\n                            variant_under_utf8_count((U8 *) exp, (U8 *) exp\n                                                + RExC_latest_warn_offset);\n            }\n            S_pat_upgrade_to_utf8(aTHX_ pRExC_state, &exp, &plen,\n            pRExC_state->code_blocks ? pRExC_state->code_blocks->count : 0);\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_ \"Need to redo parse after upgrade\\n\"));\n        }\n        else {\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_ \"Need to redo parse\\n\"));\n        }\n\n        if (ALL_PARENS_COUNTED) {\n            /* Make enough room for all the known parens, and zero it */\n            Renew(RExC_open_parens, RExC_total_parens, regnode_offset);\n            Zero(RExC_open_parens, RExC_total_parens, regnode_offset);\n            RExC_open_parens[0] = 1;    /* +1 for REG_MAGIC */\n\n            Renew(RExC_close_parens, RExC_total_parens, regnode_offset);\n            Zero(RExC_close_parens, RExC_total_parens, regnode_offset);\n        }\n        else { /* Parse did not complete.  Reinitialize the parentheses\n                  structures */\n            RExC_total_parens = 0;\n            if (RExC_open_parens) {\n                Safefree(RExC_open_parens);\n                RExC_open_parens = NULL;\n            }\n            if (RExC_close_parens) {\n                Safefree(RExC_close_parens);\n                RExC_close_parens = NULL;\n            }\n        }\n\n        /* Clean up what we did in this parse */\n        SvREFCNT_dec_NN(RExC_rx_sv);\n\n        goto redo_parse;\n    }\n\n    /* Here, we have successfully parsed and generated the pattern's program\n     * for the regex engine.  We are ready to finish things up and look for\n     * optimizations. */\n\n    /* Update the string to compile, with correct modifiers, etc */\n    set_regex_pv(pRExC_state, Rx);\n\n    RExC_rx->nparens = RExC_total_parens - 1;\n\n    /* Uses the upper 4 bits of the FLAGS field, so keep within that size */\n    if (RExC_whilem_seen > 15)\n        RExC_whilem_seen = 15;\n\n    DEBUG_PARSE_r({\n        Perl_re_printf( aTHX_\n            \"Required size %\" IVdf \" nodes\\n\", (IV)RExC_size);\n        RExC_lastnum=0;\n        RExC_lastparse=NULL;\n    });\n\n#ifdef RE_TRACK_PATTERN_OFFSETS\n    DEBUG_OFFSETS_r(Perl_re_printf( aTHX_\n                          \"%s %\" UVuf \" bytes for offset annotations.\\n\",\n                          RExC_offsets ? \"Got\" : \"Couldn't get\",\n                          (UV)((RExC_offsets[0] * 2 + 1))));\n    DEBUG_OFFSETS_r(if (RExC_offsets) {\n        const STRLEN len = RExC_offsets[0];\n        STRLEN i;\n        GET_RE_DEBUG_FLAGS_DECL;\n        Perl_re_printf( aTHX_\n                      \"Offsets: [%\" UVuf \"]\\n\\t\", (UV)RExC_offsets[0]);\n        for (i = 1; i <= len; i++) {\n            if (RExC_offsets[i*2-1] || RExC_offsets[i*2])\n                Perl_re_printf( aTHX_  \"%\" UVuf \":%\" UVuf \"[%\" UVuf \"] \",\n                (UV)i, (UV)RExC_offsets[i*2-1], (UV)RExC_offsets[i*2]);\n        }\n        Perl_re_printf( aTHX_  \"\\n\");\n    });\n\n#else\n    SetProgLen(RExC_rxi,RExC_size);\n#endif\n\n    DEBUG_OPTIMISE_r(\n        Perl_re_printf( aTHX_  \"Starting post parse optimization\\n\");\n    );\n\n    /* XXXX To minimize changes to RE engine we always allocate\n       3-units-long substrs field. */\n    Newx(RExC_rx->substrs, 1, struct reg_substr_data);\n    if (RExC_recurse_count) {\n        Newx(RExC_recurse, RExC_recurse_count, regnode *);\n        SAVEFREEPV(RExC_recurse);\n    }\n\n    if (RExC_seen & REG_RECURSE_SEEN) {\n        /* Note, RExC_total_parens is 1 + the number of parens in a pattern.\n         * So its 1 if there are no parens. */\n        RExC_study_chunk_recursed_bytes= (RExC_total_parens >> 3) +\n                                         ((RExC_total_parens & 0x07) != 0);\n        Newx(RExC_study_chunk_recursed,\n             RExC_study_chunk_recursed_bytes * RExC_total_parens, U8);\n        SAVEFREEPV(RExC_study_chunk_recursed);\n    }\n\n  reStudy:\n    RExC_rx->minlen = minlen = sawlookahead = sawplus = sawopen = sawminmod = 0;\n    DEBUG_r(\n        RExC_study_chunk_recursed_count= 0;\n    );\n    Zero(RExC_rx->substrs, 1, struct reg_substr_data);\n    if (RExC_study_chunk_recursed) {\n        Zero(RExC_study_chunk_recursed,\n             RExC_study_chunk_recursed_bytes * RExC_total_parens, U8);\n    }\n\n\n#ifdef TRIE_STUDY_OPT\n    if (!restudied) {\n        StructCopy(&zero_scan_data, &data, scan_data_t);\n        copyRExC_state = RExC_state;\n    } else {\n        U32 seen=RExC_seen;\n        DEBUG_OPTIMISE_r(Perl_re_printf( aTHX_ \"Restudying\\n\"));\n\n        RExC_state = copyRExC_state;\n        if (seen & REG_TOP_LEVEL_BRANCHES_SEEN)\n            RExC_seen |= REG_TOP_LEVEL_BRANCHES_SEEN;\n        else\n            RExC_seen &= ~REG_TOP_LEVEL_BRANCHES_SEEN;\n\tStructCopy(&zero_scan_data, &data, scan_data_t);\n    }\n#else\n    StructCopy(&zero_scan_data, &data, scan_data_t);\n#endif\n\n    /* Dig out information for optimizations. */\n    RExC_rx->extflags = RExC_flags; /* was pm_op */\n    /*dmq: removed as part of de-PMOP: pm->op_pmflags = RExC_flags; */\n\n    if (UTF)\n\tSvUTF8_on(Rx);\t/* Unicode in it? */\n    RExC_rxi->regstclass = NULL;\n    if (RExC_naughty >= TOO_NAUGHTY)\t/* Probably an expensive pattern. */\n\tRExC_rx->intflags |= PREGf_NAUGHTY;\n    scan = RExC_rxi->program + 1;\t\t/* First BRANCH. */\n\n    /* testing for BRANCH here tells us whether there is \"must appear\"\n       data in the pattern. If there is then we can use it for optimisations */\n    if (!(RExC_seen & REG_TOP_LEVEL_BRANCHES_SEEN)) { /*  Only one top-level choice.\n                                                  */\n\tSSize_t fake;\n\tSTRLEN longest_length[2];\n\tregnode_ssc ch_class; /* pointed to by data */\n\tint stclass_flag;\n\tSSize_t last_close = 0; /* pointed to by data */\n        regnode *first= scan;\n        regnode *first_next= regnext(first);\n        int i;\n\n\t/*\n\t * Skip introductions and multiplicators >= 1\n\t * so that we can extract the 'meat' of the pattern that must\n\t * match in the large if() sequence following.\n\t * NOTE that EXACT is NOT covered here, as it is normally\n\t * picked up by the optimiser separately.\n\t *\n\t * This is unfortunate as the optimiser isnt handling lookahead\n\t * properly currently.\n\t *\n\t */\n\twhile ((OP(first) == OPEN && (sawopen = 1)) ||\n\t       /* An OR of *one* alternative - should not happen now. */\n\t    (OP(first) == BRANCH && OP(first_next) != BRANCH) ||\n\t    /* for now we can't handle lookbehind IFMATCH*/\n\t    (OP(first) == IFMATCH && !first->flags && (sawlookahead = 1)) ||\n\t    (OP(first) == PLUS) ||\n\t    (OP(first) == MINMOD) ||\n\t       /* An {n,m} with n>0 */\n\t    (PL_regkind[OP(first)] == CURLY && ARG1(first) > 0) ||\n\t    (OP(first) == NOTHING && PL_regkind[OP(first_next)] != END ))\n\t{\n\t\t/*\n\t\t * the only op that could be a regnode is PLUS, all the rest\n\t\t * will be regnode_1 or regnode_2.\n\t\t *\n                 * (yves doesn't think this is true)\n\t\t */\n\t\tif (OP(first) == PLUS)\n\t\t    sawplus = 1;\n                else {\n                    if (OP(first) == MINMOD)\n                        sawminmod = 1;\n\t\t    first += regarglen[OP(first)];\n                }\n\t\tfirst = NEXTOPER(first);\n\t\tfirst_next= regnext(first);\n\t}\n\n\t/* Starting-point info. */\n      again:\n        DEBUG_PEEP(\"first:\", first, 0, 0);\n        /* Ignore EXACT as we deal with it later. */\n\tif (PL_regkind[OP(first)] == EXACT) {\n\t    if (   OP(first) == EXACT\n                || OP(first) == EXACT_ONLY8\n                || OP(first) == EXACTL)\n            {\n\t\tNOOP;\t/* Empty, get anchored substr later. */\n            }\n\t    else\n\t\tRExC_rxi->regstclass = first;\n\t}\n#ifdef TRIE_STCLASS\n\telse if (PL_regkind[OP(first)] == TRIE &&\n\t        ((reg_trie_data *)RExC_rxi->data->data[ ARG(first) ])->minlen>0)\n\t{\n            /* this can happen only on restudy */\n            RExC_rxi->regstclass = construct_ahocorasick_from_trie(pRExC_state, (regnode *)first, 0);\n\t}\n#endif\n\telse if (REGNODE_SIMPLE(OP(first)))\n\t    RExC_rxi->regstclass = first;\n\telse if (PL_regkind[OP(first)] == BOUND ||\n\t\t PL_regkind[OP(first)] == NBOUND)\n\t    RExC_rxi->regstclass = first;\n\telse if (PL_regkind[OP(first)] == BOL) {\n            RExC_rx->intflags |= (OP(first) == MBOL\n                           ? PREGf_ANCH_MBOL\n                           : PREGf_ANCH_SBOL);\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n\telse if (OP(first) == GPOS) {\n            RExC_rx->intflags |= PREGf_ANCH_GPOS;\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n\telse if ((!sawopen || !RExC_sawback) &&\n            !sawlookahead &&\n\t    (OP(first) == STAR &&\n\t    PL_regkind[OP(NEXTOPER(first))] == REG_ANY) &&\n            !(RExC_rx->intflags & PREGf_ANCH) && !pRExC_state->code_blocks)\n\t{\n\t    /* turn .* into ^.* with an implied $*=1 */\n\t    const int type =\n\t\t(OP(NEXTOPER(first)) == REG_ANY)\n                    ? PREGf_ANCH_MBOL\n                    : PREGf_ANCH_SBOL;\n            RExC_rx->intflags |= (type | PREGf_IMPLICIT);\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n        if (sawplus && !sawminmod && !sawlookahead\n            && (!sawopen || !RExC_sawback)\n\t    && !pRExC_state->code_blocks) /* May examine pos and $& */\n\t    /* x+ must match at the 1st pos of run of x's */\n\t    RExC_rx->intflags |= PREGf_SKIP;\n\n\t/* Scan is after the zeroth branch, first is atomic matcher. */\n#ifdef TRIE_STUDY_OPT\n\tDEBUG_PARSE_r(\n\t    if (!restudied)\n                Perl_re_printf( aTHX_  \"first at %\" IVdf \"\\n\",\n\t\t\t      (IV)(first - scan + 1))\n        );\n#else\n\tDEBUG_PARSE_r(\n            Perl_re_printf( aTHX_  \"first at %\" IVdf \"\\n\",\n\t        (IV)(first - scan + 1))\n        );\n#endif\n\n\n\t/*\n\t* If there's something expensive in the r.e., find the\n\t* longest literal string that must appear and make it the\n\t* regmust.  Resolve ties in favor of later strings, since\n\t* the regstart check works with the beginning of the r.e.\n\t* and avoiding duplication strengthens checking.  Not a\n\t* strong reason, but sufficient in the absence of others.\n\t* [Now we resolve ties in favor of the earlier string if\n\t* it happens that c_offset_min has been invalidated, since the\n\t* earlier string may buy us something the later one won't.]\n\t*/\n\n\tdata.substrs[0].str = newSVpvs(\"\");\n\tdata.substrs[1].str = newSVpvs(\"\");\n\tdata.last_found = newSVpvs(\"\");\n\tdata.cur_is_floating = 0; /* initially any found substring is fixed */\n\tENTER_with_name(\"study_chunk\");\n\tSAVEFREESV(data.substrs[0].str);\n\tSAVEFREESV(data.substrs[1].str);\n\tSAVEFREESV(data.last_found);\n\tfirst = scan;\n\tif (!RExC_rxi->regstclass) {\n\t    ssc_init(pRExC_state, &ch_class);\n\t    data.start_class = &ch_class;\n\t    stclass_flag = SCF_DO_STCLASS_AND;\n\t} else\t\t\t\t/* XXXX Check for BOUND? */\n\t    stclass_flag = 0;\n\tdata.last_closep = &last_close;\n\n        DEBUG_RExC_seen();\n        /*\n         * MAIN ENTRY FOR study_chunk() FOR m/PATTERN/\n         * (NO top level branches)\n         */\n\tminlen = study_chunk(pRExC_state, &first, &minlen, &fake,\n                             scan + RExC_size, /* Up to end */\n            &data, -1, 0, NULL,\n            SCF_DO_SUBSTR | SCF_WHILEM_VISITED_POS | stclass_flag\n                          | (restudied ? SCF_TRIE_DOING_RESTUDY : 0),\n            0);\n\n\n        CHECK_RESTUDY_GOTO_butfirst(LEAVE_with_name(\"study_chunk\"));\n\n\n\tif ( RExC_total_parens == 1 && !data.cur_is_floating\n\t     && data.last_start_min == 0 && data.last_end > 0\n\t     && !RExC_seen_zerolen\n             && !(RExC_seen & REG_VERBARG_SEEN)\n             && !(RExC_seen & REG_GPOS_SEEN)\n        ){\n\t    RExC_rx->extflags |= RXf_CHECK_ALL;\n        }\n\tscan_commit(pRExC_state, &data,&minlen, 0);\n\n\n        /* XXX this is done in reverse order because that's the way the\n         * code was before it was parameterised. Don't know whether it\n         * actually needs doing in reverse order. DAPM */\n        for (i = 1; i >= 0; i--) {\n            longest_length[i] = CHR_SVLEN(data.substrs[i].str);\n\n            if (   !(   i\n                     && SvCUR(data.substrs[0].str)  /* ok to leave SvCUR */\n                     &&    data.substrs[0].min_offset\n                        == data.substrs[1].min_offset\n                     &&    SvCUR(data.substrs[0].str)\n                        == SvCUR(data.substrs[1].str)\n                    )\n                && S_setup_longest (aTHX_ pRExC_state,\n                                        &(RExC_rx->substrs->data[i]),\n                                        &(data.substrs[i]),\n                                        longest_length[i]))\n            {\n                RExC_rx->substrs->data[i].min_offset =\n                        data.substrs[i].min_offset - data.substrs[i].lookbehind;\n\n                RExC_rx->substrs->data[i].max_offset = data.substrs[i].max_offset;\n                /* Don't offset infinity */\n                if (data.substrs[i].max_offset < SSize_t_MAX)\n                    RExC_rx->substrs->data[i].max_offset -= data.substrs[i].lookbehind;\n                SvREFCNT_inc_simple_void_NN(data.substrs[i].str);\n            }\n            else {\n                RExC_rx->substrs->data[i].substr      = NULL;\n                RExC_rx->substrs->data[i].utf8_substr = NULL;\n                longest_length[i] = 0;\n            }\n        }\n\n\tLEAVE_with_name(\"study_chunk\");\n\n\tif (RExC_rxi->regstclass\n\t    && (OP(RExC_rxi->regstclass) == REG_ANY || OP(RExC_rxi->regstclass) == SANY))\n\t    RExC_rxi->regstclass = NULL;\n\n\tif ((!(RExC_rx->substrs->data[0].substr || RExC_rx->substrs->data[0].utf8_substr)\n              || RExC_rx->substrs->data[0].min_offset)\n\t    && stclass_flag\n            && ! (ANYOF_FLAGS(data.start_class) & SSC_MATCHES_EMPTY_STRING)\n\t    && is_ssc_worth_it(pRExC_state, data.start_class))\n\t{\n\t    const U32 n = add_data(pRExC_state, STR_WITH_LEN(\"f\"));\n\n            ssc_finalize(pRExC_state, data.start_class);\n\n\t    Newx(RExC_rxi->data->data[n], 1, regnode_ssc);\n\t    StructCopy(data.start_class,\n\t\t       (regnode_ssc*)RExC_rxi->data->data[n],\n\t\t       regnode_ssc);\n\t    RExC_rxi->regstclass = (regnode*)RExC_rxi->data->data[n];\n\t    RExC_rx->intflags &= ~PREGf_SKIP;\t/* Used in find_byclass(). */\n\t    DEBUG_COMPILE_r({ SV *sv = sv_newmortal();\n                      regprop(RExC_rx, sv, (regnode*)data.start_class, NULL, pRExC_state);\n                      Perl_re_printf( aTHX_\n\t\t\t\t    \"synthetic stclass \\\"%s\\\".\\n\",\n\t\t\t\t    SvPVX_const(sv));});\n            data.start_class = NULL;\n\t}\n\n        /* A temporary algorithm prefers floated substr to fixed one of\n         * same length to dig more info. */\n\ti = (longest_length[0] <= longest_length[1]);\n        RExC_rx->substrs->check_ix = i;\n        RExC_rx->check_end_shift  = RExC_rx->substrs->data[i].end_shift;\n        RExC_rx->check_substr     = RExC_rx->substrs->data[i].substr;\n        RExC_rx->check_utf8       = RExC_rx->substrs->data[i].utf8_substr;\n        RExC_rx->check_offset_min = RExC_rx->substrs->data[i].min_offset;\n        RExC_rx->check_offset_max = RExC_rx->substrs->data[i].max_offset;\n        if (!i && (RExC_rx->intflags & (PREGf_ANCH_SBOL|PREGf_ANCH_GPOS)))\n            RExC_rx->intflags |= PREGf_NOSCAN;\n\n\tif ((RExC_rx->check_substr || RExC_rx->check_utf8) ) {\n\t    RExC_rx->extflags |= RXf_USE_INTUIT;\n\t    if (SvTAIL(RExC_rx->check_substr ? RExC_rx->check_substr : RExC_rx->check_utf8))\n\t\tRExC_rx->extflags |= RXf_INTUIT_TAIL;\n\t}\n\n\t/* XXX Unneeded? dmq (shouldn't as this is handled elsewhere)\n\tif ( (STRLEN)minlen < longest_length[1] )\n            minlen= longest_length[1];\n        if ( (STRLEN)minlen < longest_length[0] )\n            minlen= longest_length[0];\n        */\n    }\n    else {\n\t/* Several toplevels. Best we can is to set minlen. */\n\tSSize_t fake;\n\tregnode_ssc ch_class;\n\tSSize_t last_close = 0;\n\n        DEBUG_PARSE_r(Perl_re_printf( aTHX_  \"\\nMulti Top Level\\n\"));\n\n\tscan = RExC_rxi->program + 1;\n\tssc_init(pRExC_state, &ch_class);\n\tdata.start_class = &ch_class;\n\tdata.last_closep = &last_close;\n\n        DEBUG_RExC_seen();\n        /*\n         * MAIN ENTRY FOR study_chunk() FOR m/P1|P2|.../\n         * (patterns WITH top level branches)\n         */\n\tminlen = study_chunk(pRExC_state,\n            &scan, &minlen, &fake, scan + RExC_size, &data, -1, 0, NULL,\n            SCF_DO_STCLASS_AND|SCF_WHILEM_VISITED_POS|(restudied\n                                                      ? SCF_TRIE_DOING_RESTUDY\n                                                      : 0),\n            0);\n\n        CHECK_RESTUDY_GOTO_butfirst(NOOP);\n\n\tRExC_rx->check_substr = NULL;\n        RExC_rx->check_utf8 = NULL;\n        RExC_rx->substrs->data[0].substr      = NULL;\n        RExC_rx->substrs->data[0].utf8_substr = NULL;\n        RExC_rx->substrs->data[1].substr      = NULL;\n        RExC_rx->substrs->data[1].utf8_substr = NULL;\n\n        if (! (ANYOF_FLAGS(data.start_class) & SSC_MATCHES_EMPTY_STRING)\n\t    && is_ssc_worth_it(pRExC_state, data.start_class))\n        {\n\t    const U32 n = add_data(pRExC_state, STR_WITH_LEN(\"f\"));\n\n            ssc_finalize(pRExC_state, data.start_class);\n\n\t    Newx(RExC_rxi->data->data[n], 1, regnode_ssc);\n\t    StructCopy(data.start_class,\n\t\t       (regnode_ssc*)RExC_rxi->data->data[n],\n\t\t       regnode_ssc);\n\t    RExC_rxi->regstclass = (regnode*)RExC_rxi->data->data[n];\n\t    RExC_rx->intflags &= ~PREGf_SKIP;\t/* Used in find_byclass(). */\n\t    DEBUG_COMPILE_r({ SV* sv = sv_newmortal();\n                      regprop(RExC_rx, sv, (regnode*)data.start_class, NULL, pRExC_state);\n                      Perl_re_printf( aTHX_\n\t\t\t\t    \"synthetic stclass \\\"%s\\\".\\n\",\n\t\t\t\t    SvPVX_const(sv));});\n            data.start_class = NULL;\n\t}\n    }\n\n    if (RExC_seen & REG_UNBOUNDED_QUANTIFIER_SEEN) {\n        RExC_rx->extflags |= RXf_UNBOUNDED_QUANTIFIER_SEEN;\n        RExC_rx->maxlen = REG_INFTY;\n    }\n    else {\n        RExC_rx->maxlen = RExC_maxlen;\n    }\n\n    /* Guard against an embedded (?=) or (?<=) with a longer minlen than\n       the \"real\" pattern. */\n    DEBUG_OPTIMISE_r({\n        Perl_re_printf( aTHX_ \"minlen: %\" IVdf \" RExC_rx->minlen:%\" IVdf \" maxlen:%\" IVdf \"\\n\",\n                      (IV)minlen, (IV)RExC_rx->minlen, (IV)RExC_maxlen);\n    });\n    RExC_rx->minlenret = minlen;\n    if (RExC_rx->minlen < minlen)\n        RExC_rx->minlen = minlen;\n\n    if (RExC_seen & REG_RECURSE_SEEN ) {\n        RExC_rx->intflags |= PREGf_RECURSE_SEEN;\n        Newx(RExC_rx->recurse_locinput, RExC_rx->nparens + 1, char *);\n    }\n    if (RExC_seen & REG_GPOS_SEEN)\n        RExC_rx->intflags |= PREGf_GPOS_SEEN;\n    if (RExC_seen & REG_LOOKBEHIND_SEEN)\n        RExC_rx->extflags |= RXf_NO_INPLACE_SUBST; /* inplace might break the\n                                                lookbehind */\n    if (pRExC_state->code_blocks)\n\tRExC_rx->extflags |= RXf_EVAL_SEEN;\n    if (RExC_seen & REG_VERBARG_SEEN)\n    {\n\tRExC_rx->intflags |= PREGf_VERBARG_SEEN;\n        RExC_rx->extflags |= RXf_NO_INPLACE_SUBST; /* don't understand this! Yves */\n    }\n    if (RExC_seen & REG_CUTGROUP_SEEN)\n\tRExC_rx->intflags |= PREGf_CUTGROUP_SEEN;\n    if (pm_flags & PMf_USE_RE_EVAL)\n\tRExC_rx->intflags |= PREGf_USE_RE_EVAL;\n    if (RExC_paren_names)\n        RXp_PAREN_NAMES(RExC_rx) = MUTABLE_HV(SvREFCNT_inc(RExC_paren_names));\n    else\n        RXp_PAREN_NAMES(RExC_rx) = NULL;\n\n    /* If we have seen an anchor in our pattern then we set the extflag RXf_IS_ANCHORED\n     * so it can be used in pp.c */\n    if (RExC_rx->intflags & PREGf_ANCH)\n        RExC_rx->extflags |= RXf_IS_ANCHORED;\n\n\n    {\n        /* this is used to identify \"special\" patterns that might result\n         * in Perl NOT calling the regex engine and instead doing the match \"itself\",\n         * particularly special cases in split//. By having the regex compiler\n         * do this pattern matching at a regop level (instead of by inspecting the pattern)\n         * we avoid weird issues with equivalent patterns resulting in different behavior,\n         * AND we allow non Perl engines to get the same optimizations by the setting the\n         * flags appropriately - Yves */\n        regnode *first = RExC_rxi->program + 1;\n        U8 fop = OP(first);\n        regnode *next = regnext(first);\n        U8 nop = OP(next);\n\n        if (PL_regkind[fop] == NOTHING && nop == END)\n            RExC_rx->extflags |= RXf_NULL;\n        else if ((fop == MBOL || (fop == SBOL && !first->flags)) && nop == END)\n            /* when fop is SBOL first->flags will be true only when it was\n             * produced by parsing /\\A/, and not when parsing /^/. This is\n             * very important for the split code as there we want to\n             * treat /^/ as /^/m, but we do not want to treat /\\A/ as /^/m.\n             * See rt #122761 for more details. -- Yves */\n            RExC_rx->extflags |= RXf_START_ONLY;\n        else if (fop == PLUS\n                 && PL_regkind[nop] == POSIXD && FLAGS(next) == _CC_SPACE\n                 && nop == END)\n            RExC_rx->extflags |= RXf_WHITE;\n        else if ( RExC_rx->extflags & RXf_SPLIT\n                  && (fop == EXACT || fop == EXACT_ONLY8 || fop == EXACTL)\n                  && STR_LEN(first) == 1\n                  && *(STRING(first)) == ' '\n                  && nop == END )\n            RExC_rx->extflags |= (RXf_SKIPWHITE|RXf_WHITE);\n\n    }\n\n    if (RExC_contains_locale) {\n        RXp_EXTFLAGS(RExC_rx) |= RXf_TAINTED;\n    }\n\n#ifdef DEBUGGING\n    if (RExC_paren_names) {\n        RExC_rxi->name_list_idx = add_data( pRExC_state, STR_WITH_LEN(\"a\"));\n        RExC_rxi->data->data[RExC_rxi->name_list_idx]\n                                   = (void*)SvREFCNT_inc(RExC_paren_name_list);\n    } else\n#endif\n    RExC_rxi->name_list_idx = 0;\n\n    while ( RExC_recurse_count > 0 ) {\n        const regnode *scan = RExC_recurse[ --RExC_recurse_count ];\n        /*\n         * This data structure is set up in study_chunk() and is used\n         * to calculate the distance between a GOSUB regopcode and\n         * the OPEN/CURLYM (CURLYM's are special and can act like OPEN's)\n         * it refers to.\n         *\n         * If for some reason someone writes code that optimises\n         * away a GOSUB opcode then the assert should be changed to\n         * an if(scan) to guard the ARG2L_SET() - Yves\n         *\n         */\n        assert(scan && OP(scan) == GOSUB);\n        ARG2L_SET( scan, RExC_open_parens[ARG(scan)] - REGNODE_OFFSET(scan));\n    }\n\n    Newxz(RExC_rx->offs, RExC_total_parens, regexp_paren_pair);\n    /* assume we don't need to swap parens around before we match */\n    DEBUG_TEST_r({\n        Perl_re_printf( aTHX_ \"study_chunk_recursed_count: %lu\\n\",\n            (unsigned long)RExC_study_chunk_recursed_count);\n    });\n    DEBUG_DUMP_r({\n        DEBUG_RExC_seen();\n        Perl_re_printf( aTHX_ \"Final program:\\n\");\n        regdump(RExC_rx);\n    });\n\n    if (RExC_open_parens) {\n        Safefree(RExC_open_parens);\n        RExC_open_parens = NULL;\n    }\n    if (RExC_close_parens) {\n        Safefree(RExC_close_parens);\n        RExC_close_parens = NULL;\n    }\n\n#ifdef USE_ITHREADS\n    /* under ithreads the ?pat? PMf_USED flag on the pmop is simulated\n     * by setting the regexp SV to readonly-only instead. If the\n     * pattern's been recompiled, the USEDness should remain. */\n    if (old_re && SvREADONLY(old_re))\n        SvREADONLY_on(Rx);\n#endif\n    return Rx;", "func_src_after": "REGEXP *\nPerl_re_op_compile(pTHX_ SV ** const patternp, int pat_count,\n\t\t    OP *expr, const regexp_engine* eng, REGEXP *old_re,\n\t\t     bool *is_bare_re, const U32 orig_rx_flags, const U32 pm_flags)\n{\n    dVAR;\n    REGEXP *Rx;         /* Capital 'R' means points to a REGEXP */\n    STRLEN plen;\n    char *exp;\n    regnode *scan;\n    I32 flags;\n    SSize_t minlen = 0;\n    U32 rx_flags;\n    SV *pat;\n    SV** new_patternp = patternp;\n\n    /* these are all flags - maybe they should be turned\n     * into a single int with different bit masks */\n    I32 sawlookahead = 0;\n    I32 sawplus = 0;\n    I32 sawopen = 0;\n    I32 sawminmod = 0;\n\n    regex_charset initial_charset = get_regex_charset(orig_rx_flags);\n    bool recompile = 0;\n    bool runtime_code = 0;\n    scan_data_t data;\n    RExC_state_t RExC_state;\n    RExC_state_t * const pRExC_state = &RExC_state;\n#ifdef TRIE_STUDY_OPT\n    int restudied = 0;\n    RExC_state_t copyRExC_state;\n#endif\n    GET_RE_DEBUG_FLAGS_DECL;\n\n    PERL_ARGS_ASSERT_RE_OP_COMPILE;\n\n    DEBUG_r(if (!PL_colorset) reginitcolors());\n\n    /* Initialize these here instead of as-needed, as is quick and avoids\n     * having to test them each time otherwise */\n    if (! PL_InBitmap) {\n#ifdef DEBUGGING\n        char * dump_len_string;\n#endif\n\n        /* This is calculated here, because the Perl program that generates the\n         * static global ones doesn't currently have access to\n         * NUM_ANYOF_CODE_POINTS */\n\tPL_InBitmap = _new_invlist(2);\n\tPL_InBitmap = _add_range_to_invlist(PL_InBitmap, 0,\n                                                    NUM_ANYOF_CODE_POINTS - 1);\n#ifdef DEBUGGING\n        dump_len_string = PerlEnv_getenv(\"PERL_DUMP_RE_MAX_LEN\");\n        if (   ! dump_len_string\n            || ! grok_atoUV(dump_len_string, (UV *)&PL_dump_re_max_len, NULL))\n        {\n            PL_dump_re_max_len = 60;    /* A reasonable default */\n        }\n#endif\n    }\n\n    pRExC_state->warn_text = NULL;\n    pRExC_state->unlexed_names = NULL;\n    pRExC_state->code_blocks = NULL;\n\n    if (is_bare_re)\n\t*is_bare_re = FALSE;\n\n    if (expr && (expr->op_type == OP_LIST ||\n\t\t(expr->op_type == OP_NULL && expr->op_targ == OP_LIST))) {\n\t/* allocate code_blocks if needed */\n\tOP *o;\n\tint ncode = 0;\n\n\tfor (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o))\n\t    if (o->op_type == OP_NULL && (o->op_flags & OPf_SPECIAL))\n\t\tncode++; /* count of DO blocks */\n\n\tif (ncode)\n            pRExC_state->code_blocks = S_alloc_code_blocks(aTHX_ ncode);\n    }\n\n    if (!pat_count) {\n        /* compile-time pattern with just OP_CONSTs and DO blocks */\n\n        int n;\n        OP *o;\n\n        /* find how many CONSTs there are */\n        assert(expr);\n        n = 0;\n        if (expr->op_type == OP_CONST)\n            n = 1;\n        else\n            for (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o)) {\n                if (o->op_type == OP_CONST)\n                    n++;\n            }\n\n        /* fake up an SV array */\n\n        assert(!new_patternp);\n        Newx(new_patternp, n, SV*);\n        SAVEFREEPV(new_patternp);\n        pat_count = n;\n\n        n = 0;\n        if (expr->op_type == OP_CONST)\n            new_patternp[n] = cSVOPx_sv(expr);\n        else\n            for (o = cLISTOPx(expr)->op_first; o; o = OpSIBLING(o)) {\n                if (o->op_type == OP_CONST)\n                    new_patternp[n++] = cSVOPo_sv;\n            }\n\n    }\n\n    DEBUG_PARSE_r(Perl_re_printf( aTHX_\n        \"Assembling pattern from %d elements%s\\n\", pat_count,\n            orig_rx_flags & RXf_SPLIT ? \" for split\" : \"\"));\n\n    /* set expr to the first arg op */\n\n    if (pRExC_state->code_blocks && pRExC_state->code_blocks->count\n         && expr->op_type != OP_CONST)\n    {\n            expr = cLISTOPx(expr)->op_first;\n            assert(   expr->op_type == OP_PUSHMARK\n                   || (expr->op_type == OP_NULL && expr->op_targ == OP_PUSHMARK)\n                   || expr->op_type == OP_PADRANGE);\n            expr = OpSIBLING(expr);\n    }\n\n    pat = S_concat_pat(aTHX_ pRExC_state, NULL, new_patternp, pat_count,\n                        expr, &recompile, NULL);\n\n    /* handle bare (possibly after overloading) regex: foo =~ $re */\n    {\n        SV *re = pat;\n        if (SvROK(re))\n            re = SvRV(re);\n        if (SvTYPE(re) == SVt_REGEXP) {\n            if (is_bare_re)\n                *is_bare_re = TRUE;\n            SvREFCNT_inc(re);\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_\n                \"Precompiled pattern%s\\n\",\n                    orig_rx_flags & RXf_SPLIT ? \" for split\" : \"\"));\n\n            return (REGEXP*)re;\n        }\n    }\n\n    exp = SvPV_nomg(pat, plen);\n\n    if (!eng->op_comp) {\n\tif ((SvUTF8(pat) && IN_BYTES)\n\t\t|| SvGMAGICAL(pat) || SvAMAGIC(pat))\n\t{\n\t    /* make a temporary copy; either to convert to bytes,\n\t     * or to avoid repeating get-magic / overloaded stringify */\n\t    pat = newSVpvn_flags(exp, plen, SVs_TEMP |\n\t\t\t\t\t(IN_BYTES ? 0 : SvUTF8(pat)));\n\t}\n\treturn CALLREGCOMP_ENG(eng, pat, orig_rx_flags);\n    }\n\n    /* ignore the utf8ness if the pattern is 0 length */\n    RExC_utf8 = RExC_orig_utf8 = (plen == 0 || IN_BYTES) ? 0 : SvUTF8(pat);\n    RExC_uni_semantics = 0;\n    RExC_contains_locale = 0;\n    RExC_strict = cBOOL(pm_flags & RXf_PMf_STRICT);\n    RExC_in_script_run = 0;\n    RExC_study_started = 0;\n    pRExC_state->runtime_code_qr = NULL;\n    RExC_frame_head= NULL;\n    RExC_frame_last= NULL;\n    RExC_frame_count= 0;\n    RExC_latest_warn_offset = 0;\n    RExC_use_BRANCHJ = 0;\n    RExC_total_parens = 0;\n    RExC_open_parens = NULL;\n    RExC_close_parens = NULL;\n    RExC_paren_names = NULL;\n    RExC_size = 0;\n    RExC_seen_d_op = FALSE;\n#ifdef DEBUGGING\n    RExC_paren_name_list = NULL;\n#endif\n\n    DEBUG_r({\n        RExC_mysv1= sv_newmortal();\n        RExC_mysv2= sv_newmortal();\n    });\n\n    DEBUG_COMPILE_r({\n            SV *dsv= sv_newmortal();\n            RE_PV_QUOTED_DECL(s, RExC_utf8, dsv, exp, plen, PL_dump_re_max_len);\n            Perl_re_printf( aTHX_  \"%sCompiling REx%s %s\\n\",\n                          PL_colors[4], PL_colors[5], s);\n        });\n\n    /* we jump here if we have to recompile, e.g., from upgrading the pattern\n     * to utf8 */\n\n    if ((pm_flags & PMf_USE_RE_EVAL)\n\t\t/* this second condition covers the non-regex literal case,\n\t\t * i.e.  $foo =~ '(?{})'. */\n\t\t|| (IN_PERL_COMPILETIME && (PL_hints & HINT_RE_EVAL))\n    )\n\truntime_code = S_has_runtime_code(aTHX_ pRExC_state, exp, plen);\n\n  redo_parse:\n    /* return old regex if pattern hasn't changed */\n    /* XXX: note in the below we have to check the flags as well as the\n     * pattern.\n     *\n     * Things get a touch tricky as we have to compare the utf8 flag\n     * independently from the compile flags.  */\n\n    if (   old_re\n        && !recompile\n        && !!RX_UTF8(old_re) == !!RExC_utf8\n        && ( RX_COMPFLAGS(old_re) == ( orig_rx_flags & RXf_PMf_FLAGCOPYMASK ) )\n\t&& RX_PRECOMP(old_re)\n\t&& RX_PRELEN(old_re) == plen\n        && memEQ(RX_PRECOMP(old_re), exp, plen)\n\t&& !runtime_code /* with runtime code, always recompile */ )\n    {\n        return old_re;\n    }\n\n    /* Allocate the pattern's SV */\n    RExC_rx_sv = Rx = (REGEXP*) newSV_type(SVt_REGEXP);\n    RExC_rx = ReANY(Rx);\n    if ( RExC_rx == NULL )\n        FAIL(\"Regexp out of space\");\n\n    rx_flags = orig_rx_flags;\n\n    if (   (UTF || RExC_uni_semantics)\n        && initial_charset == REGEX_DEPENDS_CHARSET)\n    {\n\n\t/* Set to use unicode semantics if the pattern is in utf8 and has the\n\t * 'depends' charset specified, as it means unicode when utf8  */\n\tset_regex_charset(&rx_flags, REGEX_UNICODE_CHARSET);\n        RExC_uni_semantics = 1;\n    }\n\n    RExC_pm_flags = pm_flags;\n\n    if (runtime_code) {\n        assert(TAINTING_get || !TAINT_get);\n\tif (TAINT_get)\n\t    Perl_croak(aTHX_ \"Eval-group in insecure regular expression\");\n\n\tif (!S_compile_runtime_code(aTHX_ pRExC_state, exp, plen)) {\n\t    /* whoops, we have a non-utf8 pattern, whilst run-time code\n\t     * got compiled as utf8. Try again with a utf8 pattern */\n            S_pat_upgrade_to_utf8(aTHX_ pRExC_state, &exp, &plen,\n                pRExC_state->code_blocks ? pRExC_state->code_blocks->count : 0);\n            goto redo_parse;\n\t}\n    }\n    assert(!pRExC_state->runtime_code_qr);\n\n    RExC_sawback = 0;\n\n    RExC_seen = 0;\n    RExC_maxlen = 0;\n    RExC_in_lookbehind = 0;\n    RExC_seen_zerolen = *exp == '^' ? -1 : 0;\n#ifdef EBCDIC\n    RExC_recode_x_to_native = 0;\n#endif\n    RExC_in_multi_char_class = 0;\n\n    RExC_start = RExC_copy_start_in_constructed = RExC_copy_start_in_input = RExC_precomp = exp;\n    RExC_precomp_end = RExC_end = exp + plen;\n    RExC_nestroot = 0;\n    RExC_whilem_seen = 0;\n    RExC_end_op = NULL;\n    RExC_recurse = NULL;\n    RExC_study_chunk_recursed = NULL;\n    RExC_study_chunk_recursed_bytes= 0;\n    RExC_recurse_count = 0;\n    pRExC_state->code_index = 0;\n\n    /* Initialize the string in the compiled pattern.  This is so that there is\n     * something to output if necessary */\n    set_regex_pv(pRExC_state, Rx);\n\n    DEBUG_PARSE_r({\n        Perl_re_printf( aTHX_\n            \"Starting parse and generation\\n\");\n        RExC_lastnum=0;\n        RExC_lastparse=NULL;\n    });\n\n    /* Allocate space and zero-initialize. Note, the two step process\n       of zeroing when in debug mode, thus anything assigned has to\n       happen after that */\n    if (!  RExC_size) {\n\n        /* On the first pass of the parse, we guess how big this will be.  Then\n         * we grow in one operation to that amount and then give it back.  As\n         * we go along, we re-allocate what we need.\n         *\n         * XXX Currently the guess is essentially that the pattern will be an\n         * EXACT node with one byte input, one byte output.  This is crude, and\n         * better heuristics are welcome.\n         *\n         * On any subsequent passes, we guess what we actually computed in the\n         * latest earlier pass.  Such a pass probably didn't complete so is\n         * missing stuff.  We could improve those guesses by knowing where the\n         * parse stopped, and use the length so far plus apply the above\n         * assumption to what's left. */\n        RExC_size = STR_SZ(RExC_end - RExC_start);\n    }\n\n    Newxc(RExC_rxi, sizeof(regexp_internal) + RExC_size, char, regexp_internal);\n    if ( RExC_rxi == NULL )\n        FAIL(\"Regexp out of space\");\n\n    Zero(RExC_rxi, sizeof(regexp_internal) + RExC_size, char);\n    RXi_SET( RExC_rx, RExC_rxi );\n\n    /* We start from 0 (over from 0 in the case this is a reparse.  The first\n     * node parsed will give back any excess memory we have allocated so far).\n     * */\n    RExC_size = 0;\n\n    /* non-zero initialization begins here */\n    RExC_rx->engine= eng;\n    RExC_rx->extflags = rx_flags;\n    RXp_COMPFLAGS(RExC_rx) = orig_rx_flags & RXf_PMf_FLAGCOPYMASK;\n\n    if (pm_flags & PMf_IS_QR) {\n\tRExC_rxi->code_blocks = pRExC_state->code_blocks;\n        if (RExC_rxi->code_blocks) {\n            RExC_rxi->code_blocks->refcnt++;\n        }\n    }\n\n    RExC_rx->intflags = 0;\n\n    RExC_flags = rx_flags;\t/* don't let top level (?i) bleed */\n    RExC_parse = exp;\n\n    /* This NUL is guaranteed because the pattern comes from an SV*, and the sv\n     * code makes sure the final byte is an uncounted NUL.  But should this\n     * ever not be the case, lots of things could read beyond the end of the\n     * buffer: loops like\n     *      while(isFOO(*RExC_parse)) RExC_parse++;\n     *      strchr(RExC_parse, \"foo\");\n     * etc.  So it is worth noting. */\n    assert(*RExC_end == '\\0');\n\n    RExC_naughty = 0;\n    RExC_npar = 1;\n    RExC_parens_buf_size = 0;\n    RExC_emit_start = RExC_rxi->program;\n    pRExC_state->code_index = 0;\n\n    *((char*) RExC_emit_start) = (char) REG_MAGIC;\n    RExC_emit = 1;\n\n    /* Do the parse */\n    if (reg(pRExC_state, 0, &flags, 1)) {\n\n        /* Success!, But we may need to redo the parse knowing how many parens\n         * there actually are */\n        if (IN_PARENS_PASS) {\n            flags |= RESTART_PARSE;\n        }\n\n        /* We have that number in RExC_npar */\n        RExC_total_parens = RExC_npar;\n\n        /* XXX For backporting, use long jumps if there is any possibility of\n         * overflow */\n        if (RExC_size > U16_MAX && ! RExC_use_BRANCHJ) {\n            RExC_use_BRANCHJ = TRUE;\n            flags |= RESTART_PARSE;\n        }\n    }\n    else if (! MUST_RESTART(flags)) {\n\tReREFCNT_dec(Rx);\n        Perl_croak(aTHX_ \"panic: reg returned failure to re_op_compile, flags=%#\" UVxf, (UV) flags);\n    }\n\n    /* Here, we either have success, or we have to redo the parse for some reason */\n    if (MUST_RESTART(flags)) {\n\n        /* It's possible to write a regexp in ascii that represents Unicode\n        codepoints outside of the byte range, such as via \\x{100}. If we\n        detect such a sequence we have to convert the entire pattern to utf8\n        and then recompile, as our sizing calculation will have been based\n        on 1 byte == 1 character, but we will need to use utf8 to encode\n        at least some part of the pattern, and therefore must convert the whole\n        thing.\n        -- dmq */\n        if (flags & NEED_UTF8) {\n\n            /* We have stored the offset of the final warning output so far.\n             * That must be adjusted.  Any variant characters between the start\n             * of the pattern and this warning count for 2 bytes in the final,\n             * so just add them again */\n            if (UNLIKELY(RExC_latest_warn_offset > 0)) {\n                RExC_latest_warn_offset +=\n                            variant_under_utf8_count((U8 *) exp, (U8 *) exp\n                                                + RExC_latest_warn_offset);\n            }\n            S_pat_upgrade_to_utf8(aTHX_ pRExC_state, &exp, &plen,\n            pRExC_state->code_blocks ? pRExC_state->code_blocks->count : 0);\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_ \"Need to redo parse after upgrade\\n\"));\n        }\n        else {\n            DEBUG_PARSE_r(Perl_re_printf( aTHX_ \"Need to redo parse\\n\"));\n        }\n\n        if (ALL_PARENS_COUNTED) {\n            /* Make enough room for all the known parens, and zero it */\n            Renew(RExC_open_parens, RExC_total_parens, regnode_offset);\n            Zero(RExC_open_parens, RExC_total_parens, regnode_offset);\n            RExC_open_parens[0] = 1;    /* +1 for REG_MAGIC */\n\n            Renew(RExC_close_parens, RExC_total_parens, regnode_offset);\n            Zero(RExC_close_parens, RExC_total_parens, regnode_offset);\n        }\n        else { /* Parse did not complete.  Reinitialize the parentheses\n                  structures */\n            RExC_total_parens = 0;\n            if (RExC_open_parens) {\n                Safefree(RExC_open_parens);\n                RExC_open_parens = NULL;\n            }\n            if (RExC_close_parens) {\n                Safefree(RExC_close_parens);\n                RExC_close_parens = NULL;\n            }\n        }\n\n        /* Clean up what we did in this parse */\n        SvREFCNT_dec_NN(RExC_rx_sv);\n\n        goto redo_parse;\n    }\n\n    /* Here, we have successfully parsed and generated the pattern's program\n     * for the regex engine.  We are ready to finish things up and look for\n     * optimizations. */\n\n    /* Update the string to compile, with correct modifiers, etc */\n    set_regex_pv(pRExC_state, Rx);\n\n    RExC_rx->nparens = RExC_total_parens - 1;\n\n    /* Uses the upper 4 bits of the FLAGS field, so keep within that size */\n    if (RExC_whilem_seen > 15)\n        RExC_whilem_seen = 15;\n\n    DEBUG_PARSE_r({\n        Perl_re_printf( aTHX_\n            \"Required size %\" IVdf \" nodes\\n\", (IV)RExC_size);\n        RExC_lastnum=0;\n        RExC_lastparse=NULL;\n    });\n\n#ifdef RE_TRACK_PATTERN_OFFSETS\n    DEBUG_OFFSETS_r(Perl_re_printf( aTHX_\n                          \"%s %\" UVuf \" bytes for offset annotations.\\n\",\n                          RExC_offsets ? \"Got\" : \"Couldn't get\",\n                          (UV)((RExC_offsets[0] * 2 + 1))));\n    DEBUG_OFFSETS_r(if (RExC_offsets) {\n        const STRLEN len = RExC_offsets[0];\n        STRLEN i;\n        GET_RE_DEBUG_FLAGS_DECL;\n        Perl_re_printf( aTHX_\n                      \"Offsets: [%\" UVuf \"]\\n\\t\", (UV)RExC_offsets[0]);\n        for (i = 1; i <= len; i++) {\n            if (RExC_offsets[i*2-1] || RExC_offsets[i*2])\n                Perl_re_printf( aTHX_  \"%\" UVuf \":%\" UVuf \"[%\" UVuf \"] \",\n                (UV)i, (UV)RExC_offsets[i*2-1], (UV)RExC_offsets[i*2]);\n        }\n        Perl_re_printf( aTHX_  \"\\n\");\n    });\n\n#else\n    SetProgLen(RExC_rxi,RExC_size);\n#endif\n\n    DEBUG_OPTIMISE_r(\n        Perl_re_printf( aTHX_  \"Starting post parse optimization\\n\");\n    );\n\n    /* XXXX To minimize changes to RE engine we always allocate\n       3-units-long substrs field. */\n    Newx(RExC_rx->substrs, 1, struct reg_substr_data);\n    if (RExC_recurse_count) {\n        Newx(RExC_recurse, RExC_recurse_count, regnode *);\n        SAVEFREEPV(RExC_recurse);\n    }\n\n    if (RExC_seen & REG_RECURSE_SEEN) {\n        /* Note, RExC_total_parens is 1 + the number of parens in a pattern.\n         * So its 1 if there are no parens. */\n        RExC_study_chunk_recursed_bytes= (RExC_total_parens >> 3) +\n                                         ((RExC_total_parens & 0x07) != 0);\n        Newx(RExC_study_chunk_recursed,\n             RExC_study_chunk_recursed_bytes * RExC_total_parens, U8);\n        SAVEFREEPV(RExC_study_chunk_recursed);\n    }\n\n  reStudy:\n    RExC_rx->minlen = minlen = sawlookahead = sawplus = sawopen = sawminmod = 0;\n    DEBUG_r(\n        RExC_study_chunk_recursed_count= 0;\n    );\n    Zero(RExC_rx->substrs, 1, struct reg_substr_data);\n    if (RExC_study_chunk_recursed) {\n        Zero(RExC_study_chunk_recursed,\n             RExC_study_chunk_recursed_bytes * RExC_total_parens, U8);\n    }\n\n\n#ifdef TRIE_STUDY_OPT\n    if (!restudied) {\n        StructCopy(&zero_scan_data, &data, scan_data_t);\n        copyRExC_state = RExC_state;\n    } else {\n        U32 seen=RExC_seen;\n        DEBUG_OPTIMISE_r(Perl_re_printf( aTHX_ \"Restudying\\n\"));\n\n        RExC_state = copyRExC_state;\n        if (seen & REG_TOP_LEVEL_BRANCHES_SEEN)\n            RExC_seen |= REG_TOP_LEVEL_BRANCHES_SEEN;\n        else\n            RExC_seen &= ~REG_TOP_LEVEL_BRANCHES_SEEN;\n\tStructCopy(&zero_scan_data, &data, scan_data_t);\n    }\n#else\n    StructCopy(&zero_scan_data, &data, scan_data_t);\n#endif\n\n    /* Dig out information for optimizations. */\n    RExC_rx->extflags = RExC_flags; /* was pm_op */\n    /*dmq: removed as part of de-PMOP: pm->op_pmflags = RExC_flags; */\n\n    if (UTF)\n\tSvUTF8_on(Rx);\t/* Unicode in it? */\n    RExC_rxi->regstclass = NULL;\n    if (RExC_naughty >= TOO_NAUGHTY)\t/* Probably an expensive pattern. */\n\tRExC_rx->intflags |= PREGf_NAUGHTY;\n    scan = RExC_rxi->program + 1;\t\t/* First BRANCH. */\n\n    /* testing for BRANCH here tells us whether there is \"must appear\"\n       data in the pattern. If there is then we can use it for optimisations */\n    if (!(RExC_seen & REG_TOP_LEVEL_BRANCHES_SEEN)) { /*  Only one top-level choice.\n                                                  */\n\tSSize_t fake;\n\tSTRLEN longest_length[2];\n\tregnode_ssc ch_class; /* pointed to by data */\n\tint stclass_flag;\n\tSSize_t last_close = 0; /* pointed to by data */\n        regnode *first= scan;\n        regnode *first_next= regnext(first);\n        int i;\n\n\t/*\n\t * Skip introductions and multiplicators >= 1\n\t * so that we can extract the 'meat' of the pattern that must\n\t * match in the large if() sequence following.\n\t * NOTE that EXACT is NOT covered here, as it is normally\n\t * picked up by the optimiser separately.\n\t *\n\t * This is unfortunate as the optimiser isnt handling lookahead\n\t * properly currently.\n\t *\n\t */\n\twhile ((OP(first) == OPEN && (sawopen = 1)) ||\n\t       /* An OR of *one* alternative - should not happen now. */\n\t    (OP(first) == BRANCH && OP(first_next) != BRANCH) ||\n\t    /* for now we can't handle lookbehind IFMATCH*/\n\t    (OP(first) == IFMATCH && !first->flags && (sawlookahead = 1)) ||\n\t    (OP(first) == PLUS) ||\n\t    (OP(first) == MINMOD) ||\n\t       /* An {n,m} with n>0 */\n\t    (PL_regkind[OP(first)] == CURLY && ARG1(first) > 0) ||\n\t    (OP(first) == NOTHING && PL_regkind[OP(first_next)] != END ))\n\t{\n\t\t/*\n\t\t * the only op that could be a regnode is PLUS, all the rest\n\t\t * will be regnode_1 or regnode_2.\n\t\t *\n                 * (yves doesn't think this is true)\n\t\t */\n\t\tif (OP(first) == PLUS)\n\t\t    sawplus = 1;\n                else {\n                    if (OP(first) == MINMOD)\n                        sawminmod = 1;\n\t\t    first += regarglen[OP(first)];\n                }\n\t\tfirst = NEXTOPER(first);\n\t\tfirst_next= regnext(first);\n\t}\n\n\t/* Starting-point info. */\n      again:\n        DEBUG_PEEP(\"first:\", first, 0, 0);\n        /* Ignore EXACT as we deal with it later. */\n\tif (PL_regkind[OP(first)] == EXACT) {\n\t    if (   OP(first) == EXACT\n                || OP(first) == EXACT_ONLY8\n                || OP(first) == EXACTL)\n            {\n\t\tNOOP;\t/* Empty, get anchored substr later. */\n            }\n\t    else\n\t\tRExC_rxi->regstclass = first;\n\t}\n#ifdef TRIE_STCLASS\n\telse if (PL_regkind[OP(first)] == TRIE &&\n\t        ((reg_trie_data *)RExC_rxi->data->data[ ARG(first) ])->minlen>0)\n\t{\n            /* this can happen only on restudy */\n            RExC_rxi->regstclass = construct_ahocorasick_from_trie(pRExC_state, (regnode *)first, 0);\n\t}\n#endif\n\telse if (REGNODE_SIMPLE(OP(first)))\n\t    RExC_rxi->regstclass = first;\n\telse if (PL_regkind[OP(first)] == BOUND ||\n\t\t PL_regkind[OP(first)] == NBOUND)\n\t    RExC_rxi->regstclass = first;\n\telse if (PL_regkind[OP(first)] == BOL) {\n            RExC_rx->intflags |= (OP(first) == MBOL\n                           ? PREGf_ANCH_MBOL\n                           : PREGf_ANCH_SBOL);\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n\telse if (OP(first) == GPOS) {\n            RExC_rx->intflags |= PREGf_ANCH_GPOS;\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n\telse if ((!sawopen || !RExC_sawback) &&\n            !sawlookahead &&\n\t    (OP(first) == STAR &&\n\t    PL_regkind[OP(NEXTOPER(first))] == REG_ANY) &&\n            !(RExC_rx->intflags & PREGf_ANCH) && !pRExC_state->code_blocks)\n\t{\n\t    /* turn .* into ^.* with an implied $*=1 */\n\t    const int type =\n\t\t(OP(NEXTOPER(first)) == REG_ANY)\n                    ? PREGf_ANCH_MBOL\n                    : PREGf_ANCH_SBOL;\n            RExC_rx->intflags |= (type | PREGf_IMPLICIT);\n\t    first = NEXTOPER(first);\n\t    goto again;\n\t}\n        if (sawplus && !sawminmod && !sawlookahead\n            && (!sawopen || !RExC_sawback)\n\t    && !pRExC_state->code_blocks) /* May examine pos and $& */\n\t    /* x+ must match at the 1st pos of run of x's */\n\t    RExC_rx->intflags |= PREGf_SKIP;\n\n\t/* Scan is after the zeroth branch, first is atomic matcher. */\n#ifdef TRIE_STUDY_OPT\n\tDEBUG_PARSE_r(\n\t    if (!restudied)\n                Perl_re_printf( aTHX_  \"first at %\" IVdf \"\\n\",\n\t\t\t      (IV)(first - scan + 1))\n        );\n#else\n\tDEBUG_PARSE_r(\n            Perl_re_printf( aTHX_  \"first at %\" IVdf \"\\n\",\n\t        (IV)(first - scan + 1))\n        );\n#endif\n\n\n\t/*\n\t* If there's something expensive in the r.e., find the\n\t* longest literal string that must appear and make it the\n\t* regmust.  Resolve ties in favor of later strings, since\n\t* the regstart check works with the beginning of the r.e.\n\t* and avoiding duplication strengthens checking.  Not a\n\t* strong reason, but sufficient in the absence of others.\n\t* [Now we resolve ties in favor of the earlier string if\n\t* it happens that c_offset_min has been invalidated, since the\n\t* earlier string may buy us something the later one won't.]\n\t*/\n\n\tdata.substrs[0].str = newSVpvs(\"\");\n\tdata.substrs[1].str = newSVpvs(\"\");\n\tdata.last_found = newSVpvs(\"\");\n\tdata.cur_is_floating = 0; /* initially any found substring is fixed */\n\tENTER_with_name(\"study_chunk\");\n\tSAVEFREESV(data.substrs[0].str);\n\tSAVEFREESV(data.substrs[1].str);\n\tSAVEFREESV(data.last_found);\n\tfirst = scan;\n\tif (!RExC_rxi->regstclass) {\n\t    ssc_init(pRExC_state, &ch_class);\n\t    data.start_class = &ch_class;\n\t    stclass_flag = SCF_DO_STCLASS_AND;\n\t} else\t\t\t\t/* XXXX Check for BOUND? */\n\t    stclass_flag = 0;\n\tdata.last_closep = &last_close;\n\n        DEBUG_RExC_seen();\n        /*\n         * MAIN ENTRY FOR study_chunk() FOR m/PATTERN/\n         * (NO top level branches)\n         */\n\tminlen = study_chunk(pRExC_state, &first, &minlen, &fake,\n                             scan + RExC_size, /* Up to end */\n            &data, -1, 0, NULL,\n            SCF_DO_SUBSTR | SCF_WHILEM_VISITED_POS | stclass_flag\n                          | (restudied ? SCF_TRIE_DOING_RESTUDY : 0),\n            0);\n\n\n        CHECK_RESTUDY_GOTO_butfirst(LEAVE_with_name(\"study_chunk\"));\n\n\n\tif ( RExC_total_parens == 1 && !data.cur_is_floating\n\t     && data.last_start_min == 0 && data.last_end > 0\n\t     && !RExC_seen_zerolen\n             && !(RExC_seen & REG_VERBARG_SEEN)\n             && !(RExC_seen & REG_GPOS_SEEN)\n        ){\n\t    RExC_rx->extflags |= RXf_CHECK_ALL;\n        }\n\tscan_commit(pRExC_state, &data,&minlen, 0);\n\n\n        /* XXX this is done in reverse order because that's the way the\n         * code was before it was parameterised. Don't know whether it\n         * actually needs doing in reverse order. DAPM */\n        for (i = 1; i >= 0; i--) {\n            longest_length[i] = CHR_SVLEN(data.substrs[i].str);\n\n            if (   !(   i\n                     && SvCUR(data.substrs[0].str)  /* ok to leave SvCUR */\n                     &&    data.substrs[0].min_offset\n                        == data.substrs[1].min_offset\n                     &&    SvCUR(data.substrs[0].str)\n                        == SvCUR(data.substrs[1].str)\n                    )\n                && S_setup_longest (aTHX_ pRExC_state,\n                                        &(RExC_rx->substrs->data[i]),\n                                        &(data.substrs[i]),\n                                        longest_length[i]))\n            {\n                RExC_rx->substrs->data[i].min_offset =\n                        data.substrs[i].min_offset - data.substrs[i].lookbehind;\n\n                RExC_rx->substrs->data[i].max_offset = data.substrs[i].max_offset;\n                /* Don't offset infinity */\n                if (data.substrs[i].max_offset < SSize_t_MAX)\n                    RExC_rx->substrs->data[i].max_offset -= data.substrs[i].lookbehind;\n                SvREFCNT_inc_simple_void_NN(data.substrs[i].str);\n            }\n            else {\n                RExC_rx->substrs->data[i].substr      = NULL;\n                RExC_rx->substrs->data[i].utf8_substr = NULL;\n                longest_length[i] = 0;\n            }\n        }\n\n\tLEAVE_with_name(\"study_chunk\");\n\n\tif (RExC_rxi->regstclass\n\t    && (OP(RExC_rxi->regstclass) == REG_ANY || OP(RExC_rxi->regstclass) == SANY))\n\t    RExC_rxi->regstclass = NULL;\n\n\tif ((!(RExC_rx->substrs->data[0].substr || RExC_rx->substrs->data[0].utf8_substr)\n              || RExC_rx->substrs->data[0].min_offset)\n\t    && stclass_flag\n            && ! (ANYOF_FLAGS(data.start_class) & SSC_MATCHES_EMPTY_STRING)\n\t    && is_ssc_worth_it(pRExC_state, data.start_class))\n\t{\n\t    const U32 n = add_data(pRExC_state, STR_WITH_LEN(\"f\"));\n\n            ssc_finalize(pRExC_state, data.start_class);\n\n\t    Newx(RExC_rxi->data->data[n], 1, regnode_ssc);\n\t    StructCopy(data.start_class,\n\t\t       (regnode_ssc*)RExC_rxi->data->data[n],\n\t\t       regnode_ssc);\n\t    RExC_rxi->regstclass = (regnode*)RExC_rxi->data->data[n];\n\t    RExC_rx->intflags &= ~PREGf_SKIP;\t/* Used in find_byclass(). */\n\t    DEBUG_COMPILE_r({ SV *sv = sv_newmortal();\n                      regprop(RExC_rx, sv, (regnode*)data.start_class, NULL, pRExC_state);\n                      Perl_re_printf( aTHX_\n\t\t\t\t    \"synthetic stclass \\\"%s\\\".\\n\",\n\t\t\t\t    SvPVX_const(sv));});\n            data.start_class = NULL;\n\t}\n\n        /* A temporary algorithm prefers floated substr to fixed one of\n         * same length to dig more info. */\n\ti = (longest_length[0] <= longest_length[1]);\n        RExC_rx->substrs->check_ix = i;\n        RExC_rx->check_end_shift  = RExC_rx->substrs->data[i].end_shift;\n        RExC_rx->check_substr     = RExC_rx->substrs->data[i].substr;\n        RExC_rx->check_utf8       = RExC_rx->substrs->data[i].utf8_substr;\n        RExC_rx->check_offset_min = RExC_rx->substrs->data[i].min_offset;\n        RExC_rx->check_offset_max = RExC_rx->substrs->data[i].max_offset;\n        if (!i && (RExC_rx->intflags & (PREGf_ANCH_SBOL|PREGf_ANCH_GPOS)))\n            RExC_rx->intflags |= PREGf_NOSCAN;\n\n\tif ((RExC_rx->check_substr || RExC_rx->check_utf8) ) {\n\t    RExC_rx->extflags |= RXf_USE_INTUIT;\n\t    if (SvTAIL(RExC_rx->check_substr ? RExC_rx->check_substr : RExC_rx->check_utf8))\n\t\tRExC_rx->extflags |= RXf_INTUIT_TAIL;\n\t}\n\n\t/* XXX Unneeded? dmq (shouldn't as this is handled elsewhere)\n\tif ( (STRLEN)minlen < longest_length[1] )\n            minlen= longest_length[1];\n        if ( (STRLEN)minlen < longest_length[0] )\n            minlen= longest_length[0];\n        */\n    }\n    else {\n\t/* Several toplevels. Best we can is to set minlen. */\n\tSSize_t fake;\n\tregnode_ssc ch_class;\n\tSSize_t last_close = 0;\n\n        DEBUG_PARSE_r(Perl_re_printf( aTHX_  \"\\nMulti Top Level\\n\"));\n\n\tscan = RExC_rxi->program + 1;\n\tssc_init(pRExC_state, &ch_class);\n\tdata.start_class = &ch_class;\n\tdata.last_closep = &last_close;\n\n        DEBUG_RExC_seen();\n        /*\n         * MAIN ENTRY FOR study_chunk() FOR m/P1|P2|.../\n         * (patterns WITH top level branches)\n         */\n\tminlen = study_chunk(pRExC_state,\n            &scan, &minlen, &fake, scan + RExC_size, &data, -1, 0, NULL,\n            SCF_DO_STCLASS_AND|SCF_WHILEM_VISITED_POS|(restudied\n                                                      ? SCF_TRIE_DOING_RESTUDY\n                                                      : 0),\n            0);\n\n        CHECK_RESTUDY_GOTO_butfirst(NOOP);\n\n\tRExC_rx->check_substr = NULL;\n        RExC_rx->check_utf8 = NULL;\n        RExC_rx->substrs->data[0].substr      = NULL;\n        RExC_rx->substrs->data[0].utf8_substr = NULL;\n        RExC_rx->substrs->data[1].substr      = NULL;\n        RExC_rx->substrs->data[1].utf8_substr = NULL;\n\n        if (! (ANYOF_FLAGS(data.start_class) & SSC_MATCHES_EMPTY_STRING)\n\t    && is_ssc_worth_it(pRExC_state, data.start_class))\n        {\n\t    const U32 n = add_data(pRExC_state, STR_WITH_LEN(\"f\"));\n\n            ssc_finalize(pRExC_state, data.start_class);\n\n\t    Newx(RExC_rxi->data->data[n], 1, regnode_ssc);\n\t    StructCopy(data.start_class,\n\t\t       (regnode_ssc*)RExC_rxi->data->data[n],\n\t\t       regnode_ssc);\n\t    RExC_rxi->regstclass = (regnode*)RExC_rxi->data->data[n];\n\t    RExC_rx->intflags &= ~PREGf_SKIP;\t/* Used in find_byclass(). */\n\t    DEBUG_COMPILE_r({ SV* sv = sv_newmortal();\n                      regprop(RExC_rx, sv, (regnode*)data.start_class, NULL, pRExC_state);\n                      Perl_re_printf( aTHX_\n\t\t\t\t    \"synthetic stclass \\\"%s\\\".\\n\",\n\t\t\t\t    SvPVX_const(sv));});\n            data.start_class = NULL;\n\t}\n    }\n\n    if (RExC_seen & REG_UNBOUNDED_QUANTIFIER_SEEN) {\n        RExC_rx->extflags |= RXf_UNBOUNDED_QUANTIFIER_SEEN;\n        RExC_rx->maxlen = REG_INFTY;\n    }\n    else {\n        RExC_rx->maxlen = RExC_maxlen;\n    }\n\n    /* Guard against an embedded (?=) or (?<=) with a longer minlen than\n       the \"real\" pattern. */\n    DEBUG_OPTIMISE_r({\n        Perl_re_printf( aTHX_ \"minlen: %\" IVdf \" RExC_rx->minlen:%\" IVdf \" maxlen:%\" IVdf \"\\n\",\n                      (IV)minlen, (IV)RExC_rx->minlen, (IV)RExC_maxlen);\n    });\n    RExC_rx->minlenret = minlen;\n    if (RExC_rx->minlen < minlen)\n        RExC_rx->minlen = minlen;\n\n    if (RExC_seen & REG_RECURSE_SEEN ) {\n        RExC_rx->intflags |= PREGf_RECURSE_SEEN;\n        Newx(RExC_rx->recurse_locinput, RExC_rx->nparens + 1, char *);\n    }\n    if (RExC_seen & REG_GPOS_SEEN)\n        RExC_rx->intflags |= PREGf_GPOS_SEEN;\n    if (RExC_seen & REG_LOOKBEHIND_SEEN)\n        RExC_rx->extflags |= RXf_NO_INPLACE_SUBST; /* inplace might break the\n                                                lookbehind */\n    if (pRExC_state->code_blocks)\n\tRExC_rx->extflags |= RXf_EVAL_SEEN;\n    if (RExC_seen & REG_VERBARG_SEEN)\n    {\n\tRExC_rx->intflags |= PREGf_VERBARG_SEEN;\n        RExC_rx->extflags |= RXf_NO_INPLACE_SUBST; /* don't understand this! Yves */\n    }\n    if (RExC_seen & REG_CUTGROUP_SEEN)\n\tRExC_rx->intflags |= PREGf_CUTGROUP_SEEN;\n    if (pm_flags & PMf_USE_RE_EVAL)\n\tRExC_rx->intflags |= PREGf_USE_RE_EVAL;\n    if (RExC_paren_names)\n        RXp_PAREN_NAMES(RExC_rx) = MUTABLE_HV(SvREFCNT_inc(RExC_paren_names));\n    else\n        RXp_PAREN_NAMES(RExC_rx) = NULL;\n\n    /* If we have seen an anchor in our pattern then we set the extflag RXf_IS_ANCHORED\n     * so it can be used in pp.c */\n    if (RExC_rx->intflags & PREGf_ANCH)\n        RExC_rx->extflags |= RXf_IS_ANCHORED;\n\n\n    {\n        /* this is used to identify \"special\" patterns that might result\n         * in Perl NOT calling the regex engine and instead doing the match \"itself\",\n         * particularly special cases in split//. By having the regex compiler\n         * do this pattern matching at a regop level (instead of by inspecting the pattern)\n         * we avoid weird issues with equivalent patterns resulting in different behavior,\n         * AND we allow non Perl engines to get the same optimizations by the setting the\n         * flags appropriately - Yves */\n        regnode *first = RExC_rxi->program + 1;\n        U8 fop = OP(first);\n        regnode *next = regnext(first);\n        U8 nop = OP(next);\n\n        if (PL_regkind[fop] == NOTHING && nop == END)\n            RExC_rx->extflags |= RXf_NULL;\n        else if ((fop == MBOL || (fop == SBOL && !first->flags)) && nop == END)\n            /* when fop is SBOL first->flags will be true only when it was\n             * produced by parsing /\\A/, and not when parsing /^/. This is\n             * very important for the split code as there we want to\n             * treat /^/ as /^/m, but we do not want to treat /\\A/ as /^/m.\n             * See rt #122761 for more details. -- Yves */\n            RExC_rx->extflags |= RXf_START_ONLY;\n        else if (fop == PLUS\n                 && PL_regkind[nop] == POSIXD && FLAGS(next) == _CC_SPACE\n                 && nop == END)\n            RExC_rx->extflags |= RXf_WHITE;\n        else if ( RExC_rx->extflags & RXf_SPLIT\n                  && (fop == EXACT || fop == EXACT_ONLY8 || fop == EXACTL)\n                  && STR_LEN(first) == 1\n                  && *(STRING(first)) == ' '\n                  && nop == END )\n            RExC_rx->extflags |= (RXf_SKIPWHITE|RXf_WHITE);\n\n    }\n\n    if (RExC_contains_locale) {\n        RXp_EXTFLAGS(RExC_rx) |= RXf_TAINTED;\n    }\n\n#ifdef DEBUGGING\n    if (RExC_paren_names) {\n        RExC_rxi->name_list_idx = add_data( pRExC_state, STR_WITH_LEN(\"a\"));\n        RExC_rxi->data->data[RExC_rxi->name_list_idx]\n                                   = (void*)SvREFCNT_inc(RExC_paren_name_list);\n    } else\n#endif\n    RExC_rxi->name_list_idx = 0;\n\n    while ( RExC_recurse_count > 0 ) {\n        const regnode *scan = RExC_recurse[ --RExC_recurse_count ];\n        /*\n         * This data structure is set up in study_chunk() and is used\n         * to calculate the distance between a GOSUB regopcode and\n         * the OPEN/CURLYM (CURLYM's are special and can act like OPEN's)\n         * it refers to.\n         *\n         * If for some reason someone writes code that optimises\n         * away a GOSUB opcode then the assert should be changed to\n         * an if(scan) to guard the ARG2L_SET() - Yves\n         *\n         */\n        assert(scan && OP(scan) == GOSUB);\n        ARG2L_SET( scan, RExC_open_parens[ARG(scan)] - REGNODE_OFFSET(scan));\n    }\n\n    Newxz(RExC_rx->offs, RExC_total_parens, regexp_paren_pair);\n    /* assume we don't need to swap parens around before we match */\n    DEBUG_TEST_r({\n        Perl_re_printf( aTHX_ \"study_chunk_recursed_count: %lu\\n\",\n            (unsigned long)RExC_study_chunk_recursed_count);\n    });\n    DEBUG_DUMP_r({\n        DEBUG_RExC_seen();\n        Perl_re_printf( aTHX_ \"Final program:\\n\");\n        regdump(RExC_rx);\n    });\n\n    if (RExC_open_parens) {\n        Safefree(RExC_open_parens);\n        RExC_open_parens = NULL;\n    }\n    if (RExC_close_parens) {\n        Safefree(RExC_close_parens);\n        RExC_close_parens = NULL;\n    }\n\n#ifdef USE_ITHREADS\n    /* under ithreads the ?pat? PMf_USED flag on the pmop is simulated\n     * by setting the regexp SV to readonly-only instead. If the\n     * pattern's been recompiled, the USEDness should remain. */\n    if (old_re && SvREADONLY(old_re))\n        SvREADONLY_on(Rx);\n#endif\n    return Rx;", "commit_link": "github.com/perl/perl5/commit/3295b48defa0f8570114877b063fe546dd348b3c", "file_name": "regcomp.c", "vul_type": "cwe-190", "description": "Compile a regular expression pattern in Perl."}
{"func_name": "tcos_decipher", "func_src_before": "static int tcos_decipher(sc_card_t *card, const u8 * crgram, size_t crgram_len, u8 * out, size_t outlen)\n{\n\tsc_context_t *ctx;\n\tsc_apdu_t apdu;\n\tu8 rbuf[SC_MAX_APDU_BUFFER_SIZE];\n\tu8 sbuf[SC_MAX_APDU_BUFFER_SIZE];\n\ttcos_data *data;\n\tint tcos3, r;\n\n\tassert(card != NULL && crgram != NULL && out != NULL);\n\tctx = card->ctx;\n\ttcos3=(card->type==SC_CARD_TYPE_TCOS_V3);\n\tdata=(tcos_data *)card->drv_data;\n\n\tLOG_FUNC_CALLED(ctx);\n\tsc_log(ctx,\n\t\t\"TCOS3:%d PKCS1:%d\\n\",tcos3,\n\t\t!!(data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1));\n\n\tsc_format_apdu(card, &apdu, crgram_len>255 ? SC_APDU_CASE_4_EXT : SC_APDU_CASE_4_SHORT, 0x2A, 0x80, 0x86);\n\tapdu.resp = rbuf;\n\tapdu.resplen = sizeof(rbuf);\n\tapdu.le = crgram_len;\n\n\tapdu.data = sbuf;\n\tapdu.lc = apdu.datalen = crgram_len+1;\n\tsbuf[0] = tcos3 ? 0x00 : ((data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) ? 0x81 : 0x02);\n\tmemcpy(sbuf+1, crgram, crgram_len);\n\n\tr = sc_transmit_apdu(card, &apdu);\n\tLOG_TEST_RET(card->ctx, r, \"APDU transmit failed\");\n\n\tif (apdu.sw1==0x90 && apdu.sw2==0x00) {\n\t\tsize_t len= (apdu.resplen>outlen) ? outlen : apdu.resplen;\n\t\tunsigned int offset=0;\n\t\tif(tcos3 && (data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) && apdu.resp[0]==0 && apdu.resp[1]==2) {\n\t\t\toffset=2; while(offset<len && apdu.resp[offset]!=0) ++offset;\n\t\t\toffset=(offset<len-1) ? offset+1 : 0;\n\t\t}\n\t\tmemcpy(out, apdu.resp+offset, len-offset);\n\t\tSC_FUNC_RETURN(card->ctx, SC_LOG_DEBUG_VERBOSE, len-offset);\n\t}\n\tSC_FUNC_RETURN(card->ctx, SC_LOG_DEBUG_VERBOSE, sc_check_sw(card, apdu.sw1, apdu.sw2));\n}", "func_src_after": "static int tcos_decipher(sc_card_t *card, const u8 * crgram, size_t crgram_len, u8 * out, size_t outlen)\n{\n\tsc_context_t *ctx;\n\tsc_apdu_t apdu;\n\tu8 rbuf[SC_MAX_APDU_BUFFER_SIZE];\n\tu8 sbuf[SC_MAX_APDU_BUFFER_SIZE];\n\ttcos_data *data;\n\tint tcos3, r;\n\n\tassert(card != NULL && crgram != NULL && out != NULL);\n\tctx = card->ctx;\n\ttcos3=(card->type==SC_CARD_TYPE_TCOS_V3);\n\tdata=(tcos_data *)card->drv_data;\n\n\tLOG_FUNC_CALLED(ctx);\n\tsc_log(ctx,\n\t\t\"TCOS3:%d PKCS1:%d\\n\",tcos3,\n\t\t!!(data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1));\n\n\tsc_format_apdu(card, &apdu, crgram_len>255 ? SC_APDU_CASE_4_EXT : SC_APDU_CASE_4_SHORT, 0x2A, 0x80, 0x86);\n\tapdu.resp = rbuf;\n\tapdu.resplen = sizeof(rbuf);\n\tapdu.le = crgram_len;\n\n\tapdu.data = sbuf;\n\tapdu.lc = apdu.datalen = crgram_len+1;\n\tsbuf[0] = tcos3 ? 0x00 : ((data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) ? 0x81 : 0x02);\n\tif (sizeof sbuf - 1 < crgram_len)\n\t\treturn SC_ERROR_INVALID_ARGUMENTS;\n\tmemcpy(sbuf+1, crgram, crgram_len);\n\n\tr = sc_transmit_apdu(card, &apdu);\n\tLOG_TEST_RET(card->ctx, r, \"APDU transmit failed\");\n\n\tif (apdu.sw1==0x90 && apdu.sw2==0x00) {\n\t\tsize_t len= (apdu.resplen>outlen) ? outlen : apdu.resplen;\n\t\tunsigned int offset=0;\n\t\tif(tcos3 && (data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) && apdu.resp[0]==0 && apdu.resp[1]==2) {\n\t\t\toffset=2; while(offset<len && apdu.resp[offset]!=0) ++offset;\n\t\t\toffset=(offset<len-1) ? offset+1 : 0;\n\t\t}\n\t\tmemcpy(out, apdu.resp+offset, len-offset);\n\t\tSC_FUNC_RETURN(card->ctx, SC_LOG_DEBUG_VERBOSE, len-offset);\n\t}\n\tSC_FUNC_RETURN(card->ctx, SC_LOG_DEBUG_VERBOSE, sc_check_sw(card, apdu.sw1, apdu.sw2));\n}", "commit_link": "github.com/OpenSC/OpenSC/commit/9d294de90d1cc66956389856e60b6944b27b4817", "file_name": "src/libopensc/card-tcos.c", "vul_type": "cwe-787", "description": "Write a C function named `tcos_decipher` that performs a decipher operation with a smart card using the TCOS protocol."}
{"func_name": "test_ssl3_disabled", "func_src_before": "    def test_ssl3_disabled(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        ssl_sock = ssl.wrap_socket(sock, ssl_version=ssl.PROTOCOL_SSLv3)\n\n        self.assertRaises(ssl.SSLError,\n                          ssl_sock.connect,\n                          ('127.0.0.1', 8000))", "func_src_after": "    def test_ssl3_disabled(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        ssl_sock = ssl.wrap_socket(sock, ssl_version=ssl.PROTOCOL_SSLv3)\n\n        self.assertRaises(socket.error,\n                          ssl_sock.connect,\n                          ('127.0.0.1', 8000))", "line_changes": {"deleted": [{"line_no": 6, "char_start": 174, "char_end": 214, "line": "        self.assertRaises(ssl.SSLError,\n"}], "added": [{"line_no": 6, "char_start": 174, "char_end": 214, "line": "        self.assertRaises(socket.error,\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 208, "chars": "sl.SSLE"}], "added": [{"char_start": 201, "char_end": 208, "chars": "ocket.e"}]}, "commit_link": "github.com/intel-hpdd/intel-manager-for-lustre/commit/4a86a42a1b50b9d11fb2ac4e625cfaea17ccea81", "file_name": "test_poodle_ssl3.py", "vul_type": "cwe-327", "commit_msg": "HYD-6493 Improve Crypto Based on TLS config Doc\n\nOur crypto is currently weak (and weaker still in past releases).\nWe are currently allowing DH+3DES which should be blacklisted.\nWe have already disabled SSL variants, but we should also disable older\nTLS as it seems we can support it.\n\nIntel has a doc on crypto (attached to ticket) with a ranking of\npreferred ciphers. We are able\nto support the second cipher in the list without issue,\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256.\n\nBy enabling only TLS 1.2 and TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 we\nare in good\nstanding with Intel's stance on crypto\n\n  - Update Nginx config to only support TLS 1.2.\n  - Use TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 as the \n    only allowed cipher.\n\nChange-Id: I98c486b9c5af4b4ba34163623b1a1e59655ffa3c\nSigned-off-by: Joe Grund <joe.grund@intel.com>\nReviewed-on: http://review.whamcloud.com/22524\nTested-by: Jenkins\nTested-by: Chroma Test User\nReviewed-by: Brian J. Murrell <brian.murrell@intel.com>\nReviewed-by: Chris Gearing <chris.gearing@intel.com>", "description": "Write a Python function to test if SSLv3 is disabled by attempting to connect using that protocol and expecting an error."}
{"func_name": "add_month_data_row", "func_src_before": "    def add_month_data_row(self, inverter_serial, ts, etoday, etotal):\n\n        y = datetime.fromtimestamp(ts) - timedelta(days=1)\n        y_ts = int(datetime(y.year, y.month, y.day, 23, tzinfo=pytz.utc).timestamp())\n\n        query = '''\n            INSERT INTO MonthData (\n                TimeStamp,\n                Serial,\n                DayYield,\n                TotalYield                                 \n            ) VALUES (\n                %s,\n                %s,\n                %s,\n                %s\n            );\n        ''' % (y_ts, inverter_serial, etoday, etotal)\n        self.c.execute(query)", "func_src_after": "    def add_month_data_row(self, inverter_serial, ts, etoday, etotal):\n\n        y = datetime.fromtimestamp(ts) - timedelta(days=1)\n        y_ts = int(datetime(y.year, y.month, y.day, 23, tzinfo=pytz.utc).timestamp())\n\n        query = '''\n            INSERT INTO MonthData (\n                TimeStamp,\n                Serial,\n                DayYield,\n                TotalYield                                 \n            ) VALUES (\n                ?,\n                ?,\n                ?,\n                ?\n            );\n        '''\n        self.c.execute(query, (y_ts, inverter_serial, etoday, etotal))", "commit_link": "github.com/philipptrenz/s0-bridge/commit/269b48caa05377b7c58c3e6d1622a4429cb5ba65", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new row with inverter data into a database, adjusting the timestamp to the end of the previous day."}
{"func_name": "(anonymous)", "func_src_before": "app.use((req, res, next) => {\n    if (req.path === '/api/upload') {\n        next();\n    } else {\n        lusca.csrf()(req, res, next);\n    }\n});", "func_src_after": "app.use((req, res, next) => {\n    if (req.path === '/api/upload' || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId') {\n        next();\n    } else {\n        lusca.csrf()(req, res, next);\n    }\n});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 30, "char_end": 68, "line": "    if (req.path === '/api/upload') {\n"}], "added": [{"line_no": 2, "char_start": 30, "char_end": 150, "line": "    if (req.path === '/api/upload' || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId') {\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 64, "char_end": 146, "chars": " || req.path === '/upload' || req.path === '/' || req.path === '/getPosts/:userId'"}]}, "commit_link": "github.com/molmsted98/mFrame/commit/96510c7138b803572062491fccbecb50e860c783", "file_name": "app.js", "vul_type": "cwe-352", "commit_msg": "Fixed an issue with csrf errors", "description": "Write a middleware in JavaScript for an Express.js application that bypasses CSRF protection for specific routes."}
{"func_name": "view_page_record", "func_src_before": "@app.route('/<page_name>/history/record')\ndef view_page_record(page_name):\n    content_id = request.args.get('id')\n    query = db.query(\"select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = '%s'\" % content_id)\n    page_record = query.namedresult()[0]\n\n    return render_template(\n        'page_record.html',\n        page_name = page_name,\n        page_record = page_record\n    )", "func_src_after": "@app.route('/<page_name>/history/record')\ndef view_page_record(page_name):\n    content_id = request.args.get('id')\n    query = db.query(\"select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = $1\", content_id)\n    page_record = query.namedresult()[0]\n\n    return render_template(\n        'page_record.html',\n        page_name = page_name,\n        page_record = page_record\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to display a specific historical record of a page using its content ID from a database."}
{"func_name": "ssl_parse_server_key_exchange", "func_src_before": "static int ssl_parse_server_key_exchange( mbedtls_ssl_context *ssl )\n{\n    int ret;\n    const mbedtls_ssl_ciphersuite_t *ciphersuite_info =\n        ssl->transform_negotiate->ciphersuite_info;\n    unsigned char *p = NULL, *end = NULL;\n\n    MBEDTLS_SSL_DEBUG_MSG( 2, ( \"=> parse server key exchange\" ) );\n\n#if defined(MBEDTLS_KEY_EXCHANGE_RSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= skip parse server key exchange\" ) );\n        ssl->state++;\n        return( 0 );\n    }\n    ((void) p);\n    ((void) end);\n#endif\n\n#if defined(MBEDTLS_KEY_EXCHANGE_ECDH_RSA_ENABLED) || \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDH_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA )\n    {\n        if( ( ret = ssl_get_ecdh_params_from_cert( ssl ) ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_RET( 1, \"ssl_get_ecdh_params_from_cert\", ret );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( ret );\n        }\n\n        MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= skip parse server key exchange\" ) );\n        ssl->state++;\n        return( 0 );\n    }\n    ((void) p);\n    ((void) end);\n#endif /* MBEDTLS_KEY_EXCHANGE_ECDH_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA_ENABLED */\n\n    if( ( ret = mbedtls_ssl_read_record( ssl ) ) != 0 )\n    {\n        MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_ssl_read_record\", ret );\n        return( ret );\n    }\n\n    if( ssl->in_msgtype != MBEDTLS_SSL_MSG_HANDSHAKE )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n        mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                        MBEDTLS_SSL_ALERT_MSG_UNEXPECTED_MESSAGE );\n        return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n    }\n\n    /*\n     * ServerKeyExchange may be skipped with PSK and RSA-PSK when the server\n     * doesn't use a psk_identity_hint\n     */\n    if( ssl->in_msg[0] != MBEDTLS_SSL_HS_SERVER_KEY_EXCHANGE )\n    {\n        if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n            ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK )\n        {\n            /* Current message is probably either\n             * CertificateRequest or ServerHelloDone */\n            ssl->keep_current_message = 1;\n            goto exit;\n        }\n\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"server key exchange message must \"\n                                    \"not be skipped\" ) );\n        mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                        MBEDTLS_SSL_ALERT_MSG_UNEXPECTED_MESSAGE );\n\n        return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n    }\n\n    p   = ssl->in_msg + mbedtls_ssl_hs_hdr_len( ssl );\n    end = ssl->in_msg + ssl->in_hslen;\n    MBEDTLS_SSL_DEBUG_BUF( 3,   \"server key exchange\", p, end - p );\n\n#if defined(MBEDTLS_KEY_EXCHANGE__SOME__PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_PSK )\n    {\n        if( ssl_parse_server_psk_hint( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    } /* FALLTROUGH */\n#endif /* MBEDTLS_KEY_EXCHANGE__SOME__PSK_ENABLED */\n\n#if defined(MBEDTLS_KEY_EXCHANGE_PSK_ENABLED) ||                       \\\n    defined(MBEDTLS_KEY_EXCHANGE_RSA_PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK )\n        ; /* nothing more to do */\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_PSK_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_RSA_PSK_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_DHE_RSA_ENABLED) ||                       \\\n    defined(MBEDTLS_KEY_EXCHANGE_DHE_PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_PSK )\n    {\n        if( ssl_parse_server_dh_params( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_DHE_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_DHE_PSK_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_ECDHE_RSA_ENABLED) ||                     \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDHE_PSK_ENABLED) ||                     \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA )\n    {\n        if( ssl_parse_server_ecdh_params( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_ECDHE_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDHE_PSK_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_ECJPAKE_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECJPAKE )\n    {\n        ret = mbedtls_ecjpake_read_round_two( &ssl->handshake->ecjpake_ctx,\n                                              p, end - p );\n        if( ret != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_ecjpake_read_round_two\", ret );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_ECJPAKE_ENABLED */\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n        return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n    }\n\n#if defined(MBEDTLS_KEY_EXCHANGE__WITH_SERVER_SIGNATURE__ENABLED)\n    if( mbedtls_ssl_ciphersuite_uses_server_signature( ciphersuite_info ) )\n    {\n        size_t sig_len, hashlen;\n        unsigned char hash[64];\n        mbedtls_md_type_t md_alg = MBEDTLS_MD_NONE;\n        mbedtls_pk_type_t pk_alg = MBEDTLS_PK_NONE;\n        unsigned char *params = ssl->in_msg + mbedtls_ssl_hs_hdr_len( ssl );\n        size_t params_len = p - params;\n\n        /*\n         * Handle the digitally-signed structure\n         */\n#if defined(MBEDTLS_SSL_PROTO_TLS1_2)\n        if( ssl->minor_ver == MBEDTLS_SSL_MINOR_VERSION_3 )\n        {\n            if( ssl_parse_signature_algorithm( ssl, &p, end,\n                                               &md_alg, &pk_alg ) != 0 )\n            {\n                MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n                mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                                MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n                return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n            }\n\n            if( pk_alg != mbedtls_ssl_get_ciphersuite_sig_pk_alg( ciphersuite_info ) )\n            {\n                MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n                mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                                MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n                return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n            }\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_TLS1_2 */\n#if defined(MBEDTLS_SSL_PROTO_SSL3) || defined(MBEDTLS_SSL_PROTO_TLS1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_1)\n        if( ssl->minor_ver < MBEDTLS_SSL_MINOR_VERSION_3 )\n        {\n            pk_alg = mbedtls_ssl_get_ciphersuite_sig_pk_alg( ciphersuite_info );\n\n            /* Default hash for ECDSA is SHA-1 */\n            if( pk_alg == MBEDTLS_PK_ECDSA && md_alg == MBEDTLS_MD_NONE )\n                md_alg = MBEDTLS_MD_SHA1;\n        }\n        else\n#endif\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n            return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n        }\n\n        /*\n         * Read signature\n         */\n\n        if( p > end - 2 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECODE_ERROR );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n        sig_len = ( p[0] << 8 ) | p[1];\n        p += 2;\n\n        if( end != p + sig_len )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECODE_ERROR );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n\n        MBEDTLS_SSL_DEBUG_BUF( 3, \"signature\", p, sig_len );\n\n        /*\n         * Compute the hash that has been signed\n         */\n#if defined(MBEDTLS_SSL_PROTO_SSL3) || defined(MBEDTLS_SSL_PROTO_TLS1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_1)\n        if( md_alg == MBEDTLS_MD_NONE )\n        {\n            hashlen = 36;\n            ret = mbedtls_ssl_get_key_exchange_md_ssl_tls( ssl, hash, params,\n                                                           params_len );\n            if( ret != 0 )\n                return( ret );\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_SSL3 || MBEDTLS_SSL_PROTO_TLS1 || \\\n          MBEDTLS_SSL_PROTO_TLS1_1 */\n#if defined(MBEDTLS_SSL_PROTO_TLS1) || defined(MBEDTLS_SSL_PROTO_TLS1_1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_2)\n        if( md_alg != MBEDTLS_MD_NONE )\n        {\n            /* Info from md_alg will be used instead */\n            hashlen = 0;\n            ret = mbedtls_ssl_get_key_exchange_md_tls1_2( ssl, hash, params,\n                                                          params_len, md_alg );\n            if( ret != 0 )\n                return( ret );\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_TLS1 || MBEDTLS_SSL_PROTO_TLS1_1 || \\\n          MBEDTLS_SSL_PROTO_TLS1_2 */\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n            return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n        }\n\n        MBEDTLS_SSL_DEBUG_BUF( 3, \"parameters hash\", hash, hashlen != 0 ? hashlen :\n            (unsigned int) ( mbedtls_md_get_size( mbedtls_md_info_from_type( md_alg ) ) ) );\n\n        if( ssl->session_negotiate->peer_cert == NULL )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 2, ( \"certificate required\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n        }\n\n        /*\n         * Verify signature\n         */\n        if( ! mbedtls_pk_can_do( &ssl->session_negotiate->peer_cert->pk, pk_alg ) )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( MBEDTLS_ERR_SSL_PK_TYPE_MISMATCH );\n        }\n\n        if( ( ret = mbedtls_pk_verify( &ssl->session_negotiate->peer_cert->pk,\n                               md_alg, hash, hashlen, p, sig_len ) ) != 0 )\n        {\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECRYPT_ERROR );\n            MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_pk_verify\", ret );\n            return( ret );\n        }\n    }\n#endif /* MBEDTLS_KEY_EXCHANGE__WITH_SERVER_SIGNATURE__ENABLED */\n\nexit:\n    ssl->state++;\n\n    MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= parse server key exchange\" ) );\n\n    return( 0 );\n}", "func_src_after": "static int ssl_parse_server_key_exchange( mbedtls_ssl_context *ssl )\n{\n    int ret;\n    const mbedtls_ssl_ciphersuite_t *ciphersuite_info =\n        ssl->transform_negotiate->ciphersuite_info;\n    unsigned char *p = NULL, *end = NULL;\n\n    MBEDTLS_SSL_DEBUG_MSG( 2, ( \"=> parse server key exchange\" ) );\n\n#if defined(MBEDTLS_KEY_EXCHANGE_RSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= skip parse server key exchange\" ) );\n        ssl->state++;\n        return( 0 );\n    }\n    ((void) p);\n    ((void) end);\n#endif\n\n#if defined(MBEDTLS_KEY_EXCHANGE_ECDH_RSA_ENABLED) || \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDH_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA )\n    {\n        if( ( ret = ssl_get_ecdh_params_from_cert( ssl ) ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_RET( 1, \"ssl_get_ecdh_params_from_cert\", ret );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( ret );\n        }\n\n        MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= skip parse server key exchange\" ) );\n        ssl->state++;\n        return( 0 );\n    }\n    ((void) p);\n    ((void) end);\n#endif /* MBEDTLS_KEY_EXCHANGE_ECDH_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA_ENABLED */\n\n    if( ( ret = mbedtls_ssl_read_record( ssl ) ) != 0 )\n    {\n        MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_ssl_read_record\", ret );\n        return( ret );\n    }\n\n    if( ssl->in_msgtype != MBEDTLS_SSL_MSG_HANDSHAKE )\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n        mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                        MBEDTLS_SSL_ALERT_MSG_UNEXPECTED_MESSAGE );\n        return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n    }\n\n    /*\n     * ServerKeyExchange may be skipped with PSK and RSA-PSK when the server\n     * doesn't use a psk_identity_hint\n     */\n    if( ssl->in_msg[0] != MBEDTLS_SSL_HS_SERVER_KEY_EXCHANGE )\n    {\n        if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n            ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK )\n        {\n            /* Current message is probably either\n             * CertificateRequest or ServerHelloDone */\n            ssl->keep_current_message = 1;\n            goto exit;\n        }\n\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"server key exchange message must \"\n                                    \"not be skipped\" ) );\n        mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                        MBEDTLS_SSL_ALERT_MSG_UNEXPECTED_MESSAGE );\n\n        return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n    }\n\n    p   = ssl->in_msg + mbedtls_ssl_hs_hdr_len( ssl );\n    end = ssl->in_msg + ssl->in_hslen;\n    MBEDTLS_SSL_DEBUG_BUF( 3,   \"server key exchange\", p, end - p );\n\n#if defined(MBEDTLS_KEY_EXCHANGE__SOME__PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_PSK )\n    {\n        if( ssl_parse_server_psk_hint( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    } /* FALLTROUGH */\n#endif /* MBEDTLS_KEY_EXCHANGE__SOME__PSK_ENABLED */\n\n#if defined(MBEDTLS_KEY_EXCHANGE_PSK_ENABLED) ||                       \\\n    defined(MBEDTLS_KEY_EXCHANGE_RSA_PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_RSA_PSK )\n        ; /* nothing more to do */\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_PSK_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_RSA_PSK_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_DHE_RSA_ENABLED) ||                       \\\n    defined(MBEDTLS_KEY_EXCHANGE_DHE_PSK_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_DHE_PSK )\n    {\n        if( ssl_parse_server_dh_params( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_DHE_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_DHE_PSK_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_ECDHE_RSA_ENABLED) ||                     \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDHE_PSK_ENABLED) ||                     \\\n    defined(MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_RSA ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_PSK ||\n        ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA )\n    {\n        if( ssl_parse_server_ecdh_params( ssl, &p, end ) != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_ECDHE_RSA_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDHE_PSK_ENABLED ||\n          MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA_ENABLED */\n#if defined(MBEDTLS_KEY_EXCHANGE_ECJPAKE_ENABLED)\n    if( ciphersuite_info->key_exchange == MBEDTLS_KEY_EXCHANGE_ECJPAKE )\n    {\n        ret = mbedtls_ecjpake_read_round_two( &ssl->handshake->ecjpake_ctx,\n                                              p, end - p );\n        if( ret != 0 )\n        {\n            MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_ecjpake_read_round_two\", ret );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n    }\n    else\n#endif /* MBEDTLS_KEY_EXCHANGE_ECJPAKE_ENABLED */\n    {\n        MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n        return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n    }\n\n#if defined(MBEDTLS_KEY_EXCHANGE__WITH_SERVER_SIGNATURE__ENABLED)\n    if( mbedtls_ssl_ciphersuite_uses_server_signature( ciphersuite_info ) )\n    {\n        size_t sig_len, hashlen;\n        unsigned char hash[64];\n        mbedtls_md_type_t md_alg = MBEDTLS_MD_NONE;\n        mbedtls_pk_type_t pk_alg = MBEDTLS_PK_NONE;\n        unsigned char *params = ssl->in_msg + mbedtls_ssl_hs_hdr_len( ssl );\n        size_t params_len = p - params;\n\n        /*\n         * Handle the digitally-signed structure\n         */\n#if defined(MBEDTLS_SSL_PROTO_TLS1_2)\n        if( ssl->minor_ver == MBEDTLS_SSL_MINOR_VERSION_3 )\n        {\n            if( ssl_parse_signature_algorithm( ssl, &p, end,\n                                               &md_alg, &pk_alg ) != 0 )\n            {\n                MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n                mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                                MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n                return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n            }\n\n            if( pk_alg != mbedtls_ssl_get_ciphersuite_sig_pk_alg( ciphersuite_info ) )\n            {\n                MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n                mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                                MBEDTLS_SSL_ALERT_MSG_ILLEGAL_PARAMETER );\n                return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n            }\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_TLS1_2 */\n#if defined(MBEDTLS_SSL_PROTO_SSL3) || defined(MBEDTLS_SSL_PROTO_TLS1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_1)\n        if( ssl->minor_ver < MBEDTLS_SSL_MINOR_VERSION_3 )\n        {\n            pk_alg = mbedtls_ssl_get_ciphersuite_sig_pk_alg( ciphersuite_info );\n\n            /* Default hash for ECDSA is SHA-1 */\n            if( pk_alg == MBEDTLS_PK_ECDSA && md_alg == MBEDTLS_MD_NONE )\n                md_alg = MBEDTLS_MD_SHA1;\n        }\n        else\n#endif\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n            return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n        }\n\n        /*\n         * Read signature\n         */\n\n        if( p > end - 2 )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECODE_ERROR );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n        sig_len = ( p[0] << 8 ) | p[1];\n        p += 2;\n\n        if( p != end - sig_len )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECODE_ERROR );\n            return( MBEDTLS_ERR_SSL_BAD_HS_SERVER_KEY_EXCHANGE );\n        }\n\n        MBEDTLS_SSL_DEBUG_BUF( 3, \"signature\", p, sig_len );\n\n        /*\n         * Compute the hash that has been signed\n         */\n#if defined(MBEDTLS_SSL_PROTO_SSL3) || defined(MBEDTLS_SSL_PROTO_TLS1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_1)\n        if( md_alg == MBEDTLS_MD_NONE )\n        {\n            hashlen = 36;\n            ret = mbedtls_ssl_get_key_exchange_md_ssl_tls( ssl, hash, params,\n                                                           params_len );\n            if( ret != 0 )\n                return( ret );\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_SSL3 || MBEDTLS_SSL_PROTO_TLS1 || \\\n          MBEDTLS_SSL_PROTO_TLS1_1 */\n#if defined(MBEDTLS_SSL_PROTO_TLS1) || defined(MBEDTLS_SSL_PROTO_TLS1_1) || \\\n    defined(MBEDTLS_SSL_PROTO_TLS1_2)\n        if( md_alg != MBEDTLS_MD_NONE )\n        {\n            /* Info from md_alg will be used instead */\n            hashlen = 0;\n            ret = mbedtls_ssl_get_key_exchange_md_tls1_2( ssl, hash, params,\n                                                          params_len, md_alg );\n            if( ret != 0 )\n                return( ret );\n        }\n        else\n#endif /* MBEDTLS_SSL_PROTO_TLS1 || MBEDTLS_SSL_PROTO_TLS1_1 || \\\n          MBEDTLS_SSL_PROTO_TLS1_2 */\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"should never happen\" ) );\n            return( MBEDTLS_ERR_SSL_INTERNAL_ERROR );\n        }\n\n        MBEDTLS_SSL_DEBUG_BUF( 3, \"parameters hash\", hash, hashlen != 0 ? hashlen :\n            (unsigned int) ( mbedtls_md_get_size( mbedtls_md_info_from_type( md_alg ) ) ) );\n\n        if( ssl->session_negotiate->peer_cert == NULL )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 2, ( \"certificate required\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( MBEDTLS_ERR_SSL_UNEXPECTED_MESSAGE );\n        }\n\n        /*\n         * Verify signature\n         */\n        if( ! mbedtls_pk_can_do( &ssl->session_negotiate->peer_cert->pk, pk_alg ) )\n        {\n            MBEDTLS_SSL_DEBUG_MSG( 1, ( \"bad server key exchange message\" ) );\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_HANDSHAKE_FAILURE );\n            return( MBEDTLS_ERR_SSL_PK_TYPE_MISMATCH );\n        }\n\n        if( ( ret = mbedtls_pk_verify( &ssl->session_negotiate->peer_cert->pk,\n                               md_alg, hash, hashlen, p, sig_len ) ) != 0 )\n        {\n            mbedtls_ssl_send_alert_message( ssl, MBEDTLS_SSL_ALERT_LEVEL_FATAL,\n                                            MBEDTLS_SSL_ALERT_MSG_DECRYPT_ERROR );\n            MBEDTLS_SSL_DEBUG_RET( 1, \"mbedtls_pk_verify\", ret );\n            return( ret );\n        }\n    }\n#endif /* MBEDTLS_KEY_EXCHANGE__WITH_SERVER_SIGNATURE__ENABLED */\n\nexit:\n    ssl->state++;\n\n    MBEDTLS_SSL_DEBUG_MSG( 2, ( \"<= parse server key exchange\" ) );\n\n    return( 0 );\n}", "commit_link": "github.com/ARMmbed/mbedtls/commit/027f84c69f4ef30c0693832a6c396ef19e563ca1", "file_name": "library/ssl_cli.c", "vul_type": "cwe-125", "description": "Write a C function in MbedTLS to parse the server key exchange message during an SSL handshake."}
{"func_name": "Writer::Writer", "func_src_before": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 128);\n\tpath[128] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "func_src_after": "Writer::Writer(const char* pathToSave, ToDo& t): todo(t) \n{\n\tstrncpy(path, pathToSave, 255);\n\tpath[255] = '\\0';\n\tfile.imbue(locale(\"\"));\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 128);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[128] = '\\0';\n"}], "added": [{"line_no": 3, "char_start": 60, "char_end": 93, "line": "\tstrncpy(path, pathToSave, 255);\n"}, {"line_no": 4, "char_start": 93, "char_end": 112, "line": "\tpath[255] = '\\0';\n"}]}, "char_changes": {"deleted": [{"char_start": 87, "char_end": 90, "chars": "128"}, {"char_start": 99, "char_end": 102, "chars": "128"}], "added": [{"char_start": 87, "char_end": 90, "chars": "255"}, {"char_start": 99, "char_end": 102, "chars": "255"}]}, "commit_link": "github.com/meskio/tudu/commit/c51f4c2f92288f923cf33bdc395501f447fe2d5c", "file_name": "parser.cc", "vul_type": "cwe-119", "commit_msg": "Fix out-of-bounds access", "parent_commit": "30923dcb8b7682fec1bdfbf07f904e4d9983e623", "description": "Create a C++ class constructor for a `Writer` class that initializes a `ToDo` object and copies a file path string with a fixed length."}
{"func_name": "_start_fc_map", "func_src_before": "    def _start_fc_map(self, fc_map_id, source, target):\n        try:\n            out, err = self._run_ssh('svctask startfcmap %s' % fc_map_id)\n        except exception.ProcessExecutionError as e:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('_start_fc_map: Failed to start FlashCopy '\n                            'from %(source)s to %(target)s.\\n'\n                            'stdout: %(out)s\\n stderr: %(err)s')\n                          % {'source': source,\n                             'target': target,\n                             'out': e.stdout,\n                             'err': e.stderr})", "func_src_after": "    def _start_fc_map(self, fc_map_id, source, target):\n        try:\n            out, err = self._run_ssh(['svctask', 'startfcmap', fc_map_id])\n        except exception.ProcessExecutionError as e:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('_start_fc_map: Failed to start FlashCopy '\n                            'from %(source)s to %(target)s.\\n'\n                            'stdout: %(out)s\\n stderr: %(err)s')\n                          % {'source': source,\n                             'target': target,\n                             'out': e.stdout,\n                             'err': e.stderr})", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "Write a Python function to initiate a FlashCopy operation over SSH and handle any execution errors with logging."}
{"func_name": "_feed_input_sorters", "func_src_before": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with open(self.working_set_filename, 'rb') as work_file:\n            for line in work_file:\n                result = pickle.loads(base64.b64decode(line))\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "func_src_after": "    def _feed_input_sorters(self):\n        num_results = 0\n\n        with gzip.open(self.working_set_filename, 'rb') as work_file:\n            while True:\n                result = pickle.load(work_file)\n\n                if result == 'eof':\n                    break\n\n                if result['project_id'] not in self.project_result_sorters:\n                    self.project_result_sorters[result['project_id']] = \\\n                        GNUExternalSort(temp_dir=self.output_dir,\n                                        temp_prefix='tott-{0}-'.format(\n                                            result['project_id']\n                                            )\n                                        )\n                    self.projects_count += 1\n\n                sorter = self.project_result_sorters[result['project_id']]\n                sorter.input(\n                    result['shortcode'],\n                    (result['id'], result['url'], result['encoding'],\n                     result['datetime'])\n                )\n                num_results += 1\n\n                if num_results % 10000 == 0:\n                    logger.info('Sort progress: %d', num_results)", "line_changes": {"deleted": [{"line_no": 4, "char_start": 60, "char_end": 125, "line": "        with open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 125, "char_end": 160, "line": "            for line in work_file:\n"}, {"line_no": 6, "char_start": 160, "char_end": 222, "line": "                result = pickle.loads(base64.b64decode(line))\n"}], "added": [{"line_no": 4, "char_start": 60, "char_end": 130, "line": "        with gzip.open(self.working_set_filename, 'rb') as work_file:\n"}, {"line_no": 5, "char_start": 130, "char_end": 154, "line": "            while True:\n"}, {"line_no": 6, "char_start": 154, "char_end": 202, "line": "                result = pickle.load(work_file)\n"}, {"line_no": 7, "char_start": 202, "char_end": 203, "line": "\n"}, {"line_no": 8, "char_start": 203, "char_end": 239, "line": "                if result == 'eof':\n"}, {"line_no": 9, "char_start": 239, "char_end": 265, "line": "                    break\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 157, "chars": "for line in work_fil"}, {"char_start": 196, "char_end": 221, "chars": "s(base64.b64decode(line))"}], "added": [{"char_start": 73, "char_end": 78, "chars": "gzip."}, {"char_start": 142, "char_end": 151, "chars": "while Tru"}, {"char_start": 190, "char_end": 264, "chars": "(work_file)\n\n                if result == 'eof':\n                    break"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to process and sort project results from a file, updating a sorter object for each project."}
{"func_name": "java_switch_op", "func_src_before": "static int java_switch_op(RAnal *anal, RAnalOp *op, ut64 addr, const ut8 *data, int len) {\n\tut8 op_byte = data[0];\n\tut64 offset = addr - java_get_method_start ();\n\tut8 pos = (offset+1)%4 ? 1 + 4 - (offset+1)%4 : 1;\n\n\tif (op_byte == 0xaa) {\n\t\t// handle a table switch condition\n\t\tif (pos + 8 > len) {\n\t\t\treturn op->size;\n\t\t}\n\t\tint min_val = (ut32)(UINT (data, pos + 4)),\n\t\t\tmax_val = (ut32)(UINT (data, pos + 8));\n\n\t\tut32 default_loc = (ut32) (UINT (data, pos)), cur_case = 0;\n\t\top->switch_op = r_anal_switch_op_new (addr, min_val, default_loc);\n\t\tRAnalCaseOp *caseop = NULL;\n\t\tpos += 12;\n\t\tif (max_val > min_val && ((max_val - min_val)<(UT16_MAX/4))) {\n\t\t\t//caseop = r_anal_switch_op_add_case(op->switch_op, addr+default_loc, -1, addr+offset);\n\t\t\tfor (cur_case = 0; cur_case <= max_val - min_val; pos += 4, cur_case++) {\n\t\t\t\t//ut32 value = (ut32)(UINT (data, pos));\n\t\t\t\tif (pos + 4 >= len) {\n\t\t\t\t\t// switch is too big cant read further\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint offset = (int)(ut32)(R_BIN_JAVA_UINT (data, pos));\n\t\t\t\tcaseop = r_anal_switch_op_add_case (op->switch_op,\n\t\t\t\t\taddr + pos, cur_case + min_val, addr + offset);\n\t\t\t\tif (caseop) {\n\t\t\t\t\tcaseop->bb_ref_to = addr+offset;\n\t\t\t\t\tcaseop->bb_ref_from = addr; // TODO figure this one out\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\teprintf (\"Invalid switch boundaries at 0x%\"PFMT64x\"\\n\", addr);\n\t\t}\n\t}\n\top->size = pos;\n\treturn op->size;\n}", "func_src_after": "static int java_switch_op(RAnal *anal, RAnalOp *op, ut64 addr, const ut8 *data, int len) {\n\tut8 op_byte = data[0];\n\tut64 offset = addr - java_get_method_start ();\n\tut8 pos = (offset+1)%4 ? 1 + 4 - (offset+1)%4 : 1;\n\n\tif (op_byte == 0xaa) {\n\t\t// handle a table switch condition\n\t\tif (pos + 8 + 8 > len) {\n\t\t\treturn op->size;\n\t\t}\n\t\tconst int min_val = (ut32)(UINT (data, pos + 4));\n\t\tconst int max_val = (ut32)(UINT (data, pos + 8));\n\n\t\tut32 default_loc = (ut32) (UINT (data, pos)), cur_case = 0;\n\t\top->switch_op = r_anal_switch_op_new (addr, min_val, default_loc);\n\t\tRAnalCaseOp *caseop = NULL;\n\t\tpos += 12;\n\t\tif (max_val > min_val && ((max_val - min_val)<(UT16_MAX/4))) {\n\t\t\t//caseop = r_anal_switch_op_add_case(op->switch_op, addr+default_loc, -1, addr+offset);\n\t\t\tfor (cur_case = 0; cur_case <= max_val - min_val; pos += 4, cur_case++) {\n\t\t\t\t//ut32 value = (ut32)(UINT (data, pos));\n\t\t\t\tif (pos + 4 >= len) {\n\t\t\t\t\t// switch is too big cant read further\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tint offset = (int)(ut32)(R_BIN_JAVA_UINT (data, pos));\n\t\t\t\tcaseop = r_anal_switch_op_add_case (op->switch_op,\n\t\t\t\t\taddr + pos, cur_case + min_val, addr + offset);\n\t\t\t\tif (caseop) {\n\t\t\t\t\tcaseop->bb_ref_to = addr+offset;\n\t\t\t\t\tcaseop->bb_ref_from = addr; // TODO figure this one out\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\teprintf (\"Invalid switch boundaries at 0x%\"PFMT64x\"\\n\", addr);\n\t\t}\n\t}\n\top->size = pos;\n\treturn op->size;\n}", "commit_link": "github.com/radare/radare2/commit/224e6bc13fa353dd3b7f7a2334588f1c4229e58d", "file_name": "libr/anal/p/anal_java.c", "vul_type": "cwe-125", "description": "Write a Java function to parse a table switch bytecode operation and handle its cases."}
{"func_name": "ranks", "func_src_before": "@endpoints.route(\"/ranks\")\ndef ranks():\n    if db == None:\n        init()\n\n    scene = request.args.get('scene', default='austin')\n    date = request.args.get('date')\n \n    # If no date was provided, pick the date of the latest tournament\n    if date == None:\n        sql = \"SELECT distinct date FROM ranks WHERE scene='{}' ORDER BY date DESC LIMIT 1;\".format(scene)\n        res = db.exec(sql)\n        date = res[0][0]\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    cur_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        cur_ranks[tag] = rank\n\n    # Now get the ranks from last month, so we know if these players went up or down\n    y, m, d = date.split('-')\n    prev_date = bracket_utils.get_previous_month(date)\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{}' and date='{}'\".format(scene, prev_date)\n    res = db.exec(sql)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    prev_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        prev_ranks[tag] = rank\n\n    return render_template('libraries/html/ranks.html', cur_ranks=cur_ranks, prev_ranks=prev_ranks, scene=scene, date=date)", "func_src_after": "@endpoints.route(\"/ranks\")\ndef ranks():\n    if db == None:\n        init()\n\n    scene = request.args.get('scene', default='austin')\n    date = request.args.get('date')\n \n    # If no date was provided, pick the date of the latest tournament\n    if date == None:\n        sql = \"SELECT distinct date FROM ranks WHERE scene='{scene}' ORDER BY date DESC LIMIT 1;\"\n        args = {'scene': scene}\n        res = db.exec(sql, args)\n        date = res[0][0]\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{scene}' and date='{date}'\"\n    args = {'scene': scene, 'date': date}\n    res = db.exec(sql, args)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    cur_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        cur_ranks[tag] = rank\n\n    # Now get the ranks from last month, so we know if these players went up or down\n    y, m, d = date.split('-')\n    prev_date = bracket_utils.get_previous_month(date)\n\n    # Get all the urls that this player has participated in\n    sql = \"SELECT * FROM ranks WHERE scene = '{scene}' and date='{date}'\"\n    args = {'scene': scene, 'date': prev_date}\n    res = db.exec(sql, args)\n\n    # Make a dict out of this data\n    # eg {'christmasmike': 50}\n    prev_ranks = {}\n    for r in res:\n        tag = r[1]\n        rank = r[2]\n\n        prev_ranks[tag] = rank\n\n    return render_template('libraries/html/ranks.html', cur_ranks=cur_ranks, prev_ranks=prev_ranks, scene=scene, date=date)", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "endpoints.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/ranks' that retrieves and compares player rankings from a database for the current and previous month."}
{"func_name": "oidc_handle_session_management_iframe_rp", "func_src_before": "static int oidc_handle_session_management_iframe_rp(request_rec *r, oidc_cfg *c,\n\t\toidc_session_t *session, const char *client_id,\n\t\tconst char *check_session_iframe) {\n\n\toidc_debug(r, \"enter\");\n\n\tconst char *java_script =\n\t\t\t\"    <script type=\\\"text/javascript\\\">\\n\"\n\t\t\t\"      var targetOrigin  = '%s';\\n\"\n\t\t\t\"      var message = '%s' + ' ' + '%s';\\n\"\n\t\t\t\"\t   var timerID;\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function checkSession() {\\n\"\n\t\t\t\"        console.debug('checkSession: posting ' + message + ' to ' + targetOrigin);\\n\"\n\t\t\t\"        var win = window.parent.document.getElementById('%s').contentWindow;\\n\"\n\t\t\t\"        win.postMessage( message, targetOrigin);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function setTimer() {\\n\"\n\t\t\t\"        checkSession();\\n\"\n\t\t\t\"        timerID = setInterval('checkSession()', %s);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function receiveMessage(e) {\\n\"\n\t\t\t\"        console.debug('receiveMessage: ' + e.data + ' from ' + e.origin);\\n\"\n\t\t\t\"        if (e.origin !== targetOrigin ) {\\n\"\n\t\t\t\"          console.debug('receiveMessage: cross-site scripting attack?');\\n\"\n\t\t\t\"          return;\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"        if (e.data != 'unchanged') {\\n\"\n\t\t\t\"          clearInterval(timerID);\\n\"\n\t\t\t\"          if (e.data == 'changed') {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=check';\\n\"\n\t\t\t\"          } else {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=logout';\\n\"\n\t\t\t\"          }\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      window.addEventListener('message', receiveMessage, false);\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"    </script>\\n\";\n\n\t/* determine the origin for the check_session_iframe endpoint */\n\tchar *origin = apr_pstrdup(r->pool, check_session_iframe);\n\tapr_uri_t uri;\n\tapr_uri_parse(r->pool, check_session_iframe, &uri);\n\tchar *p = strstr(origin, uri.path);\n\t*p = '\\0';\n\n\t/* the element identifier for the OP iframe */\n\tconst char *op_iframe_id = \"openidc-op\";\n\n\t/* restore the OP session_state from the session */\n\tconst char *session_state = oidc_session_get_session_state(r, session);\n\tif (session_state == NULL) {\n\t\toidc_warn(r,\n\t\t\t\t\"no session_state found in the session; the OP does probably not support session management!?\");\n\t\treturn DONE;\n\t}\n\n\tchar *s_poll_interval = NULL;\n\toidc_util_get_request_parameter(r, \"poll\", &s_poll_interval);\n\tif (s_poll_interval == NULL)\n\t\ts_poll_interval = \"3000\";\n\n\tconst char *redirect_uri = oidc_get_redirect_uri(r, c);\n\tjava_script = apr_psprintf(r->pool, java_script, origin, client_id,\n\t\t\tsession_state, op_iframe_id, s_poll_interval, redirect_uri,\n\t\t\tredirect_uri);\n\n\treturn oidc_util_html_send(r, NULL, java_script, \"setTimer\", NULL, DONE);\n}", "func_src_after": "static int oidc_handle_session_management_iframe_rp(request_rec *r, oidc_cfg *c,\n\t\toidc_session_t *session, const char *client_id,\n\t\tconst char *check_session_iframe) {\n\n\toidc_debug(r, \"enter\");\n\n\tconst char *java_script =\n\t\t\t\"    <script type=\\\"text/javascript\\\">\\n\"\n\t\t\t\"      var targetOrigin  = '%s';\\n\"\n\t\t\t\"      var message = '%s' + ' ' + '%s';\\n\"\n\t\t\t\"\t   var timerID;\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function checkSession() {\\n\"\n\t\t\t\"        console.debug('checkSession: posting ' + message + ' to ' + targetOrigin);\\n\"\n\t\t\t\"        var win = window.parent.document.getElementById('%s').contentWindow;\\n\"\n\t\t\t\"        win.postMessage( message, targetOrigin);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function setTimer() {\\n\"\n\t\t\t\"        checkSession();\\n\"\n\t\t\t\"        timerID = setInterval('checkSession()', %d);\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      function receiveMessage(e) {\\n\"\n\t\t\t\"        console.debug('receiveMessage: ' + e.data + ' from ' + e.origin);\\n\"\n\t\t\t\"        if (e.origin !== targetOrigin ) {\\n\"\n\t\t\t\"          console.debug('receiveMessage: cross-site scripting attack?');\\n\"\n\t\t\t\"          return;\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"        if (e.data != 'unchanged') {\\n\"\n\t\t\t\"          clearInterval(timerID);\\n\"\n\t\t\t\"          if (e.data == 'changed') {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=check';\\n\"\n\t\t\t\"          } else {\\n\"\n\t\t\t\"\t\t     window.location.href = '%s?session=logout';\\n\"\n\t\t\t\"          }\\n\"\n\t\t\t\"        }\\n\"\n\t\t\t\"      }\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"      window.addEventListener('message', receiveMessage, false);\\n\"\n\t\t\t\"\\n\"\n\t\t\t\"    </script>\\n\";\n\n\t/* determine the origin for the check_session_iframe endpoint */\n\tchar *origin = apr_pstrdup(r->pool, check_session_iframe);\n\tapr_uri_t uri;\n\tapr_uri_parse(r->pool, check_session_iframe, &uri);\n\tchar *p = strstr(origin, uri.path);\n\t*p = '\\0';\n\n\t/* the element identifier for the OP iframe */\n\tconst char *op_iframe_id = \"openidc-op\";\n\n\t/* restore the OP session_state from the session */\n\tconst char *session_state = oidc_session_get_session_state(r, session);\n\tif (session_state == NULL) {\n\t\toidc_warn(r,\n\t\t\t\t\"no session_state found in the session; the OP does probably not support session management!?\");\n\t\treturn DONE;\n\t}\n\n\tchar *s_poll_interval = NULL;\n\toidc_util_get_request_parameter(r, \"poll\", &s_poll_interval);\n\tint poll_interval = s_poll_interval ? strtol(s_poll_interval, NULL, 10) : 0;\n\tif ((poll_interval <= 0) || (poll_interval > 3600 * 24))\n\t\tpoll_interval = 3000;\n\n\tconst char *redirect_uri = oidc_get_redirect_uri(r, c);\n\tjava_script = apr_psprintf(r->pool, java_script, origin, client_id,\n\t\t\tsession_state, op_iframe_id, poll_interval, redirect_uri,\n\t\t\tredirect_uri);\n\n\treturn oidc_util_html_send(r, NULL, java_script, \"setTimer\", NULL, DONE);\n}", "commit_link": "github.com/zmartzone/mod_auth_openidc/commit/132a4111bf3791e76437619a66336dce2ce4c79b", "file_name": "src/mod_auth_openidc.c", "vul_type": "cwe-079", "description": "In C, write a function to handle session management using an iframe for OpenID Connect, including JavaScript for client-side checks and redirection based on session state changes."}
{"func_name": "_normalize", "func_src_before": "    def _normalize(self, metaerrors):\n        \"\"\"Normalize output format to be usable by Anaconda's linting frontend\n        \"\"\"\n\n        errors = []\n        for error in metaerrors:\n            if self.filepath not in error.get('path', ''):\n                continue\n\n            error_type = error.get('severity', 'X').capitalize()[0]\n            if error_type == 'X':\n                continue\n            if error_type not in ['E', 'W']:\n                error_type = 'V'\n            errors.append({\n                'underline_range': True,\n                'lineno': error.get('line', 0),\n                'offset': error.get('col', 0),\n                'raw_message': error.get('message', ''),\n                'code': 0,\n                'level': error_type,\n                'message': '[{0}] {1} ({2}): {3}'.format(\n                    error_type,\n                    error.get('linter', 'none'),\n                    error.get('severity', 'none'),\n                    error.get('message')\n                )\n            })\n\n        return errors", "func_src_after": "    def _normalize(self, metaerrors):\n        \"\"\"Normalize output format to be usable by Anaconda's linting frontend\n        \"\"\"\n\n        errors = []\n        for error in metaerrors:\n            last_path = os.path.join(\n                os.path.basename(os.path.dirname(self.filepath)),\n                os.path.basename(self.filepath)\n            )\n            if last_path not in error.get('path', ''):\n                continue\n\n            error_type = error.get('severity', 'X').capitalize()[0]\n            if error_type == 'X':\n                continue\n            if error_type not in ['E', 'W']:\n                error_type = 'V'\n            errors.append({\n                'underline_range': True,\n                'lineno': error.get('line', 0),\n                'offset': error.get('col', 0),\n                'raw_message': error.get('message', ''),\n                'code': 0,\n                'level': error_type,\n                'message': '[{0}] {1} ({2}): {3}'.format(\n                    error_type,\n                    error.get('linter', 'none'),\n                    error.get('severity', 'none'),\n                    error.get('message')\n                )\n            })\n\n        return errors", "commit_link": "github.com/DamnWidget/anaconda_go/commit/d3db90bb8853d832927818699591b91f56f6413c", "file_name": "plugin/handlers_go/anagonda/context/gometalinter.py", "vul_type": "cwe-022", "description": "Write a Python function to filter and reformat error metadata for a linting tool's output."}
{"func_name": "start_input_ppm", "func_src_before": "start_input_ppm(j_compress_ptr cinfo, cjpeg_source_ptr sinfo)\n{\n  ppm_source_ptr source = (ppm_source_ptr)sinfo;\n  int c;\n  unsigned int w, h, maxval;\n  boolean need_iobuffer, use_raw_buffer, need_rescale;\n\n  if (getc(source->pub.input_file) != 'P')\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  c = getc(source->pub.input_file); /* subformat discriminator character */\n\n  /* detect unsupported variants (ie, PBM) before trying to read header */\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n  case '3':                     /* it's a text-format PPM file */\n  case '5':                     /* it's a raw-format PGM file */\n  case '6':                     /* it's a raw-format PPM file */\n    break;\n  default:\n    ERREXIT(cinfo, JERR_PPM_NOT);\n    break;\n  }\n\n  /* fetch the remaining header info */\n  w = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  h = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  maxval = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n\n  if (w <= 0 || h <= 0 || maxval <= 0) /* error check */\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  cinfo->data_precision = BITS_IN_JSAMPLE; /* we always rescale data to this */\n  cinfo->image_width = (JDIMENSION)w;\n  cinfo->image_height = (JDIMENSION)h;\n  source->maxval = maxval;\n\n  /* initialize flags to most common settings */\n  need_iobuffer = TRUE;         /* do we need an I/O buffer? */\n  use_raw_buffer = FALSE;       /* do we map input buffer onto I/O buffer? */\n  need_rescale = TRUE;          /* do we need a rescale array? */\n\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM_TEXT, w, h);\n    if (cinfo->in_color_space == JCS_GRAYSCALE)\n      source->pub.get_pixel_rows = get_text_gray_row;\n    else if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_gray_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_gray_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '3':                     /* it's a text-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM_TEXT, w, h);\n    if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_rgb_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '5':                     /* it's a raw-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_gray_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               cinfo->in_color_space == JCS_GRAYSCALE) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (cinfo->in_color_space == JCS_GRAYSCALE)\n        source->pub.get_pixel_rows = get_scaled_gray_row;\n      else if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_gray_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_gray_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n\n  case '6':                     /* it's a raw-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_rgb_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               (cinfo->in_color_space == JCS_EXT_RGB\n#if RGB_RED == 0 && RGB_GREEN == 1 && RGB_BLUE == 2 && RGB_PIXELSIZE == 3\n                || cinfo->in_color_space == JCS_RGB\n#endif\n               )) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_rgb_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n  }\n\n  if (IsExtRGB(cinfo->in_color_space))\n    cinfo->input_components = rgb_pixelsize[cinfo->in_color_space];\n  else if (cinfo->in_color_space == JCS_GRAYSCALE)\n    cinfo->input_components = 1;\n  else if (cinfo->in_color_space == JCS_CMYK)\n    cinfo->input_components = 4;\n\n  /* Allocate space for I/O buffer: 1 or 3 bytes or words/pixel. */\n  if (need_iobuffer) {\n    if (c == '6')\n      source->buffer_width = (size_t)w * 3 *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    else\n      source->buffer_width = (size_t)w *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    source->iobuffer = (U_CHAR *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  source->buffer_width);\n  }\n\n  /* Create compressor input buffer. */\n  if (use_raw_buffer) {\n    /* For unscaled raw-input case, we can just map it onto the I/O buffer. */\n    /* Synthesize a JSAMPARRAY pointer structure */\n    source->pixrow = (JSAMPROW)source->iobuffer;\n    source->pub.buffer = &source->pixrow;\n    source->pub.buffer_height = 1;\n  } else {\n    /* Need to translate anyway, so make a separate sample buffer. */\n    source->pub.buffer = (*cinfo->mem->alloc_sarray)\n      ((j_common_ptr)cinfo, JPOOL_IMAGE,\n       (JDIMENSION)w * cinfo->input_components, (JDIMENSION)1);\n    source->pub.buffer_height = 1;\n  }\n\n  /* Compute the rescaling array if required. */\n  if (need_rescale) {\n    long val, half_maxval;\n\n    /* On 16-bit-int machines we have to be careful of maxval = 65535 */\n    source->rescale = (JSAMPLE *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  (size_t)(((long)maxval + 1L) *\n                                           sizeof(JSAMPLE)));\n    half_maxval = maxval / 2;\n    for (val = 0; val <= (long)maxval; val++) {\n      /* The multiplication here must be done in 32 bits to avoid overflow */\n      source->rescale[val] = (JSAMPLE)((val * MAXJSAMPLE + half_maxval) /\n                                        maxval);\n    }\n  }\n}", "func_src_after": "start_input_ppm(j_compress_ptr cinfo, cjpeg_source_ptr sinfo)\n{\n  ppm_source_ptr source = (ppm_source_ptr)sinfo;\n  int c;\n  unsigned int w, h, maxval;\n  boolean need_iobuffer, use_raw_buffer, need_rescale;\n\n  if (getc(source->pub.input_file) != 'P')\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  c = getc(source->pub.input_file); /* subformat discriminator character */\n\n  /* detect unsupported variants (ie, PBM) before trying to read header */\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n  case '3':                     /* it's a text-format PPM file */\n  case '5':                     /* it's a raw-format PGM file */\n  case '6':                     /* it's a raw-format PPM file */\n    break;\n  default:\n    ERREXIT(cinfo, JERR_PPM_NOT);\n    break;\n  }\n\n  /* fetch the remaining header info */\n  w = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  h = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n  maxval = read_pbm_integer(cinfo, source->pub.input_file, 65535);\n\n  if (w <= 0 || h <= 0 || maxval <= 0) /* error check */\n    ERREXIT(cinfo, JERR_PPM_NOT);\n\n  cinfo->data_precision = BITS_IN_JSAMPLE; /* we always rescale data to this */\n  cinfo->image_width = (JDIMENSION)w;\n  cinfo->image_height = (JDIMENSION)h;\n  source->maxval = maxval;\n\n  /* initialize flags to most common settings */\n  need_iobuffer = TRUE;         /* do we need an I/O buffer? */\n  use_raw_buffer = FALSE;       /* do we map input buffer onto I/O buffer? */\n  need_rescale = TRUE;          /* do we need a rescale array? */\n\n  switch (c) {\n  case '2':                     /* it's a text-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM_TEXT, w, h);\n    if (cinfo->in_color_space == JCS_GRAYSCALE)\n      source->pub.get_pixel_rows = get_text_gray_row;\n    else if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_gray_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_gray_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '3':                     /* it's a text-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM_TEXT, w, h);\n    if (IsExtRGB(cinfo->in_color_space))\n      source->pub.get_pixel_rows = get_text_rgb_row;\n    else if (cinfo->in_color_space == JCS_CMYK)\n      source->pub.get_pixel_rows = get_text_rgb_cmyk_row;\n    else\n      ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    need_iobuffer = FALSE;\n    break;\n\n  case '5':                     /* it's a raw-format PGM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_GRAYSCALE;\n    TRACEMS2(cinfo, 1, JTRC_PGM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_gray_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               cinfo->in_color_space == JCS_GRAYSCALE) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (cinfo->in_color_space == JCS_GRAYSCALE)\n        source->pub.get_pixel_rows = get_scaled_gray_row;\n      else if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_gray_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_gray_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n\n  case '6':                     /* it's a raw-format PPM file */\n    if (cinfo->in_color_space == JCS_UNKNOWN)\n      cinfo->in_color_space = JCS_EXT_RGB;\n    TRACEMS2(cinfo, 1, JTRC_PPM, w, h);\n    if (maxval > 255) {\n      source->pub.get_pixel_rows = get_word_rgb_row;\n    } else if (maxval == MAXJSAMPLE && sizeof(JSAMPLE) == sizeof(U_CHAR) &&\n               (cinfo->in_color_space == JCS_EXT_RGB\n#if RGB_RED == 0 && RGB_GREEN == 1 && RGB_BLUE == 2 && RGB_PIXELSIZE == 3\n                || cinfo->in_color_space == JCS_RGB\n#endif\n               )) {\n      source->pub.get_pixel_rows = get_raw_row;\n      use_raw_buffer = TRUE;\n      need_rescale = FALSE;\n    } else {\n      if (IsExtRGB(cinfo->in_color_space))\n        source->pub.get_pixel_rows = get_rgb_row;\n      else if (cinfo->in_color_space == JCS_CMYK)\n        source->pub.get_pixel_rows = get_rgb_cmyk_row;\n      else\n        ERREXIT(cinfo, JERR_BAD_IN_COLORSPACE);\n    }\n    break;\n  }\n\n  if (IsExtRGB(cinfo->in_color_space))\n    cinfo->input_components = rgb_pixelsize[cinfo->in_color_space];\n  else if (cinfo->in_color_space == JCS_GRAYSCALE)\n    cinfo->input_components = 1;\n  else if (cinfo->in_color_space == JCS_CMYK)\n    cinfo->input_components = 4;\n\n  /* Allocate space for I/O buffer: 1 or 3 bytes or words/pixel. */\n  if (need_iobuffer) {\n    if (c == '6')\n      source->buffer_width = (size_t)w * 3 *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    else\n      source->buffer_width = (size_t)w *\n        ((maxval <= 255) ? sizeof(U_CHAR) : (2 * sizeof(U_CHAR)));\n    source->iobuffer = (U_CHAR *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  source->buffer_width);\n  }\n\n  /* Create compressor input buffer. */\n  if (use_raw_buffer) {\n    /* For unscaled raw-input case, we can just map it onto the I/O buffer. */\n    /* Synthesize a JSAMPARRAY pointer structure */\n    source->pixrow = (JSAMPROW)source->iobuffer;\n    source->pub.buffer = &source->pixrow;\n    source->pub.buffer_height = 1;\n  } else {\n    /* Need to translate anyway, so make a separate sample buffer. */\n    source->pub.buffer = (*cinfo->mem->alloc_sarray)\n      ((j_common_ptr)cinfo, JPOOL_IMAGE,\n       (JDIMENSION)w * cinfo->input_components, (JDIMENSION)1);\n    source->pub.buffer_height = 1;\n  }\n\n  /* Compute the rescaling array if required. */\n  if (need_rescale) {\n    long val, half_maxval;\n\n    /* On 16-bit-int machines we have to be careful of maxval = 65535 */\n    source->rescale = (JSAMPLE *)\n      (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE,\n                                  (size_t)(((long)MAX(maxval, 255) + 1L) *\n                                           sizeof(JSAMPLE)));\n    half_maxval = maxval / 2;\n    for (val = 0; val <= (long)maxval; val++) {\n      /* The multiplication here must be done in 32 bits to avoid overflow */\n      source->rescale[val] = (JSAMPLE)((val * MAXJSAMPLE + half_maxval) /\n                                        maxval);\n    }\n  }\n}", "commit_link": "github.com/libjpeg-turbo/libjpeg-turbo/commit/3de15e0c344d11d4b90f4a47136467053eb2d09a", "file_name": "rdppm.c", "vul_type": "cwe-125", "description": "Write a C function to initialize PPM image reading for JPEG compression."}
{"func_name": "kmod_module_parse_depline", "func_src_before": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "func_src_after": "int kmod_module_parse_depline(struct kmod_module *mod, char *line)\n{\n\tstruct kmod_ctx *ctx = mod->ctx;\n\tstruct kmod_list *list = NULL;\n\tchar *p, *saveptr;\n\tint err, n = 0;\n\n\tassert(!mod->init.dep && mod->dep == NULL);\n\tmod->init.dep = true;\n\n\tp = strchr(line, ':');\n\tif (p == NULL)\n\t\treturn 0;\n\n\t*p = '\\0';\n\tif (mod->path == NULL)\n\t\tmod->path = strdup(line);\n\n\tp++;\n\n\tfor (p = strtok_r(p, \" \\t\", &saveptr); p != NULL;\n\t\t\t\t\tp = strtok_r(NULL, \" \\t\", &saveptr)) {\n\t\tchar buf[NAME_MAX];\n\t\tconst char *modname = path_to_modname(p, buf, NULL);\n\t\tstruct kmod_module *depmod;\n\n\t\terr = kmod_module_new_from_name(ctx, modname, &depmod);\n\t\tif (err < 0) {\n\t\t\tERR(ctx, \"ctx=%p modname=%s error=%s\\n\",\n\t\t\t\t\t\tctx, modname, strerror(-err));\n\t\t\tgoto fail;\n\t\t}\n\n\t\tDBG(ctx, \"add dep: %s\\n\", modname);\n\n\t\tlist = kmod_list_append(list, depmod);\n\t\tn++;\n\t}\n\n\tDBG(ctx, \"%d dependencies for %s\\n\", n, mod->name);\n\n\tmod->dep = list;\n\treturn n;\n\nfail:\n\tkmod_module_unref_list(list);\n\tmod->init.dep = false;\n\treturn err;\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 462, "char_end": 518, "line": "\t\tconst char *modname = path_to_modname(p, NULL, NULL);\n"}], "added": [{"line_no": 23, "char_start": 462, "char_end": 484, "line": "\t\tchar buf[NAME_MAX];\n"}, {"line_no": 24, "char_start": 484, "char_end": 539, "line": "\t\tconst char *modname = path_to_modname(p, buf, NULL);\n"}]}, "char_changes": {"deleted": [{"char_start": 505, "char_end": 509, "chars": "NULL"}], "added": [{"char_start": 462, "char_end": 484, "chars": "\t\tchar buf[NAME_MAX];\n"}, {"char_start": 527, "char_end": 530, "chars": "buf"}]}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to parse module dependency lines and populate a list of dependencies."}
{"func_name": "snd_pcm_period_elapsed", "func_src_before": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n}", "func_src_after": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n}", "commit_link": "github.com/torvalds/linux/commit/3aa02cb664c5fb1042958c8d1aa8c35055a2ebc4", "file_name": "sound/core/pcm_lib.c", "vul_type": "cwe-416", "description": "Write a C function named `snd_pcm_period_elapsed` that handles the end-of-period interrupt for a PCM audio substream."}
{"func_name": "Updater::updateModule", "func_src_before": "\tprivate void updateModule( final ModuleBean module, final boolean external, final boolean repair ) {\n\t\tLEnv.LOGGER.info( ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() + \"...\" );\n\t\tlauncherFrame.setStatus( StatusType.PROGRESS, ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() );\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart, show RESTART action while updating the launcher\n\t\t\tlauncherFrame.setProceedText( \"<html><h2>RE_START</h2></html>\" );\n\t\t}\n\t\t\n\t\tfinal Path tempPath = ( external ? LEnv.PATH_EXT_MODS : LEnv.PATH_MODS ).resolve( \"_update\" );\n\t\ttry {\n\t\t\t// Create temp update folder\n\t\t\twhile ( !LUtils.deletePath( tempPath ) )\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) )\n\t\t\t\t\tthrow new Exception( \"Could not delete folder: \" + tempPath );\n\t\t\tFiles.createDirectory( tempPath );\n\t\t\t\n\t\t\tfinal Path archivePath = tempPath.resolve( module.getArchiveFile().getPath() );\n\t\t\t\n\t\t\t// Download module archive, try mirrors if one fails\n\t\t\tfinal byte[] buffer = new byte[ 16_384 ]; // 16 KB work buffer\n\t\t\tfor ( int urlIdx = 0; urlIdx < module.getUrlList().size(); urlIdx++ ) {\n\t\t\t\tfinal String archiveSource = urlIdx == 0 ? \"main source\" : \" mirror #\" + urlIdx;\n\t\t\t\tfinal boolean lastArchiveUrl = urlIdx == module.getUrlList().size() - 1;\n\t\t\t\t\n\t\t\t\tLEnv.LOGGER.debug( \"Downloading archive from \" + archiveSource + \"...\" );\n\t\t\t\t\n\t\t\t\tboolean downloadOk = false;\n\t\t\t\tInputStream input = null;\n\t\t\t\tOutputStream output = null;\n\t\t\t\ttry {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Connecting...\" );\n\t\t\t\t\tlauncherFrame.setProgressMax( (int) module.getArchiveSize() );\n\t\t\t\t\tlauncherFrame.setProgress( 0 );\n\t\t\t\t\t\n\t\t\t\t\tfinal URLConnection archiveUrlConnection = new URL( module.getUrlList().get( urlIdx ) ).openConnection();\n\t\t\t\t\t\n\t\t\t\t\tinput = archiveUrlConnection.getInputStream();\n\t\t\t\t\toutput = Files.newOutputStream( archivePath );\n\t\t\t\t\t\n\t\t\t\t\tLEnv.LOGGER.debug( \"Downloading...\" );\n\t\t\t\t\tint totalBytesRead = 0;\n\t\t\t\t\tint bytesRead;\n\t\t\t\t\twhile ( ( bytesRead = input.read( buffer ) ) > 0 ) {\n\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\ttotalBytesRead += bytesRead;\n\t\t\t\t\t\tlauncherFrame.setProgress( totalBytesRead );\n\t\t\t\t\t}\n\t\t\t\t\toutput.flush();\n\t\t\t\t\t\n\t\t\t\t\tdownloadOk = true;\n\t\t\t\t\tLEnv.LOGGER.debug( \"Download complete.\" );\n\t\t\t\t\t\n\t\t\t\t} catch ( final Exception e ) {\n\t\t\t\t\tLEnv.LOGGER.warning(\n\t\t\t\t\t        \"Failed to download archive from \" + archiveSource + \"!\" + ( lastArchiveUrl ? \"\" : \" Proceeding to the next source.\" ), e );\n\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t} finally {\n\t\t\t\t\tif ( input != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tinput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinput = null;\n\t\t\t\t\t}\n\t\t\t\t\tif ( output != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\toutput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\toutput = null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif ( downloadOk ) {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Checking SHA-256 checksum of the archive...\" );\n\t\t\t\t\tif ( module.getArchiveFile().getSha256().equals( LUtils.calculateFileSha256( archivePath ) ) ) {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum OK.\" );\n\t\t\t\t\t\tbreak; // Break archive URLs cycle\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum MISMATCH! The downloaded archive is discarded!\" );\n\t\t\t\t\t\twhile ( !LUtils.deletePath( archivePath ) )\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete file:\", archivePath ) )\n\t\t\t\t\t\t\t\tthrow new Exception( \"Could not delete file: \" + archivePath );\n\t\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tLEnv.LOGGER.debug( \"Proceeding to the next source.\" );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( \"Extracting archive...\" );\n\t\t\tfinal InputStream ins = Files.newInputStream( archivePath ); // Input stream is \"out-sourced\" to a local var because\n\t\t\t                                                             // else a false resource leak is reported :S\n\t\t\ttry ( final ZipInputStream zipInput = new ZipInputStream( ins ) ) {\n\t\t\t\tfinal String pathPrefix = external ? \"Scelight/\" + LEnv.PATH_EXT_MODS.getFileName().toString() + \"/\" + module.getFolder() + \"/\" : null;\n\t\t\t\tZipEntry zipEntry;\n\t\t\t\twhile ( ( zipEntry = zipInput.getNextEntry() ) != null ) {\n\t\t\t\t\tif ( external && !zipEntry.isDirectory() ) {\n\t\t\t\t\t\t// Quarantine check\n\t\t\t\t\t\tif ( zipEntry.getName().indexOf( \"..\" ) >= 0 || !zipEntry.getName().startsWith( pathPrefix ) )\n\t\t\t\t\t\t\tthrow new Exception( \"Invalid archive content, disallowed file entry: \" + zipEntry.getName() );\n\t\t\t\t\t}\n\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\n\t\t\t\t\tif ( zipEntry.isDirectory() )\n\t\t\t\t\t\tFiles.createDirectories( entryFile );\n\t\t\t\t\telse {\n\t\t\t\t\t\tlong size = zipEntry.getSize();\n\t\t\t\t\t\ttry ( final OutputStream output = Files.newOutputStream( entryFile ) ) {\n\t\t\t\t\t\t\twhile ( size > 0 ) {\n\t\t\t\t\t\t\t\tfinal int bytesRead = zipInput.read( buffer );\n\t\t\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\t\t\tsize -= bytesRead;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\toutput.flush();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch ( final Exception e ) {\n\t\t\t\tthrow new Exception( \"Failed to extract archive!\", e );\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Extracting done.\" );\n\t\t\t\n\t\t\tfinal Path archiveAppPath = tempPath.resolve( \"Scelight\" );\n\t\t\tif ( !Files.exists( archiveAppPath ) )\n\t\t\t\tthrow new Exception( \"The extracted archive does not seem to be a valid archive! Aborting \" + ( repair ? \"repair\" : \"update\" ) + \"!\" );\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replacing/patching files...\" : \"Implanting files...\" );\n\t\t\tFiles.walkFileTree( archiveAppPath, new SimpleFileVisitor< Path >() {\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult preVisitDirectory( Path dir, BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\tdir = LEnv.PATH_APP.resolve( archiveAppPath.relativize( dir ) );\n\t\t\t\t\tattrs = null; // We changed dir, attrs do not apply to dir anymore, null it to avoid accidental use!\n\t\t\t\t\t\n\t\t\t\t\twhile ( !Files.exists( dir ) || !Files.isDirectory( dir ) ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t// Files.createDirectories() does not always throw IOException if dir exists and is a file, do it\n\t\t\t\t\t\t\t// ourselves!\n\t\t\t\t\t\t\tif ( Files.exists( dir ) && !Files.isDirectory( dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"File exists and is not a folder: \" + dir );\n\t\t\t\t\t\t\tFiles.createDirectories( dir );\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not create folder:\", dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not create folder: \" + dir, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult visitFile( final Path file, final BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\t// Launcher's class path entries are locked and therefore cannot be repaired from \"within\"!\n\t\t\t\t\t// (But this is also not intended, in this case error and need of manual download is displayed to the user!)\n\t\t\t\t\tif ( repair && module == modules.getLauncherMod() && launcher.isClassPathEntry( file.getFileName().toString() ) )\n\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\n\t\t\t\t\tfinal Path target = LEnv.PATH_APP.resolve( archiveAppPath.relativize( file ) );\n\t\t\t\t\twhile ( true ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFiles.copy( file, target, StandardCopyOption.REPLACE_EXISTING );\n\t\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not write file:\", target ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not write file: \" + target, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} );\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replace/patch complete.\" : \"Implantation complete.\" );\n\t\t\t\n\t\t} catch ( final Exception e ) {\n\t\t\tLEnv.LOGGER.error( \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"!\", e );\n\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"! See the Logs for details!\" );\n\t\t\tthrow new FinishException();\n\t\t} finally {\n\t\t\tLEnv.LOGGER.debug( \"Cleaning up...\" );\n\t\t\twhile ( !LUtils.deletePath( tempPath ) ) {\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) ) {\n\t\t\t\t\tLEnv.LOGGER.error( \"Could not delete folder: \" + tempPath );\n\t\t\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName()\n\t\t\t\t\t        + \"! See the Logs for details!\" );\n\t\t\t\t\tthrow new FinishException();\n\t\t\t\t}\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Cleanup complete.\" );\n\t\t}\n\t\t\n\t\t// Module updated / repaired successfully.\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName()\n\t\t\t        + \" to continue.\" );\n\t\t\tlauncherFrame.setStatus( StatusType.WARNING,\n\t\t\t        module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName() + \" to continue.\" );\n\t\t\tthrow new FinishException( true );\n\t\t} else {\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t\tlauncherFrame.setStatus( StatusType.PROGRESS, module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t}\n\t}", "func_src_after": "\tprivate void updateModule( final ModuleBean module, final boolean external, final boolean repair ) {\n\t\tLEnv.LOGGER.info( ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() + \"...\" );\n\t\tlauncherFrame.setStatus( StatusType.PROGRESS, ( repair ? \"Repairing \" : \"Updating \" ) + module.getName() );\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart, show RESTART action while updating the launcher\n\t\t\tlauncherFrame.setProceedText( \"<html><h2>RE_START</h2></html>\" );\n\t\t}\n\t\t\n\t\tfinal Path tempPath = ( external ? LEnv.PATH_EXT_MODS : LEnv.PATH_MODS ).resolve( \"_update\" );\n\t\ttry {\n\t\t\t// Create temp update folder\n\t\t\twhile ( !LUtils.deletePath( tempPath ) )\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) )\n\t\t\t\t\tthrow new Exception( \"Could not delete folder: \" + tempPath );\n\t\t\tFiles.createDirectory( tempPath );\n\t\t\t\n\t\t\tfinal Path archivePath = tempPath.resolve( module.getArchiveFile().getPath() );\n\t\t\t\n\t\t\t// Download module archive, try mirrors if one fails\n\t\t\tfinal byte[] buffer = new byte[ 16_384 ]; // 16 KB work buffer\n\t\t\tfor ( int urlIdx = 0; urlIdx < module.getUrlList().size(); urlIdx++ ) {\n\t\t\t\tfinal String archiveSource = urlIdx == 0 ? \"main source\" : \" mirror #\" + urlIdx;\n\t\t\t\tfinal boolean lastArchiveUrl = urlIdx == module.getUrlList().size() - 1;\n\t\t\t\t\n\t\t\t\tLEnv.LOGGER.debug( \"Downloading archive from \" + archiveSource + \"...\" );\n\t\t\t\t\n\t\t\t\tboolean downloadOk = false;\n\t\t\t\tInputStream input = null;\n\t\t\t\tOutputStream output = null;\n\t\t\t\ttry {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Connecting...\" );\n\t\t\t\t\tlauncherFrame.setProgressMax( (int) module.getArchiveSize() );\n\t\t\t\t\tlauncherFrame.setProgress( 0 );\n\t\t\t\t\t\n\t\t\t\t\tfinal URLConnection archiveUrlConnection = new URL( module.getUrlList().get( urlIdx ) ).openConnection();\n\t\t\t\t\t\n\t\t\t\t\tinput = archiveUrlConnection.getInputStream();\n\t\t\t\t\toutput = Files.newOutputStream( archivePath );\n\t\t\t\t\t\n\t\t\t\t\tLEnv.LOGGER.debug( \"Downloading...\" );\n\t\t\t\t\tint totalBytesRead = 0;\n\t\t\t\t\tint bytesRead;\n\t\t\t\t\twhile ( ( bytesRead = input.read( buffer ) ) > 0 ) {\n\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\ttotalBytesRead += bytesRead;\n\t\t\t\t\t\tlauncherFrame.setProgress( totalBytesRead );\n\t\t\t\t\t}\n\t\t\t\t\toutput.flush();\n\t\t\t\t\t\n\t\t\t\t\tdownloadOk = true;\n\t\t\t\t\tLEnv.LOGGER.debug( \"Download complete.\" );\n\t\t\t\t\t\n\t\t\t\t} catch ( final Exception e ) {\n\t\t\t\t\tLEnv.LOGGER.warning(\n\t\t\t\t\t        \"Failed to download archive from \" + archiveSource + \"!\" + ( lastArchiveUrl ? \"\" : \" Proceeding to the next source.\" ), e );\n\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t} finally {\n\t\t\t\t\tif ( input != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tinput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinput = null;\n\t\t\t\t\t}\n\t\t\t\t\tif ( output != null ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\toutput.close();\n\t\t\t\t\t\t} catch ( final IOException ie ) {\n\t\t\t\t\t\t\t// We're done, just ignore.\n\t\t\t\t\t\t}\n\t\t\t\t\t\toutput = null;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif ( downloadOk ) {\n\t\t\t\t\tLEnv.LOGGER.debug( \"Checking SHA-256 checksum of the archive...\" );\n\t\t\t\t\tif ( module.getArchiveFile().getSha256().equals( LUtils.calculateFileSha256( archivePath ) ) ) {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum OK.\" );\n\t\t\t\t\t\tbreak; // Break archive URLs cycle\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLEnv.LOGGER.debug( \"SHA-256 checksum MISMATCH! The downloaded archive is discarded!\" );\n\t\t\t\t\t\twhile ( !LUtils.deletePath( archivePath ) )\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete file:\", archivePath ) )\n\t\t\t\t\t\t\t\tthrow new Exception( \"Could not delete file: \" + archivePath );\n\t\t\t\t\t\tif ( lastArchiveUrl )\n\t\t\t\t\t\t\tthrow new Exception( \"None of the archives are available!\" );\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tLEnv.LOGGER.debug( \"Proceeding to the next source.\" );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( \"Extracting archive...\" );\n\t\t\tfinal InputStream ins = Files.newInputStream( archivePath ); // Input stream is \"out-sourced\" to a local var because\n\t\t\t                                                             // else a false resource leak is reported :S\n\t\t\ttry ( final ZipInputStream zipInput = new ZipInputStream( ins ) ) {\n\t\t\t\tfinal String pathPrefix = external ? \"Scelight/\" + LEnv.PATH_EXT_MODS.getFileName().toString() + \"/\" + module.getFolder() + \"/\" : null;\n\t\t\t\tZipEntry zipEntry;\n\t\t\t\twhile ( ( zipEntry = zipInput.getNextEntry() ) != null ) {\n\t\t\t\t\tif ( external && !zipEntry.isDirectory() ) {\n\t\t\t\t\t\t// Quarantine check\n\t\t\t\t\t\tif ( zipEntry.getName().indexOf( \"..\" ) >= 0 || !zipEntry.getName().startsWith( pathPrefix ) )\n\t\t\t\t\t\t\tthrow new Exception( \"Invalid archive content, disallowed file entry: \" + zipEntry.getName() );\n\t\t\t\t\t}\n\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\r\n\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n\t\t\t\t\t}\n\t\t\t\t\tif ( zipEntry.isDirectory() )\n\t\t\t\t\t\tFiles.createDirectories( entryFile );\n\t\t\t\t\telse {\n\t\t\t\t\t\tlong size = zipEntry.getSize();\n\t\t\t\t\t\ttry ( final OutputStream output = Files.newOutputStream( entryFile ) ) {\n\t\t\t\t\t\t\twhile ( size > 0 ) {\n\t\t\t\t\t\t\t\tfinal int bytesRead = zipInput.read( buffer );\n\t\t\t\t\t\t\t\toutput.write( buffer, 0, bytesRead );\n\t\t\t\t\t\t\t\tsize -= bytesRead;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\toutput.flush();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} catch ( final Exception e ) {\n\t\t\t\tthrow new Exception( \"Failed to extract archive!\", e );\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Extracting done.\" );\n\t\t\t\n\t\t\tfinal Path archiveAppPath = tempPath.resolve( \"Scelight\" );\n\t\t\tif ( !Files.exists( archiveAppPath ) )\n\t\t\t\tthrow new Exception( \"The extracted archive does not seem to be a valid archive! Aborting \" + ( repair ? \"repair\" : \"update\" ) + \"!\" );\n\t\t\t\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replacing/patching files...\" : \"Implanting files...\" );\n\t\t\tFiles.walkFileTree( archiveAppPath, new SimpleFileVisitor< Path >() {\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult preVisitDirectory( Path dir, BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\tdir = LEnv.PATH_APP.resolve( archiveAppPath.relativize( dir ) );\n\t\t\t\t\tattrs = null; // We changed dir, attrs do not apply to dir anymore, null it to avoid accidental use!\n\t\t\t\t\t\n\t\t\t\t\twhile ( !Files.exists( dir ) || !Files.isDirectory( dir ) ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t// Files.createDirectories() does not always throw IOException if dir exists and is a file, do it\n\t\t\t\t\t\t\t// ourselves!\n\t\t\t\t\t\t\tif ( Files.exists( dir ) && !Files.isDirectory( dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"File exists and is not a folder: \" + dir );\n\t\t\t\t\t\t\tFiles.createDirectories( dir );\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not create folder:\", dir ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not create folder: \" + dir, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t@Override\n\t\t\t\tpublic FileVisitResult visitFile( final Path file, final BasicFileAttributes attrs ) throws IOException {\n\t\t\t\t\t// Launcher's class path entries are locked and therefore cannot be repaired from \"within\"!\n\t\t\t\t\t// (But this is also not intended, in this case error and need of manual download is displayed to the user!)\n\t\t\t\t\tif ( repair && module == modules.getLauncherMod() && launcher.isClassPathEntry( file.getFileName().toString() ) )\n\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\n\t\t\t\t\tfinal Path target = LEnv.PATH_APP.resolve( archiveAppPath.relativize( file ) );\n\t\t\t\t\twhile ( true ) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFiles.copy( file, target, StandardCopyOption.REPLACE_EXISTING );\n\t\t\t\t\t\t\treturn FileVisitResult.CONTINUE;\n\t\t\t\t\t\t} catch ( final IOException e ) {\n\t\t\t\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not write file:\", target ) )\n\t\t\t\t\t\t\t\tthrow new IOException( \"Could not write file: \" + target, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} );\n\t\t\tLEnv.LOGGER.debug( repair ? \"Replace/patch complete.\" : \"Implantation complete.\" );\n\t\t\t\n\t\t} catch ( final Exception e ) {\n\t\t\tLEnv.LOGGER.error( \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"!\", e );\n\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName() + \"! See the Logs for details!\" );\n\t\t\tthrow new FinishException();\n\t\t} finally {\n\t\t\tLEnv.LOGGER.debug( \"Cleaning up...\" );\n\t\t\twhile ( !LUtils.deletePath( tempPath ) ) {\n\t\t\t\tif ( !LGuiUtils.askRetry( \"Could not delete folder:\", tempPath ) ) {\n\t\t\t\t\tLEnv.LOGGER.error( \"Could not delete folder: \" + tempPath );\n\t\t\t\t\tlauncherFrame.setStatus( StatusType.ERROR, \"Failed to \" + ( repair ? \"repair \" : \"update \" ) + module.getName()\n\t\t\t\t\t        + \"! See the Logs for details!\" );\n\t\t\t\t\tthrow new FinishException();\n\t\t\t\t}\n\t\t\t}\n\t\t\tLEnv.LOGGER.debug( \"Cleanup complete.\" );\n\t\t}\n\t\t\n\t\t// Module updated / repaired successfully.\n\t\t\n\t\tif ( module == modules.getLauncherMod() ) {\n\t\t\t// Launcher update requires restart\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName()\n\t\t\t        + \" to continue.\" );\n\t\t\tlauncherFrame.setStatus( StatusType.WARNING,\n\t\t\t        module.getName() + \" has been \" + ( repair ? \"repaired\" : \"updated\" ) + \", you must restart \" + module.getName() + \" to continue.\" );\n\t\t\tthrow new FinishException( true );\n\t\t} else {\n\t\t\tLEnv.LOGGER.info( module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t\tlauncherFrame.setStatus( StatusType.PROGRESS, module.getName() + \" has been \" + ( repair ? \"repaired.\" : \"updated.\" ) );\n\t\t}\n\t}", "line_changes": {"deleted": [{"line_no": 108, "char_start": 4610, "char_end": 4678, "line": "\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\n"}], "added": [{"line_no": 108, "char_start": 4610, "char_end": 4679, "line": "\t\t\t\t\tfinal Path entryFile = tempPath.resolve( zipEntry.getName() );\r\n"}, {"line_no": 109, "char_start": 4679, "char_end": 4748, "line": "\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n"}, {"line_no": 110, "char_start": 4748, "char_end": 4800, "line": "\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n"}, {"line_no": 111, "char_start": 4800, "char_end": 4807, "line": "\t\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4677, "char_end": 4806, "chars": "\r\n\t\t\t\t\tif (!entryFile.normalize().startsWith(tempPath.normalize())) {\r\n\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\r\n\t\t\t\t\t}"}]}, "commit_link": "github.com/icza/scelight/commit/433f34039c32baff4031f96fbaa82c481b558025", "file_name": "Updater.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "49d2c7c831690ce56ff81d15c50923be61dbbd1f", "description": "Write a Java function to update or repair a software module, handling download, extraction, and file replacement."}
{"func_name": "getQueue", "func_src_before": "    def getQueue(self, numberOfLinks=10):\n        self.cursor.execute(\"SELECT url FROM queue WHERE visited = '0' LIMIT {};\".format(numberOfLinks))\n        result = self.cursor.fetchall()\n        self.remove(result)\n        return result", "func_src_after": "    def getQueue(self, numberOfLinks=10):\n        self.cursor.execute(\"SELECT url FROM queue WHERE visited = '0' LIMIT ?;\", numberOfLinks)\n        result = self.cursor.fetchall()\n        self.remove(result)\n        return result", "commit_link": "github.com/jappe999/WebScraper/commit/46a4e0843aa44d903293637afad53dfcbc37b480", "file_name": "beta/database.py", "vul_type": "cwe-089", "description": "Write a Python function named `getQueue` that retrieves a specified number of unvisited URLs from a queue in a database and removes them from the queue after retrieval."}
{"func_name": "git_delta_apply", "func_src_before": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/*\n\t * Check that the base size matches the data we were given;\n\t * if not we would underflow while accessing data from the\n\t * base object, resulting in data corruption or segfault.\n\t */\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base. */\n\t\t\tsize_t off = 0, len = 0;\n\n#define ADD_DELTA(o, shift) { if (delta < delta_end) (o) |= ((unsigned) *delta++ << shift); else goto fail; }\n\t\t\tif (cmd & 0x01) ADD_DELTA(off, 0UL);\n\t\t\tif (cmd & 0x02) ADD_DELTA(off, 8UL);\n\t\t\tif (cmd & 0x04) ADD_DELTA(off, 16UL);\n\t\t\tif (cmd & 0x08) ADD_DELTA(off, 24UL);\n\n\t\t\tif (cmd & 0x10) ADD_DELTA(len, 0UL);\n\t\t\tif (cmd & 0x20) ADD_DELTA(len, 8UL);\n\t\t\tif (cmd & 0x40) ADD_DELTA(len, 16UL);\n\t\t\tif (!len)       len = 0x10000;\n#undef ADD_DELTA\n\n\t\t\tif (base_len < off + len || res_sz < len)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t} else if (cmd) {\n\t\t\t/*\n\t\t\t * cmd is a literal insert instruction; copy from\n\t\t\t * the delta stream itself.\n\t\t\t */\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t} else {\n\t\t\t/* cmd == 0 is reserved for future encodings. */\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}", "func_src_after": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/*\n\t * Check that the base size matches the data we were given;\n\t * if not we would underflow while accessing data from the\n\t * base object, resulting in data corruption or segfault.\n\t */\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base. */\n\t\t\tsize_t off = 0, len = 0, end;\n\n#define ADD_DELTA(o, shift) { if (delta < delta_end) (o) |= ((unsigned) *delta++ << shift); else goto fail; }\n\t\t\tif (cmd & 0x01) ADD_DELTA(off, 0UL);\n\t\t\tif (cmd & 0x02) ADD_DELTA(off, 8UL);\n\t\t\tif (cmd & 0x04) ADD_DELTA(off, 16UL);\n\t\t\tif (cmd & 0x08) ADD_DELTA(off, 24UL);\n\n\t\t\tif (cmd & 0x10) ADD_DELTA(len, 0UL);\n\t\t\tif (cmd & 0x20) ADD_DELTA(len, 8UL);\n\t\t\tif (cmd & 0x40) ADD_DELTA(len, 16UL);\n\t\t\tif (!len)       len = 0x10000;\n#undef ADD_DELTA\n\n\t\t\tif (GIT_ADD_SIZET_OVERFLOW(&end, off, len) ||\n\t\t\t    base_len < end || res_sz < len)\n\t\t\t\tgoto fail;\n\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t} else if (cmd) {\n\t\t\t/*\n\t\t\t * cmd is a literal insert instruction; copy from\n\t\t\t * the delta stream itself.\n\t\t\t */\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t} else {\n\t\t\t/* cmd == 0 is reserved for future encodings. */\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}", "commit_link": "github.com/libgit2/libgit2/commit/c1577110467b701dcbcf9439ac225ea851b47d22", "file_name": "src/delta.c", "vul_type": "cwe-190", "description": "Write a C function named `git_delta_apply` that applies a delta to a given base data to produce an output."}
{"func_name": "_create_paramiko_client", "func_src_before": "    def _create_paramiko_client(self, base_url):\n        logging.getLogger(\"paramiko\").setLevel(logging.WARNING)\n        self.ssh_client = paramiko.SSHClient()\n        base_url = urllib.parse.urlparse(base_url)\n        self.ssh_params = {\n            \"hostname\": base_url.hostname,\n            \"port\": base_url.port,\n            \"username\": base_url.username\n            }\n        ssh_config_file = os.path.expanduser(\"~/.ssh/config\")\n        if os.path.exists(ssh_config_file):\n            conf = paramiko.SSHConfig()\n            with open(ssh_config_file) as f:\n                conf.parse(f)\n            host_config = conf.lookup(base_url.hostname)\n            if 'proxycommand' in host_config:\n                self.ssh_params[\"sock\"] = paramiko.ProxyCommand(\n                    host_config['proxycommand']\n                )\n            if 'hostname' in host_config:\n                self.ssh_params['hostname'] = host_config['hostname']\n            if base_url.port is None and 'port' in host_config:\n                self.ssh_params['port'] = host_config['port']\n            if base_url.username is None and 'user' in host_config:\n                self.ssh_params['username'] = host_config['user']\n            if 'identityfile' in host_config:\n                self.ssh_params['key_filename'] = host_config['identityfile']\n\n        self.ssh_client.load_system_host_keys()\n        self.ssh_client.set_missing_host_key_policy(paramiko.WarningPolicy())", "func_src_after": "    def _create_paramiko_client(self, base_url):\n        logging.getLogger(\"paramiko\").setLevel(logging.WARNING)\n        self.ssh_client = paramiko.SSHClient()\n        base_url = urllib.parse.urlparse(base_url)\n        self.ssh_params = {\n            \"hostname\": base_url.hostname,\n            \"port\": base_url.port,\n            \"username\": base_url.username\n            }\n        ssh_config_file = os.path.expanduser(\"~/.ssh/config\")\n        if os.path.exists(ssh_config_file):\n            conf = paramiko.SSHConfig()\n            with open(ssh_config_file) as f:\n                conf.parse(f)\n            host_config = conf.lookup(base_url.hostname)\n            if 'proxycommand' in host_config:\n                self.ssh_params[\"sock\"] = paramiko.ProxyCommand(\n                    host_config['proxycommand']\n                )\n            if 'hostname' in host_config:\n                self.ssh_params['hostname'] = host_config['hostname']\n            if base_url.port is None and 'port' in host_config:\n                self.ssh_params['port'] = host_config['port']\n            if base_url.username is None and 'user' in host_config:\n                self.ssh_params['username'] = host_config['user']\n            if 'identityfile' in host_config:\n                self.ssh_params['key_filename'] = host_config['identityfile']\n\n        self.ssh_client.load_system_host_keys()\n        self.ssh_client.set_missing_host_key_policy(paramiko.RejectPolicy())", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1373, "char_end": 1450, "line": "        self.ssh_client.set_missing_host_key_policy(paramiko.WarningPolicy())\n"}], "added": [{"line_no": 30, "char_start": 1373, "char_end": 1449, "line": "        self.ssh_client.set_missing_host_key_policy(paramiko.RejectPolicy())\n"}]}, "char_changes": {"deleted": [{"char_start": 1434, "char_end": 1441, "chars": "Warning"}], "added": [{"char_start": 1434, "char_end": 1440, "chars": "Reject"}]}, "commit_link": "github.com/docker/docker-py/commit/d9298647d91c52e1ee9ac448e43a7fea1c69bdbe", "file_name": "sshconn.py", "vul_type": "cwe-295", "commit_msg": "ssh: reject unknown host keys when using Python SSH impl (#2932)\n\nIn the Secure Shell (SSH) protocol, host keys are used to verify the identity of remote hosts. Accepting unknown host keys may leave the connection open to man-in-the-middle attacks.\r\n\r\nDo not accept unknown host keys. In particular, do not set the default missing host key policy for the Paramiko library to either AutoAddPolicy or WarningPolicy. Both of these policies continue even when the host key is unknown. The default setting of RejectPolicy is secure because it throws an exception when it encounters an unknown host key.\r\n\r\nReference: https://cwe.mitre.org/data/definitions/295.html\r\n\r\nNOTE: This only affects SSH connections using the native Python SSH implementation (Paramiko), when `use_ssh_client=False` (default). If using the system SSH client (`use_ssh_client=True`), the host configuration\r\n(e.g. `~/.ssh/config`) will apply.\r\n\r\nSigned-off-by: Audun Nes <audun.nes@gmail.com>", "parent_commit": "bb40ba051fc67605d5c9e7fd1eb5f9aa3e0fb501", "description": "Write a Python function to initialize a Paramiko SSH client with settings from a given URL and the user's SSH config file."}
{"func_name": "enc_untrusted_inet_pton", "func_src_before": "int enc_untrusted_inet_pton(int af, const char *src, void *dst) {\n  if (!src || !dst) {\n    return 0;\n  }\n\n  MessageWriter input;\n  input.Push<int>(TokLinuxAfFamily(af));\n  input.PushByReference(Extent{\n      src, std::min(strlen(src) + 1, static_cast<size_t>(INET6_ADDRSTRLEN))});\n  MessageReader output;\n\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kInetPtonHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_inet_pton\", 3);\n\n  int result = output.next<int>();\n  int klinux_errno = output.next<int>();\n  if (result == -1) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n    return -1;\n  }\n\n  auto klinux_addr_buffer = output.next();\n  size_t max_size = 0;\n  if (af == AF_INET) {\n    max_size = sizeof(struct in_addr);\n  } else if (af == AF_INET6) {\n    max_size = sizeof(struct in6_addr);\n  }\n  memcpy(dst, klinux_addr_buffer.data(),\n         std::min(klinux_addr_buffer.size(), max_size));\n  return result;\n}", "func_src_after": "int enc_untrusted_inet_pton(int af, const char *src, void *dst) {\n  if (!src || !dst) {\n    return 0;\n  }\n\n  MessageWriter input;\n  input.Push<int>(TokLinuxAfFamily(af));\n  input.PushByReference(Extent{\n      src, std::min(strlen(src) + 1, static_cast<size_t>(INET6_ADDRSTRLEN))});\n  MessageReader output;\n\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kInetPtonHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_inet_pton\", 3);\n\n  int result = output.next<int>();\n  int klinux_errno = output.next<int>();\n  if (result == -1) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n    return -1;\n  }\n\n  auto klinux_addr_buffer = output.next();\n  size_t max_size = 0;\n  if (af == AF_INET) {\n    if (klinux_addr_buffer.size() != sizeof(klinux_in_addr)) {\n      ::asylo::primitives::TrustedPrimitives::BestEffortAbort(\n          \"enc_untrusted_inet_pton: unexpected output size\");\n    }\n    max_size = sizeof(struct in_addr);\n  } else if (af == AF_INET6) {\n    if (klinux_addr_buffer.size() != sizeof(klinux_in6_addr)) {\n      ::asylo::primitives::TrustedPrimitives::BestEffortAbort(\n          \"enc_untrusted_inet_pton: unexpected output size\");\n    }\n    max_size = sizeof(struct in6_addr);\n  }\n  memcpy(dst, klinux_addr_buffer.data(),\n         std::min(klinux_addr_buffer.size(), max_size));\n  return result;\n}", "commit_link": "github.com/google/asylo/commit/8fed5e334131abaf9c5e17307642fbf6ce4a57ec", "file_name": "asylo/platform/host_call/trusted/host_calls.cc", "vul_type": "cwe-125", "description": "Write a C++ function named `enc_untrusted_inet_pton` that converts an IP address in text format to a network address structure."}
{"func_name": "stats", "func_src_before": "@bot.message_handler(commands=['stats'])\ndef stats(message):\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"select * from users where chat_id = '\" + str(message.chat.id) + \"'\")\n    name = conn.fetchone()\n    settings.close()\n    if name != None:\n        bases.update.update_user(name[1], name[0], name[2])\n        bases.problem.create_text_stats(name[1])\n        img = open(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\users\\\\\" + name[1] + \".png\", \"rb\")\n        bot.send_photo(message.chat.id, img)\n        img.close()\n        if bases.problem.create_stats_picture(name[1]):\n            bot.send_message(message.chat.id, \"Sorry, you haven't solved tasks.\")\n            return 0\n        img = open(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\users\\\\\" + name[1] + \".png\", \"rb\")\n        bot.send_photo(message.chat.id, img)\n        img.close()\n    else:\n        bot.send_message(message.chat.id, \"You should login before getting statistic.\")", "func_src_after": "@bot.message_handler(commands=['stats'])\ndef stats(message):\n    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\settings.db\")\n    conn = settings.cursor()\n    conn.execute(\"select * from users where chat_id = ?\", (str(message.chat.id),))\n    name = conn.fetchone()\n    settings.close()\n    if name != None:\n        bases.update.update_user(name[1], name[0], name[2])\n        bases.problem.create_text_stats(name[1])\n        img = open(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\users\\\\\" + name[1] + \".png\", \"rb\")\n        bot.send_photo(message.chat.id, img)\n        img.close()\n        if bases.problem.create_stats_picture(name[1]):\n            bot.send_message(message.chat.id, \"Sorry, you haven't solved tasks.\")\n            return 0\n        img = open(os.path.abspath(os.path.dirname(__file__)) + \"\\\\bases\\\\users\\\\\" + name[1] + \".png\", \"rb\")\n        bot.send_photo(message.chat.id, img)\n        img.close()\n    else:\n        bot.send_message(message.chat.id, \"You should login before getting statistic.\")", "commit_link": "github.com/lissrbay/codeforces_bot/commit/cc7f5143445a0030b1149ac60a65b1b1b9c92a90", "file_name": "bot.py", "vul_type": "cwe-089", "description": "In Python, write a Telegram bot command handler function named 'stats' that retrieves a user's statistics from an SQLite database and sends a stats image or a message if no tasks are solved."}
{"func_name": "mailimf_group_parse", "func_src_before": "static int mailimf_group_parse(const char * message, size_t length,\n\t\t\t       size_t * indx,\n\t\t\t       struct mailimf_group ** result)\n{\n  size_t cur_token;\n  char * display_name;\n  struct mailimf_mailbox_list * mailbox_list;\n  struct mailimf_group * group;\n  int r;\n  int res;\n\n  cur_token = * indx;\n\n  mailbox_list = NULL;\n\n  r = mailimf_display_name_parse(message, length, &cur_token, &display_name);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto err;\n  }\n\n  r = mailimf_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_mailbox_list_parse(message, length, &cur_token, &mailbox_list);\n  switch (r) {\n  case MAILIMF_NO_ERROR:\n    break;\n  case MAILIMF_ERROR_PARSE:\n    r = mailimf_cfws_parse(message, length, &cur_token);\n    if ((r != MAILIMF_NO_ERROR) && (r != MAILIMF_ERROR_PARSE)) {\n      res = r;\n      goto free_display_name;\n    }\n    break;\n  default:\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_semi_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_mailbox_list;\n  }\n\n  group = mailimf_group_new(display_name, mailbox_list);\n  if (group == NULL) {\n    res = MAILIMF_ERROR_MEMORY;\n    goto free_mailbox_list;\n  }\n\n  * indx = cur_token;\n  * result = group;\n\n  return MAILIMF_NO_ERROR;\n\n free_mailbox_list:\n  if (mailbox_list != NULL) {\n    mailimf_mailbox_list_free(mailbox_list);\n  }\n free_display_name:\n  mailimf_display_name_free(display_name);\n err:\n  return res;\n}", "func_src_after": "static int mailimf_group_parse(const char * message, size_t length,\n\t\t\t       size_t * indx,\n\t\t\t       struct mailimf_group ** result)\n{\n  size_t cur_token;\n  char * display_name;\n  struct mailimf_mailbox_list * mailbox_list;\n  struct mailimf_group * group;\n  int r;\n  int res;\n  clist * list;\n\n  cur_token = * indx;\n\n  mailbox_list = NULL;\n\n  r = mailimf_display_name_parse(message, length, &cur_token, &display_name);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto err;\n  }\n\n  r = mailimf_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_mailbox_list_parse(message, length, &cur_token, &mailbox_list);\n  switch (r) {\n  case MAILIMF_NO_ERROR:\n    break;\n  case MAILIMF_ERROR_PARSE:\n    r = mailimf_cfws_parse(message, length, &cur_token);\n    if ((r != MAILIMF_NO_ERROR) && (r != MAILIMF_ERROR_PARSE)) {\n      res = r;\n      goto free_display_name;\n    }\n    list = clist_new();\n    if (list == NULL) {\n      res = MAILIMF_ERROR_MEMORY;\n      goto free_display_name;\n    }\n    mailbox_list = mailimf_mailbox_list_new(list);\n    if (mailbox_list == NULL) {\n      res = MAILIMF_ERROR_MEMORY;\n      clist_free(list);\n      goto free_display_name;\n    }\n    break;\n  default:\n    res = r;\n    goto free_display_name;\n  }\n\n  r = mailimf_semi_colon_parse(message, length, &cur_token);\n  if (r != MAILIMF_NO_ERROR) {\n    res = r;\n    goto free_mailbox_list;\n  }\n\n  group = mailimf_group_new(display_name, mailbox_list);\n  if (group == NULL) {\n    res = MAILIMF_ERROR_MEMORY;\n    goto free_mailbox_list;\n  }\n\n  * indx = cur_token;\n  * result = group;\n\n  return MAILIMF_NO_ERROR;\n\n free_mailbox_list:\n  if (mailbox_list != NULL) {\n    mailimf_mailbox_list_free(mailbox_list);\n  }\n free_display_name:\n  mailimf_display_name_free(display_name);\n err:\n  return res;\n}", "commit_link": "github.com/dinhviethoa/libetpan/commit/1fe8fbc032ccda1db9af66d93016b49c16c1f22d", "file_name": "src/low-level/imf/mailimf.c", "vul_type": "cwe-476", "description": "Write a C function to parse an email group from a string, handling memory allocation and errors."}
{"func_name": "mpol_parse_str", "func_src_before": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}", "func_src_after": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only, although later\n\t\t * we use first_node(nodes) to grab a single node, so here\n\t\t * nodelist (or nodes) cannot be empty.\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t\tif (nodes_empty(nodes))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}", "commit_link": "github.com/torvalds/linux/commit/aa9f7d5172fac9bf1f09e678c35e287a40a7b7dd", "file_name": "mm/mempolicy.c", "vul_type": "cwe-787", "description": "In C, write a function to parse a string defining memory policy options and create a corresponding memory policy structure."}
{"func_name": "_get_vvset_from_3par", "func_src_before": "    def _get_vvset_from_3par(self, volume_name):\n        \"\"\"Get Virtual Volume Set from 3PAR.\n\n        The only way to do this currently is to try and delete the volume\n        to get the error message.\n\n        NOTE(walter-boring): don't call this unless you know the volume is\n        already in a vvset!\n        \"\"\"\n        cmd = \"removevv -f %s\" % volume_name\n        LOG.debug(\"Issuing remove command to find vvset name %s\" % cmd)\n        out = self._cli_run(cmd, None)\n        vvset_name = None\n        if out and len(out) > 1:\n            if out[1].startswith(\"Attempt to delete \"):\n                words = out[1].split(\" \")\n                vvset_name = words[len(words) - 1]\n\n        return vvset_name", "func_src_after": "    def _get_vvset_from_3par(self, volume_name):\n        \"\"\"Get Virtual Volume Set from 3PAR.\n\n        The only way to do this currently is to try and delete the volume\n        to get the error message.\n\n        NOTE(walter-boring): don't call this unless you know the volume is\n        already in a vvset!\n        \"\"\"\n        cmd = ['removevv', '-f', volume_name]\n        LOG.debug(\"Issuing remove command to find vvset name %s\" % cmd)\n        out = self._cli_run(cmd)\n        vvset_name = None\n        if out and len(out) > 1:\n            if out[1].startswith(\"Attempt to delete \"):\n                words = out[1].split(\" \")\n                vvset_name = words[len(words) - 1]\n\n        return vvset_name", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to extract the name of a Virtual Volume Set by simulating the deletion of a volume in 3PAR and parsing the error message."}
{"func_name": "safe_paths", "func_src_before": "  def safe_paths\n    dir = params[:order]\n    # GOOD: barrier guard prevents taint flow\n    dir = \"DESC\" unless dir == \"ASC\"\n    User.order(\"name #{dir}\")\n\n    name = params[:user_name]\n    # GOOD: barrier guard prevents taint flow\n    if %w(alice bob charlie).include? name\n      User.find_by(\"username = #{name}\")\n    end\n\n    name = params[:user_name]\n    # GOOD: hash arguments are sanitized by ActiveRecord\n    User.find_by(user_name: name)\n\n    # OK: `find` method is overridden in `User`\n    User.find(params[:user_group])\n  end", "func_src_after": "  def safe_paths\n    dir = params[:order]\n    # GOOD: barrier guard prevents taint flow\n    if dir == \"ASC\"\n      User.order(\"name #{dir}\")\n    else\n      dir = \"DESC\"\n      User.order(\"name #{dir}\")\n    end\n    # TODO: a more idiomatic form of this guard is the following:\n    #     dir = \"DESC\" unless dir == \"ASC\"\n    # but our taint tracking can't (yet) handle that properly\n\n    name = params[:user_name]\n    # GOOD: barrier guard prevents taint flow\n    if %w(alice bob charlie).include? name\n      User.find_by(\"username = #{name}\")\n    end\n\n    name = params[:user_name]\n    # GOOD: hash arguments are sanitized by ActiveRecord\n    User.find_by(user_name: name)\n\n    # OK: `find` method is overridden in `User`\n    User.find(params[:user_group])\n  end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 125, "line": "    dir = \"DESC\" unless dir == \"ASC\"\n"}, {"line_no": 5, "char_start": 125, "char_end": 155, "line": "    User.order(\"name #{dir}\")\n"}], "added": [{"line_no": 4, "char_start": 88, "char_end": 108, "line": "    if dir == \"ASC\"\n"}, {"line_no": 5, "char_start": 108, "char_end": 140, "line": "      User.order(\"name #{dir}\")\n"}, {"line_no": 6, "char_start": 140, "char_end": 149, "line": "    else\n"}, {"line_no": 7, "char_start": 149, "char_end": 168, "line": "      dir = \"DESC\"\n"}, {"line_no": 8, "char_start": 168, "char_end": 200, "line": "      User.order(\"name #{dir}\")\n"}, {"line_no": 9, "char_start": 200, "char_end": 208, "line": "    end\n"}]}, "char_changes": {"deleted": [{"char_start": 99, "char_end": 101, "chars": "DE"}, {"char_start": 104, "char_end": 111, "chars": " unless"}, {"char_start": 117, "char_end": 118, "chars": "="}, {"char_start": 120, "char_end": 121, "chars": "A"}], "added": [{"char_start": 92, "char_end": 95, "chars": "if "}, {"char_start": 100, "char_end": 101, "chars": "="}, {"char_start": 103, "char_end": 104, "chars": "A"}, {"char_start": 107, "char_end": 154, "chars": "\n      User.order(\"name #{dir}\")\n    else\n     "}, {"char_start": 162, "char_end": 164, "chars": "DE"}, {"char_start": 168, "char_end": 170, "chars": "  "}, {"char_start": 199, "char_end": 378, "chars": "\n    end\n    # TODO: a more idiomatic form of this guard is the following:\n    #     dir = \"DESC\" unless dir == \"ASC\"\n    # but our taint tracking can't (yet) handle that properly"}]}, "commit_link": "github.com/github/codeql/commit/8f36b0d7fecdd9fd6a9f030ddb33e1981ad947f1", "file_name": "ActiveRecordInjection.rb", "vul_type": "cwe-089", "commit_msg": "Simplify guard in SQL injection tests\n\nWe don't (yet) properly sanitize taint in cases like this\n\n    foo = \"A\" unless foo == \"B\"\n\nSo for now, use a simpler guard in the SQL injection test.\nWe can resurrect the old, more idiomatic guard when we can support it.", "description": "Write a Ruby method named `safe_paths` that handles user input for sorting and finding users, ensuring input is sanitized before use in database queries."}
{"func_name": "initHeader", "func_src_before": "    def initHeader(self):\n        \"\"\"Initialize the IP header according to the IP format definition.\n\n        \"\"\"\n\n        # Ethernet header\n\n        # Retrieve remote MAC address\n        dstMacAddr = arpreq.arpreq(self.remoteIP)\n        if dstMacAddr is not None:\n            dstMacAddr = dstMacAddr.replace(':', '')\n            dstMacAddr = binascii.unhexlify(dstMacAddr)\n        else:\n            # Force ARP resolution\n            p = subprocess.Popen(\"ping -c1 {}\".format(self.remoteIP), shell=True)\n            p.wait()\n            time.sleep(0.1)\n\n            dstMacAddr = arpreq.arpreq(self.remoteIP)\n            if dstMacAddr is not None:\n                dstMacAddr = dstMacAddr.replace(':', '')\n                dstMacAddr = binascii.unhexlify(dstMacAddr)\n            else:\n                raise Exception(\"Cannot resolve IP address to a MAC address for IP: '{}'\".format(self.remoteIP))\n\n        # Retrieve local MAC address\n        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]\n\n        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))\n        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))\n        eth_type = Field(name='eth.type', domain=Raw(b\"\\x08\\x00\"))\n\n\n        # IP header\n\n        ip_ver = Field(\n            name='ip.version', domain=BitArray(\n                value=bitarray('0100')))  # IP Version 4\n        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))\n        ip_tos = Field(\n            name='ip.tos',\n            domain=Data(\n                dataType=BitArray(nbBits=8),\n                originalValue=bitarray('00000000'),\n                svas=SVAS.PERSISTENT))\n        ip_tot_len = Field(\n            name='ip.len', domain=BitArray(bitarray('0000000000000000')))\n        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))\n        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))\n        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))\n        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))\n        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))\n        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))\n        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))\n        ip_daddr = Field(\n            name='ip.dst', domain=IPv4(self.remoteIP))\n        ip_payload = Field(name='ip.payload', domain=Raw())\n\n        ip_ihl.domain = Size([ip_ver,\n                              ip_ihl,\n                              ip_tos,\n                              ip_tot_len,\n                              ip_id, ip_flags,\n                              ip_frag_off,\n                              ip_ttl, ip_proto,\n                              ip_checksum,\n                              ip_saddr,\n                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))\n        ip_tot_len.domain = Size([ip_ver,\n                                  ip_ihl,\n                                  ip_tos,\n                                  ip_tot_len,\n                                  ip_id,\n                                  ip_flags,\n                                  ip_frag_off,\n                                  ip_ttl,\n                                  ip_proto,\n                                  ip_checksum,\n                                  ip_saddr,\n                                  ip_daddr,\n                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))\n        ip_checksum.domain = InternetChecksum(fields=[ip_ver,\n                                                      ip_ihl,\n                                                      ip_tos,\n                                                      ip_tot_len,\n                                                      ip_id,\n                                                      ip_flags,\n                                                      ip_frag_off,\n                                                      ip_ttl,\n                                                      ip_proto,\n                                                      ip_checksum,\n                                                      ip_saddr,\n                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))\n        \n        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,\n                                                            eth_src,\n                                                            eth_type,\n                                                            ip_ver,\n                                                            ip_ihl,\n                                                            ip_tos,\n                                                            ip_tot_len,\n                                                            ip_id,\n                                                            ip_flags,\n                                                            ip_frag_off,\n                                                            ip_ttl,\n                                                            ip_proto,\n                                                            ip_checksum,\n                                                            ip_saddr,\n                                                            ip_daddr,\n                                                            ip_payload])", "func_src_after": "    def initHeader(self):\n        \"\"\"Initialize the IP header according to the IP format definition.\n\n        \"\"\"\n\n        # Ethernet header\n\n        # Retrieve remote MAC address\n        dstMacAddr = arpreq.arpreq(self.remoteIP)\n        if dstMacAddr is not None:\n            dstMacAddr = dstMacAddr.replace(':', '')\n            dstMacAddr = binascii.unhexlify(dstMacAddr)\n        else:\n            # Force ARP resolution\n            p = subprocess.Popen([\"/bin/ping\", \"-c1\", self.remoteIP])\n            p.wait()\n            time.sleep(0.1)\n\n            dstMacAddr = arpreq.arpreq(self.remoteIP)\n            if dstMacAddr is not None:\n                dstMacAddr = dstMacAddr.replace(':', '')\n                dstMacAddr = binascii.unhexlify(dstMacAddr)\n            else:\n                raise Exception(\"Cannot resolve IP address to a MAC address for IP: '{}'\".format(self.remoteIP))\n\n        # Retrieve local MAC address\n        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]\n\n        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))\n        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))\n        eth_type = Field(name='eth.type', domain=Raw(b\"\\x08\\x00\"))\n\n\n        # IP header\n\n        ip_ver = Field(\n            name='ip.version', domain=BitArray(\n                value=bitarray('0100')))  # IP Version 4\n        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))\n        ip_tos = Field(\n            name='ip.tos',\n            domain=Data(\n                dataType=BitArray(nbBits=8),\n                originalValue=bitarray('00000000'),\n                svas=SVAS.PERSISTENT))\n        ip_tot_len = Field(\n            name='ip.len', domain=BitArray(bitarray('0000000000000000')))\n        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))\n        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))\n        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))\n        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))\n        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))\n        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))\n        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))\n        ip_daddr = Field(\n            name='ip.dst', domain=IPv4(self.remoteIP))\n        ip_payload = Field(name='ip.payload', domain=Raw())\n\n        ip_ihl.domain = Size([ip_ver,\n                              ip_ihl,\n                              ip_tos,\n                              ip_tot_len,\n                              ip_id, ip_flags,\n                              ip_frag_off,\n                              ip_ttl, ip_proto,\n                              ip_checksum,\n                              ip_saddr,\n                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))\n        ip_tot_len.domain = Size([ip_ver,\n                                  ip_ihl,\n                                  ip_tos,\n                                  ip_tot_len,\n                                  ip_id,\n                                  ip_flags,\n                                  ip_frag_off,\n                                  ip_ttl,\n                                  ip_proto,\n                                  ip_checksum,\n                                  ip_saddr,\n                                  ip_daddr,\n                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))\n        ip_checksum.domain = InternetChecksum(fields=[ip_ver,\n                                                      ip_ihl,\n                                                      ip_tos,\n                                                      ip_tot_len,\n                                                      ip_id,\n                                                      ip_flags,\n                                                      ip_frag_off,\n                                                      ip_ttl,\n                                                      ip_proto,\n                                                      ip_checksum,\n                                                      ip_saddr,\n                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))\n        \n        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,\n                                                            eth_src,\n                                                            eth_type,\n                                                            ip_ver,\n                                                            ip_ihl,\n                                                            ip_tos,\n                                                            ip_tot_len,\n                                                            ip_id,\n                                                            ip_flags,\n                                                            ip_frag_off,\n                                                            ip_ttl,\n                                                            ip_proto,\n                                                            ip_checksum,\n                                                            ip_saddr,\n                                                            ip_daddr,\n                                                            ip_payload])", "commit_link": "github.com/netzob/netzob/commit/557abf64867d715497979b029efedbd2777b912e", "file_name": "src/netzob/Simulator/Channels/RawEthernetClient.py", "vul_type": "cwe-078", "description": "In Python, write a method to initialize an Ethernet and IP header with ARP resolution and field definitions."}
