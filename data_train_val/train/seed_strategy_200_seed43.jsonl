{"func_name": "do_file_clean", "func_src_before": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "func_src_after": "static void do_file_clean(int flags, char *name)\n{\n\tchar *page;\n\tchar fn[30];\n\tsnprintf(fn, 30, TMPDIR \"~test%d\", tmpcount++);\n\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n\tif (fd < 0)\n\t\terr(\"open temp file\");\n\twrite(fd, fn, 4);\n\tpage = checked_mmap(NULL, PS, PROT_READ|PROT_WRITE, MAP_SHARED|flags,\n\t\tfd, 0);\n\tfsync(fd);\n\tclose(fd);\n\ttestmem(name, page, MREAD_OK);\n\t /* reread page from disk */\n\tprintf(\"\\t reading %x\\n\", *(unsigned char *)page);\n\ttestmem(name, page, MWRITE_OK);\n}", "line_changes": {"deleted": [{"line_no": 6, "char_start": 127, "char_end": 171, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT);\n"}], "added": [{"line_no": 6, "char_start": 127, "char_end": 177, "line": "\tint fd = open(fn, O_RDWR|O_TRUNC|O_CREAT, 0664);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 168, "char_end": 174, "chars": ", 0664"}]}, "commit_link": "github.com/andikleen/mce-test/commit/4da149ab9462e1e8750e2db7a93fa88429eab91a", "file_name": "tinjpage.c", "vul_type": "cwe-732", "commit_msg": "mce-test: Add file attributes to open\n\nThis fixes a compile warning.\n\nopen(2) manpage says:\n... mode specifies the permissions to use in case a new\nfile is created. This argument must be  supplied  when\nO_CREAT is specified  in  flags; ...\n\nSigned-off-by: Thomas Renninger <trenn@suse.de>\nSigned-off-by: Chen Gong <gong.chen@linux.intel.com>", "parent_commit": "cfdf0d9f9564de49ec38c794fa4b151babea98f1", "description": "Write a C function to create a temporary file, map it into memory, perform read/write tests, and print a value from the mapped memory."}
{"func_name": "add_article_action", "func_src_before": "def add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = request.POST[\"notes\"]\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response", "func_src_after": "def add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = escape(request.POST[\"notes\"])\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response", "commit_link": "github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "file_name": "c3shop/frontpage/management/reservation_actions.py", "vul_type": "cwe-079", "description": "Write a Python function to handle adding single or multiple articles to a reservation, with redirection and error handling."}
{"func_name": "UiApplication::SecurityConfiguration::configure", "func_src_before": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n\n            http.headers().httpStrictTransportSecurity();\n            http.csrf().disable();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "func_src_after": "        @Override\n        protected void configure(HttpSecurity http) throws Exception {\n\n            String[] patterns = new String[] {\"/index.html\", \"/home.html\", \"/login1.html\", \"/xss2.html\", \"/\", \"/xss\",\n                \"/login\", \"/xss1.html\", \"/resource\", \"/postcustomer\", \"/getallcustomer\", \"/getinfo\", \"/postxss\"};\n\n            // @formatter:off\n            //http.httpBasic();\n            //http.authorizeRequests().antMatchers(\"/**\").permitAll(); //.anyRequest().authenticated();\n            //http.csrf().disable();\n\n\n            http.headers().httpStrictTransportSecurity();\n            http.httpBasic().and().authorizeRequests().antMatchers(patterns).permitAll().anyRequest().authenticated();\n            http.csrf().and().addFilterAfter(new CsrfGrantingFilter(), SessionManagementFilter.class);\n            //http.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n\n            // @formatter:on\n        }", "line_changes": {"deleted": [{"line_no": 12, "char_start": 548, "char_end": 583, "line": "            http.csrf().disable();\n"}], "added": [{"line_no": 11, "char_start": 526, "char_end": 527, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 548, "char_end": 583, "chars": "            http.csrf().disable();\n"}], "added": [{"char_start": 489, "char_end": 527, "chars": "            //http.csrf().disable();\n\n"}]}, "commit_link": "github.com/barbaraisabelvieira/VulnerableDemoApp/commit/a64bbb524722f63134826a7d0424d71518a0aae6", "file_name": "UiApplication.java", "vul_type": "cwe-352", "commit_msg": "Fixing CSRF - enabled now", "parent_commit": "1faf96211c58f44a8510497e08d31757160c6367", "description": "Write a Java method using Spring Security to configure HTTP security, allowing unrestricted access to specific URL patterns and requiring authentication for all other requests."}
{"func_name": "openscript", "func_src_before": "openscript(\n    char_u\t*name,\n    int\t\tdirectly)\t/* when TRUE execute directly */\n{\n    if (curscript + 1 == NSCRIPT)\n    {\n\temsg(_(e_nesting));\n\treturn;\n    }\n#ifdef FEAT_EVAL\n    if (ignore_script)\n\t/* Not reading from script, also don't open one.  Warning message? */\n\treturn;\n#endif\n\n    if (scriptin[curscript] != NULL)\t/* already reading script */\n\t++curscript;\n\t\t\t\t/* use NameBuff for expanded name */\n    expand_env(name, NameBuff, MAXPATHL);\n    if ((scriptin[curscript] = mch_fopen((char *)NameBuff, READBIN)) == NULL)\n    {\n\tsemsg(_(e_notopen), name);\n\tif (curscript)\n\t    --curscript;\n\treturn;\n    }\n    if (save_typebuf() == FAIL)\n\treturn;\n\n    /*\n     * Execute the commands from the file right now when using \":source!\"\n     * after \":global\" or \":argdo\" or in a loop.  Also when another command\n     * follows.  This means the display won't be updated.  Don't do this\n     * always, \"make test\" would fail.\n     */\n    if (directly)\n    {\n\toparg_T\toa;\n\tint\toldcurscript;\n\tint\tsave_State = State;\n\tint\tsave_restart_edit = restart_edit;\n\tint\tsave_insertmode = p_im;\n\tint\tsave_finish_op = finish_op;\n\tint\tsave_msg_scroll = msg_scroll;\n\n\tState = NORMAL;\n\tmsg_scroll = FALSE;\t/* no msg scrolling in Normal mode */\n\trestart_edit = 0;\t/* don't go to Insert mode */\n\tp_im = FALSE;\t\t/* don't use 'insertmode' */\n\tclear_oparg(&oa);\n\tfinish_op = FALSE;\n\n\toldcurscript = curscript;\n\tdo\n\t{\n\t    update_topline_cursor();\t// update cursor position and topline\n\t    normal_cmd(&oa, FALSE);\t// execute one command\n\t    vpeekc();\t\t\t// check for end of file\n\t}\n\twhile (scriptin[oldcurscript] != NULL);\n\n\tState = save_State;\n\tmsg_scroll = save_msg_scroll;\n\trestart_edit = save_restart_edit;\n\tp_im = save_insertmode;\n\tfinish_op = save_finish_op;\n    }\n}", "func_src_after": "openscript(\n    char_u\t*name,\n    int\t\tdirectly)\t/* when TRUE execute directly */\n{\n    if (curscript + 1 == NSCRIPT)\n    {\n\temsg(_(e_nesting));\n\treturn;\n    }\n\n    // Disallow sourcing a file in the sandbox, the commands would be executed\n    // later, possibly outside of the sandbox.\n    if (check_secure())\n\treturn;\n\n#ifdef FEAT_EVAL\n    if (ignore_script)\n\t/* Not reading from script, also don't open one.  Warning message? */\n\treturn;\n#endif\n\n    if (scriptin[curscript] != NULL)\t/* already reading script */\n\t++curscript;\n\t\t\t\t/* use NameBuff for expanded name */\n    expand_env(name, NameBuff, MAXPATHL);\n    if ((scriptin[curscript] = mch_fopen((char *)NameBuff, READBIN)) == NULL)\n    {\n\tsemsg(_(e_notopen), name);\n\tif (curscript)\n\t    --curscript;\n\treturn;\n    }\n    if (save_typebuf() == FAIL)\n\treturn;\n\n    /*\n     * Execute the commands from the file right now when using \":source!\"\n     * after \":global\" or \":argdo\" or in a loop.  Also when another command\n     * follows.  This means the display won't be updated.  Don't do this\n     * always, \"make test\" would fail.\n     */\n    if (directly)\n    {\n\toparg_T\toa;\n\tint\toldcurscript;\n\tint\tsave_State = State;\n\tint\tsave_restart_edit = restart_edit;\n\tint\tsave_insertmode = p_im;\n\tint\tsave_finish_op = finish_op;\n\tint\tsave_msg_scroll = msg_scroll;\n\n\tState = NORMAL;\n\tmsg_scroll = FALSE;\t/* no msg scrolling in Normal mode */\n\trestart_edit = 0;\t/* don't go to Insert mode */\n\tp_im = FALSE;\t\t/* don't use 'insertmode' */\n\tclear_oparg(&oa);\n\tfinish_op = FALSE;\n\n\toldcurscript = curscript;\n\tdo\n\t{\n\t    update_topline_cursor();\t// update cursor position and topline\n\t    normal_cmd(&oa, FALSE);\t// execute one command\n\t    vpeekc();\t\t\t// check for end of file\n\t}\n\twhile (scriptin[oldcurscript] != NULL);\n\n\tState = save_State;\n\tmsg_scroll = save_msg_scroll;\n\trestart_edit = save_restart_edit;\n\tp_im = save_insertmode;\n\tfinish_op = save_finish_op;\n    }\n}", "commit_link": "github.com/vim/vim/commit/53575521406739cf20bbe4e384d88e7dca11f040", "file_name": "src/getchar.c", "vul_type": "cwe-078", "description": "In C, write a function `openscript` that takes a script name and a flag to execute directly, handling script nesting and environment expansion."}
{"func_name": "mpeg4video_probe", "func_src_before": "static int mpeg4video_probe(AVProbeData *probe_packet)\n{\n    uint32_t temp_buffer = -1;\n    int VO = 0, VOL = 0, VOP = 0, VISO = 0, res = 0;\n    int i;\n\n    for (i = 0; i < probe_packet->buf_size; i++) {\n        temp_buffer = (temp_buffer << 8) + probe_packet->buf[i];\n        if ((temp_buffer & 0xffffff00) != 0x100)\n            continue;\n\n        if (temp_buffer == VOP_START_CODE)\n            VOP++;\n        else if (temp_buffer == VISUAL_OBJECT_START_CODE)\n            VISO++;\n        else if (temp_buffer < 0x120)\n            VO++;\n        else if (temp_buffer < 0x130)\n            VOL++;\n        else if (!(0x1AF < temp_buffer && temp_buffer < 0x1B7) &&\n                 !(0x1B9 < temp_buffer && temp_buffer < 0x1C4))\n            res++;\n    }\n\n    if (VOP >= VISO && VOP >= VOL && VO >= VOL && VOL > 0 && res == 0)\n        return AVPROBE_SCORE_EXTENSION;\n    return 0;\n}", "func_src_after": "static int mpeg4video_probe(AVProbeData *probe_packet)\n{\n    uint32_t temp_buffer = -1;\n    int VO = 0, VOL = 0, VOP = 0, VISO = 0, res = 0;\n    int i;\n\n    for (i = 0; i < probe_packet->buf_size; i++) {\n        temp_buffer = (temp_buffer << 8) + probe_packet->buf[i];\n        if (temp_buffer & 0xfffffe00)\n            continue;\n        if (temp_buffer < 2)\n            continue;\n\n        if (temp_buffer == VOP_START_CODE)\n            VOP++;\n        else if (temp_buffer == VISUAL_OBJECT_START_CODE)\n            VISO++;\n        else if (temp_buffer >= 0x100 && temp_buffer < 0x120)\n            VO++;\n        else if (temp_buffer >= 0x120 && temp_buffer < 0x130)\n            VOL++;\n        else if (!(0x1AF < temp_buffer && temp_buffer < 0x1B7) &&\n                 !(0x1B9 < temp_buffer && temp_buffer < 0x1C4))\n            res++;\n    }\n\n    if (VOP >= VISO && VOP >= VOL && VO >= VOL && VOL > 0 && res == 0)\n        return AVPROBE_SCORE_EXTENSION;\n    return 0;\n}", "commit_link": "github.com/libav/libav/commit/e5b019725f53b79159931d3a7317107cbbfd0860", "file_name": "libavformat/m4vdec.c", "vul_type": "cwe-476", "description": "Write a C function named `mpeg4video_probe` that checks for MPEG-4 video signatures in a buffer and returns a score based on specific criteria."}
{"func_name": "add_empty_commit", "func_src_before": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        os.system(f'git clone {folder} {tmpdirname}')\n        os.chdir(tmpdirname)\n        os.system(f'git checkout -b {which_branch}')\n        os.system(\"git \" +\n                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n                  \"--allow-empty -m 'initial empty commit' \" +\n                  \"--author='submitty <submitty@example.com>'\")\n        os.system(f'git push origin {which_branch}')\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "func_src_after": "def add_empty_commit(folder,which_branch):\n\n    assert (os.path.isdir(folder))\n    os.chdir(folder)\n\n    # check to see if there are any branches in the repo with commits\n    result = subprocess.run(['git', 'branch', '-v'], stdout=subprocess.PIPE)\n    s = result.stdout.decode('utf-8')\n    if s != \"\":\n        # do nothing if there is at least one branch with a commit\n        print('NOTE: this repo is non-empty (has a commit on at least one branch)')\n        return\n\n    # otherwise clone to a non-bare repo and add an empty commit\n    # to the specified branch\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        subprocess.run(['git', 'clone', folder, tmpdirname])\n        os.chdir(tmpdirname)\n        subprocess.run(['git', 'checkout', '-b', which_branch])\n        subprocess.run(['git',\n            '-c', 'user.name=submitty',\n            '-c', 'user.email=submitty@example.com',\n            'commit', '--allow-empty',\n            '-m', 'initial empty commit',\n            '--author=submitty <submitty@example.com>'])\n        subprocess.run(['git', 'push', 'origin', which_branch])\n\n    print(f'Made new empty commit on branch {which_branch} in repo {folder}')", "line_changes": {"deleted": [{"line_no": 17, "char_start": 618, "char_end": 672, "line": "        os.system(f'git clone {folder} {tmpdirname}')\n"}, {"line_no": 19, "char_start": 701, "char_end": 754, "line": "        os.system(f'git checkout -b {which_branch}')\n"}, {"line_no": 20, "char_start": 754, "char_end": 781, "line": "        os.system(\"git \" +\n"}, {"line_no": 21, "char_start": 781, "char_end": 868, "line": "                  \"-c user.name=submitty -c user.email=submitty@example.com commit \" +\n"}, {"line_no": 22, "char_start": 868, "char_end": 931, "line": "                  \"--allow-empty -m 'initial empty commit' \" +\n"}, {"line_no": 23, "char_start": 931, "char_end": 995, "line": "                  \"--author='submitty <submitty@example.com>'\")\n"}, {"line_no": 24, "char_start": 995, "char_end": 1048, "line": "        os.system(f'git push origin {which_branch}')\n"}], "added": [{"line_no": 17, "char_start": 618, "char_end": 679, "line": "        subprocess.run(['git', 'clone', folder, tmpdirname])\n"}, {"line_no": 19, "char_start": 708, "char_end": 772, "line": "        subprocess.run(['git', 'checkout', '-b', which_branch])\n"}, {"line_no": 20, "char_start": 772, "char_end": 803, "line": "        subprocess.run(['git',\n"}, {"line_no": 21, "char_start": 803, "char_end": 843, "line": "            '-c', 'user.name=submitty',\n"}, {"line_no": 22, "char_start": 843, "char_end": 896, "line": "            '-c', 'user.email=submitty@example.com',\n"}, {"line_no": 23, "char_start": 896, "char_end": 935, "line": "            'commit', '--allow-empty',\n"}, {"line_no": 24, "char_start": 935, "char_end": 977, "line": "            '-m', 'initial empty commit',\n"}, {"line_no": 25, "char_start": 977, "char_end": 1034, "line": "            '--author=submitty <submitty@example.com>'])\n"}, {"line_no": 26, "char_start": 1034, "char_end": 1098, "line": "        subprocess.run(['git', 'push', 'origin', which_branch])\n"}]}, "char_changes": {"deleted": [{"char_start": 626, "char_end": 642, "chars": "os.system(f'git "}, {"char_start": 648, "char_end": 649, "chars": "{"}, {"char_start": 655, "char_end": 658, "chars": "} {"}, {"char_start": 668, "char_end": 670, "chars": "}'"}, {"char_start": 709, "char_end": 738, "chars": "os.system(f'git checkout -b {"}, {"char_start": 750, "char_end": 752, "chars": "}'"}, {"char_start": 762, "char_end": 785, "chars": "os.system(\"git \" +\n    "}, {"char_start": 797, "char_end": 803, "chars": "  \"-c "}, {"char_start": 821, "char_end": 825, "chars": " -c "}, {"char_start": 856, "char_end": 867, "chars": " commit \" +"}, {"char_start": 880, "char_end": 887, "chars": "      \""}, {"char_start": 900, "char_end": 903, "chars": " -m"}, {"char_start": 926, "char_end": 934, "chars": " \" +\n   "}, {"char_start": 946, "char_end": 950, "chars": "   \""}, {"char_start": 959, "char_end": 960, "chars": "'"}, {"char_start": 992, "char_end": 993, "chars": "\""}, {"char_start": 1003, "char_end": 1024, "chars": "os.system(f'git push "}, {"char_start": 1031, "char_end": 1032, "chars": "{"}, {"char_start": 1044, "char_end": 1046, "chars": "}'"}], "added": [{"char_start": 626, "char_end": 650, "chars": "subprocess.run(['git', '"}, {"char_start": 655, "char_end": 657, "chars": "',"}, {"char_start": 664, "char_end": 666, "chars": ", "}, {"char_start": 676, "char_end": 677, "chars": "]"}, {"char_start": 716, "char_end": 757, "chars": "subprocess.run(['git', 'checkout', '-b', "}, {"char_start": 769, "char_end": 770, "chars": "]"}, {"char_start": 780, "char_end": 802, "chars": "subprocess.run(['git',"}, {"char_start": 815, "char_end": 822, "chars": "'-c', '"}, {"char_start": 840, "char_end": 862, "chars": "',\n            '-c', '"}, {"char_start": 893, "char_end": 896, "chars": "',\n"}, {"char_start": 908, "char_end": 919, "chars": "'commit', '"}, {"char_start": 932, "char_end": 952, "chars": "',\n            '-m',"}, {"char_start": 975, "char_end": 976, "chars": ","}, {"char_start": 989, "char_end": 990, "chars": "'"}, {"char_start": 1031, "char_end": 1032, "chars": "]"}, {"char_start": 1042, "char_end": 1074, "chars": "subprocess.run(['git', 'push', '"}, {"char_start": 1080, "char_end": 1082, "chars": "',"}, {"char_start": 1095, "char_end": 1096, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to create an empty commit on a specified branch in a Git repository if the repository has no commits."}
{"func_name": "build_filter_params", "func_src_before": "  def build_filter_params\n    @conditions = \"state in('published', 'withdrawn')\"\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions += \" AND published = #{@search[:published].to_i}\"\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "func_src_after": "  def build_filter_params\n    @conditions = [\"state in('published', 'withdrawn')\"]\n    if params[:search]\n      @search = params[:search]\n\n      if @search[:published_at] and %r{(\\d\\d\\d\\d)-(\\d\\d)} =~ @search[:published_at]\n        @conditions[0] += \" AND published_at LIKE ? \"\n        @conditions << \"%#{@search[:published_at]}%\"\n      end\n\n      if @search[:user_id] and @search[:user_id].to_i > 0\n        @conditions[0] += \" AND user_id = ? \"\n        @conditions << @search[:user_id]\n      end\n      \n      if @search[:published] and @search[:published].to_s =~ /0|1/\n        @conditions[0] += \" AND published = ? \"\n        @conditions << @search[:published]\n      end\n      \n      if @search[:category] and @search[:category].to_i > 0\n        @conditions[0] += \" AND categorizations.category_id = ? \"\n        @conditions << @search[:category]\n      end\n  \n    else\n      @search = { :category => nil, :user_id => nil, :published_at => nil, :published => nil }\n    end    \n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 81, "line": "    @conditions = \"state in('published', 'withdrawn')\"\n"}, {"line_no": 7, "char_start": 221, "char_end": 299, "line": "        @conditions += \" AND published_at LIKE '%#{@search[:published_at]}%'\"\n"}, {"line_no": 11, "char_start": 368, "char_end": 434, "line": "        @conditions += \" AND user_id = #{@search[:user_id].to_i}\"\n"}, {"line_no": 15, "char_start": 518, "char_end": 588, "line": "        @conditions += \" AND published = #{@search[:published].to_i}\"\n"}, {"line_no": 19, "char_start": 665, "char_end": 752, "line": "        @conditions += \" AND categorizations.category_id = #{@search[:category].to_i}\"\n"}], "added": [{"line_no": 2, "char_start": 26, "char_end": 83, "line": "    @conditions = [\"state in('published', 'withdrawn')\"]\n"}, {"line_no": 7, "char_start": 223, "char_end": 277, "line": "        @conditions[0] += \" AND published_at LIKE ? \"\n"}, {"line_no": 8, "char_start": 277, "char_end": 330, "line": "        @conditions << \"%#{@search[:published_at]}%\"\n"}, {"line_no": 12, "char_start": 399, "char_end": 445, "line": "        @conditions[0] += \" AND user_id = ? \"\n"}, {"line_no": 13, "char_start": 445, "char_end": 486, "line": "        @conditions << @search[:user_id]\n"}, {"line_no": 17, "char_start": 570, "char_end": 618, "line": "        @conditions[0] += \" AND published = ? \"\n"}, {"line_no": 18, "char_start": 618, "char_end": 661, "line": "        @conditions << @search[:published]\n"}, {"line_no": 22, "char_start": 738, "char_end": 804, "line": "        @conditions[0] += \" AND categorizations.category_id = ? \"\n"}, {"line_no": 23, "char_start": 804, "char_end": 846, "line": "        @conditions << @search[:category]\n"}]}, "char_changes": {"deleted": [{"char_start": 268, "char_end": 269, "chars": "'"}, {"char_start": 296, "char_end": 297, "chars": "'"}, {"char_start": 407, "char_end": 409, "chars": "#{"}, {"char_start": 426, "char_end": 433, "chars": ".to_i}\""}, {"char_start": 559, "char_end": 561, "chars": "#{"}, {"char_start": 580, "char_end": 587, "chars": ".to_i}\""}, {"char_start": 724, "char_end": 726, "chars": "#{"}, {"char_start": 744, "char_end": 751, "chars": ".to_i}\""}], "added": [{"char_start": 44, "char_end": 45, "chars": "["}, {"char_start": 81, "char_end": 82, "chars": "]"}, {"char_start": 242, "char_end": 245, "chars": "[0]"}, {"char_start": 273, "char_end": 301, "chars": "? \"\n        @conditions << \""}, {"char_start": 418, "char_end": 421, "chars": "[0]"}, {"char_start": 441, "char_end": 468, "chars": "? \"\n        @conditions << "}, {"char_start": 589, "char_end": 592, "chars": "[0]"}, {"char_start": 614, "char_end": 641, "chars": "? \"\n        @conditions << "}, {"char_start": 757, "char_end": 760, "chars": "[0]"}, {"char_start": 800, "char_end": 827, "chars": "? \"\n        @conditions << "}]}, "commit_link": "github.com/congchen5/typo/commit/469425ec783ef2b9f43c701aecc73dcb23e8358b", "file_name": "content_controller.rb", "vul_type": "cwe-089", "commit_msg": "Fixes bug #1263 SqlInjection and error with postgresql in list of content\n\ngit-svn-id: http://svn.typosphere.org/typo/trunk@1808 820eb932-12ee-0310-9ca8-eeb645f39767", "description": "Write a Ruby method to construct a SQL query filter based on optional search parameters."}
{"func_name": "mif_process_cmpt", "func_src_before": "static int mif_process_cmpt(mif_hdr_t *hdr, char *buf)\n{\n\tjas_tvparser_t *tvp;\n\tmif_cmpt_t *cmpt;\n\tint id;\n\n\tcmpt = 0;\n\ttvp = 0;\n\n\tif (!(cmpt = mif_cmpt_create())) {\n\t\tgoto error;\n\t}\n\tcmpt->tlx = 0;\n\tcmpt->tly = 0;\n\tcmpt->sampperx = 0;\n\tcmpt->samppery = 0;\n\tcmpt->width = 0;\n\tcmpt->height = 0;\n\tcmpt->prec = 0;\n\tcmpt->sgnd = -1;\n\tcmpt->data = 0;\n\n\tif (!(tvp = jas_tvparser_create(buf))) {\n\t\tgoto error;\n\t}\n\twhile (!(id = jas_tvparser_next(tvp))) {\n\t\tswitch (jas_taginfo_nonull(jas_taginfos_lookup(mif_tags,\n\t\t  jas_tvparser_gettag(tvp)))->id) {\n\t\tcase MIF_TLX:\n\t\t\tcmpt->tlx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_TLY:\n\t\t\tcmpt->tly = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_WIDTH:\n\t\t\tcmpt->width = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HEIGHT:\n\t\t\tcmpt->height = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HSAMP:\n\t\t\tcmpt->sampperx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_VSAMP:\n\t\t\tcmpt->samppery = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_PREC:\n\t\t\tcmpt->prec = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_SGND:\n\t\t\tcmpt->sgnd = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_DATA:\n\t\t\tif (!(cmpt->data = jas_strdup(jas_tvparser_getval(tvp)))) {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tjas_tvparser_destroy(tvp);\n\tif (!cmpt->sampperx || !cmpt->samppery) {\n\t\tgoto error;\n\t}\n\tif (mif_hdr_addcmpt(hdr, hdr->numcmpts, cmpt)) {\n\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tif (cmpt) {\n\t\tmif_cmpt_destroy(cmpt);\n\t}\n\tif (tvp) {\n\t\tjas_tvparser_destroy(tvp);\n\t}\n\treturn -1;\n}", "func_src_after": "static int mif_process_cmpt(mif_hdr_t *hdr, char *buf)\n{\n\tjas_tvparser_t *tvp;\n\tmif_cmpt_t *cmpt;\n\tint id;\n\n\tcmpt = 0;\n\ttvp = 0;\n\n\tif (!(cmpt = mif_cmpt_create())) {\n\t\tgoto error;\n\t}\n\tcmpt->tlx = 0;\n\tcmpt->tly = 0;\n\tcmpt->sampperx = 0;\n\tcmpt->samppery = 0;\n\tcmpt->width = 0;\n\tcmpt->height = 0;\n\tcmpt->prec = 0;\n\tcmpt->sgnd = -1;\n\tcmpt->data = 0;\n\n\tif (!(tvp = jas_tvparser_create(buf))) {\n\t\tgoto error;\n\t}\n\twhile (!(id = jas_tvparser_next(tvp))) {\n\t\tswitch (jas_taginfo_nonull(jas_taginfos_lookup(mif_tags,\n\t\t  jas_tvparser_gettag(tvp)))->id) {\n\t\tcase MIF_TLX:\n\t\t\tcmpt->tlx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_TLY:\n\t\t\tcmpt->tly = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_WIDTH:\n\t\t\tcmpt->width = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HEIGHT:\n\t\t\tcmpt->height = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_HSAMP:\n\t\t\tcmpt->sampperx = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_VSAMP:\n\t\t\tcmpt->samppery = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_PREC:\n\t\t\tcmpt->prec = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_SGND:\n\t\t\tcmpt->sgnd = atoi(jas_tvparser_getval(tvp));\n\t\t\tbreak;\n\t\tcase MIF_DATA:\n\t\t\tif (!(cmpt->data = jas_strdup(jas_tvparser_getval(tvp)))) {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!cmpt->sampperx || !cmpt->samppery) {\n\t\tgoto error;\n\t}\n\tif (mif_hdr_addcmpt(hdr, hdr->numcmpts, cmpt)) {\n\t\tgoto error;\n\t}\n\tjas_tvparser_destroy(tvp);\n\treturn 0;\n\nerror:\n\tif (cmpt) {\n\t\tmif_cmpt_destroy(cmpt);\n\t}\n\tif (tvp) {\n\t\tjas_tvparser_destroy(tvp);\n\t}\n\treturn -1;\n}", "commit_link": "github.com/mdadams/jasper/commit/df5d2867e8004e51e18b89865bc4aa69229227b3", "file_name": "src/libjasper/mif/mif_cod.c", "vul_type": "cwe-416", "description": "Write a C function to parse component information from a buffer and add it to a MIF header, handling errors appropriately."}
{"func_name": "gps_tracker", "func_src_before": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "func_src_after": "void gps_tracker( void )\n{\n\tssize_t unused;\n    int gpsd_sock;\n    char line[256], *temp;\n    struct sockaddr_in gpsd_addr;\n    int ret, is_json, pos;\n    fd_set read_fd;\n    struct timeval timeout;\n\n    /* attempt to connect to localhost, port 2947 */\n\n    pos = 0;\n    gpsd_sock = socket( AF_INET, SOCK_STREAM, 0 );\n\n    if( gpsd_sock < 0 ) {\n        return;\n    }\n\n    gpsd_addr.sin_family      = AF_INET;\n    gpsd_addr.sin_port        = htons( 2947 );\n    gpsd_addr.sin_addr.s_addr = inet_addr( \"127.0.0.1\" );\n\n    if( connect( gpsd_sock, (struct sockaddr *) &gpsd_addr,\n                 sizeof( gpsd_addr ) ) < 0 ) {\n        return;\n    }\n\n    // Check if it's GPSd < 2.92 or the new one\n    // 2.92+ immediately send stuff\n    // < 2.92 requires to send PVTAD command\n    FD_ZERO(&read_fd);\n    FD_SET(gpsd_sock, &read_fd);\n    timeout.tv_sec = 1;\n    timeout.tv_usec = 0;\n    is_json = select(gpsd_sock + 1, &read_fd, NULL, NULL, &timeout);\n    if (is_json) {\n    \t/*\n\t\t\t{\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n\t\t\t?WATCH={\"json\":true};\n\t\t\t{\"class\":\"DEVICES\",\"devices\":[]}\n    \t */\n\n\n    \t// Get the crap and ignore it: {\"class\":\"VERSION\",\"release\":\"2.95\",\"rev\":\"2010-11-16T21:12:35\",\"proto_major\":3,\"proto_minor\":3}\n    \tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n    \t\treturn;\n\n    \tis_json = (line[0] == '{');\n    \tif (is_json) {\n\t\t\t// Send ?WATCH={\"json\":true};\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tstrcpy(line, \"?WATCH={\\\"json\\\":true};\\n\");\n\t\t\tif( send( gpsd_sock, line, 22, 0 ) != 22 )\n\t\t\t\treturn;\n\n\t\t\t// Check that we have devices\n\t\t\tmemset(line, 0, sizeof(line));\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\t// Stop processing if there is no device\n\t\t\tif (strncmp(line, \"{\\\"class\\\":\\\"DEVICES\\\",\\\"devices\\\":[]}\", 32) == 0) {\n\t\t\t\tclose(gpsd_sock);\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpos = strlen(line);\n\t\t\t}\n    \t}\n    }\n\n    /* loop reading the GPS coordinates */\n\n    while( G.do_exit == 0 )\n    {\n        usleep( 500000 );\n        memset( G.gps_loc, 0, sizeof( float ) * 5 );\n\n        /* read position, speed, heading, altitude */\n        if (is_json) {\n        \t// Format definition: http://catb.org/gpsd/gpsd_json.html\n\n        \tif (pos == sizeof( line )) {\n        \t\tmemset(line, 0, sizeof(line));\n        \t\tpos = 0;\n        \t}\n\n        \t// New version, JSON\n        \tif( recv( gpsd_sock, line + pos, sizeof( line ) - pos - 1, 0 ) <= 0 )\n        \t\treturn;\n\n        \t// search for TPV class: {\"class\":\"TPV\"\n        \ttemp = strstr(line, \"{\\\"class\\\":\\\"TPV\\\"\");\n        \tif (temp == NULL) {\n        \t\tcontinue;\n        \t}\n\n        \t// Make sure the data we have is complete\n        \tif (strchr(temp, '}') == NULL) {\n        \t\t// Move the data at the beginning of the buffer;\n        \t\tpos = strlen(temp);\n        \t\tif (temp != line) {\n        \t\t\tmemmove(line, temp, pos);\n        \t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n        \t\t}\n        \t}\n\n\t\t\t// Example line: {\"class\":\"TPV\",\"tag\":\"MID2\",\"device\":\"/dev/ttyUSB0\",\"time\":1350957517.000,\"ept\":0.005,\"lat\":46.878936576,\"lon\":-115.832602964,\"alt\":1968.382,\"track\":0.0000,\"speed\":0.000,\"climb\":0.000,\"mode\":3}\n\n        \t// Latitude\n        \ttemp = strstr(temp, \"\\\"lat\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[0]);\n\n\t\t\t// Longitude\n\t\t\ttemp = strstr(temp, \"\\\"lon\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[1]);\n\n\t\t\t// Altitude\n\t\t\ttemp = strstr(temp, \"\\\"alt\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[4]);\n\n\t\t\t// Speed\n\t\t\ttemp = strstr(temp, \"\\\"speed\\\":\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = sscanf(temp + 6, \"%f\", &G.gps_loc[2]);\n\n\t\t\t// No more heading\n\n\t\t\t// Get the next TPV class\n\t\t\ttemp = strstr(temp, \"{\\\"class\\\":\\\"TPV\\\"\");\n\t\t\tif (temp == NULL) {\n\t\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\t\tpos = 0;\n\t\t\t} else {\n\t\t\t\tpos = strlen(temp);\n\t\t\t\tmemmove(line, temp, pos);\n\t\t\t\tmemset(line + pos, 0, sizeof(line) - pos);\n\t\t\t}\n\n        } else {\n        \tmemset( line, 0, sizeof( line ) );\n\n\t\t\tsnprintf( line,  sizeof( line ) - 1, \"PVTAD\\r\\n\" );\n\t\t\tif( send( gpsd_sock, line, 7, 0 ) != 7 )\n\t\t\t\treturn;\n\n\t\t\tmemset( line, 0, sizeof( line ) );\n\t\t\tif( recv( gpsd_sock, line, sizeof( line ) - 1, 0 ) <= 0 )\n\t\t\t\treturn;\n\n\t\t\tif( memcmp( line, \"GPSD,P=\", 7 ) != 0 )\n\t\t\t\tcontinue;\n\n\t\t\t/* make sure the coordinates are present */\n\n\t\t\tif( line[7] == '?' )\n\t\t\t\tcontinue;\n\n\t\t\tret = sscanf( line + 7, \"%f %f\", &G.gps_loc[0], &G.gps_loc[1] );\n\n\t\t\tif( ( temp = strstr( line, \"V=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[2] ); /* speed */\n\n\t\t\tif( ( temp = strstr( line, \"T=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[3] ); /* heading */\n\n\t\t\tif( ( temp = strstr( line, \"A=\" ) ) == NULL ) continue;\n\t\t\tret = sscanf( temp + 2, \"%f\", &G.gps_loc[4] ); /* altitude */\n        }\n\n        if (G.record_data)\n\t\t\tfputs( line, G.f_gps );\n\n\t\tG.save_gps = 1;\n\n        if (G.do_exit == 0)\n\t\t{\n\t\t\tunused = write( G.gc_pipe[1], G.gps_loc, sizeof( float ) * 5 );\n\t\t\tkill( getppid(), SIGUSR2 );\n\t\t}\n    }\n}", "commit_link": "github.com/aircrack-ng/aircrack-ng/commit/ff70494dd389ba570dbdbf36f217c28d4381c6b5/", "file_name": "src/airodump-ng.c", "vul_type": "cwe-787", "description": "Write a C function named `gps_tracker` that connects to a GPS daemon on localhost and reads GPS coordinates in a loop."}
{"func_name": "ReadMATImage", "func_src_before": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "func_src_after": "static Image *ReadMATImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image *image, *image2=NULL,\n   *rotated_image;\n  PixelPacket *q;\n\n  unsigned int status;\n  MATHeader MATLAB_HDR;\n  size_t size;\n  size_t CellType;\n  QuantumInfo *quantum_info;\n  ImageInfo *clone_info;\n  int i;\n  ssize_t ldblk;\n  unsigned char *BImgBuff = NULL;\n  double MinVal, MaxVal;\n  size_t Unknown6;\n  unsigned z, z2;\n  unsigned Frames;\n  int logging;\n  int sample_size;\n  MagickOffsetType filepos=0x80;\n  BlobInfo *blob;\n  size_t one;\n\n  unsigned int (*ReadBlobXXXLong)(Image *image);\n  unsigned short (*ReadBlobXXXShort)(Image *image);\n  void (*ReadBlobDoublesXXX)(Image * image, size_t len, double *data);\n  void (*ReadBlobFloatsXXX)(Image * image, size_t len, float *data);\n\n\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  logging = LogMagickEvent(CoderEvent,GetMagickModule(),\"enter\");\n\n  /*\n     Open image file.\n   */\n  image = AcquireImage(image_info);\n\n  status = OpenBlob(image_info, image, ReadBinaryBlobMode, exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n     Read MATLAB image.\n   */\n  clone_info=CloneImageInfo(image_info);\n  if(ReadBlob(image,124,(unsigned char *) &MATLAB_HDR.identific) != 124)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  MATLAB_HDR.Version = ReadBlobLSBShort(image);\n  if(ReadBlob(image,2,(unsigned char *) &MATLAB_HDR.EndianIndicator) != 2)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"  Endian %c%c\",\n        MATLAB_HDR.EndianIndicator[0],MATLAB_HDR.EndianIndicator[1]);\n  if (!strncmp(MATLAB_HDR.EndianIndicator, \"IM\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobLSBLong;\n    ReadBlobXXXShort = ReadBlobLSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesLSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsLSB;\n    image->endian = LSBEndian;\n  }\n  else if (!strncmp(MATLAB_HDR.EndianIndicator, \"MI\", 2))\n  {\n    ReadBlobXXXLong = ReadBlobMSBLong;\n    ReadBlobXXXShort = ReadBlobMSBShort;\n    ReadBlobDoublesXXX = ReadBlobDoublesMSB;\n    ReadBlobFloatsXXX = ReadBlobFloatsMSB;\n    image->endian = MSBEndian;\n  }\n  else\n    goto MATLAB_KO;    /* unsupported endian */\n\n  if (strncmp(MATLAB_HDR.identific, \"MATLAB\", 6))\nMATLAB_KO: ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n\n  filepos = TellBlob(image);\n  while(!EOFBlob(image)) /* object parser loop */\n  {\n    Frames = 1;\n    (void) SeekBlob(image,filepos,SEEK_SET);\n    /* printf(\"pos=%X\\n\",TellBlob(image)); */\n\n    MATLAB_HDR.DataType = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    MATLAB_HDR.ObjectSize = ReadBlobXXXLong(image);\n    if(EOFBlob(image)) break;\n    filepos += MATLAB_HDR.ObjectSize + 4 + 4;\n\n    image2 = image;\n#if defined(MAGICKCORE_ZLIB_DELEGATE)\n    if(MATLAB_HDR.DataType == miCOMPRESSED)\n    {\n      image2 = DecompressBlock(image,MATLAB_HDR.ObjectSize,clone_info,exception);\n      if(image2==NULL) continue;\n      MATLAB_HDR.DataType = ReadBlobXXXLong(image2); /* replace compressed object type. */\n    }\n#endif\n\n    if(MATLAB_HDR.DataType!=miMATRIX) continue;  /* skip another objects. */\n\n    MATLAB_HDR.unknown1 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.unknown2 = ReadBlobXXXLong(image2);\n\n    MATLAB_HDR.unknown5 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.StructureClass = MATLAB_HDR.unknown5 & 0xFF;\n    MATLAB_HDR.StructureFlag = (MATLAB_HDR.unknown5>>8) & 0xFF;\n\n    MATLAB_HDR.unknown3 = ReadBlobXXXLong(image2);\n    if(image!=image2)\n      MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);  /* ??? don't understand why ?? */\n    MATLAB_HDR.unknown4 = ReadBlobXXXLong(image2);\n    MATLAB_HDR.DimFlag = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeX = ReadBlobXXXLong(image2);\n    MATLAB_HDR.SizeY = ReadBlobXXXLong(image2);\n\n\n    switch(MATLAB_HDR.DimFlag)\n    {\n      case  8: z2=z=1; break;      /* 2D matrix*/\n      case 12: z2=z = ReadBlobXXXLong(image2);  /* 3D matrix RGB*/\n           Unknown6 = ReadBlobXXXLong(image2);\n           (void) Unknown6;\n         if(z!=3) ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n         break;\n      case 16: z2=z = ReadBlobXXXLong(image2);  /* 4D matrix animation */\n         if(z!=3 && z!=1)\n           ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n          Frames = ReadBlobXXXLong(image2);\n         break;\n      default: ThrowReaderException(CoderError, \"MultidimensionalMatricesAreNotSupported\");\n    }\n\n    MATLAB_HDR.Flag1 = ReadBlobXXXShort(image2);\n    MATLAB_HDR.NameFlag = ReadBlobXXXShort(image2);\n\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"MATLAB_HDR.StructureClass %d\",MATLAB_HDR.StructureClass);\n    if (MATLAB_HDR.StructureClass != mxCHAR_CLASS &&\n        MATLAB_HDR.StructureClass != mxSINGLE_CLASS &&    /* float + complex float */\n        MATLAB_HDR.StructureClass != mxDOUBLE_CLASS &&    /* double + complex double */\n        MATLAB_HDR.StructureClass != mxINT8_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT8_CLASS &&    /* uint8 + uint8 3D */\n        MATLAB_HDR.StructureClass != mxINT16_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT16_CLASS &&    /* uint16 + uint16 3D */\n        MATLAB_HDR.StructureClass != mxINT32_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT32_CLASS &&    /* uint32 + uint32 3D */\n        MATLAB_HDR.StructureClass != mxINT64_CLASS &&\n        MATLAB_HDR.StructureClass != mxUINT64_CLASS)    /* uint64 + uint64 3D */\n      ThrowReaderException(CoderError,\"UnsupportedCellTypeInTheMatrix\");\n\n    switch (MATLAB_HDR.NameFlag)\n    {\n      case 0:\n        size = ReadBlobXXXLong(image2);  /* Object name string size */\n        size = 4 * (ssize_t) ((size + 3 + 1) / 4);\n        (void) SeekBlob(image2, size, SEEK_CUR);\n        break;\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n        (void) ReadBlob(image2, 4, (unsigned char *) &size); /* Object name string */\n        break;\n      default:\n        goto MATLAB_KO;\n    }\n\n    CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n    if (logging)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"MATLAB_HDR.CellType: %.20g\",(double) CellType);\n\n    (void) ReadBlob(image2, 4, (unsigned char *) &size);     /* data size */\n\nNEXT_FRAME:\n    switch (CellType)\n    {\n      case miINT8:\n      case miUINT8:\n        sample_size = 8;\n        if(MATLAB_HDR.StructureFlag & FLAG_LOGICAL)\n          image->depth = 1;\n        else\n          image->depth = 8;         /* Byte type cell */\n        ldblk = (ssize_t) MATLAB_HDR.SizeX;\n        break;\n      case miINT16:\n      case miUINT16:\n        sample_size = 16;\n        image->depth = 16;        /* Word type cell */\n        ldblk = (ssize_t) (2 * MATLAB_HDR.SizeX);\n        break;\n      case miINT32:\n      case miUINT32:\n        sample_size = 32;\n        image->depth = 32;        /* Dword type cell */\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miINT64:\n      case miUINT64:\n        sample_size = 64;\n        image->depth = 64;        /* Qword type cell */\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      case miSINGLE:\n        sample_size = 32;\n        image->depth = 32;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {              /* complex float type cell */\n  }\n        ldblk = (ssize_t) (4 * MATLAB_HDR.SizeX);\n        break;\n      case miDOUBLE:\n        sample_size = 64;\n        image->depth = 64;        /* double type cell */\n        (void) SetImageOption(clone_info,\"quantum:format\",\"floating-point\");\nDisableMSCWarning(4127)\n        if (sizeof(double) != 8)\nRestoreMSCWarning\n          ThrowReaderException(CoderError, \"IncompatibleSizeOfDouble\");\n        if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n  {                         /* complex double type cell */\n  }\n        ldblk = (ssize_t) (8 * MATLAB_HDR.SizeX);\n        break;\n      default:\n        ThrowReaderException(CoderError, \"UnsupportedCellTypeInTheMatrix\");\n    }\n    (void) sample_size;\n    image->columns = MATLAB_HDR.SizeX;\n    image->rows = MATLAB_HDR.SizeY;\n    quantum_info=AcquireQuantumInfo(clone_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    one=1;\n    image->colors = one << image->depth;\n    if (image->columns == 0 || image->rows == 0)\n      goto MATLAB_KO;\n      /* Image is gray when no complex flag is set and 2D Matrix */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      {\n        SetImageColorspace(image,GRAYColorspace);\n        image->type=GrayscaleType;\n      }\n\n\n    /*\n      If ping is true, then only set image size and colors without\n      reading any image data.\n    */\n    if (image_info->ping)\n    {\n      size_t temp = image->columns;\n      image->columns = image->rows;\n      image->rows = temp;\n      goto done_reading; /* !!!!!! BAD  !!!! */\n    }\n    status=SetImageExtent(image,image->columns,image->rows);\n    if (status == MagickFalse)\n      {\n        InheritException(exception,&image->exception);\n        return(DestroyImageList(image));\n      }\n\n  /* ----- Load raster data ----- */\n    BImgBuff = (unsigned char *) AcquireQuantumMemory((size_t) (ldblk),sizeof(double));    /* Ldblk was set in the check phase */\n    if (BImgBuff == NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n    MinVal = 0;\n    MaxVal = 0;\n    if (CellType==miDOUBLE || CellType==miSINGLE)        /* Find Min and Max Values for floats */\n    {\n      CalcMinMax(image2, image_info->endian,  MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &quantum_info->minimum, &quantum_info->maximum);\n    }\n\n    /* Main loop for reading all scanlines */\n    if(z==1) z=0; /* read grey scanlines */\n    /* else read color scanlines */\n    do\n    {\n      for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n      {\n        q=GetAuthenticPixels(image,0,MATLAB_HDR.SizeY-i-1,image->columns,1,exception);\n        if (q == (PixelPacket *) NULL)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT set image pixels returns unexpected NULL on a row %u.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto done_reading;    /* Skip image rotation, when cannot set image pixels    */\n  }\n        if(ReadBlob(image2,ldblk,(unsigned char *)BImgBuff) != (ssize_t) ldblk)\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"  MAT cannot read scanrow %u from a file.\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n        if((CellType==miINT8 || CellType==miUINT8) && (MATLAB_HDR.StructureFlag & FLAG_LOGICAL))\n        {\n          FixLogical((unsigned char *)BImgBuff,ldblk);\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n    {\nImportQuantumPixelsFailed:\n      if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  MAT failed to ImportQuantumPixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n      break;\n    }\n        }\n        else\n        {\n          if(ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,z2qtype[z],BImgBuff,exception) <= 0)\n      goto ImportQuantumPixelsFailed;\n\n\n          if (z<=1 &&       /* fix only during a last pass z==0 || z==1 */\n          (CellType==miINT8 || CellType==miINT16 || CellType==miINT32 || CellType==miINT64))\n      FixSignedValues(q,MATLAB_HDR.SizeX);\n        }\n\n        if (!SyncAuthenticPixels(image,exception))\n  {\n    if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  MAT failed to sync image pixels for a row %u\", (unsigned)(MATLAB_HDR.SizeY-i-1));\n    goto ExitLoop;\n  }\n      }\n    } while(z-- >= 2);\n    quantum_info=DestroyQuantumInfo(quantum_info);\nExitLoop:\n\n\n    /* Read complex part of numbers here */\n    if (MATLAB_HDR.StructureFlag & FLAG_COMPLEX)\n    {        /* Find Min and Max Values for complex parts of floats */\n      CellType = ReadBlobXXXLong(image2);    /* Additional object type */\n      i = ReadBlobXXXLong(image2);           /* size of a complex part - toss away*/\n\n      if (CellType==miDOUBLE || CellType==miSINGLE)\n      {\n        CalcMinMax(image2,  image_info->endian, MATLAB_HDR.SizeX, MATLAB_HDR.SizeY, CellType, ldblk, BImgBuff, &MinVal, &MaxVal);\n      }\n\n      if (CellType==miDOUBLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobDoublesXXX(image2, ldblk, (double *)BImgBuff);\n          InsertComplexDoubleRow((double *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n\n      if (CellType==miSINGLE)\n        for (i = 0; i < (ssize_t) MATLAB_HDR.SizeY; i++)\n  {\n          ReadBlobFloatsXXX(image2, ldblk, (float *)BImgBuff);\n          InsertComplexFloatRow((float *)BImgBuff, i, image, MinVal, MaxVal);\n  }\n    }\n\n      /* Image is gray when no complex flag is set and 2D Matrix AGAIN!!! */\n    if ((MATLAB_HDR.DimFlag == 8) &&\n        ((MATLAB_HDR.StructureFlag & FLAG_COMPLEX) == 0))\n      image->type=GrayscaleType;\n    if (image->depth == 1)\n      image->type=BilevelType;\n\n    if(image2==image)\n        image2 = NULL;    /* Remove shadow copy to an image before rotation. */\n\n      /*  Rotate image. */\n    rotated_image = RotateImage(image, 90.0, exception);\n    if (rotated_image != (Image *) NULL)\n    {\n        /* Remove page offsets added by RotateImage */\n      rotated_image->page.x=0;\n      rotated_image->page.y=0;\n\n      blob = rotated_image->blob;\n      rotated_image->blob = image->blob;\n      rotated_image->colors = image->colors;\n      image->blob = blob;\n      AppendImageToList(&image,rotated_image);\n      DeleteImageFromList(&image);\n    }\n\ndone_reading:\n\n    if(image2!=NULL)\n      if(image2!=image)\n      {\n        DeleteImageFromList(&image2);\n  if(clone_info)\n  {\n          if(clone_info->file)\n    {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) remove_utf8(clone_info->filename);\n    }\n        }\n      }\n\n      /* Allocate next image structure. */\n    AcquireNextImage(image_info,image);\n    if (image->next == (Image *) NULL) break;\n    image=SyncNextImageInList(image);\n    image->columns=image->rows=0;\n    image->colors=0;\n\n      /* row scan buffer is no longer needed */\n    RelinquishMagickMemory(BImgBuff);\n    BImgBuff = NULL;\n\n    if(--Frames>0)\n    {\n      z = z2;\n      if(image2==NULL) image2 = image;\n      goto NEXT_FRAME;\n    }\n\n    if(image2!=NULL)\n      if(image2!=image)   /* Does shadow temporary decompressed image exist? */\n      {\n/*  CloseBlob(image2); */\n        DeleteImageFromList(&image2);\n        if(clone_info)\n        {\n          if(clone_info->file)\n          {\n            fclose(clone_info->file);\n            clone_info->file = NULL;\n            (void) unlink(clone_info->filename);\n          }\n         }\n       }\n  }\n  clone_info=DestroyImageInfo(clone_info);\n\n  RelinquishMagickMemory(BImgBuff);\n  CloseBlob(image);\n\n\n  {\n    Image *p;\n    ssize_t scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n\n    /*\n      Fix scene numbers\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=scene++;\n  }\n\n  if(clone_info != NULL)  /* cleanup garbage file from compression */\n  {\n    if(clone_info->file)\n    {\n      fclose(clone_info->file);\n      clone_info->file = NULL;\n      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/b173a352397877775c51c9a0e9d59eb6ce24c455", "file_name": "coders/mat.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a MATLAB image file in ImageMagick."}
{"func_name": "PHYSICALPATH_FUNC", "func_src_before": "PHYSICALPATH_FUNC(mod_alias_physical_handler) {\n\tplugin_data *p = p_d;\n\tint uri_len, basedir_len;\n\tchar *uri_ptr;\n\tsize_t k;\n\n\tif (buffer_is_empty(con->physical.path)) return HANDLER_GO_ON;\n\n\tmod_alias_patch_connection(srv, con, p);\n\n\t/* not to include the tailing slash */\n\tbasedir_len = buffer_string_length(con->physical.basedir);\n\tif ('/' == con->physical.basedir->ptr[basedir_len-1]) --basedir_len;\n\turi_len = buffer_string_length(con->physical.path) - basedir_len;\n\turi_ptr = con->physical.path->ptr + basedir_len;\n\n\tfor (k = 0; k < p->conf.alias->used; k++) {\n\t\tdata_string *ds = (data_string *)p->conf.alias->data[k];\n\t\tint alias_len = buffer_string_length(ds->key);\n\n\t\tif (alias_len > uri_len) continue;\n\t\tif (buffer_is_empty(ds->key)) continue;\n\n\t\tif (0 == (con->conf.force_lowercase_filenames ?\n\t\t\t\t\tstrncasecmp(uri_ptr, ds->key->ptr, alias_len) :\n\t\t\t\t\tstrncmp(uri_ptr, ds->key->ptr, alias_len))) {\n\t\t\t/* matched */\n\n\t\t\tbuffer_copy_buffer(con->physical.basedir, ds->value);\n\t\t\tbuffer_copy_buffer(srv->tmp_buf, ds->value);\n\t\t\tbuffer_append_string(srv->tmp_buf, uri_ptr + alias_len);\n\t\t\tbuffer_copy_buffer(con->physical.path, srv->tmp_buf);\n\n\t\t\treturn HANDLER_GO_ON;\n\t\t}\n\t}\n\n\t/* not found */\n\treturn HANDLER_GO_ON;\n}", "func_src_after": "PHYSICALPATH_FUNC(mod_alias_physical_handler) {\n\tplugin_data *p = p_d;\n\tint uri_len, basedir_len;\n\tchar *uri_ptr;\n\tsize_t k;\n\n\tif (buffer_is_empty(con->physical.path)) return HANDLER_GO_ON;\n\n\tmod_alias_patch_connection(srv, con, p);\n\n\t/* not to include the tailing slash */\n\tbasedir_len = buffer_string_length(con->physical.basedir);\n\tif ('/' == con->physical.basedir->ptr[basedir_len-1]) --basedir_len;\n\turi_len = buffer_string_length(con->physical.path) - basedir_len;\n\turi_ptr = con->physical.path->ptr + basedir_len;\n\n\tfor (k = 0; k < p->conf.alias->used; k++) {\n\t\tdata_string *ds = (data_string *)p->conf.alias->data[k];\n\t\tint alias_len = buffer_string_length(ds->key);\n\n\t\tif (alias_len > uri_len) continue;\n\t\tif (buffer_is_empty(ds->key)) continue;\n\n\t\tif (0 == (con->conf.force_lowercase_filenames ?\n\t\t\t\t\tstrncasecmp(uri_ptr, ds->key->ptr, alias_len) :\n\t\t\t\t\tstrncmp(uri_ptr, ds->key->ptr, alias_len))) {\n\t\t\t/* matched */\n\n\t\t\t/* check for path traversal in url-path following alias if key\n\t\t\t * does not end in slash, but replacement value ends in slash */\n\t\t\tif (uri_ptr[alias_len] == '.') {\n\t\t\t\tchar *s = uri_ptr + alias_len + 1;\n\t\t\t\tif (*s == '.') ++s;\n\t\t\t\tif (*s == '/' || *s == '\\0') {\n\t\t\t\t\tsize_t vlen = buffer_string_length(ds->value);\n\t\t\t\t\tif (0 != alias_len && ds->key->ptr[alias_len-1] != '/'\n\t\t\t\t\t    && 0 != vlen && ds->value->ptr[vlen-1] == '/') {\n\t\t\t\t\t\tcon->http_status = 403;\n\t\t\t\t\t\treturn HANDLER_FINISHED;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbuffer_copy_buffer(con->physical.basedir, ds->value);\n\t\t\tbuffer_copy_buffer(srv->tmp_buf, ds->value);\n\t\t\tbuffer_append_string(srv->tmp_buf, uri_ptr + alias_len);\n\t\t\tbuffer_copy_buffer(con->physical.path, srv->tmp_buf);\n\n\t\t\treturn HANDLER_GO_ON;\n\t\t}\n\t}\n\n\t/* not found */\n\treturn HANDLER_GO_ON;\n}", "commit_link": "github.com/lighttpd/lighttpd1.4/commit/2105dae0f9d7a964375ce681e53cb165375f84c1", "file_name": "src/mod_alias.c", "vul_type": "cwe-022", "description": "Write a C function named `mod_alias_physical_handler` that rewrites URL paths based on predefined aliases and handles path traversal security."}
{"func_name": "top_proxies", "func_src_before": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id='\"+proxy_id+\"' LIMIT 1\"\n        cur.execute(query)\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as='\"+proxy_id+\"'\"\n        cur.execute(query)\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "func_src_after": "@app.route('/top_proxies')\ndef top_proxies():\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT sum(amount) FROM holders\"\n    cur.execute(query)\n    total = cur.fetchone()\n    total_votes = total[0]\n\n    query = \"SELECT voting_as FROM holders WHERE voting_as<>'1.2.5' group by voting_as\"\n    cur.execute(query)\n    results = cur.fetchall()\n    #con.close()\n\n    proxies = []\n\n    for p in range(0, len(results)):\n\n        proxy_line = [0] * 5\n\n        proxy_id = results[p][0]\n        proxy_line[0] = proxy_id\n\n        query = \"SELECT account_name, amount FROM holders WHERE account_id=%s LIMIT 1\"\n        cur.execute(query, (proxy_id,))\n        proxy = cur.fetchone()\n\n        try:\n            proxy_name = proxy[0]\n            proxy_amount = proxy[1]\n        except:\n            proxy_name = \"unknown\"\n            proxy_amount = 0\n\n\n        proxy_line[1] = proxy_name\n\n        query = \"SELECT amount, account_id FROM holders WHERE voting_as=%s\"\n        cur.execute(query, (proxy_id,))\n        results2 = cur.fetchall()\n\n        proxy_line[2] = int(proxy_amount)\n\n        for p2 in range(0, len(results2)):\n            amount = results2[p2][0]\n            account_id = results2[p2][1]\n            proxy_line[2] = proxy_line[2] + int(amount)  # total proxy votes\n            proxy_line[3] = proxy_line[3] + 1       # followers\n\n        if proxy_line[3] > 2:\n            percentage = float(float(proxy_line[2]) * 100.0/ float(total_votes))\n            proxy_line[4] = percentage\n            proxies.append(proxy_line)\n\n    con.close()\n\n    proxies = sorted(proxies, key=lambda k: int(k[2]))\n    r_proxies = proxies[::-1]\n\n    return jsonify(filter(None, r_proxies))", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/top_proxies' that queries a PostgreSQL database to retrieve and return a JSON list of top proxy voters and their details."}
{"func_name": "AuthenticationConfig::configure", "func_src_before": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .csrf()\n\t\t        .disable()\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "func_src_after": "\t@Override\n\tprotected void configure(final HttpSecurity http) throws Exception {\n\t\thttp\n\t\t        .authorizeRequests()\n    \t\t\t\t.antMatchers(\"/css/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/js/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/fonts/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/plugins/**\").permitAll()\n    \t\t\t\t.antMatchers(\"/dist/**\").permitAll()\n                    .antMatchers(\"/login\").permitAll()\n    \t\t\t\t.anyRequest().authenticated()\n    \t\t\t\t.and()\n\t\t\t\t.formLogin()\n\t\t\t\t    .loginPage(\"/login\")\n\t\t\t\t    .permitAll()\n\t\t\t\t    .and()\n\t\t\t\t.logout()                                    \n                    .permitAll();\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 88, "char_end": 106, "line": "\t\t        .csrf()\n"}, {"line_no": 5, "char_start": 106, "char_end": 127, "line": "\t\t        .disable()\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 88, "char_end": 127, "chars": "\t\t        .csrf()\n\t\t        .disable()\n"}], "added": []}, "commit_link": "github.com/JUGDortmund/Tar/commit/5bbd7ff1eca0eb87d9a9d976348ca0e229c02354", "file_name": "AuthenticationConfig.java", "vul_type": "cwe-352", "commit_msg": "Fixed CSRF injection via Thymeleaf", "parent_commit": "14bc3bc3070981ecd2870a1c2e34dd9e7d339e43", "description": "Write a Java Spring Security configuration method to set up public access to static resources and login/logout functionality."}
{"func_name": "check_clus_chain", "func_src_before": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tclus_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %u bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %u bytes\",\n\t\t\t\t\t\tcount *\n\t\t\t\t\t\texfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %u bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "func_src_after": "static int check_clus_chain(struct exfat *exfat, struct exfat_inode *node)\n{\n\tstruct exfat_dentry *stream_de;\n\tclus_t clus, prev, next;\n\tuint64_t count, max_count;\n\n\tclus = node->first_clus;\n\tprev = EXFAT_EOF_CLUSTER;\n\tcount = 0;\n\tmax_count = DIV_ROUND_UP(node->size, exfat->clus_size);\n\n\tif (node->size == 0 && node->first_clus == EXFAT_FREE_CLUSTER)\n\t\treturn 0;\n\n\t/* the first cluster is wrong */\n\tif ((node->size == 0 && node->first_clus != EXFAT_FREE_CLUSTER) ||\n\t\t(node->size > 0 && !heap_clus(exfat, node->first_clus))) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_FIRST_CLUS, \"first cluster is wrong\"))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\twhile (clus != EXFAT_EOF_CLUSTER) {\n\t\tif (count >= max_count) {\n\t\t\tif (node->is_contiguous)\n\t\t\t\tbreak;\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_SMALLER_SIZE,\n\t\t\t\t\t\"more clusters are allocated. \"\n\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/*\n\t\t * This cluster is already allocated. it may be shared with\n\t\t * the other file, or there is a loop in cluster chain.\n\t\t */\n\t\tif (EXFAT_BITMAP_GET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\tER_FILE_DUPLICATED_CLUS,\n\t\t\t\t\t\"cluster is already allocated for \"\n\t\t\t\t\t\"the other file. truncated to %\"\n\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\tgoto truncate_file;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* This cluster is allocated or not */\n\t\tif (get_next_clus(exfat, node, clus, &next))\n\t\t\tgoto truncate_file;\n\t\tif (!node->is_contiguous) {\n\t\t\tif (!heap_clus(exfat, next) &&\n\t\t\t\t\tnext != EXFAT_EOF_CLUSTER) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"broken cluster chain. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!EXFAT_BITMAP_GET(exfat->disk_bitmap,\n\t\t\t\t\tclus - EXFAT_FIRST_CLUSTER)) {\n\t\t\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\t\t\t\tER_FILE_INVALID_CLUS,\n\t\t\t\t\t\t\"cluster is marked as free. \"\n\t\t\t\t\t\t\"truncate to %\"\n\t\t\t\t\t\tPRIu64 \" bytes\",\n\t\t\t\t\t\tcount * exfat->clus_size))\n\t\t\t\t\tgoto truncate_file;\n\n\t\t\t\telse\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tcount++;\n\t\tEXFAT_BITMAP_SET(exfat->alloc_bitmap,\n\t\t\t\tclus - EXFAT_FIRST_CLUSTER);\n\t\tprev = clus;\n\t\tclus = next;\n\t}\n\n\tif (count < max_count) {\n\t\tif (repair_file_ask(&exfat->de_iter, node,\n\t\t\tER_FILE_LARGER_SIZE, \"less clusters are allocated. \"\n\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n\t\t\tcount * exfat->clus_size))\n\t\t\tgoto truncate_file;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\ntruncate_file:\n\tnode->size = count * exfat->clus_size;\n\tif (!heap_clus(exfat, prev))\n\t\tnode->first_clus = EXFAT_FREE_CLUSTER;\n\n\texfat_de_iter_get_dirty(&exfat->de_iter, 1, &stream_de);\n\tif (count * exfat->clus_size <\n\t\t\tle64_to_cpu(stream_de->stream_valid_size))\n\t\tstream_de->stream_valid_size = cpu_to_le64(\n\t\t\t\tcount * exfat->clus_size);\n\tif (!heap_clus(exfat, prev))\n\t\tstream_de->stream_start_clu = EXFAT_FREE_CLUSTER;\n\tstream_de->stream_size = cpu_to_le64(\n\t\t\tcount * exfat->clus_size);\n\n\t/* remaining clusters will be freed while FAT is compared with\n\t * alloc_bitmap.\n\t */\n\tif (!node->is_contiguous && heap_clus(exfat, prev))\n\t\treturn set_fat(exfat, prev, EXFAT_EOF_CLUSTER);\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 5, "char_start": 136, "char_end": 162, "line": "\tclus_t count, max_count;\n"}, {"line_no": 32, "char_start": 888, "char_end": 917, "line": "\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 48, "char_start": 1333, "char_end": 1379, "line": "\t\t\t\t\t\"the other file. truncated to %u bytes\",\n"}, {"line_no": 64, "char_start": 1783, "char_end": 1813, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 65, "char_start": 1813, "char_end": 1827, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 66, "char_start": 1827, "char_end": 1852, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 78, "char_start": 2116, "char_end": 2146, "line": "\t\t\t\t\t\t\"truncate to %u bytes\",\n"}, {"line_no": 79, "char_start": 2146, "char_end": 2160, "line": "\t\t\t\t\t\tcount *\n"}, {"line_no": 80, "char_start": 2160, "char_end": 2185, "line": "\t\t\t\t\t\texfat->clus_size))\n"}, {"line_no": 98, "char_start": 2496, "char_end": 2524, "line": "\t\t\t\"truncates to %u bytes\",\n"}], "added": [{"line_no": 5, "char_start": 136, "char_end": 164, "line": "\tuint64_t count, max_count;\n"}, {"line_no": 32, "char_start": 890, "char_end": 928, "line": "\t\t\t\t\t\"truncate to %\" PRIu64 \" bytes\",\n"}, {"line_no": 48, "char_start": 1344, "char_end": 1382, "line": "\t\t\t\t\t\"the other file. truncated to %\"\n"}, {"line_no": 49, "char_start": 1382, "char_end": 1404, "line": "\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 65, "char_start": 1808, "char_end": 1830, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 66, "char_start": 1830, "char_end": 1853, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 67, "char_start": 1853, "char_end": 1886, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 79, "char_start": 2150, "char_end": 2172, "line": "\t\t\t\t\t\t\"truncate to %\"\n"}, {"line_no": 80, "char_start": 2172, "char_end": 2195, "line": "\t\t\t\t\t\tPRIu64 \" bytes\",\n"}, {"line_no": 81, "char_start": 2195, "char_end": 2228, "line": "\t\t\t\t\t\tcount * exfat->clus_size))\n"}, {"line_no": 99, "char_start": 2539, "char_end": 2576, "line": "\t\t\t\"truncates to %\" PRIu64 \" bytes\",\n"}]}, "char_changes": {"deleted": [{"char_start": 137, "char_end": 141, "chars": "clus"}, {"char_start": 907, "char_end": 908, "chars": "u"}, {"char_start": 1369, "char_end": 1370, "chars": "u"}, {"char_start": 1803, "char_end": 1804, "chars": "u"}, {"char_start": 1826, "char_end": 1833, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2136, "char_end": 2137, "chars": "u"}, {"char_start": 2159, "char_end": 2166, "chars": "\n\t\t\t\t\t\t"}, {"char_start": 2514, "char_end": 2515, "chars": "u"}], "added": [{"char_start": 137, "char_end": 143, "chars": "uint64"}, {"char_start": 909, "char_end": 919, "chars": "\" PRIu64 \""}, {"char_start": 1380, "char_end": 1395, "chars": "\"\n\t\t\t\t\tPRIu64 \""}, {"char_start": 1828, "char_end": 1844, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 1866, "char_end": 1867, "chars": " "}, {"char_start": 2170, "char_end": 2186, "chars": "\"\n\t\t\t\t\t\tPRIu64 \""}, {"char_start": 2208, "char_end": 2209, "chars": " "}, {"char_start": 2557, "char_end": 2567, "chars": "\" PRIu64 \""}]}, "commit_link": "github.com/exfatprogs/exfatprogs/commit/c1f48157c38df8d958ab81012abae2470750a785", "file_name": "fsck.c", "vul_type": "cwe-190", "commit_msg": "fsck: fix integer overflow in calculating size\n\nThe size must be 64-bit integer\n\nSigned-off-by: Hyunchul Lee <hyc.lee@gmail.com>", "parent_commit": "edf7a39b7252f4b63915292420aaa63a750f22d9", "description": "Write a C function to validate and repair the cluster chain of a file in an exFAT file system."}
{"func_name": "test_feature_tags", "func_src_before": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "func_src_after": "def test_feature_tags():\n\n    with open(util.base_dir() + \"/mapper.yaml\", \"r\") as mapper_file:\n        mapper_content = mapper_file.read()\n    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n    testmappers = [x for x in mapper_yaml[\"testmapper\"]]\n    mapper_tests = [\n        list(x.keys())[0] for tm in testmappers for x in mapper_yaml[\"testmapper\"][tm]\n    ]\n\n    def check_ver(tag):\n        for ver_prefix, ver_len in [\n            [\"ver\", 3],\n            [\"rhelver\", 2],\n            [\"fedoraver\", 1],\n        ]:\n            if not tag.startswith(ver_prefix):\n                continue\n            op, ver = misc.test_version_tag_parse(tag, ver_prefix)\n            assert type(op) is str\n            assert type(ver) is list\n            assert op in [\"+\", \"+=\", \"-\", \"-=\"]\n            assert ver\n            assert all([type(v) is int for v in ver])\n            assert all([v >= 0 for v in ver])\n            assert len(ver) <= ver_len\n            assert tag.startswith(ver_prefix + op)\n            return True\n        return tag in [\n            \"rhel_pkg\",\n            \"not_with_rhel_pkg\",\n            \"fedora_pkg\",\n            \"not_with_fedora_pkg\",\n        ]\n\n    def check_bugzilla(tag):\n        if tag.startswith(\"rhbz\"):\n            assert re.match(\"^rhbz[0-9]+$\", tag)\n            return True\n        if tag.startswith(\"gnomebz\"):\n            assert re.match(\"^gnomebz[0-9]+$\", tag)\n            return True\n        return False\n\n    def check_registry(tag):\n        return tag in tag_registry.tag_registry\n\n    def check_mapper(tag):\n        return tag in mapper_tests\n\n    for feature in [\"nmcli\", \"nmtui\"]:\n        all_tags = misc.test_load_tags_from_features(feature)\n\n        tag_registry_used = set()\n        unique_tags = set()\n        for tags in all_tags:\n            assert tags\n            assert type(tags) is list\n            test_in_mapper = False\n            for tag in tags:\n                assert type(tag) is str\n                assert tag\n                assert re.match(\"^[-a-z_.A-Z0-9+=]+$\", tag)\n                assert re.match(\"^\" + misc.TEST_NAME_VALID_CHAR_REGEX + \"+$\", tag)\n                assert tags.count(tag) == 1, f'tag \"{tag}\" is not unique in {tags}'\n                is_ver = check_ver(tag)\n                is_bugzilla = check_bugzilla(tag)\n                is_registry = check_registry(tag)\n                is_mapper = check_mapper(tag)\n                test_in_mapper = test_in_mapper or is_mapper\n                if is_registry:\n                    tag_registry_used.add(tag)\n                assert (\n                    is_ver or is_bugzilla or is_registry or is_mapper\n                ), f'tag \"{tag}\" has no effect'\n                assert [is_ver, is_bugzilla, is_registry, is_mapper].count(True) == 1, (\n                    f'tag \"{tag}\" is multipurpose ({\"mapper, \" if is_mapper else \"\"}'\n                    f'{\"registry, \" if is_registry else \"\"}{\"ver, \" if is_ver else \"\"}'\n                    f'{\"bugzilla, \" if is_bugzilla else \"\"})'\n                )\n\n            assert test_in_mapper, f\"none of {tags} is in mapper\"\n\n            tt = tuple(tags)\n            if tt in unique_tags:\n                pytest.fail(f'tags \"{tags}\" are duplicate over the {feature} tests')\n            unique_tags.add(tt)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 139, "char_end": 183, "line": "    mapper_yaml = yaml.load(mapper_content)\n"}], "added": [{"line_no": 5, "char_start": 139, "char_end": 207, "line": "    mapper_yaml = yaml.load(mapper_content, Loader=yaml.BaseLoader)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 181, "char_end": 205, "chars": ", Loader=yaml.BaseLoader"}]}, "commit_link": "github.com/NetworkManager/NetworkManager-ci/commit/e8a0a1686315dc988b3600ce80ff3953cccb7b4b", "file_name": "test.py", "vul_type": "cwe-502", "commit_msg": "nmci/test: avoid deprecation warning for yaml.load()\n\nAvoids:\n\n  nmci/test.py::test_feature_tags\n    /TMP/NetworkManager-ci/nmci/test.py:411: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n      mapper_yaml = yaml.load(mapper_content)\n\n  -- Docs: https://docs.pytest.org/en/stable/warnings.html", "parent_commit": "5e80ca2fac5f2fa62e0daccc6703b69694a97f51", "description": "Write a Python function to validate feature tags against various criteria, including version tags, bugzilla tags, and a tag registry."}
{"func_name": "build_board", "func_src_before": "def build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\" % game)\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)", "func_src_after": "def build_board(conn, game,size):\n    # we'll build the empty board, and then fill in with the move list that\n    # we get from the DB.\n    board = []\n    for i in range(size):\n        board.append([\"\"]*size)\n\n\n    # search for all moves that have happenend during this game.\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT x,y,letter FROM moves WHERE gameID = %d;\", (game,))\n\n    counts = {\"X\":0, \"O\":0}\n    for move in cursor.fetchall():\n        (x,y,letter) = move\n\n        x = int(x)\n        y = int(y)\n        assert x >= 0 and x < size\n        assert y >= 0 and y < size\n\n        assert letter in \"XO\"\n\n        assert board[x][y] == \"\"\n        board[x][y] = letter\n\n        counts[letter] += 1\n\n    cursor.close()\n\n    assert counts[\"X\"] >= counts[\"O\"]\n    assert counts[\"X\"] <= counts[\"O\"]+1\n\n    if counts[\"X\"] == counts[\"O\"]:\n        nextPlayer = 0\n    else:\n        nextPlayer = 1\n    letter = \"XO\"[nextPlayer]\n\n    return (board,nextPlayer,letter)", "commit_link": "github.com/russ-lewis/ttt_-_python_cgi/commit/6096f43fd4b2d91211eec4614b7960c0816900da", "file_name": "cgi/common.py", "vul_type": "cwe-089", "description": "In Python, write a function to construct a tic-tac-toe board from a database, tracking the next player's turn based on move history."}
{"func_name": "edit_coordinator", "func_src_before": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "func_src_after": "@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json_for_html(),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "commit_link": "github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "file_name": "apps/oozie/src/oozie/views/editor2.py", "vul_type": "cwe-079", "description": "In Python, write a view function `edit_coordinator` that checks access permissions, fetches coordinator and workflow details, and renders them to a template."}
{"func_name": "addKey", "func_src_before": "def addKey(client):\n\t\"\"\"Adds a new key with the specified name and contents.\n\tReturns an error if a key with the specified name already exists.\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal CREATED\n\n\tvalidateClient(client)\n\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateNewKeyData(token_data)\n\n\t# Use 'x' flag so we can throw an error if a key with this name already exists\n\ttry:\n\t\twith open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:\n\t\t\tf.write(token_data['key'])\n\texcept FileExistsError:\n\t\traise FoxlockError(BAD_REQUEST, \"Key '%s' already exists\" % token_data['name'])\n\n\treturn 'Key successfully created', CREATED", "func_src_after": "def addKey(client):\n\t\"\"\"Adds a new key with the specified name and contents.\n\tReturns an error if a key with the specified name already exists.\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal CREATED\n\n\tvalidateClient(client)\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateNewKeyData(token_data)\n\tvalidateKeyName(token_data['name'])\n\n\t# Use 'x' flag so we can throw an error if a key with this name already exists\n\ttry:\n\t\twith open('keys/%s/%s.key' % (client, token_data['name']), 'x') as f:\n\t\t\tf.write(token_data['key'])\n\texcept FileExistsError:\n\t\traise FoxlockError(BAD_REQUEST, \"Key '%s' already exists\" % token_data['name'])\n\n\treturn 'Key successfully created', CREATED", "commit_link": "github.com/Mimickal/FoxLock/commit/7c665e556987f4e2c1a75e143a1e80ae066ad833", "file_name": "impl.py", "vul_type": "cwe-022", "description": "Write a Python function named `addKey` that creates a new key file for a client if it doesn't already exist, or raises an error if the key file already exists."}
{"func_name": "enc_untrusted_read", "func_src_before": "ssize_t enc_untrusted_read(int fd, void *buf, size_t count) {\n  return static_cast<ssize_t>(EnsureInitializedAndDispatchSyscall(\n      asylo::system_call::kSYS_read, fd, buf, count));\n}", "func_src_after": "ssize_t enc_untrusted_read(int fd, void *buf, size_t count) {\n  ssize_t ret = static_cast<ssize_t>(EnsureInitializedAndDispatchSyscall(\n      asylo::system_call::kSYS_read, fd, buf, count));\n  if (ret != -1 && ret > count) {\n    ::asylo::primitives::TrustedPrimitives::BestEffortAbort(\n        \"enc_untrusted_read: read result exceeds requested\");\n  }\n  return ret;\n}", "commit_link": "github.com/google/asylo/commit/b1d120a2c7d7446d2cc58d517e20a1b184b82200", "file_name": "asylo/platform/host_call/trusted/host_calls.cc", "vul_type": "cwe-125", "description": "Write a C++ function named `enc_untrusted_read` that wraps a syscall for reading from a file descriptor, with error handling for reading more bytes than requested."}
{"func_name": "dumptable", "func_src_before": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumptable(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tuser, pw := getCredentials(r)\n\tdatabase := parray[0]\n\ttable := parray[1]\n\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcols, err := rows.Columns()\n\tcheckY(err)\n\tfmt.Fprintln(w, \"<p>\"+\"# \"+strings.Join(cols, \" \")+\"</p>\")\n\n\t/*  credits:\n\t * \thttp://stackoverflow.com/questions/19991541/dumping-mysql-tables-to-json-with-golang\n\t * \thttp://go-database-sql.org/varcols.html\n\t */\n\n\traw := make([]interface{}, len(cols))\n\tval := make([]interface{}, len(cols))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\tfor rows.Next() {\n\n\t\tfmt.Fprint(w, linkDeeper(r.URL.Path, strconv.Itoa(n), strconv.Itoa(n)))\n\t\terr = rows.Scan(raw...)\n\t\tcheckY(err)\n\n\t\tfor _, col := range val {\n\t\t\tif col != nil {\n\t\t\t\tfmt.Fprintf(w, \"%s \", string(col.([]byte)))\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(w, \"<br>\")\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 241, "char_end": 299, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 14, "char_start": 313, "char_end": 345, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 11, "char_start": 241, "char_end": 292, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 14, "char_start": 306, "char_end": 343, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 289, "char_end": 297, "chars": " + table"}], "added": [{"char_start": 288, "char_end": 289, "chars": "?"}, {"char_start": 336, "char_end": 341, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to display all records from a specified MySQL table in an HTTP response."}
{"func_name": "filter_session_io", "func_src_before": "filter_session_io(struct io *io, int evt, void *arg)\n{\n\tstruct filter_session *fs = arg;\n\tchar *line = NULL;\n\tssize_t len;\n\n\tlog_trace(TRACE_IO, \"filter session: %p: %s %s\", fs, io_strevent(evt),\n\t    io_strio(io));\n\n\tswitch (evt) {\n\tcase IO_DATAIN:\n\tnextline:\n\t\tline = io_getline(fs->io, &len);\n\t\t/* No complete line received */\n\t\tif (line == NULL)\n\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\n\tcase IO_DISCONNECTED:\n\t\tio_free(fs->io);\n\t\tfs->io = NULL;\n\t\tbreak;\n\t}\n}", "func_src_after": "filter_session_io(struct io *io, int evt, void *arg)\n{\n\tstruct filter_session *fs = arg;\n\tchar *line = NULL;\n\tssize_t len;\n\n\tlog_trace(TRACE_IO, \"filter session: %p: %s %s\", fs, io_strevent(evt),\n\t    io_strio(io));\n\n\tswitch (evt) {\n\tcase IO_DATAIN:\n\tnextline:\n\t\tline = io_getline(fs->io, &len);\n\t\t/* No complete line received */\n\t\tif (line == NULL)\n\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\t}\n}", "commit_link": "github.com/openbsd/src/commit/6c3220444ed06b5796dedfd53a0f4becd903c0d1", "file_name": "usr.sbin/smtpd/lka_filter.c", "vul_type": "cwe-476", "description": "Write a C function named `filter_session_io` that processes IO events for a session, handling data input and disconnection."}
{"func_name": "testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError", "func_src_before": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    unrelated_source_path = tempfile.mktemp()\n    with open(unrelated_source_path, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "func_src_after": "  def testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError(self):\n    # Create an unrelated source file.\n    fd, unrelated_source_path = tempfile.mkstemp()\n    with open(fd, \"wt\") as source_file:\n      source_file.write(\"print('hello, world')\\n\")\n\n    self.assertEqual({},\n                     source_utils.annotate_source(self.dump,\n                                                  unrelated_source_path))\n\n    # Clean up unrelated source file.\n    os.remove(unrelated_source_path)", "line_changes": {"deleted": [{"line_no": 3, "char_start": 111, "char_end": 157, "line": "    unrelated_source_path = tempfile.mktemp()\n"}, {"line_no": 4, "char_start": 157, "char_end": 216, "line": "    with open(unrelated_source_path, \"wt\") as source_file:\n"}], "added": [{"line_no": 3, "char_start": 111, "char_end": 162, "line": "    fd, unrelated_source_path = tempfile.mkstemp()\n"}, {"line_no": 4, "char_start": 162, "char_end": 202, "line": "    with open(fd, \"wt\") as source_file:\n"}]}, "char_changes": {"deleted": [{"char_start": 171, "char_end": 192, "chars": "unrelated_source_path"}], "added": [{"char_start": 114, "char_end": 118, "chars": " fd,"}, {"char_start": 154, "char_end": 155, "chars": "s"}, {"char_start": 176, "char_end": 178, "chars": "fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/3752cc4c6ba6b69f04f857c6047adde9e8487bd6", "file_name": "source_utils_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359237\nChange-Id: I7fa45e888deff612ca53a4f8610cfad8f28e9671", "description": "Write a Python function to test that calling a source annotation utility with an unrelated source file does not produce an error, and clean up the file afterwards."}
{"func_name": "DefaultArchiveExtractor::extract", "func_src_before": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "func_src_after": "    @Override\n    public void extract(String archive, String destinationDirectory) throws ArchiveExtractionException {\n        final File archiveFile = new File(archive);\n\n        try (FileInputStream fis = new FileInputStream(archiveFile)) {\n            if (\"msi\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                String command = \"msiexec /a \" + archiveFile.getAbsolutePath() + \" /qn TARGETDIR=\\\"\"\n                        + destinationDirectory + \"\\\"\";\n                Process child = Runtime.getRuntime().exec(command);\n                try {\n                    int result = child.waitFor();\n                    if (result != 0) {\n                        throw new ArchiveExtractionException(\n                                \"Could not extract \" + archiveFile.getAbsolutePath() + \"; return code \" + result);\n                    }\n                } catch (InterruptedException e) {\n                    throw new ArchiveExtractionException(\n                            \"Unexpected interruption of while waiting for extraction process\", e);\n                }\n            } else if (\"zip\".equals(FileUtils.getExtension(archiveFile.getAbsolutePath()))) {\n                ZipFile zipFile = new ZipFile(archiveFile);\n                try {\n                    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n                    while (entries.hasMoreElements()) {\n                        ZipEntry entry = entries.nextElement();\n                        final File destPath = new File(destinationDirectory, entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }\n                        prepDestination(destPath, entry.isDirectory());\n                        if (!entry.isDirectory()) {\n                            InputStream in = null;\n                            OutputStream out = null;\n                            try {\n                                in = zipFile.getInputStream(entry);\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(in, out);\n                            } finally {\n                                IOUtils.closeQuietly(in);\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                    }\n                } finally {\n                    zipFile.close();\n                }\n            } else {\n                // TarArchiveInputStream can be constructed with a normal FileInputStream if\n                // we ever need to extract regular '.tar' files.\n                TarArchiveInputStream tarIn = null;\n                try {\n                    tarIn = new TarArchiveInputStream(new GzipCompressorInputStream(fis));\n\n                    TarArchiveEntry tarEntry = tarIn.getNextTarEntry();\n                    String canonicalDestinationDirectory = new File(destinationDirectory).getCanonicalPath();\n                    while (tarEntry != null) {\n                        // Create a file for this tarEntry\n                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n                        prepDestination(destPath, tarEntry.isDirectory());\n\n                        if (!startsWithPath(destPath.getCanonicalPath(), canonicalDestinationDirectory)) {\n                            throw new IOException(\n                                    \"Expanding \" + tarEntry.getName() + \" would create file outside of \" + canonicalDestinationDirectory\n                            );\n                        }\n\n                        if (!tarEntry.isDirectory()) {\n                            destPath.createNewFile();\n                            boolean isExecutable = (tarEntry.getMode() & 0100) > 0;\n                            destPath.setExecutable(isExecutable);\n\n                            OutputStream out = null;\n                            try {\n                                out = new FileOutputStream(destPath);\n                                IOUtils.copy(tarIn, out);\n                            } finally {\n                                IOUtils.closeQuietly(out);\n                            }\n                        }\n                        tarEntry = tarIn.getNextTarEntry();\n                    }\n                } finally {\n                    IOUtils.closeQuietly(tarIn);\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveExtractionException(\"Could not extract archive: '\"\n                    + archive\n                    + \"'\", e);\n        }\n    }", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1467, "char_end": 1580, "line": "                        final File destPath = new File(destinationDirectory + File.separator + entry.getName());\n"}, {"line_no": 55, "char_start": 2986, "char_end": 3102, "line": "                        final File destPath = new File(destinationDirectory + File.separator + tarEntry.getName());\n"}], "added": [{"line_no": 26, "char_start": 1467, "char_end": 1562, "line": "                        final File destPath = new File(destinationDirectory, entry.getName());\n"}, {"line_no": 27, "char_start": 1562, "char_end": 1657, "line": "                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n"}, {"line_no": 28, "char_start": 1657, "char_end": 1730, "line": "                            throw new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 29, "char_start": 1730, "char_end": 1756, "line": "                        }\n"}, {"line_no": 58, "char_start": 3162, "char_end": 3260, "line": "                        final File destPath = new File(destinationDirectory, tarEntry.getName());\n"}]}, "char_changes": {"deleted": [{"char_start": 1542, "char_end": 1579, "chars": " + File.separator + entry.getName());"}, {"char_start": 3061, "char_end": 3080, "chars": " + File.separator +"}], "added": [{"char_start": 1542, "char_end": 1755, "chars": ", entry.getName());\n                        if (!destPath.toPath().normalize().startsWith(destinationDirectory)) {\n                            throw new RuntimeException(\"Bad zip entry\");\n                        }"}, {"char_start": 3237, "char_end": 3238, "chars": ","}]}, "commit_link": "github.com/eirslett/frontend-maven-plugin/commit/9f53f7617b41d89cbef1a29342c6b6b7441fa78a", "file_name": "ArchiveExtractor.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java method to extract files from an archive (ZIP, MSI, or TAR.GZ) to a specified directory."}
{"func_name": "MultiPartInputFile::Data::chunkOffsetReconstruction", "func_src_before": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber> static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}", "func_src_after": "MultiPartInputFile::Data::chunkOffsetReconstruction(OPENEXR_IMF_INTERNAL_NAMESPACE::IStream& is, const vector<InputPartData*>& parts)\n{\n    //\n    // Reconstruct broken chunk offset tables. Stop once we received any exception.\n    //\n\n    Int64 position = is.tellg();\n\n    \n    //\n    // check we understand all the parts available: if not, we cannot continue\n    // exceptions thrown here should trickle back up to the constructor\n    //\n    \n    for (size_t i = 0; i < parts.size(); i++)\n    {\n        Header& header=parts[i]->header;\n        \n        //\n        // do we have a valid type entry?\n        // we only need them for true multipart files or single part non-image (deep) files\n        //\n        if(!header.hasType() && (isMultiPart(version) || isNonImage(version)))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with missing type\");\n        }\n        if(!isSupportedType(header.type()))\n        {\n            throw IEX_NAMESPACE::ArgExc(\"cannot reconstruct incomplete file: part with unknown type \"+header.type());\n        }\n    }\n    \n    \n    // how many chunks should we read? We should stop when we reach the end\n    size_t total_chunks = 0;\n        \n    // for tiled-based parts, array of (pointers to) tileOffsets objects\n    // to create mapping between tile coordinates and chunk table indices\n    \n    \n    vector<TileOffsets*> tileOffsets(parts.size());\n    \n    // for scanline-based parts, number of scanlines in each chunk\n    vector<int> rowsizes(parts.size());\n        \n    for(size_t i = 0 ; i < parts.size() ; i++)\n    {\n        total_chunks += parts[i]->chunkOffsets.size();\n        if (isTiled(parts[i]->header.type()))\n        {\n            tileOffsets[i] = createTileOffsets(parts[i]->header);\n        }else{\n            tileOffsets[i] = NULL;\n            // (TODO) fix this so that it doesn't need to be revised for future compression types.\n            switch(parts[i]->header.compression())\n            {\n                case DWAB_COMPRESSION :\n                    rowsizes[i] = 256;\n                    break;\n                case PIZ_COMPRESSION :\n                case B44_COMPRESSION :\n                case B44A_COMPRESSION :\n                case DWAA_COMPRESSION :\n                    rowsizes[i]=32;\n                    break;\n                case ZIP_COMPRESSION :\n                case PXR24_COMPRESSION :\n                    rowsizes[i]=16;\n                    break;\n                case ZIPS_COMPRESSION :\n                case RLE_COMPRESSION :\n                case NO_COMPRESSION :\n                    rowsizes[i]=1;\n                    break;\n                default :\n                    throw(IEX_NAMESPACE::ArgExc(\"Unknown compression method in chunk offset reconstruction\"));\n            }\n        }\n     }\n        \n     try\n     {\n            \n        //\n        // \n        //\n        \n        Int64 chunk_start = position;\n        for (size_t i = 0; i < total_chunks ; i++)\n        {\n            //\n            // do we have a part number?\n            //\n            \n            int partNumber = 0;\n            if(isMultiPart(version))\n            {\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, partNumber);\n            }\n            \n            \n            \n            if(partNumber<0 || partNumber>= static_cast<int>(parts.size()))\n            {\n                throw IEX_NAMESPACE::IoExc(\"part number out of range\");\n            }\n            \n            Header& header = parts[partNumber]->header;\n\n            // size of chunk NOT including multipart field\n            \n            Int64 size_of_chunk=0;\n\n            if (isTiled(header.type()))\n            {\n                //\n                // \n                //\n                int tilex,tiley,levelx,levely;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tilex);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, tiley);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levelx);\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, levely);\n                \n                //std::cout << \"chunk_start for \" << tilex <<',' << tiley << ',' << levelx << ' ' << levely << ':' << chunk_start << std::endl;\n                    \n                \n                if(!tileOffsets[partNumber])\n                {\n                    // this shouldn't actually happen - we should have allocated a valid\n                    // tileOffsets for any part which isTiled\n                    throw IEX_NAMESPACE::IoExc(\"part not tiled\");\n                    \n                }\n                \n                if(!tileOffsets[partNumber]->isValidTile(tilex,tiley,levelx,levely))\n                {\n                    throw IEX_NAMESPACE::IoExc(\"invalid tile coordinates\");\n                }\n                \n                (*tileOffsets[partNumber])(tilex,tiley,levelx,levely)=chunk_start;\n                \n                // compute chunk sizes - different procedure for deep tiles and regular\n                // ones\n                if(header.type()==DEEPTILE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    //add 40 byte header to packed sizes (tile coordinates, packed sizes, unpacked size)\n                    size_of_chunk=packed_offset+packed_sample+40;\n                }\n                else\n                {\n                    \n                    // regular image has 20 bytes of header, 4 byte chunksize;\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);\n                    size_of_chunk=chunksize+20;\n                }\n            }\n            else\n            {\n                int y_coordinate;\n                OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, y_coordinate);\n                \n                \n                if(y_coordinate < header.dataWindow().min.y || y_coordinate > header.dataWindow().max.y)\n                {\n                   throw IEX_NAMESPACE::IoExc(\"y out of range\");\n                }\n                y_coordinate -= header.dataWindow().min.y;\n                y_coordinate /= rowsizes[partNumber];   \n                \n                if(y_coordinate < 0 || y_coordinate >= int(parts[partNumber]->chunkOffsets.size()))\n                {\n                   throw IEX_NAMESPACE::IoExc(\"chunk index out of range\");\n                }\n                \n                parts[partNumber]->chunkOffsets[y_coordinate]=chunk_start;\n                \n                if(header.type()==DEEPSCANLINE)\n                {\n                    Int64 packed_offset;\n                    Int64 packed_sample;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_offset);\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, packed_sample);\n                    \n                    \n                    size_of_chunk=packed_offset+packed_sample+28;\n                }\n                else\n                {\n                    int chunksize;\n                    OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, chunksize);   \n                    size_of_chunk=chunksize+8;\n                }\n                \n            }\n            \n            if(isMultiPart(version))\n            {\n                chunk_start+=4;\n            }\n            \n            chunk_start+=size_of_chunk;\n            \n            is.seekg(chunk_start);\n            \n        }\n        \n    }\n    catch (...)\n    {\n        //\n        // Suppress all exceptions.  This functions is\n        // called only to reconstruct the line offset\n        // table for incomplete files, and exceptions\n        // are likely.\n        //\n    }\n\n    // copy tiled part data back to chunk offsets\n    \n    for(size_t partNumber=0;partNumber<parts.size();partNumber++)\n    {\n        if(tileOffsets[partNumber])\n        {\n            size_t pos=0;\n            vector<vector<vector <Int64> > > offsets = tileOffsets[partNumber]->getOffsets();\n            for (size_t l = 0; l < offsets.size(); l++)\n                for (size_t y = 0; y < offsets[l].size(); y++)\n                    for (size_t x = 0; x < offsets[l][y].size(); x++)\n                    {\n                        parts[ partNumber ]->chunkOffsets[pos] = offsets[l][y][x];\n                        pos++;\n                    }\n           delete tileOffsets[partNumber];\n        }\n    }\n\n    is.clear();\n    is.seekg (position);\n}", "commit_link": "github.com/AcademySoftwareFoundation/openexr/commit/8b5370c688a7362673c3a5256d93695617a4cd9a", "file_name": "OpenEXR/IlmImf/ImfMultiPartInputFile.cpp", "vul_type": "cwe-787", "description": "Write a C++ function to reconstruct chunk offset tables from an OpenEXR file stream, handling exceptions silently."}
{"func_name": "set_eeprom_serial_number", "func_src_before": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 16);\n  _dirty = 1;\n\n  return 0;\n}", "func_src_after": "set_eeprom_serial_number (EEPROM_HDR *e, char *sn)\n{\n  strncpy (e->serial, sn, 12);\n  _dirty = 1;\n\n  return 0;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 16);\n"}], "added": [{"line_no": 3, "char_start": 53, "char_end": 84, "line": "  strncpy (e->serial, sn, 12);\n"}]}, "char_changes": {"deleted": [{"char_start": 80, "char_end": 81, "chars": "6"}], "added": [{"char_start": 80, "char_end": 81, "chars": "2"}]}, "commit_link": "github.com/picoflamingo/BBCape_EEPROM/commit/0b2d0afdd72e6ca35e9312bd43e29d488ae8c2e5", "file_name": "bbcape_eeprom.c", "vul_type": "cwe-119", "commit_msg": "Buffer Overflow fixed (https://github.com/picoflamingo/BBCape_EEPROM/issues/1)", "parent_commit": "21b1310205d6b2d9073efc51c6a32edbd9a08b89", "description": "Write a C function named `set_eeprom_serial_number` that copies a serial number string into an EEPROM structure and sets a dirty flag."}
{"func_name": "send_message", "func_src_before": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id='{email_id}'\"\"\".format(email_id=sender))\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "func_src_after": "@frappe.whitelist(allow_guest=True)\ndef send_message(subject=\"Website Query\", message=\"\", sender=\"\", status=\"Open\"):\n\tfrom frappe.www.contact import send_message as website_send_message\n\tlead = customer = None\n\n\twebsite_send_message(subject, message, sender)\n\n\tcustomer = frappe.db.sql(\"\"\"select distinct dl.link_name from `tabDynamic Link` dl\n\t\tleft join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'\n\t\tand c.email_id = %s\"\"\", sender)\n\n\tif not customer:\n\t\tlead = frappe.db.get_value('Lead', dict(email_id=sender))\n\t\tif not lead:\n\t\t\tnew_lead = frappe.get_doc(dict(\n\t\t\t\tdoctype='Lead',\n\t\t\t\temail_id = sender,\n\t\t\t\tlead_name = sender.split('@')[0].title()\n\t\t\t)).insert(ignore_permissions=True)\n\n\topportunity = frappe.get_doc(dict(\n\t\tdoctype ='Opportunity',\n\t\tenquiry_from = 'Customer' if customer else 'Lead',\n\t\tstatus = 'Open',\n\t\ttitle = subject,\n\t\tcontact_email = sender,\n\t\tto_discuss = message\n\t))\n\n\tif customer:\n\t\topportunity.customer = customer[0][0]\n\telif lead:\n\t\topportunity.lead = lead\n\telse:\n\t\topportunity.lead = new_lead.name\n\n\topportunity.insert(ignore_permissions=True)\n\n\tcomm = frappe.get_doc({\n\t\t\"doctype\":\"Communication\",\n\t\t\"subject\": subject,\n\t\t\"content\": message,\n\t\t\"sender\": sender,\n\t\t\"sent_or_received\": \"Received\",\n\t\t'reference_doctype': 'Opportunity',\n\t\t'reference_name': opportunity.name\n\t})\n\tcomm.insert(ignore_permissions=True)\n\n\treturn \"okay\"", "commit_link": "github.com/libracore/erpnext/commit/9acb885e60f77cd4e9ea8c98bdc39c18abcac731", "file_name": "erpnext/templates/utils.py", "vul_type": "cwe-089", "description": "Write a Python function in Frappe that handles incoming messages by creating leads, opportunities, and communications."}
{"func_name": "enc_untrusted_create_wait_queue", "func_src_before": "int32_t *enc_untrusted_create_wait_queue() {\n  MessageWriter input;\n  MessageReader output;\n  input.Push<uint64_t>(sizeof(int32_t));\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);\n  return queue;\n}", "func_src_after": "int32_t *enc_untrusted_create_wait_queue() {\n  MessageWriter input;\n  MessageReader output;\n  input.Push<uint64_t>(sizeof(int32_t));\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  if (!TrustedPrimitives::IsOutsideEnclave(queue, sizeof(int32_t))) {\n    TrustedPrimitives::BestEffortAbort(\n        \"enc_untrusted_create_wait_queue: queue should be in untrusted memory\");\n  }\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);\n  return queue;\n}", "commit_link": "github.com/google/asylo/commit/a37fb6a0e7daf30134dbbf357c9a518a1026aa02", "file_name": "asylo/platform/host_call/trusted/concurrency.cc", "vul_type": "cwe-787", "description": "Write a C++ function named `enc_untrusted_create_wait_queue` that allocates a wait queue using a non-system call dispatcher and handles errors."}
{"func_name": "tid_num_to_tag_nums", "func_src_before": "    def tid_num_to_tag_nums(self, tid_num):\n        ''' Returns list of the associated tag_nums to the given tid_num. '''\n\n        q = \"SELECT tag FROM tid_tag WHERE tid = '\" + str(tid_num) + \"'\"\n        self.query(q)\n        return [i[0] for i in self.c.fetchall()]", "func_src_after": "    def tid_num_to_tag_nums(self, tid_num):\n        ''' Returns list of the associated tag_nums to the given tid_num. '''\n\n        q = \"SELECT tag FROM tid_tag WHERE tid = ?\"\n        self.query(q, tid_num)\n        return [i[0] for i in self.c.fetchall()]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves a list of tags associated with a given ID from a database."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error, exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error, exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception, msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError, e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError), e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2184, "char_end": 2228, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2356, "char_end": 2431, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2399, "char_end": 2430, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/8c9aada477cbaa791b3fc9b1c5526d9960d10e5c", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function to initialize an SSL connection using pyOpenSSL, handling client certificates and setting SSL context options."}
{"func_name": "ImagingPcxDecode", "func_src_before": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if ((state->xsize * state->bits + 7) / 8 > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;\n\n    for (;;) {\n\n\tif (bytes < 1)\n\t    return ptr - buf;\n\n\tif ((*ptr & 0xC0) == 0xC0) {\n\n\t    /* Run */\n\t    if (bytes < 2)\n\t\treturn ptr - buf;\n\n\t    n = ptr[0] & 0x3F;\n\n\t    while (n > 0) {\n\t\tif (state->x >= state->bytes) {\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    break;\n\t\t}\n\t\tstate->buffer[state->x++] = ptr[1];\n\t\tn--;\n\t    }\n\n\t    ptr += 2; bytes -= 2;\n\n\t} else {\n\n\t    /* Literal */\n\t    state->buffer[state->x++] = ptr[0];\n\t    ptr++; bytes--;\n\n\t}\n\n\tif (state->x >= state->bytes) {\n        if (state->bytes % state->xsize && state->bytes > state->xsize) {\n            int bands = state->bytes / state->xsize;\n            int stride = state->bytes / bands;\n            int i;\n            for (i=1; i< bands; i++) {  // note -- skipping first band\n                memmove(&state->buffer[i*state->xsize],\n                        &state->buffer[i*stride],\n                        state->xsize);\n            }\n        }\n\t    /* Got a full line, unpack it */\n\t    state->shuffle((UINT8*) im->image[state->y + state->yoff] +\n\t\t\t   state->xoff * im->pixelsize, state->buffer,\n\t\t\t   state->xsize);\n\n\t    state->x = 0;\n\n\t    if (++state->y >= state->ysize) {\n\t\t/* End of file (errcode = 0) */\n\t\treturn -1;\n\t    }\n\t}\n\n    }\n}", "func_src_after": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if ((state->xsize * state->bits + 7) / 8 > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;\n\n    for (;;) {\n\n\tif (bytes < 1)\n\t    return ptr - buf;\n\n\tif ((*ptr & 0xC0) == 0xC0) {\n\n\t    /* Run */\n\t    if (bytes < 2)\n\t\treturn ptr - buf;\n\n\t    n = ptr[0] & 0x3F;\n\n\t    while (n > 0) {\n\t\tif (state->x >= state->bytes) {\n\t\t    state->errcode = IMAGING_CODEC_OVERRUN;\n\t\t    break;\n\t\t}\n\t\tstate->buffer[state->x++] = ptr[1];\n\t\tn--;\n\t    }\n\n\t    ptr += 2; bytes -= 2;\n\n\t} else {\n\n\t    /* Literal */\n\t    state->buffer[state->x++] = ptr[0];\n\t    ptr++; bytes--;\n\n\t}\n\n\tif (state->x >= state->bytes) {\n        if (state->bytes % state->xsize && state->bytes > state->xsize) {\n            int bands = state->bytes / state->xsize;\n            int stride = state->bytes / bands;\n            int i;\n            for (i=1; i< bands; i++) {  // note -- skipping first band\n                memmove(&state->buffer[i*state->xsize],\n                        &state->buffer[i*stride],\n                        state->xsize);\n            }\n        }\n\t    /* Got a full line, unpack it */\n\t    state->shuffle((UINT8*) im->image[state->y + state->yoff] +\n\t\t\t   state->xoff * im->pixelsize, state->buffer,\n\t\t\t   state->xsize);\n\n\t    state->x = 0;\n\n\t    if (++state->y >= state->ysize) {\n\t\t/* End of file (errcode = 0) */\n\t\treturn -1;\n\t    }\n\t}\n\n    }\n}", "commit_link": "github.com/python-pillow/Pillow/commit/6a83e4324738bb0452fbe8074a995b1c73f08de7#diff-9478f2787e3ae9668a15123b165c23ac", "file_name": "src/libImaging/PcxDecode.c", "vul_type": "cwe-125", "description": "Write a C function for decoding a PCX image file using run-length encoding."}
{"func_name": "tflite::ops::builtin::segment_sum::ResizeOutputTensor", "func_src_before": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  int max_index = -1;\n  const int segment_id_size = segment_ids->dims->data[0];\n  if (segment_id_size > 0) {\n    max_index = segment_ids->data.i32[segment_id_size - 1];\n  }\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "func_src_after": "TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* data,\n                                const TfLiteTensor* segment_ids,\n                                TfLiteTensor* output) {\n  // Segment ids should be of same cardinality as first input dimension and they\n  // should be increasing by at most 1, from 0 (e.g., [0, 0, 1, 2, 3] is valid)\n  const int segment_id_size = segment_ids->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, segment_id_size, data->dims->data[0]);\n  int previous_segment_id = -1;\n  for (int i = 0; i < segment_id_size; i++) {\n    const int current_segment_id = GetTensorData<int32_t>(segment_ids)[i];\n    if (i == 0) {\n      TF_LITE_ENSURE_EQ(context, current_segment_id, 0);\n    } else {\n      int delta = current_segment_id - previous_segment_id;\n      TF_LITE_ENSURE(context, delta == 0 || delta == 1);\n    }\n    previous_segment_id = current_segment_id;\n  }\n\n  const int max_index = previous_segment_id;\n\n  const int data_rank = NumDimensions(data);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));\n  output_shape->data[0] = max_index + 1;\n  for (int i = 1; i < data_rank; ++i) {\n    output_shape->data[i] = data->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}", "commit_link": "github.com/tensorflow/tensorflow/commit/204945b19e44b57906c9344c0d00120eeeae178a", "file_name": "tensorflow/lite/kernels/segment_sum.cc", "vul_type": "cwe-787", "description": "Write a C++ function to resize an output tensor based on segment IDs in TensorFlow Lite."}
{"func_name": "errorf", "func_src_before": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode(v.name))\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "func_src_after": "func (v *VM) errorf(format string, args ...interface{}) {\n\ti := v.prog[v.t.pc-1]\n\tprogRuntimeErrors.Add(v.name, 1)\n\tv.runtimeErrorMu.Lock()\n\tv.runtimeError = fmt.Sprintf(format+\"\\n\", args...)\n\tv.runtimeError += fmt.Sprintf(\n\t\t\"Error occurred at instruction %d {%s, %v}, originating in %s at line %d\\n\",\n\t\tv.t.pc-1, i.Opcode, i.Operand, v.name, i.SourceLine+1)\n\tv.runtimeError += fmt.Sprintf(\"Full input text from %q was %q\", v.input.Filename, v.input.Line)\n\tif *runtimeLogError || bool(glog.V(1)) {\n\t\tglog.Info(v.name + \": Runtime error: \" + v.runtimeError)\n\n\t\tglog.Infof(\"Set logging verbosity higher (-v1 or more) to see full VM state dump.\")\n\t}\n\tif glog.V(1) {\n\t\tglog.Infof(\"VM stack:\\n%s\", debug.Stack())\n\t\tglog.Infof(\"Dumping vm state\")\n\t\tglog.Infof(\"Name: %s\", v.name)\n\t\tglog.Infof(\"Input: %#v\", v.input)\n\t\tglog.Infof(\"Thread:\")\n\t\tglog.Infof(\" PC %v\", v.t.pc-1)\n\t\tglog.Infof(\" Matched %v\", v.t.matched)\n\t\tglog.Infof(\" Matches %v\", v.t.matches)\n\t\tglog.Infof(\" Timestamp %v\", v.t.time)\n\t\tglog.Infof(\" Stack %v\", v.t.stack)\n\t\tglog.Infof(v.DumpByteCode())\n\t}\n\tv.runtimeErrorMu.Unlock()\n\tv.terminate = true\n}", "line_changes": {"deleted": [{"line_no": 26, "char_start": 1027, "char_end": 1064, "line": "\t\tglog.Infof(v.DumpByteCode(v.name))\n"}], "added": [{"line_no": 26, "char_start": 1027, "char_end": 1058, "line": "\t\tglog.Infof(v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 1055, "char_end": 1061, "chars": "v.name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "vm.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that logs a formatted runtime error and VM state for a virtual machine object."}
{"func_name": "isValidAdmToken", "func_src_before": "def isValidAdmToken(adm_token):\n    conn, c = connectDB()\n    req = \"SELECT *  from {} where adm_token='{}'\".format(CFG(\"admintoken_table_name\"), adm_token)\n    answer = bool(queryOne(c, req))\n    closeDB(conn)\n    return answer", "func_src_after": "def isValidAdmToken(adm_token):\n    conn, c = connectDB()\n    req = \"SELECT *  from {} where adm_token=?\".format(CFG(\"admintoken_table_name\"))\n    answer = bool(queryOne(c, req, (adm_token,)))\n    closeDB(conn)\n    return answer", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function to check the validity of an admin token against a database."}
{"func_name": "updateDevice", "func_src_before": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "func_src_after": "updateDevice(const struct header * headers, time_t t)\n{\n\tstruct device ** pp = &devlist;\n\tstruct device * p = *pp;\t/* = devlist; */\n\twhile(p)\n\t{\n\t\tif(  p->headers[HEADER_NT].l == headers[HEADER_NT].l\n\t\t  && (0==memcmp(p->headers[HEADER_NT].p, headers[HEADER_NT].p, headers[HEADER_NT].l))\n\t\t  && p->headers[HEADER_USN].l == headers[HEADER_USN].l\n\t\t  && (0==memcmp(p->headers[HEADER_USN].p, headers[HEADER_USN].p, headers[HEADER_USN].l)) )\n\t\t{\n\t\t\t/*printf(\"found! %d\\n\", (int)(t - p->t));*/\n\t\t\tsyslog(LOG_DEBUG, \"device updated : %.*s\", headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t\t\tp->t = t;\n\t\t\t/* update Location ! */\n\t\t\tif(headers[HEADER_LOCATION].l > p->headers[HEADER_LOCATION].l)\n\t\t\t{\n\t\t\t\tstruct device * tmp;\n\t\t\t\ttmp = realloc(p, sizeof(struct device)\n\t\t\t\t    + headers[0].l+headers[1].l+headers[2].l);\n\t\t\t\tif(!tmp)\t/* allocation error */\n\t\t\t\t{\n\t\t\t\t\tsyslog(LOG_ERR, \"updateDevice() : memory allocation error\");\n\t\t\t\t\t*pp = p->next;\t/* remove \"p\" from the list */\n\t\t\t\t\tfree(p);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tp = tmp;\n\t\t\t\t*pp = p;\n\t\t\t}\n\t\t\tmemcpy(p->data + p->headers[0].l + p->headers[1].l,\n\t\t\t       headers[2].p, headers[2].l);\n\t\t\t/* TODO : check p->headers[HEADER_LOCATION].l */\n\t\t\treturn 0;\n\t\t}\n\t\tpp = &p->next;\n\t\tp = *pp;\t/* p = p->next; */\n\t}\n\tsyslog(LOG_INFO, \"new device discovered : %.*s\",\n\t       headers[HEADER_USN].l, headers[HEADER_USN].p);\n\t/* add */\n\t{\n\t\tchar * pc;\n\t\tint i;\n\t\tp = malloc(  sizeof(struct device)\n\t\t           + headers[0].l+headers[1].l+headers[2].l );\n\t\tif(!p) {\n\t\t\tsyslog(LOG_ERR, \"updateDevice(): cannot allocate memory\");\n\t\t\treturn -1;\n\t\t}\n\t\tp->next = devlist;\n\t\tp->t = t;\n\t\tpc = p->data;\n\t\tfor(i = 0; i < 3; i++)\n\t\t{\n\t\t\tp->headers[i].p = pc;\n\t\t\tp->headers[i].l = headers[i].l;\n\t\t\tmemcpy(pc, headers[i].p, headers[i].l);\n\t\t\tpc += headers[i].l;\n\t\t}\n\t\tdevlist = p;\n\t\tsendNotifications(NOTIF_NEW, p, NULL);\n\t}\n\treturn 1;\n}", "commit_link": "github.com/miniupnp/miniupnp/commit/cd506a67e174a45c6a202eff182a712955ed6d6f", "file_name": "minissdpd/minissdpd.c", "vul_type": "cwe-416", "description": "In C, write a function `updateDevice` to manage a device list by updating an existing device or adding a new one based on provided headers and a timestamp."}
{"func_name": "ipv4_pktinfo_prepare", "func_src_before": "void ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb)\n{\n\tstruct in_pktinfo *pktinfo = PKTINFO_SKB_CB(skb);\n\tbool prepare = (inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO) ||\n\t\t       ipv6_sk_rxinfo(sk);\n\n\tif (prepare && skb_rtable(skb)) {\n\t\t/* skb->cb is overloaded: prior to this point it is IP{6}CB\n\t\t * which has interface index (iif) as the first member of the\n\t\t * underlying inet{6}_skb_parm struct. This code then overlays\n\t\t * PKTINFO_SKB_CB and in_pktinfo also has iif as the first\n\t\t * element so the iif is picked up from the prior IPCB. If iif\n\t\t * is the loopback interface, then return the sending interface\n\t\t * (e.g., process binds socket to eth0 for Tx which is\n\t\t * redirected to loopback in the rtable/dst).\n\t\t */\n\t\tif (pktinfo->ipi_ifindex == LOOPBACK_IFINDEX)\n\t\t\tpktinfo->ipi_ifindex = inet_iif(skb);\n\n\t\tpktinfo->ipi_spec_dst.s_addr = fib_compute_spec_dst(skb);\n\t} else {\n\t\tpktinfo->ipi_ifindex = 0;\n\t\tpktinfo->ipi_spec_dst.s_addr = 0;\n\t}\n\tskb_dst_drop(skb);\n}", "func_src_after": "void ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb)\n{\n\tstruct in_pktinfo *pktinfo = PKTINFO_SKB_CB(skb);\n\tbool prepare = (inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO) ||\n\t\t       ipv6_sk_rxinfo(sk);\n\n\tif (prepare && skb_rtable(skb)) {\n\t\t/* skb->cb is overloaded: prior to this point it is IP{6}CB\n\t\t * which has interface index (iif) as the first member of the\n\t\t * underlying inet{6}_skb_parm struct. This code then overlays\n\t\t * PKTINFO_SKB_CB and in_pktinfo also has iif as the first\n\t\t * element so the iif is picked up from the prior IPCB. If iif\n\t\t * is the loopback interface, then return the sending interface\n\t\t * (e.g., process binds socket to eth0 for Tx which is\n\t\t * redirected to loopback in the rtable/dst).\n\t\t */\n\t\tif (pktinfo->ipi_ifindex == LOOPBACK_IFINDEX)\n\t\t\tpktinfo->ipi_ifindex = inet_iif(skb);\n\n\t\tpktinfo->ipi_spec_dst.s_addr = fib_compute_spec_dst(skb);\n\t} else {\n\t\tpktinfo->ipi_ifindex = 0;\n\t\tpktinfo->ipi_spec_dst.s_addr = 0;\n\t}\n\t/* We need to keep the dst for __ip_options_echo()\n\t * We could restrict the test to opt.ts_needtime || opt.srr,\n\t * but the following is good enough as IP options are not often used.\n\t */\n\tif (unlikely(IPCB(skb)->opt.optlen))\n\t\tskb_dst_force(skb);\n\telse\n\t\tskb_dst_drop(skb);\n}", "commit_link": "github.com/torvalds/linux/commit/34b2cef20f19c87999fff3da4071e66937db9644", "file_name": "net/ipv4/ip_sockglue.c", "vul_type": "cwe-476", "description": "Write a C function named `ipv4_pktinfo_prepare` that prepares packet information for an IPv4 socket in a Linux kernel environment."}
{"func_name": "(anonymous)", "func_src_before": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().append(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "func_src_after": "      this.element.find('table tbody').find('tr').each(function(index, tr) {\n        // add custom td handlers\n        var $currentRow = $(tr);\n        var $expand = null;\n        if(thisObj.options.expand_callback && thisObj.table){\n          var allTds = thisObj.table.fnGetTds($currentRow[0]);      \n          var row = [];\n          var i =0; \n          $(allTds).each(function(){ \n            row[i++] = $(this).html();\n          }); \n          $expand = thisObj.options.expand_callback(row);\n          if($expand === null){\n            var text = $currentRow.find('a.twist').text(); \n            $currentRow.find('a.twist').parent().text(text);\n            $currentRow.find('a.twist').remove();\n          }\n        }\n        \n        if(!$currentRow.data('events') || !('click' in $currentRow.data('events'))){\n          $currentRow.unbind('click').bind('click', function (e) {\n            if($(e.target).is('a') && $(e.target).hasClass('twist') && thisObj.options.expand_callback){\n              if(!$currentRow.next().hasClass('expanded')){\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded\n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n                if(!$expand.hasClass('expanded-row-inner-wrapper'))\n                  $expand.addClass('expanded-row-inner-wrapper');\n                if($expand && $expand.length > 0){\n                  $currentRow.after($('<tr>').addClass('expanded').append(\n                                  $('<td>').attr('colspan', $currentRow.find('td').length).append(\n                                    $expand)));\n                  $currentRow.find('a.twist').addClass('expanded');\n                }\n              }else{\n                thisObj.element.find('table tbody').find('tr.expanded').remove(); // remove all expanded \n                thisObj.element.find('table tbody').find('a.expanded').removeClass('expanded');\n              }\n            }else{\n              var $selectedRow = $currentRow; \n              var $rowCheckbox = $selectedRow.find('input[type=\"checkbox\"]');\n              if($rowCheckbox && $rowCheckbox.length > 0){\n                $selectedRow.toggleClass('selected-row');\n                if($selectedRow.hasClass('selected-row'))\n                  $rowCheckbox.attr('checked', true);\n                else\n                  $rowCheckbox.attr('checked', false);\n              }\n            }  \n          // checked/uncheck on checkbox\n            thisObj._onRowClick();\n            thisObj._trigger('row_click', e);\n          });\n        }\n\n        if (DEPRECATE && thisObj.options.context_menu_actions) {\n          rID = 'ri-'+S4()+S4();\n          $currentRow.attr('id', rID);\n          $.contextMenu({\n            selector: '#'+rID,\n            build: function(trigger, e) {\n              if(thisObj._countSelectedRows() <= 0)\n                return null;\n              return { items: thisObj.options.context_menu_actions()};\n            }\n          });\n        }\n      });\n      this.element.qtip();\n    },", "line_changes": {"deleted": [{"line_no": 15, "char_start": 590, "char_end": 653, "line": "            $currentRow.find('a.twist').parent().append(text);\n"}], "added": [{"line_no": 15, "char_start": 590, "char_end": 651, "line": "            $currentRow.find('a.twist').parent().text(text);\n"}]}, "char_changes": {"deleted": [{"char_start": 639, "char_end": 645, "chars": "append"}], "added": [{"char_start": 639, "char_end": 643, "chars": "text"}]}, "commit_link": "github.com/eethomas/eucalyptus/commit/e6133f9af0199452f7dd51bf6ad146e73809046f", "file_name": "eucatable.js", "vul_type": "cwe-079", "commit_msg": "XSS Additional Fix: html(..) to text(..) in eucatable and sgroup JS files", "description": "In JavaScript, write a function to handle custom interactions with table rows, including expanding details and selecting rows with checkboxes."}
{"func_name": "glyph_cache_put", "func_src_before": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index > glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "func_src_after": "BOOL glyph_cache_put(rdpGlyphCache* glyphCache, UINT32 id, UINT32 index, rdpGlyph* glyph)\n{\n\trdpGlyph* prevGlyph;\n\n\tif (id > 9)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index >= glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n\n\tWLog_Print(glyphCache->log, WLOG_DEBUG, \"GlyphCachePut: id: %\" PRIu32 \" index: %\" PRIu32 \"\", id,\n\t           index);\n\tprevGlyph = glyphCache->glyphCache[id].entries[index];\n\n\tif (prevGlyph)\n\t\tprevGlyph->Free(glyphCache->context, prevGlyph);\n\n\tglyphCache->glyphCache[id].entries[index] = glyph;\n\treturn TRUE;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/c0fd449ec0870b050d350d6d844b1ea6dad4bc7d", "file_name": "libfreerdp/cache/glyph.c", "vul_type": "cwe-125", "description": "Write a C function named `glyph_cache_put` that stores a glyph in a glyph cache, ensuring the cache ID and index are within valid bounds, and logs the operation."}
{"func_name": "update_institutions", "func_src_before": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES ('%s')\" % current_institution\n            sqlite.execute(sql)\n            conn.commit()", "func_src_after": "def update_institutions(conn, sqlite, k10plus, ai):\n    \"\"\"\n    Update the institution table.\n    \"\"\"\n    current_institutions = get_all_current_institutions(k10plus, ai)\n    old_institutions = get_all_old_institutions(conn, sqlite)\n\n    # Check if the institution table is allready filled and this is not the first checkup\n    institution_table_is_filled = len(old_institutions) > 10\n\n    for old_institution in old_institutions:\n        if institution_table_is_filled and old_institution not in current_institutions:\n            message = \"Die ISIL %s ist im aktuellen Import nicht mehr vorhanden.\\nWenn dies beabsichtigt ist, bitte die Institution aus der Datenbank loeschen.\" % old_institution\n            send_message(message)\n\n    for current_institution in current_institutions:\n        if current_institution == \" \" or '\"' in current_institution:\n                continue\n        if current_institution not in old_institutions:\n            message = \"The institution %s is new in Solr.\" % current_institution\n            if institution_table_is_filled:\n                send_message(message)\n            else:\n                logging.info(message)\n            sql = \"INSERT INTO institution (institution) VALUES (?)\"\n            sqlite.execute(sql, (current_institution,))\n            conn.commit()", "commit_link": "github.com/miku/siskin/commit/7fa398d2fea72bf2e8b4808f75df4b3d35ae959a", "file_name": "bin/solrcheckup.py", "vul_type": "cwe-089", "description": "Write a Python function to update an institution table by comparing old and new institution records, sending notifications for discrepancies, and inserting new records."}
{"func_name": "_drain_to_working_set", "func_src_before": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with open(self.working_set_filename, 'wb') as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        line = base64.b64encode(pickle.dumps({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }))\n                        work_file.write(line)\n                        work_file.write(b'\\n')\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )", "func_src_after": "    def _drain_to_working_set(self, size=1000):\n        logger.info('Draining to working set %s', self.working_set_filename)\n\n        assert not os.path.exists(self.working_set_filename)\n\n        with new_session() as session:\n            query = session.query(Result)\n\n            if self.after:\n                query = query.filter(Result.datetime > self.after)\n\n            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n                last_id = -1\n                num_results = 0\n                running = True\n\n                while running:\n                    # Optimized for SQLite scrolling window\n                    rows = query.filter(Result.id > last_id).limit(size).all()\n\n                    if not rows:\n                        break\n\n                    delete_ids = []\n\n                    for result in rows:\n                        pickle.dump({\n                            'id': result.id,\n                            'project_id': result.project_id,\n                            'shortcode': result.shortcode,\n                            'url': result.url,\n                            'encoding': result.encoding,\n                            'datetime': result.datetime,\n                        }, work_file)\n\n                        num_results += 1\n                        self.items_count += 1\n\n                        delete_ids.append(result.id)\n\n                        if num_results % 10000 == 0:\n                            logger.info('Drain progress: %d', num_results)\n\n                        if num_results % 100000 == 0:\n                            # Risky, but need to do this since WAL\n                            # performance is low on large transactions\n                            logger.info(\"Checkpoint. (Don't delete stray files if program crashes!)\")\n                            work_file.flush()\n                            session.commit()\n\n                        if self.max_items and num_results >= self.max_items:\n                            logger.info('Reached max items %d.', self.max_items)\n                            running = False\n                            break\n\n                    if self.settings['delete']:\n                        delete_query = delete(Result).where(\n                            Result.id == bindparam('id')\n                        )\n                        session.execute(\n                            delete_query,\n                            [{'id': result_id} for result_id in delete_ids]\n                        )\n\n                pickle.dump('eof', work_file)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 365, "char_end": 434, "line": "            with open(self.working_set_filename, 'wb') as work_file:\n"}, {"line_no": 27, "char_start": 839, "char_end": 902, "line": "                        line = base64.b64encode(pickle.dumps({\n"}, {"line_no": 34, "char_start": 1228, "char_end": 1256, "line": "                        }))\n"}, {"line_no": 35, "char_start": 1256, "char_end": 1302, "line": "                        work_file.write(line)\n"}, {"line_no": 36, "char_start": 1302, "char_end": 1349, "line": "                        work_file.write(b'\\n')\n"}], "added": [{"line_no": 12, "char_start": 365, "char_end": 456, "line": "            with gzip.open(self.working_set_filename, 'wb', compresslevel=1) as work_file:\n"}, {"line_no": 27, "char_start": 861, "char_end": 899, "line": "                        pickle.dump({\n"}, {"line_no": 34, "char_start": 1225, "char_end": 1263, "line": "                        }, work_file)\n"}]}, "char_changes": {"deleted": [{"char_start": 863, "char_end": 887, "chars": "line = base64.b64encode("}, {"char_start": 898, "char_end": 899, "chars": "s"}, {"char_start": 1253, "char_end": 1347, "chars": "))\n                        work_file.write(line)\n                        work_file.write(b'\\n'"}], "added": [{"char_start": 382, "char_end": 387, "chars": "gzip."}, {"char_start": 423, "char_end": 440, "chars": ", compresslevel=1"}, {"char_start": 1250, "char_end": 1261, "chars": ", work_file"}, {"char_start": 2534, "char_end": 2581, "chars": "\n\n                pickle.dump('eof', work_file)"}]}, "commit_link": "github.com/ArchiveTeam/terroroftinytown/commit/4614d62a1406ee88562486c24105f38aef48be41", "file_name": "export.py", "vul_type": "cwe-502", "commit_msg": "release: Compress the working set file\n\nInstead of base64-encoding it into lines, save and load each pickle\nsequentially which the pickle module supports which avoids unneeded\nbloat. The entire file stream is gzip compressed (level 1, fast) to\nreduce the disk space usage further.", "parent_commit": "f9f8bc584c714321328b3ec8979eeb4d78cff09b", "description": "Write a Python function to export a dataset to a file, with optional data filtering and deletion after export."}
{"func_name": "keyctl_read_key", "func_src_before": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "func_src_after": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\tif (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {\n\t\tret = -ENOKEY;\n\t\tgoto error2;\n\t}\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error2;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/37863c43b2c6464f252862bf2e9768264e961678", "file_name": "security/keys/keyctl.c", "vul_type": "cwe-476", "description": "Write a C function named `keyctl_read_key` that reads data from a specified key into a buffer, handling permissions and errors."}
{"func_name": "ParseMPLSLabelStack", "func_src_before": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.Atoi(elem)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i < 0 || i > ((1<<20)-1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "func_src_after": "func ParseMPLSLabelStack(buf string) (*MPLSLabelStack, error) {\n\telems := strings.Split(buf, \"/\")\n\tlabels := make([]uint32, 0, len(elems))\n\tif len(elems) == 0 {\n\t\tgoto ERR\n\t}\n\tfor _, elem := range elems {\n\t\ti, err := strconv.ParseUint(elem, 10, 32)\n\t\tif err != nil {\n\t\t\tgoto ERR\n\t\t}\n\t\tif i > ((1 << 20) - 1) {\n\t\t\tgoto ERR\n\t\t}\n\t\tlabels = append(labels, uint32(i))\n\t}\n\treturn NewMPLSLabelStack(labels...), nil\nERR:\n\treturn nil, NewMessageError(BGP_ERROR_UPDATE_MESSAGE_ERROR, BGP_ERROR_SUB_MALFORMED_ATTRIBUTE_LIST, nil, \"invalid mpls label stack format\")\n}", "line_changes": {"deleted": [{"line_no": 8, "char_start": 205, "char_end": 236, "line": "\t\ti, err := strconv.Atoi(elem)\n"}, {"line_no": 12, "char_start": 270, "char_end": 302, "line": "\t\tif i < 0 || i > ((1<<20)-1) {\n"}], "added": [{"line_no": 8, "char_start": 205, "char_end": 249, "line": "\t\ti, err := strconv.ParseUint(elem, 10, 32)\n"}, {"line_no": 12, "char_start": 283, "char_end": 310, "line": "\t\tif i > ((1 << 20) - 1) {\n"}]}, "char_changes": {"deleted": [{"char_start": 225, "char_end": 234, "chars": "Atoi(elem"}, {"char_start": 274, "char_end": 283, "chars": " i < 0 ||"}, {"char_start": 296, "char_end": 297, "chars": "-"}], "added": [{"char_start": 225, "char_end": 247, "chars": "ParseUint(elem, 10, 32"}, {"char_start": 295, "char_end": 296, "chars": " "}, {"char_start": 298, "char_end": 299, "chars": " "}, {"char_start": 302, "char_end": 305, "chars": " - "}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse a string of MPLS labels separated by slashes into a label stack, returning an error for invalid formats."}
{"func_name": "get_monthly_ranks_for_scene", "func_src_before": "def get_monthly_ranks_for_scene(db, scene, tag):\n\n    sql = \"SELECT date, rank FROM ranks WHERE scene='{}' AND player='{}'\".format(scene, tag)\n    res = db.exec(sql)\n\n    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]\n\n    # Build up a dict of {date: rank}\n    ranks = {}\n    for r in res:\n        ranks[r[0]] = r[1]\n\n    return ranks", "func_src_after": "def get_monthly_ranks_for_scene(db, scene, tag):\n\n    sql = \"SELECT date, rank FROM ranks WHERE scene='{scene}' AND player='{tag}'\"\n    args = {'scene': scene, 'tag': tag}\n    res = db.exec(sql, args)\n\n    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]\n\n    # Build up a dict of {date: rank}\n    ranks = {}\n    for r in res:\n        ranks[r[0]] = r[1]\n\n    return ranks", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "bracket_utils.py", "vul_type": "cwe-089", "description": "Write a Python function to retrieve a dictionary of dates and ranks for a specific scene and player from a database, ensuring the player was active in the previous month."}
{"func_name": "create_or_update_repo", "func_src_before": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "func_src_after": "def create_or_update_repo(folder, which_branch):\n    print ('--------------------------------------------')\n    print (f'Create or update repo {folder}')\n\n    if not os.path.isdir(folder):\n        # if the repo doesn't already exist, create it\n        create_new_repo(folder,which_branch)\n\n    else:\n        os.chdir(folder)\n\n        # whether or not this repo was newly created, set the default HEAD\n        # on the origin repo\n        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n\n        # if this repo has no branches with valid commits, add an\n        # empty commit to the specified branch so that the repository\n        # is not empty\n        add_empty_commit(folder,which_branch)\n        \n        \n    # set/correct the permissions of all files\n    os.chdir(folder)\n    for root, dirs, files in os.walk(folder):\n        for entry in files + dirs:\n            shutil.chown(os.path.join(root, entry), group=DAEMONCGI_GROUP)", "line_changes": {"deleted": [{"line_no": 14, "char_start": 430, "char_end": 500, "line": "        os.system(f'git symbolic-ref HEAD refs/heads/{which_branch}')\n"}], "added": [{"line_no": 14, "char_start": 430, "char_end": 516, "line": "        subprocess.run(['git', 'symbolic-ref', 'HEAD', f'refs/heads/{which_branch}'])\n"}]}, "char_changes": {"deleted": [{"char_start": 438, "char_end": 472, "chars": "os.system(f'git symbolic-ref HEAD "}], "added": [{"char_start": 438, "char_end": 487, "chars": "subprocess.run(['git', 'symbolic-ref', 'HEAD', f'"}, {"char_start": 513, "char_end": 514, "chars": "]"}]}, "commit_link": "github.com/Submitty/Submitty/commit/d6eb04149be92b6c9f334570e746cb39e65098c5", "file_name": "generate_repos.py", "vul_type": "cwe-078", "commit_msg": "[SECURITY][Bugfix:System] Prevent generate_repos injection (#7903)\n\n* Replace os.system to subprocess\r\n\r\n* Update bin/generate_repos.py\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>\r\n\r\nCo-authored-by: William Allen <16820599+williamjallen@users.noreply.github.com>", "parent_commit": "1d6aed0c90c4ad468646c17e8537b876cddae41c", "description": "Write a Python function to manage a git repository by creating or updating it and setting file permissions."}
{"func_name": "store_versioninfo_gnu_verdef", "func_src_before": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && ((char *)defs + i < end); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || vstart + sizeof (Elf_(Verdaux)) > end) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || vstart + sizeof(Elf_(Verdaux)) > end) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "func_src_after": "static Sdb *store_versioninfo_gnu_verdef(ELFOBJ *bin, Elf_(Shdr) *shdr, int sz) {\n\tconst char *section_name = \"\";\n\tconst char *link_section_name = \"\";\n\tchar *end = NULL;\n\tElf_(Shdr) *link_shdr = NULL;\n\tut8 dfs[sizeof (Elf_(Verdef))] = {0};\n\tSdb *sdb;\n\tint cnt, i;\n\tif (shdr->sh_link > bin->ehdr.e_shnum) {\n\t\treturn false;\n\t}\n\tlink_shdr = &bin->shdr[shdr->sh_link];\n\tif (shdr->sh_size < 1 || shdr->sh_size > SIZE_MAX) {\n\t\treturn false;\n\t}\n\tElf_(Verdef) *defs = calloc (shdr->sh_size, sizeof (char));\n\tif (!defs) {\n\t\treturn false;\n\t}\n\tif (bin->shstrtab && shdr->sh_name < bin->shstrtab_size) {\n\t\tsection_name = &bin->shstrtab[shdr->sh_name];\n\t}\n\tif (link_shdr && bin->shstrtab && link_shdr->sh_name < bin->shstrtab_size) {\n\t\tlink_section_name = &bin->shstrtab[link_shdr->sh_name];\n\t}\n\tif (!defs) {\n\t\tbprintf (\"Warning: Cannot allocate memory (Check Elf_(Verdef))\\n\");\n\t\treturn NULL;\n\t}\n\tsdb = sdb_new0 ();\n\tend = (char *)defs + shdr->sh_size;\n\tsdb_set (sdb, \"section_name\", section_name, 0);\n\tsdb_num_set (sdb, \"entries\", shdr->sh_info, 0);\n\tsdb_num_set (sdb, \"addr\", shdr->sh_addr, 0);\n\tsdb_num_set (sdb, \"offset\", shdr->sh_offset, 0);\n\tsdb_num_set (sdb, \"link\", shdr->sh_link, 0);\n\tsdb_set (sdb, \"link_section_name\", link_section_name, 0);\n\n\tfor (cnt = 0, i = 0; i >= 0 && cnt < shdr->sh_info && (end - (char *)defs > i); ++cnt) {\n\t\tSdb *sdb_verdef = sdb_new0 ();\n\t\tchar *vstart = ((char*)defs) + i;\n\t\tchar key[32] = {0};\n\t\tElf_(Verdef) *verdef = (Elf_(Verdef)*)vstart;\n\t\tElf_(Verdaux) aux = {0};\n\t\tint j = 0;\n\t\tint isum = 0;\n\n\t\tr_buf_read_at (bin->b, shdr->sh_offset + i, dfs, sizeof (Elf_(Verdef)));\n\t\tverdef->vd_version = READ16 (dfs, j)\n\t\tverdef->vd_flags = READ16 (dfs, j)\n\t\tverdef->vd_ndx = READ16 (dfs, j)\n\t\tverdef->vd_cnt = READ16 (dfs, j)\n\t\tverdef->vd_hash = READ32 (dfs, j)\n\t\tverdef->vd_aux = READ32 (dfs, j)\n\t\tverdef->vd_next = READ32 (dfs, j)\n\t\tint vdaux = verdef->vd_aux;\n\t\tif (vdaux < 1 || (char *)UINTPTR_MAX - vstart < vdaux) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tvstart += vdaux;\n\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tj = 0;\n\t\taux.vda_name = READ32 (vstart, j)\n\t\taux.vda_next = READ32 (vstart, j)\n\n\t\tisum = i + verdef->vd_aux;\n\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tsdb_num_set (sdb_verdef, \"idx\", i, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_version\", verdef->vd_version, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_ndx\", verdef->vd_ndx, 0);\n\t\tsdb_num_set (sdb_verdef, \"vd_cnt\", verdef->vd_cnt, 0);\n\t\tsdb_set (sdb_verdef, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\tsdb_set (sdb_verdef, \"flags\", get_ver_flags (verdef->vd_flags), 0);\n\n\t\tfor (j = 1; j < verdef->vd_cnt; ++j) {\n\t\t\tint k;\n\t\t\tSdb *sdb_parent = sdb_new0 ();\n\t\t\tisum += aux.vda_next;\n\t\t\tvstart += aux.vda_next;\n\t\t\tif (vstart > end || end - vstart < sizeof (Elf_(Verdaux))) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tk = 0;\n\t\t\taux.vda_name = READ32 (vstart, k)\n\t\t\taux.vda_next = READ32 (vstart, k)\n\t\t\tif (aux.vda_name > bin->dynstr_size) {\n\t\t\t\tsdb_free (sdb_verdef);\n\t\t\t\tsdb_free (sdb_parent);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tsdb_num_set (sdb_parent, \"idx\", isum, 0);\n\t\t\tsdb_num_set (sdb_parent, \"parent\", j, 0);\n\t\t\tsdb_set (sdb_parent, \"vda_name\", &bin->dynstr[aux.vda_name], 0);\n\t\t\tsnprintf (key, sizeof (key), \"parent%d\", j - 1);\n\t\t\tsdb_ns_set (sdb_verdef, key, sdb_parent);\n\t\t}\n\n\t\tsnprintf (key, sizeof (key), \"verdef%d\", cnt);\n\t\tsdb_ns_set (sdb, key, sdb_verdef);\n\t\tif (!verdef->vd_next) {\n\t\t\tsdb_free (sdb_verdef);\n\t\t\tgoto out_error;\n\t\t}\n\t\tif ((st32)verdef->vd_next < 1) {\n\t\t\teprintf (\"Warning: Invalid vd_next in the ELF version\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti += verdef->vd_next;\n\t}\n\tfree (defs);\n\treturn sdb;\nout_error:\n\tfree (defs);\n\tsdb_free (sdb);\n\treturn NULL;\n}", "commit_link": "github.com/radare/radare2/commit/62e39f34b2705131a2d08aff0c2e542c6a52cf0e", "file_name": "libr/bin/format/elf/elf.c", "vul_type": "cwe-476", "description": "Write a C function named `store_versioninfo_gnu_verdef` that processes GNU version definition sections from an ELF binary object."}
{"func_name": "parse_playlist", "func_src_before": "static int parse_playlist(HLSContext *c, const char *url,\n                          struct playlist *pls, AVIOContext *in)\n{\n    int ret = 0, is_segment = 0, is_variant = 0;\n    int64_t duration = 0;\n    enum KeyType key_type = KEY_NONE;\n    uint8_t iv[16] = \"\";\n    int has_iv = 0;\n    char key[MAX_URL_SIZE] = \"\";\n    char line[MAX_URL_SIZE];\n    const char *ptr;\n    int close_in = 0;\n    int64_t seg_offset = 0;\n    int64_t seg_size = -1;\n    uint8_t *new_url = NULL;\n    struct variant_info variant_info;\n    char tmp_str[MAX_URL_SIZE];\n    struct segment *cur_init_section = NULL;\n\n    if (!in) {\n#if 1\n        AVDictionary *opts = NULL;\n        close_in = 1;\n        /* Some HLS servers don't like being sent the range header */\n        av_dict_set(&opts, \"seekable\", \"0\", 0);\n\n        // broker prior HTTP options that should be consistent across requests\n        av_dict_set(&opts, \"user-agent\", c->user_agent, 0);\n        av_dict_set(&opts, \"cookies\", c->cookies, 0);\n        av_dict_set(&opts, \"headers\", c->headers, 0);\n\n        ret = avio_open2(&in, url, AVIO_FLAG_READ,\n                         c->interrupt_callback, &opts);\n        av_dict_free(&opts);\n        if (ret < 0)\n            return ret;\n#else\n        ret = open_in(c, &in, url);\n        if (ret < 0)\n            return ret;\n        close_in = 1;\n#endif\n    }\n\n    if (av_opt_get(in, \"location\", AV_OPT_SEARCH_CHILDREN, &new_url) >= 0)\n        url = new_url;\n\n    read_chomp_line(in, line, sizeof(line));\n    if (strcmp(line, \"#EXTM3U\")) {\n        ret = AVERROR_INVALIDDATA;\n        goto fail;\n    }\n\n    if (pls) {\n        free_segment_list(pls);\n        pls->finished = 0;\n        pls->type = PLS_TYPE_UNSPECIFIED;\n    }\n    while (!avio_feof(in)) {\n        read_chomp_line(in, line, sizeof(line));\n        if (av_strstart(line, \"#EXT-X-STREAM-INF:\", &ptr)) {\n            is_variant = 1;\n            memset(&variant_info, 0, sizeof(variant_info));\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_variant_args,\n                               &variant_info);\n        } else if (av_strstart(line, \"#EXT-X-KEY:\", &ptr)) {\n            struct key_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_key_args,\n                               &info);\n            key_type = KEY_NONE;\n            has_iv = 0;\n            if (!strcmp(info.method, \"AES-128\"))\n                key_type = KEY_AES_128;\n            if (!strcmp(info.method, \"SAMPLE-AES\"))\n                key_type = KEY_SAMPLE_AES;\n            if (!strncmp(info.iv, \"0x\", 2) || !strncmp(info.iv, \"0X\", 2)) {\n                ff_hex_to_data(iv, info.iv + 2);\n                has_iv = 1;\n            }\n            av_strlcpy(key, info.uri, sizeof(key));\n        } else if (av_strstart(line, \"#EXT-X-MEDIA:\", &ptr)) {\n            struct rendition_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_rendition_args,\n                               &info);\n            new_rendition(c, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-TARGETDURATION:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->target_duration = atoi(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-MEDIA-SEQUENCE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->start_seq_no = atoi(ptr);\n        } else if (av_strstart(line, \"#EXT-X-PLAYLIST-TYPE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            if (!strcmp(ptr, \"EVENT\"))\n                pls->type = PLS_TYPE_EVENT;\n            else if (!strcmp(ptr, \"VOD\"))\n                pls->type = PLS_TYPE_VOD;\n        } else if (av_strstart(line, \"#EXT-X-MAP:\", &ptr)) {\n            struct init_section_info info = {{0}};\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_init_section_args,\n                               &info);\n            cur_init_section = new_init_section(pls, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-ENDLIST\", &ptr)) {\n            if (pls)\n                pls->finished = 1;\n        } else if (av_strstart(line, \"#EXTINF:\", &ptr)) {\n            is_segment = 1;\n            duration   = atof(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-BYTERANGE:\", &ptr)) {\n            seg_size = atoi(ptr);\n            ptr = strchr(ptr, '@');\n            if (ptr)\n                seg_offset = atoi(ptr+1);\n        } else if (av_strstart(line, \"#\", NULL)) {\n            continue;\n        } else if (line[0]) {\n            if (is_variant) {\n                if (!new_variant(c, &variant_info, line, url)) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                is_variant = 0;\n            }\n            if (is_segment) {\n                struct segment *seg;\n                if (!pls) {\n                    if (!new_variant(c, 0, url, NULL)) {\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                    pls = c->playlists[c->n_playlists - 1];\n                }\n                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    AV_WB32(seg->iv + 12, seq);\n                }\n\n                if (key_type != KEY_NONE) {\n                    ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, key);\n                    seg->key = av_strdup(tmp_str);\n                    if (!seg->key) {\n                        av_free(seg);\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                } else {\n                    seg->key = NULL;\n                }\n\n                ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, line);\n                seg->url = av_strdup(tmp_str);\n                if (!seg->url) {\n                    av_free(seg->key);\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {\n                    seg->url_offset = seg_offset;\n                    seg_offset += seg_size;\n                    seg_size = -1;\n                } else {\n                    seg->url_offset = 0;\n                    seg_offset = 0;\n                }\n\n                seg->init_section = cur_init_section;\n            }\n        }\n    }\n    if (pls)\n        pls->last_load_time = av_gettime_relative();\n\nfail:\n    av_free(new_url);\n    if (close_in)\n        avio_close(in);\n    return ret;\n}", "func_src_after": "static int parse_playlist(HLSContext *c, const char *url,\n                          struct playlist *pls, AVIOContext *in)\n{\n    int ret = 0, is_segment = 0, is_variant = 0;\n    int64_t duration = 0;\n    enum KeyType key_type = KEY_NONE;\n    uint8_t iv[16] = \"\";\n    int has_iv = 0;\n    char key[MAX_URL_SIZE] = \"\";\n    char line[MAX_URL_SIZE];\n    const char *ptr;\n    int close_in = 0;\n    int64_t seg_offset = 0;\n    int64_t seg_size = -1;\n    uint8_t *new_url = NULL;\n    struct variant_info variant_info;\n    char tmp_str[MAX_URL_SIZE];\n    struct segment *cur_init_section = NULL;\n\n    if (!in) {\n#if 1\n        AVDictionary *opts = NULL;\n        close_in = 1;\n        /* Some HLS servers don't like being sent the range header */\n        av_dict_set(&opts, \"seekable\", \"0\", 0);\n\n        // broker prior HTTP options that should be consistent across requests\n        av_dict_set(&opts, \"user-agent\", c->user_agent, 0);\n        av_dict_set(&opts, \"cookies\", c->cookies, 0);\n        av_dict_set(&opts, \"headers\", c->headers, 0);\n\n        ret = avio_open2(&in, url, AVIO_FLAG_READ,\n                         c->interrupt_callback, &opts);\n        av_dict_free(&opts);\n        if (ret < 0)\n            return ret;\n#else\n        ret = open_in(c, &in, url);\n        if (ret < 0)\n            return ret;\n        close_in = 1;\n#endif\n    }\n\n    if (av_opt_get(in, \"location\", AV_OPT_SEARCH_CHILDREN, &new_url) >= 0)\n        url = new_url;\n\n    read_chomp_line(in, line, sizeof(line));\n    if (strcmp(line, \"#EXTM3U\")) {\n        ret = AVERROR_INVALIDDATA;\n        goto fail;\n    }\n\n    if (pls) {\n        free_segment_list(pls);\n        pls->finished = 0;\n        pls->type = PLS_TYPE_UNSPECIFIED;\n    }\n    while (!avio_feof(in)) {\n        read_chomp_line(in, line, sizeof(line));\n        if (av_strstart(line, \"#EXT-X-STREAM-INF:\", &ptr)) {\n            is_variant = 1;\n            memset(&variant_info, 0, sizeof(variant_info));\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_variant_args,\n                               &variant_info);\n        } else if (av_strstart(line, \"#EXT-X-KEY:\", &ptr)) {\n            struct key_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_key_args,\n                               &info);\n            key_type = KEY_NONE;\n            has_iv = 0;\n            if (!strcmp(info.method, \"AES-128\"))\n                key_type = KEY_AES_128;\n            if (!strcmp(info.method, \"SAMPLE-AES\"))\n                key_type = KEY_SAMPLE_AES;\n            if (!strncmp(info.iv, \"0x\", 2) || !strncmp(info.iv, \"0X\", 2)) {\n                ff_hex_to_data(iv, info.iv + 2);\n                has_iv = 1;\n            }\n            av_strlcpy(key, info.uri, sizeof(key));\n        } else if (av_strstart(line, \"#EXT-X-MEDIA:\", &ptr)) {\n            struct rendition_info info = {{0}};\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_rendition_args,\n                               &info);\n            new_rendition(c, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-TARGETDURATION:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->target_duration = atoi(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-MEDIA-SEQUENCE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            pls->start_seq_no = atoi(ptr);\n        } else if (av_strstart(line, \"#EXT-X-PLAYLIST-TYPE:\", &ptr)) {\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            if (!strcmp(ptr, \"EVENT\"))\n                pls->type = PLS_TYPE_EVENT;\n            else if (!strcmp(ptr, \"VOD\"))\n                pls->type = PLS_TYPE_VOD;\n        } else if (av_strstart(line, \"#EXT-X-MAP:\", &ptr)) {\n            struct init_section_info info = {{0}};\n            ret = ensure_playlist(c, &pls, url);\n            if (ret < 0)\n                goto fail;\n            ff_parse_key_value(ptr, (ff_parse_key_val_cb) handle_init_section_args,\n                               &info);\n            cur_init_section = new_init_section(pls, &info, url);\n        } else if (av_strstart(line, \"#EXT-X-ENDLIST\", &ptr)) {\n            if (pls)\n                pls->finished = 1;\n        } else if (av_strstart(line, \"#EXTINF:\", &ptr)) {\n            is_segment = 1;\n            duration   = atof(ptr) * AV_TIME_BASE;\n        } else if (av_strstart(line, \"#EXT-X-BYTERANGE:\", &ptr)) {\n            seg_size = atoi(ptr);\n            ptr = strchr(ptr, '@');\n            if (ptr)\n                seg_offset = atoi(ptr+1);\n        } else if (av_strstart(line, \"#\", NULL)) {\n            continue;\n        } else if (line[0]) {\n            if (is_variant) {\n                if (!new_variant(c, &variant_info, line, url)) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                is_variant = 0;\n            }\n            if (is_segment) {\n                struct segment *seg;\n                if (!pls) {\n                    if (!new_variant(c, 0, url, NULL)) {\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                    pls = c->playlists[c->n_playlists - 1];\n                }\n                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    AV_WB32(seg->iv + 12, seq);\n                }\n\n                if (key_type != KEY_NONE) {\n                    ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, key);\n                    seg->key = av_strdup(tmp_str);\n                    if (!seg->key) {\n                        av_free(seg);\n                        ret = AVERROR(ENOMEM);\n                        goto fail;\n                    }\n                } else {\n                    seg->key = NULL;\n                }\n\n                ff_make_absolute_url(tmp_str, sizeof(tmp_str), url, line);\n                seg->url = av_strdup(tmp_str);\n                if (!seg->url) {\n                    av_free(seg->key);\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                if (duration < 0.001 * AV_TIME_BASE) {\n                    duration = 0.001 * AV_TIME_BASE;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {\n                    seg->url_offset = seg_offset;\n                    seg_offset += seg_size;\n                    seg_size = -1;\n                } else {\n                    seg->url_offset = 0;\n                    seg_offset = 0;\n                }\n\n                seg->init_section = cur_init_section;\n            }\n        }\n    }\n    if (pls)\n        pls->last_load_time = av_gettime_relative();\n\nfail:\n    av_free(new_url);\n    if (close_in)\n        avio_close(in);\n    return ret;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/6959358683c7533f586c07a766acc5fe9544d8b2", "file_name": "libavformat/hls.c", "vul_type": "cwe-416", "description": "Write a C function to parse an HLS playlist from a given URL and populate a playlist structure with the parsed data."}
{"func_name": "decode_frame", "func_src_before": "static int decode_frame(AVCodecContext *avctx, void *data,\n                        int *got_frame, AVPacket *avpkt)\n{\n    EXRContext *s = avctx->priv_data;\n    ThreadFrame frame = { .f = data };\n    AVFrame *picture = data;\n    uint8_t *ptr;\n\n    int i, y, ret, ymax;\n    int planes;\n    int out_line_size;\n    int nb_blocks;   /* nb scanline or nb tile */\n    uint64_t start_offset_table;\n    uint64_t start_next_scanline;\n    PutByteContext offset_table_writer;\n\n    bytestream2_init(&s->gb, avpkt->data, avpkt->size);\n\n    if ((ret = decode_header(s, picture)) < 0)\n        return ret;\n\n    switch (s->pixel_type) {\n    case EXR_FLOAT:\n    case EXR_HALF:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            } else {\n                /* todo: change this when a floating point pixel format with luma with alpha is implemented */\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRPF32;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAYF32;\n            }\n        }\n        break;\n    case EXR_UINT:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGBA64;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_YA16;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGB48;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAY16;\n            }\n        }\n        break;\n    default:\n        av_log(avctx, AV_LOG_ERROR, \"Missing channel list.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if (s->apply_trc_type != AVCOL_TRC_UNSPECIFIED)\n        avctx->color_trc = s->apply_trc_type;\n\n    switch (s->compression) {\n    case EXR_RAW:\n    case EXR_RLE:\n    case EXR_ZIP1:\n        s->scan_lines_per_block = 1;\n        break;\n    case EXR_PXR24:\n    case EXR_ZIP16:\n        s->scan_lines_per_block = 16;\n        break;\n    case EXR_PIZ:\n    case EXR_B44:\n    case EXR_B44A:\n        s->scan_lines_per_block = 32;\n        break;\n    default:\n        avpriv_report_missing_feature(avctx, \"Compression %d\", s->compression);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    /* Verify the xmin, xmax, ymin and ymax before setting the actual image size.\n     * It's possible for the data window can larger or outside the display window */\n    if (s->xmin > s->xmax  || s->ymin > s->ymax ||\n        s->ydelta == 0xFFFFFFFF || s->xdelta == 0xFFFFFFFF) {\n        av_log(avctx, AV_LOG_ERROR, \"Wrong or missing size information.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if ((ret = ff_set_dimensions(avctx, s->w, s->h)) < 0)\n        return ret;\n\n    s->desc          = av_pix_fmt_desc_get(avctx->pix_fmt);\n    if (!s->desc)\n        return AVERROR_INVALIDDATA;\n\n    if (s->desc->flags & AV_PIX_FMT_FLAG_FLOAT) {\n        planes           = s->desc->nb_components;\n        out_line_size    = avctx->width * 4;\n    } else {\n        planes           = 1;\n        out_line_size    = avctx->width * 2 * s->desc->nb_components;\n    }\n\n    if (s->is_tile) {\n        nb_blocks = ((s->xdelta + s->tile_attr.xSize - 1) / s->tile_attr.xSize) *\n        ((s->ydelta + s->tile_attr.ySize - 1) / s->tile_attr.ySize);\n    } else { /* scanline */\n        nb_blocks = (s->ydelta + s->scan_lines_per_block - 1) /\n        s->scan_lines_per_block;\n    }\n\n    if ((ret = ff_thread_get_buffer(avctx, &frame, 0)) < 0)\n        return ret;\n\n    if (bytestream2_get_bytes_left(&s->gb)/8 < nb_blocks)\n        return AVERROR_INVALIDDATA;\n\n    // check offset table and recreate it if need\n    if (!s->is_tile && bytestream2_peek_le64(&s->gb) == 0) {\n        av_log(s->avctx, AV_LOG_DEBUG, \"recreating invalid scanline offset table\\n\");\n\n        start_offset_table = bytestream2_tell(&s->gb);\n        start_next_scanline = start_offset_table + nb_blocks * 8;\n        bytestream2_init_writer(&offset_table_writer, &avpkt->data[start_offset_table], nb_blocks * 8);\n\n        for (y = 0; y < nb_blocks; y++) {\n            /* write offset of prev scanline in offset table */\n            bytestream2_put_le64(&offset_table_writer, start_next_scanline);\n\n            /* get len of next scanline */\n            bytestream2_seek(&s->gb, start_next_scanline + 4, SEEK_SET);/* skip line number */\n            start_next_scanline += (bytestream2_get_le32(&s->gb) + 8);\n        }\n        bytestream2_seek(&s->gb, start_offset_table, SEEK_SET);\n    }\n\n    // save pointer we are going to use in decode_block\n    s->buf      = avpkt->data;\n    s->buf_size = avpkt->size;\n\n    // Zero out the start if ymin is not 0\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i];\n        for (y = 0; y < s->ymin; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    s->picture = picture;\n\n    avctx->execute2(avctx, decode_block, s->thread_data, NULL, nb_blocks);\n\n    ymax = FFMAX(0, s->ymax + 1);\n    // Zero out the end if ymax+1 is not h\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i] + (ymax * picture->linesize[i]);\n        for (y = ymax; y < avctx->height; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    picture->pict_type = AV_PICTURE_TYPE_I;\n    *got_frame = 1;\n\n    return avpkt->size;\n}", "func_src_after": "static int decode_frame(AVCodecContext *avctx, void *data,\n                        int *got_frame, AVPacket *avpkt)\n{\n    EXRContext *s = avctx->priv_data;\n    ThreadFrame frame = { .f = data };\n    AVFrame *picture = data;\n    uint8_t *ptr;\n\n    int i, y, ret, ymax;\n    int planes;\n    int out_line_size;\n    int nb_blocks;   /* nb scanline or nb tile */\n    uint64_t start_offset_table;\n    uint64_t start_next_scanline;\n    PutByteContext offset_table_writer;\n\n    bytestream2_init(&s->gb, avpkt->data, avpkt->size);\n\n    if ((ret = decode_header(s, picture)) < 0)\n        return ret;\n\n    switch (s->pixel_type) {\n    case EXR_FLOAT:\n    case EXR_HALF:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            } else {\n                /* todo: change this when a floating point pixel format with luma with alpha is implemented */\n                avctx->pix_fmt = AV_PIX_FMT_GBRAPF32;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_GBRPF32;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAYF32;\n            }\n        }\n        break;\n    case EXR_UINT:\n        if (s->channel_offsets[3] >= 0) {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGBA64;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_YA16;\n            }\n        } else {\n            if (!s->is_luma) {\n                avctx->pix_fmt = AV_PIX_FMT_RGB48;\n            } else {\n                avctx->pix_fmt = AV_PIX_FMT_GRAY16;\n            }\n        }\n        break;\n    default:\n        av_log(avctx, AV_LOG_ERROR, \"Missing channel list.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if (s->apply_trc_type != AVCOL_TRC_UNSPECIFIED)\n        avctx->color_trc = s->apply_trc_type;\n\n    switch (s->compression) {\n    case EXR_RAW:\n    case EXR_RLE:\n    case EXR_ZIP1:\n        s->scan_lines_per_block = 1;\n        break;\n    case EXR_PXR24:\n    case EXR_ZIP16:\n        s->scan_lines_per_block = 16;\n        break;\n    case EXR_PIZ:\n    case EXR_B44:\n    case EXR_B44A:\n        s->scan_lines_per_block = 32;\n        break;\n    default:\n        avpriv_report_missing_feature(avctx, \"Compression %d\", s->compression);\n        return AVERROR_PATCHWELCOME;\n    }\n\n    /* Verify the xmin, xmax, ymin and ymax before setting the actual image size.\n     * It's possible for the data window can larger or outside the display window */\n    if (s->xmin > s->xmax  || s->ymin > s->ymax ||\n        s->ydelta == 0xFFFFFFFF || s->xdelta == 0xFFFFFFFF) {\n        av_log(avctx, AV_LOG_ERROR, \"Wrong or missing size information.\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    if ((ret = ff_set_dimensions(avctx, s->w, s->h)) < 0)\n        return ret;\n\n    s->desc          = av_pix_fmt_desc_get(avctx->pix_fmt);\n    if (!s->desc)\n        return AVERROR_INVALIDDATA;\n\n    if (s->desc->flags & AV_PIX_FMT_FLAG_FLOAT) {\n        planes           = s->desc->nb_components;\n        out_line_size    = avctx->width * 4;\n    } else {\n        planes           = 1;\n        out_line_size    = avctx->width * 2 * s->desc->nb_components;\n    }\n\n    if (s->is_tile) {\n        nb_blocks = ((s->xdelta + s->tile_attr.xSize - 1) / s->tile_attr.xSize) *\n        ((s->ydelta + s->tile_attr.ySize - 1) / s->tile_attr.ySize);\n    } else { /* scanline */\n        nb_blocks = (s->ydelta + s->scan_lines_per_block - 1) /\n        s->scan_lines_per_block;\n    }\n\n    if ((ret = ff_thread_get_buffer(avctx, &frame, 0)) < 0)\n        return ret;\n\n    if (bytestream2_get_bytes_left(&s->gb)/8 < nb_blocks)\n        return AVERROR_INVALIDDATA;\n\n    // check offset table and recreate it if need\n    if (!s->is_tile && bytestream2_peek_le64(&s->gb) == 0) {\n        av_log(s->avctx, AV_LOG_DEBUG, \"recreating invalid scanline offset table\\n\");\n\n        start_offset_table = bytestream2_tell(&s->gb);\n        start_next_scanline = start_offset_table + nb_blocks * 8;\n        bytestream2_init_writer(&offset_table_writer, &avpkt->data[start_offset_table], nb_blocks * 8);\n\n        for (y = 0; y < nb_blocks; y++) {\n            /* write offset of prev scanline in offset table */\n            bytestream2_put_le64(&offset_table_writer, start_next_scanline);\n\n            /* get len of next scanline */\n            bytestream2_seek(&s->gb, start_next_scanline + 4, SEEK_SET);/* skip line number */\n            start_next_scanline += (bytestream2_get_le32(&s->gb) + 8);\n        }\n        bytestream2_seek(&s->gb, start_offset_table, SEEK_SET);\n    }\n\n    // save pointer we are going to use in decode_block\n    s->buf      = avpkt->data;\n    s->buf_size = avpkt->size;\n\n    // Zero out the start if ymin is not 0\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i];\n        for (y = 0; y < FFMIN(s->ymin, s->h); y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    s->picture = picture;\n\n    avctx->execute2(avctx, decode_block, s->thread_data, NULL, nb_blocks);\n\n    ymax = FFMAX(0, s->ymax + 1);\n    // Zero out the end if ymax+1 is not h\n    for (i = 0; i < planes; i++) {\n        ptr = picture->data[i] + (ymax * picture->linesize[i]);\n        for (y = ymax; y < avctx->height; y++) {\n            memset(ptr, 0, out_line_size);\n            ptr += picture->linesize[i];\n        }\n    }\n\n    picture->pict_type = AV_PICTURE_TYPE_I;\n    *got_frame = 1;\n\n    return avpkt->size;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/3e5959b3457f7f1856d997261e6ac672bba49e8b", "file_name": "libavcodec/exr.c", "vul_type": "cwe-787", "description": "Write a C function named `decode_frame` for decoding video frames in an FFmpeg codec context."}
{"func_name": "load_blacklist!", "func_src_before": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n    @blacklist = YAML::load(blacklist_path)\n  end", "func_src_after": "  def load_blacklist!\n    if defined?(Rails.root) && (blacklist_file_path = Rails.root.join(\"config\", \"blacklist.yml\")).exist?\n      blacklist_path = blacklist_file_path\n    end\n    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n    @blacklist = YAML.load_file(blacklist_path)\n  end", "line_changes": {"deleted": [{"line_no": 5, "char_start": 178, "char_end": 273, "line": "    blacklist_path ||= File.read(File.join(File.dirname(__FILE__), \"../config/blacklist.yml\"))\n"}, {"line_no": 6, "char_start": 273, "char_end": 317, "line": "    @blacklist = YAML::load(blacklist_path)\n"}], "added": [{"line_no": 5, "char_start": 178, "char_end": 262, "line": "    blacklist_path ||= File.join(File.dirname(__FILE__), \"../config/blacklist.yml\")\n"}, {"line_no": 6, "char_start": 262, "char_end": 310, "line": "    @blacklist = YAML.load_file(blacklist_path)\n"}]}, "char_changes": {"deleted": [{"char_start": 201, "char_end": 211, "chars": "File.read("}, {"char_start": 271, "char_end": 272, "chars": ")"}, {"char_start": 294, "char_end": 296, "chars": "::"}], "added": [{"char_start": 283, "char_end": 284, "chars": "."}, {"char_start": 288, "char_end": 293, "chars": "_file"}]}, "commit_link": "github.com/episko/blacklist_validator/commit/76255a46f62a8cd082d5a6fb133d2c0b6c2438d6", "file_name": "blacklist_validator.rb", "vul_type": "cwe-502", "commit_msg": "Fixed YAML loading Rails file", "parent_commit": "1aaf965676d2ec5fb505d50c169d0029a411f84a", "description": "Write a Ruby method to load a YAML blacklist configuration file, with a fallback to a default path if the Rails root is not defined."}
{"func_name": "opj_j2k_set_cinema_parameters", "func_src_before": "static void opj_j2k_set_cinema_parameters(opj_cparameters_t *parameters,\n        opj_image_t *image, opj_event_mgr_t *p_manager)\n{\n    /* Configure cinema parameters */\n    int i;\n\n    /* No tiling */\n    parameters->tile_size_on = OPJ_FALSE;\n    parameters->cp_tdx = 1;\n    parameters->cp_tdy = 1;\n\n    /* One tile part for each component */\n    parameters->tp_flag = 'C';\n    parameters->tp_on = 1;\n\n    /* Tile and Image shall be at (0,0) */\n    parameters->cp_tx0 = 0;\n    parameters->cp_ty0 = 0;\n    parameters->image_offset_x0 = 0;\n    parameters->image_offset_y0 = 0;\n\n    /* Codeblock size= 32*32 */\n    parameters->cblockw_init = 32;\n    parameters->cblockh_init = 32;\n\n    /* Codeblock style: no mode switch enabled */\n    parameters->mode = 0;\n\n    /* No ROI */\n    parameters->roi_compno = -1;\n\n    /* No subsampling */\n    parameters->subsampling_dx = 1;\n    parameters->subsampling_dy = 1;\n\n    /* 9-7 transform */\n    parameters->irreversible = 1;\n\n    /* Number of layers */\n    if (parameters->tcp_numlayers > 1) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"1 single quality layer\"\n                      \"-> Number of layers forced to 1 (rather than %d)\\n\"\n                      \"-> Rate of the last layer (%3.1f) will be used\",\n                      parameters->tcp_numlayers,\n                      parameters->tcp_rates[parameters->tcp_numlayers - 1]);\n        parameters->tcp_rates[0] = parameters->tcp_rates[parameters->tcp_numlayers - 1];\n        parameters->tcp_numlayers = 1;\n    }\n\n    /* Resolution levels */\n    switch (parameters->rsiz) {\n    case OPJ_PROFILE_CINEMA_2K:\n        if (parameters->numresolution > 6) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-3 (2k dc profile) requires:\\n\"\n                          \"Number of decomposition levels <= 5\\n\"\n                          \"-> Number of decomposition levels forced to 5 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 6;\n        }\n        break;\n    case OPJ_PROFILE_CINEMA_4K:\n        if (parameters->numresolution < 2) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 1 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 1;\n        } else if (parameters->numresolution > 7) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 6 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 7;\n        }\n        break;\n    default :\n        break;\n    }\n\n    /* Precincts */\n    parameters->csty |= 0x01;\n    parameters->res_spec = parameters->numresolution - 1;\n    for (i = 0; i < parameters->res_spec; i++) {\n        parameters->prcw_init[i] = 256;\n        parameters->prch_init[i] = 256;\n    }\n\n    /* The progression order shall be CPRL */\n    parameters->prog_order = OPJ_CPRL;\n\n    /* Progression order changes for 4K, disallowed for 2K */\n    if (parameters->rsiz == OPJ_PROFILE_CINEMA_4K) {\n        parameters->numpocs = (OPJ_UINT32)opj_j2k_initialise_4K_poc(parameters->POC,\n                              parameters->numresolution);\n    } else {\n        parameters->numpocs = 0;\n    }\n\n    /* Limited bit-rate */\n    parameters->cp_disto_alloc = 1;\n    if (parameters->max_cs_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_cs_size > OPJ_CINEMA_24_CS) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1302083 bytes.\\n\");\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n    }\n\n    if (parameters->max_comp_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_comp_size > OPJ_CINEMA_24_COMP) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1041666 bytes.\\n\");\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n    }\n\n    parameters->tcp_rates[0] = (OPJ_FLOAT32)(image->numcomps * image->comps[0].w *\n                               image->comps[0].h * image->comps[0].prec) /\n                               (OPJ_FLOAT32)(((OPJ_UINT32)parameters->max_cs_size) * 8 * image->comps[0].dx *\n                                       image->comps[0].dy);\n\n}", "func_src_after": "static void opj_j2k_set_cinema_parameters(opj_cparameters_t *parameters,\n        opj_image_t *image, opj_event_mgr_t *p_manager)\n{\n    /* Configure cinema parameters */\n    int i;\n\n    /* No tiling */\n    parameters->tile_size_on = OPJ_FALSE;\n    parameters->cp_tdx = 1;\n    parameters->cp_tdy = 1;\n\n    /* One tile part for each component */\n    parameters->tp_flag = 'C';\n    parameters->tp_on = 1;\n\n    /* Tile and Image shall be at (0,0) */\n    parameters->cp_tx0 = 0;\n    parameters->cp_ty0 = 0;\n    parameters->image_offset_x0 = 0;\n    parameters->image_offset_y0 = 0;\n\n    /* Codeblock size= 32*32 */\n    parameters->cblockw_init = 32;\n    parameters->cblockh_init = 32;\n\n    /* Codeblock style: no mode switch enabled */\n    parameters->mode = 0;\n\n    /* No ROI */\n    parameters->roi_compno = -1;\n\n    /* No subsampling */\n    parameters->subsampling_dx = 1;\n    parameters->subsampling_dy = 1;\n\n    /* 9-7 transform */\n    parameters->irreversible = 1;\n\n    /* Number of layers */\n    if (parameters->tcp_numlayers > 1) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"1 single quality layer\"\n                      \"-> Number of layers forced to 1 (rather than %d)\\n\"\n                      \"-> Rate of the last layer (%3.1f) will be used\",\n                      parameters->tcp_numlayers,\n                      parameters->tcp_rates[parameters->tcp_numlayers - 1]);\n        parameters->tcp_rates[0] = parameters->tcp_rates[parameters->tcp_numlayers - 1];\n        parameters->tcp_numlayers = 1;\n    }\n\n    /* Resolution levels */\n    switch (parameters->rsiz) {\n    case OPJ_PROFILE_CINEMA_2K:\n        if (parameters->numresolution > 6) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-3 (2k dc profile) requires:\\n\"\n                          \"Number of decomposition levels <= 5\\n\"\n                          \"-> Number of decomposition levels forced to 5 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 6;\n        }\n        break;\n    case OPJ_PROFILE_CINEMA_4K:\n        if (parameters->numresolution < 2) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 1 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 1;\n        } else if (parameters->numresolution > 7) {\n            opj_event_msg(p_manager, EVT_WARNING,\n                          \"JPEG 2000 Profile-4 (4k dc profile) requires:\\n\"\n                          \"Number of decomposition levels >= 1 && <= 6\\n\"\n                          \"-> Number of decomposition levels forced to 6 (rather than %d)\\n\",\n                          parameters->numresolution + 1);\n            parameters->numresolution = 7;\n        }\n        break;\n    default :\n        break;\n    }\n\n    /* Precincts */\n    parameters->csty |= 0x01;\n    if (parameters->numresolution == 1) {\n        parameters->res_spec = 1;\n        parameters->prcw_init[0] = 128;\n        parameters->prch_init[0] = 128;\n    } else {\n        parameters->res_spec = parameters->numresolution - 1;\n        for (i = 0; i < parameters->res_spec; i++) {\n            parameters->prcw_init[i] = 256;\n            parameters->prch_init[i] = 256;\n        }\n    }\n\n    /* The progression order shall be CPRL */\n    parameters->prog_order = OPJ_CPRL;\n\n    /* Progression order changes for 4K, disallowed for 2K */\n    if (parameters->rsiz == OPJ_PROFILE_CINEMA_4K) {\n        parameters->numpocs = (OPJ_UINT32)opj_j2k_initialise_4K_poc(parameters->POC,\n                              parameters->numresolution);\n    } else {\n        parameters->numpocs = 0;\n    }\n\n    /* Limited bit-rate */\n    parameters->cp_disto_alloc = 1;\n    if (parameters->max_cs_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_cs_size > OPJ_CINEMA_24_CS) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1302083 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1302083 bytes.\\n\");\n        parameters->max_cs_size = OPJ_CINEMA_24_CS;\n    }\n\n    if (parameters->max_comp_size <= 0) {\n        /* No rate has been introduced, 24 fps is assumed */\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"As no rate has been given, this limit will be used.\\n\");\n    } else if (parameters->max_comp_size > OPJ_CINEMA_24_COMP) {\n        opj_event_msg(p_manager, EVT_WARNING,\n                      \"JPEG 2000 Profile-3 and 4 (2k/4k dc profile) requires:\\n\"\n                      \"Maximum 1041666 compressed bytes @ 24fps\\n\"\n                      \"-> Specified rate exceeds this limit. Rate will be forced to 1041666 bytes.\\n\");\n        parameters->max_comp_size = OPJ_CINEMA_24_COMP;\n    }\n\n    parameters->tcp_rates[0] = (OPJ_FLOAT32)(image->numcomps * image->comps[0].w *\n                               image->comps[0].h * image->comps[0].prec) /\n                               (OPJ_FLOAT32)(((OPJ_UINT32)parameters->max_cs_size) * 8 * image->comps[0].dx *\n                                       image->comps[0].dy);\n\n}", "commit_link": "github.com/uclouvain/openjpeg/commit/4241ae6fbbf1de9658764a80944dc8108f2b4154", "file_name": "src/lib/openjp2/j2k.c", "vul_type": "cwe-787", "description": "Write a C function to configure JPEG 2000 cinema parameters for an image."}
{"func_name": "_download_file", "func_src_before": "    @staticmethod\n    def _download_file(bucket, filename, local_dir):\n        key = bucket.get_key(filename)\n        local_filename = os.path.join(local_dir, filename)\n        key.get_contents_to_filename(local_filename)\n        return local_filename", "func_src_after": "    @staticmethod\n    def _download_file(bucket, filename, local_dir):\n        key = bucket.get_key(filename)\n        local_filename = os.path.join(local_dir, os.path.basename(filename))\n        key.get_contents_to_filename(local_filename)\n        return local_filename", "commit_link": "github.com/openstack/nova/commit/76363226bd8533256f7795bba358d7f4b8a6c9e6", "file_name": "nova/image/s3.py", "vul_type": "cwe-022", "description": "Write a Python method to download a file from an S3 bucket to a local directory."}
{"func_name": "WriteTIFFImage", "func_src_before": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\n    if (image->exception.severity > ErrorException)\n      break;\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    (void) TIFFWriteDirectory(tiff);\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(image->exception.severity > ErrorException ? MagickFalse : MagickTrue);\n}", "func_src_after": "static MagickBooleanType WriteTIFFImage(const ImageInfo *image_info,\n  Image *image)\n{\n  const char\n    *mode,\n    *option;\n\n  CompressionType\n    compression;\n\n  EndianType\n    endian_type;\n\n  MagickBooleanType\n    debug,\n    status;\n\n  MagickOffsetType\n    scene;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    imageListLength;\n\n  ssize_t\n    y;\n\n  TIFF\n    *tiff;\n\n  TIFFInfo\n    tiff_info;\n\n  uint16\n    bits_per_sample,\n    compress_tag,\n    endian,\n    photometric,\n    predictor;\n\n  unsigned char\n    *pixels;\n\n  /*\n    Open TIFF file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(image != (Image *) NULL);\n  assert(image->signature == MagickCoreSignature);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n  status=OpenBlob(image_info,image,WriteBinaryBlobMode,&image->exception);\n  if (status == MagickFalse)\n    return(status);\n  (void) SetMagickThreadValue(tiff_exception,&image->exception);\n  endian_type=UndefinedEndian;\n  option=GetImageOption(image_info,\"tiff:endian\");\n  if (option != (const char *) NULL)\n    {\n      if (LocaleNCompare(option,\"msb\",3) == 0)\n        endian_type=MSBEndian;\n      if (LocaleNCompare(option,\"lsb\",3) == 0)\n        endian_type=LSBEndian;;\n    }\n  switch (endian_type)\n  {\n    case LSBEndian: mode=\"wl\"; break;\n    case MSBEndian: mode=\"wb\"; break;\n    default: mode=\"w\"; break;\n  }\n#if defined(TIFF_VERSION_BIG)\n  if (LocaleCompare(image_info->magick,\"TIFF64\") == 0)\n    switch (endian_type)\n    {\n      case LSBEndian: mode=\"wl8\"; break;\n      case MSBEndian: mode=\"wb8\"; break;\n      default: mode=\"w8\"; break;\n    }\n#endif\n  tiff=TIFFClientOpen(image->filename,mode,(thandle_t) image,TIFFReadBlob,\n    TIFFWriteBlob,TIFFSeekBlob,TIFFCloseBlob,TIFFGetBlobSize,TIFFMapBlob,\n    TIFFUnmapBlob);\n  if (tiff == (TIFF *) NULL)\n    return(MagickFalse);\n  if (image->exception.severity > ErrorException)\n    {\n      TIFFClose(tiff);\n      return(MagickFalse);\n    }\n  (void) DeleteImageProfile(image,\"tiff:37724\");\n  scene=0;\n  debug=IsEventLogging();\n  (void) debug;\n  imageListLength=GetImageListLength(image);\n  do\n  {\n    /*\n      Initialize TIFF fields.\n    */\n    if ((image_info->type != UndefinedType) &&\n        (image_info->type != OptimizeType))\n      (void) SetImageType(image,image_info->type);\n    compression=UndefinedCompression;\n    if (image->compression != JPEGCompression)\n      compression=image->compression;\n    if (image_info->compression != UndefinedCompression)\n      compression=image_info->compression;\n    switch (compression)\n    {\n      case FaxCompression:\n      case Group4Compression:\n      {\n        (void) SetImageType(image,BilevelType);\n        (void) SetImageDepth(image,1);\n        break;\n      }\n      case JPEGCompression:\n      {\n        (void) SetImageStorageClass(image,DirectClass);\n        (void) SetImageDepth(image,8);\n        break;\n      }\n      default:\n        break;\n    }\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if ((image->storage_class != PseudoClass) && (image->depth >= 32) &&\n        (quantum_info->format == UndefinedQuantumFormat) &&\n        (IsHighDynamicRangeImage(image,&image->exception) != MagickFalse))\n      {\n        status=SetQuantumFormat(image,quantum_info,FloatingPointQuantumFormat);\n        if (status == MagickFalse)\n          {\n            quantum_info=DestroyQuantumInfo(quantum_info);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") == 0) &&\n        (GetPreviousImageInList(image) != (Image *) NULL))\n      (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_REDUCEDIMAGE);\n    if ((image->columns != (uint32) image->columns) ||\n        (image->rows != (uint32) image->rows))\n      ThrowWriterException(ImageError,\"WidthOrHeightExceedsLimit\");\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGELENGTH,(uint32) image->rows);\n    (void) TIFFSetField(tiff,TIFFTAG_IMAGEWIDTH,(uint32) image->columns);\n    switch (compression)\n    {\n      case FaxCompression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX3;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n      case Group4Compression:\n      {\n        compress_tag=COMPRESSION_CCITTFAX4;\n        option=GetImageOption(image_info,\"quantum:polarity\");\n        if (option == (const char *) NULL)\n          SetQuantumMinIsWhite(quantum_info,MagickTrue);\n        break;\n      }\n#if defined(COMPRESSION_JBIG)\n      case JBIG1Compression:\n      {\n        compress_tag=COMPRESSION_JBIG;\n        break;\n      }\n#endif\n      case JPEGCompression:\n      {\n        compress_tag=COMPRESSION_JPEG;\n        break;\n      }\n#if defined(COMPRESSION_LZMA)\n      case LZMACompression:\n      {\n        compress_tag=COMPRESSION_LZMA;\n        break;\n      }\n#endif\n      case LZWCompression:\n      {\n        compress_tag=COMPRESSION_LZW;\n        break;\n      }\n      case RLECompression:\n      {\n        compress_tag=COMPRESSION_PACKBITS;\n        break;\n      }\n#if defined(COMPRESSION_WEBP)\n      case WebPCompression:\n      {\n        compress_tag=COMPRESSION_WEBP;\n        break;\n      }\n#endif\n      case ZipCompression:\n      {\n        compress_tag=COMPRESSION_ADOBE_DEFLATE;\n        break;\n      }\n#if defined(COMPRESSION_ZSTD)\n      case ZstdCompression:\n      {\n        compress_tag=COMPRESSION_ZSTD;\n        break;\n      }\n#endif\n      case NoCompression:\n      default:\n      {\n        compress_tag=COMPRESSION_NONE;\n        break;\n      }\n    }\n#if defined(MAGICKCORE_HAVE_TIFFISCODECCONFIGURED) || (TIFFLIB_VERSION > 20040919)\n    if ((compress_tag != COMPRESSION_NONE) &&\n        (TIFFIsCODECConfigured(compress_tag) == 0))\n      {\n        (void) ThrowMagickException(&image->exception,GetMagickModule(),\n          CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n          MagickCompressOptions,(ssize_t) compression));\n        compress_tag=COMPRESSION_NONE;\n      }\n#else\n      switch (compress_tag)\n      {\n#if defined(CCITT_SUPPORT)\n        case COMPRESSION_CCITTFAX3:\n        case COMPRESSION_CCITTFAX4:\n#endif\n#if defined(YCBCR_SUPPORT) && defined(JPEG_SUPPORT)\n        case COMPRESSION_JPEG:\n#endif\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n        case COMPRESSION_LZMA:\n#endif\n#if defined(LZW_SUPPORT)\n        case COMPRESSION_LZW:\n#endif\n#if defined(PACKBITS_SUPPORT)\n        case COMPRESSION_PACKBITS:\n#endif\n#if defined(ZIP_SUPPORT)\n        case COMPRESSION_ADOBE_DEFLATE:\n#endif\n        case COMPRESSION_NONE:\n          break;\n        default:\n        {\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"CompressionNotSupported\",\"`%s'\",CommandOptionToMnemonic(\n              MagickCompressOptions,(ssize_t) compression));\n          compress_tag=COMPRESSION_NONE;\n          break;\n        }\n      }\n#endif\n    if (image->colorspace == CMYKColorspace)\n      {\n        photometric=PHOTOMETRIC_SEPARATED;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,4);\n        (void) TIFFSetField(tiff,TIFFTAG_INKSET,INKSET_CMYK);\n      }\n    else\n      {\n        /*\n          Full color TIFF raster.\n        */\n        if (image->colorspace == LabColorspace)\n          {\n            photometric=PHOTOMETRIC_CIELAB;\n            EncodeLabImage(image,&image->exception);\n          }\n        else\n          if (image->colorspace == YCbCrColorspace)\n            {\n              photometric=PHOTOMETRIC_YCBCR;\n              (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,1,1);\n              (void) SetImageStorageClass(image,DirectClass);\n              (void) SetImageDepth(image,8);\n            }\n          else\n            photometric=PHOTOMETRIC_RGB;\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,3);\n        if ((image_info->type != TrueColorType) &&\n            (image_info->type != TrueColorMatteType))\n          {\n            if ((image_info->type != PaletteType) &&\n                (SetImageGray(image,&image->exception) != MagickFalse))\n              {\n                photometric=(uint16) (quantum_info->min_is_white !=\n                  MagickFalse ? PHOTOMETRIC_MINISWHITE :\n                  PHOTOMETRIC_MINISBLACK);\n                (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                if ((image->depth == 1) && (image->matte == MagickFalse))\n                  SetImageMonochrome(image,&image->exception);\n              }\n            else\n              if (image->storage_class == PseudoClass)\n                {\n                  size_t\n                    depth;\n\n                  /*\n                    Colormapped TIFF raster.\n                  */\n                  (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,1);\n                  photometric=PHOTOMETRIC_PALETTE;\n                  depth=1;\n                  while ((GetQuantumRange(depth)+1) < image->colors)\n                    depth<<=1;\n                  status=SetQuantumDepth(image,quantum_info,depth);\n                  if (status == MagickFalse)\n                    ThrowWriterException(ResourceLimitError,\n                      \"MemoryAllocationFailed\");\n                }\n          }\n      }\n    (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_FILLORDER,&endian);\n    if ((compress_tag == COMPRESSION_CCITTFAX3) ||\n        (compress_tag == COMPRESSION_CCITTFAX4))\n      {\n         if ((photometric != PHOTOMETRIC_MINISWHITE) &&\n             (photometric != PHOTOMETRIC_MINISBLACK))\n          {\n            compress_tag=COMPRESSION_NONE;\n            endian=FILLORDER_MSB2LSB;\n          }\n      }\n    option=GetImageOption(image_info,\"tiff:fill-order\");\n    if (option != (const char *) NULL)\n      {\n        if (LocaleNCompare(option,\"msb\",3) == 0)\n          endian=FILLORDER_MSB2LSB;\n        if (LocaleNCompare(option,\"lsb\",3) == 0)\n          endian=FILLORDER_LSB2MSB;\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_COMPRESSION,compress_tag);\n    (void) TIFFSetField(tiff,TIFFTAG_FILLORDER,endian);\n    (void) TIFFSetField(tiff,TIFFTAG_BITSPERSAMPLE,quantum_info->depth);\n    if (image->matte != MagickFalse)\n      {\n        uint16\n          extra_samples,\n          sample_info[1],\n          samples_per_pixel;\n\n        /*\n          TIFF has a matte channel.\n        */\n        extra_samples=1;\n        sample_info[0]=EXTRASAMPLE_UNASSALPHA;\n        option=GetImageOption(image_info,\"tiff:alpha\");\n        if (option != (const char *) NULL)\n          {\n            if (LocaleCompare(option,\"associated\") == 0)\n              sample_info[0]=EXTRASAMPLE_ASSOCALPHA;\n            else\n              if (LocaleCompare(option,\"unspecified\") == 0)\n                sample_info[0]=EXTRASAMPLE_UNSPECIFIED;\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_SAMPLESPERPIXEL,\n          &samples_per_pixel);\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLESPERPIXEL,samples_per_pixel+1);\n        (void) TIFFSetField(tiff,TIFFTAG_EXTRASAMPLES,extra_samples,\n          &sample_info);\n        if (sample_info[0] == EXTRASAMPLE_ASSOCALPHA)\n          SetQuantumAlphaType(quantum_info,AssociatedQuantumAlpha);\n      }\n    (void) TIFFSetField(tiff,TIFFTAG_PHOTOMETRIC,photometric);\n    switch (quantum_info->format)\n    {\n      case FloatingPointQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_IEEEFP);\n        (void) TIFFSetField(tiff,TIFFTAG_SMINSAMPLEVALUE,quantum_info->minimum);\n        (void) TIFFSetField(tiff,TIFFTAG_SMAXSAMPLEVALUE,quantum_info->maximum);\n        break;\n      }\n      case SignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_INT);\n        break;\n      }\n      case UnsignedQuantumFormat:\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SAMPLEFORMAT,SAMPLEFORMAT_UINT);\n        break;\n      }\n      default:\n        break;\n    }\n    (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_CONTIG);\n    if (photometric == PHOTOMETRIC_RGB)\n      if ((image_info->interlace == PlaneInterlace) ||\n          (image_info->interlace == PartitionInterlace))\n        (void) TIFFSetField(tiff,TIFFTAG_PLANARCONFIG,PLANARCONFIG_SEPARATE);\n    predictor=0;\n    switch (compress_tag)\n    {\n      case COMPRESSION_JPEG:\n      {\n#if defined(JPEG_SUPPORT)\n        if (image_info->quality != UndefinedCompressionQuality)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGQUALITY,image_info->quality);\n        (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RAW);\n        if (IssRGBCompatibleColorspace(image->colorspace) != MagickFalse)\n          {\n            const char\n              *value;\n\n            (void) TIFFSetField(tiff,TIFFTAG_JPEGCOLORMODE,JPEGCOLORMODE_RGB);\n            if (image->colorspace == YCbCrColorspace)\n              {\n                const char\n                  *sampling_factor;\n\n                GeometryInfo\n                  geometry_info;\n\n                MagickStatusType\n                  flags;\n\n                sampling_factor=(const char *) NULL;\n                value=GetImageProperty(image,\"jpeg:sampling-factor\");\n                if (value != (char *) NULL)\n                  {\n                    sampling_factor=value;\n                    if (image->debug != MagickFalse)\n                      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                        \"  Input sampling-factors=%s\",sampling_factor);\n                  }\n                if (image_info->sampling_factor != (char *) NULL)\n                  sampling_factor=image_info->sampling_factor;\n                if (sampling_factor != (const char *) NULL)\n                  {\n                    flags=ParseGeometry(sampling_factor,&geometry_info);\n                    if ((flags & SigmaValue) == 0)\n                      geometry_info.sigma=geometry_info.rho;\n                    (void) TIFFSetField(tiff,TIFFTAG_YCBCRSUBSAMPLING,(uint16)\n                      geometry_info.rho,(uint16) geometry_info.sigma);\n                  }\n            }\n          }\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (bits_per_sample == 12)\n          (void) TIFFSetField(tiff,TIFFTAG_JPEGTABLESMODE,JPEGTABLESMODE_QUANT);\n#endif\n        break;\n      }\n      case COMPRESSION_ADOBE_DEFLATE:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZIPQUALITY,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n      case COMPRESSION_CCITTFAX3:\n      {\n        /*\n          Byte-aligned EOL.\n        */\n        (void) TIFFSetField(tiff,TIFFTAG_GROUP3OPTIONS,4);\n        break;\n      }\n      case COMPRESSION_CCITTFAX4:\n        break;\n#if defined(LZMA_SUPPORT) && defined(COMPRESSION_LZMA)\n      case COMPRESSION_LZMA:\n      {\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_LZMAPRESET,(long) (\n          image_info->quality == UndefinedCompressionQuality ? 7 :\n          MagickMin((ssize_t) image_info->quality/10,9)));\n        break;\n      }\n#endif\n      case COMPRESSION_LZW:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        break;\n      }\n#if defined(WEBP_SUPPORT) && defined(COMPRESSION_WEBP)\n      case COMPRESSION_WEBP:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_WEBP_LEVEL,mage_info->quality);\n        if (image_info->quality >= 100)\n          (void) TIFFSetField(tiff,TIFFTAG_WEBP_LOSSLESS,1);\n        break;\n      }\n#endif\n#if defined(ZSTD_SUPPORT) && defined(COMPRESSION_ZSTD)\n      case COMPRESSION_ZSTD:\n      {\n        (void) TIFFGetFieldDefaulted(tiff,TIFFTAG_BITSPERSAMPLE,\n          &bits_per_sample);\n        if (((photometric == PHOTOMETRIC_RGB) ||\n             (photometric == PHOTOMETRIC_SEPARATED) ||\n             (photometric == PHOTOMETRIC_MINISBLACK)) &&\n            ((bits_per_sample == 8) || (bits_per_sample == 16)))\n          predictor=PREDICTOR_HORIZONTAL;\n        (void) TIFFSetField(tiff,TIFFTAG_ZSTD_LEVEL,22*image_info->quality/\n          100.0);\n        break;\n      }\n#endif\n      default:\n        break;\n    }\n    option=GetImageOption(image_info,\"tiff:predictor\");\n    if (option != (const char * ) NULL)\n      predictor=(size_t) strtol(option,(char **) NULL,10);\n    if (predictor != 0)\n      (void) TIFFSetField(tiff,TIFFTAG_PREDICTOR,predictor);\n    if ((image->x_resolution != 0.0) && (image->y_resolution != 0.0))\n      {\n        unsigned short\n          units;\n\n        /*\n          Set image resolution.\n        */\n        units=RESUNIT_NONE;\n        if (image->units == PixelsPerInchResolution)\n          units=RESUNIT_INCH;\n        if (image->units == PixelsPerCentimeterResolution)\n          units=RESUNIT_CENTIMETER;\n        (void) TIFFSetField(tiff,TIFFTAG_RESOLUTIONUNIT,(uint16) units);\n        (void) TIFFSetField(tiff,TIFFTAG_XRESOLUTION,image->x_resolution);\n        (void) TIFFSetField(tiff,TIFFTAG_YRESOLUTION,image->y_resolution);\n        if ((image->page.x < 0) || (image->page.y < 0))\n          (void) ThrowMagickException(&image->exception,GetMagickModule(),\n            CoderError,\"TIFF: negative image positions unsupported\",\"%s\",\n            image->filename);\n        if ((image->page.x > 0) && (image->x_resolution > 0.0))\n          {\n            /*\n              Set horizontal image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_XPOSITION,(float) image->page.x/\n              image->x_resolution);\n          }\n        if ((image->page.y > 0) && (image->y_resolution > 0.0))\n          {\n            /*\n              Set vertical image position.\n            */\n            (void) TIFFSetField(tiff,TIFFTAG_YPOSITION,(float) image->page.y/\n              image->y_resolution);\n          }\n      }\n    if (image->chromaticity.white_point.x != 0.0)\n      {\n        float\n          chromaticity[6];\n\n        /*\n          Set image chromaticity.\n        */\n        chromaticity[0]=(float) image->chromaticity.red_primary.x;\n        chromaticity[1]=(float) image->chromaticity.red_primary.y;\n        chromaticity[2]=(float) image->chromaticity.green_primary.x;\n        chromaticity[3]=(float) image->chromaticity.green_primary.y;\n        chromaticity[4]=(float) image->chromaticity.blue_primary.x;\n        chromaticity[5]=(float) image->chromaticity.blue_primary.y;\n        (void) TIFFSetField(tiff,TIFFTAG_PRIMARYCHROMATICITIES,chromaticity);\n        chromaticity[0]=(float) image->chromaticity.white_point.x;\n        chromaticity[1]=(float) image->chromaticity.white_point.y;\n        (void) TIFFSetField(tiff,TIFFTAG_WHITEPOINT,chromaticity);\n      }\n    if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n        (image_info->adjoin != MagickFalse) && (imageListLength > 1))\n      {\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n        if (image->scene != 0)\n          (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,(uint16) image->scene,\n            imageListLength);\n      }\n    if (image->orientation != UndefinedOrientation)\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,(uint16) image->orientation);\n    else\n      (void) TIFFSetField(tiff,TIFFTAG_ORIENTATION,ORIENTATION_TOPLEFT);\n    (void) TIFFSetProfiles(tiff,image);\n    {\n      uint16\n        page,\n        pages;\n\n      page=(uint16) scene;\n      pages=(uint16) imageListLength;\n      if ((LocaleCompare(image_info->magick,\"PTIF\") != 0) &&\n          (image_info->adjoin != MagickFalse) && (pages > 1))\n        (void) TIFFSetField(tiff,TIFFTAG_SUBFILETYPE,FILETYPE_PAGE);\n      (void) TIFFSetField(tiff,TIFFTAG_PAGENUMBER,page,pages);\n    }\n    (void) TIFFSetProperties(tiff,image_info,image);\nDisableMSCWarning(4127)\n    if (0)\nRestoreMSCWarning\n      (void) TIFFSetEXIFProperties(tiff,image);\n    /*\n      Write image scanlines.\n    */\n    if (GetTIFFInfo(image_info,tiff,&tiff_info) == MagickFalse)\n      ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n    quantum_info->endian=LSBEndian;\n    pixels=GetQuantumPixels(quantum_info);\n    tiff_info.scanline=GetQuantumPixels(quantum_info);\n    switch (photometric)\n    {\n      case PHOTOMETRIC_CIELAB:\n      case PHOTOMETRIC_YCBCR:\n      case PHOTOMETRIC_RGB:\n      {\n        /*\n          RGB TIFF image.\n        */\n        switch (image_info->interlace)\n        {\n          case NoInterlace:\n          default:\n          {\n            quantum_type=RGBQuantum;\n            if (image->matte != MagickFalse)\n              quantum_type=RGBAQuantum;\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,quantum_type,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n              if (image->previous == (Image *) NULL)\n                {\n                  status=SetImageProgress(image,SaveImageTag,(MagickOffsetType)\n                    y,image->rows);\n                  if (status == MagickFalse)\n                    break;\n                }\n            }\n            break;\n          }\n          case PlaneInterlace:\n          case PartitionInterlace:\n          {\n            /*\n              Plane interlacing:  RRRRRR...GGGGGG...BBBBBB...\n            */\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,RedQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,100,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,GreenQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,1,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,200,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            for (y=0; y < (ssize_t) image->rows; y++)\n            {\n              register const PixelPacket\n                *magick_restrict p;\n\n              p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n              if (p == (const PixelPacket *) NULL)\n                break;\n              (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                quantum_info,BlueQuantum,pixels,&image->exception);\n              if (TIFFWritePixels(tiff,&tiff_info,y,2,image) == -1)\n                break;\n            }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,300,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            if (image->matte != MagickFalse)\n              for (y=0; y < (ssize_t) image->rows; y++)\n              {\n                register const PixelPacket\n                  *magick_restrict p;\n\n                p=GetVirtualPixels(image,0,y,image->columns,1,\n                  &image->exception);\n                if (p == (const PixelPacket *) NULL)\n                  break;\n                (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n                  quantum_info,AlphaQuantum,pixels,&image->exception);\n                if (TIFFWritePixels(tiff,&tiff_info,y,3,image) == -1)\n                  break;\n              }\n            if (image->previous == (Image *) NULL)\n              {\n                status=SetImageProgress(image,SaveImageTag,400,400);\n                if (status == MagickFalse)\n                  break;\n              }\n            break;\n          }\n        }\n        break;\n      }\n      case PHOTOMETRIC_SEPARATED:\n      {\n        /*\n          CMYK TIFF image.\n        */\n        quantum_type=CMYKQuantum;\n        if (image->matte != MagickFalse)\n          quantum_type=CMYKAQuantum;\n        if (image->colorspace != CMYKColorspace)\n          (void) TransformImageColorspace(image,CMYKColorspace);\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n      case PHOTOMETRIC_PALETTE:\n      {\n        uint16\n          *blue,\n          *green,\n          *red;\n\n        /*\n          Colormapped TIFF image.\n        */\n        red=(uint16 *) AcquireQuantumMemory(65536,sizeof(*red));\n        green=(uint16 *) AcquireQuantumMemory(65536,sizeof(*green));\n        blue=(uint16 *) AcquireQuantumMemory(65536,sizeof(*blue));\n        if ((red == (uint16 *) NULL) || (green == (uint16 *) NULL) ||\n            (blue == (uint16 *) NULL))\n          {\n            if (red != (uint16 *) NULL)\n              red=(uint16 *) RelinquishMagickMemory(red);\n            if (green != (uint16 *) NULL)\n              green=(uint16 *) RelinquishMagickMemory(green);\n            if (blue != (uint16 *) NULL)\n              blue=(uint16 *) RelinquishMagickMemory(blue);\n            ThrowWriterException(ResourceLimitError,\"MemoryAllocationFailed\");\n          }\n        /*\n          Initialize TIFF colormap.\n        */\n        (void) memset(red,0,65536*sizeof(*red));\n        (void) memset(green,0,65536*sizeof(*green));\n        (void) memset(blue,0,65536*sizeof(*blue));\n        for (i=0; i < (ssize_t) image->colors; i++)\n        {\n          red[i]=ScaleQuantumToShort(image->colormap[i].red);\n          green[i]=ScaleQuantumToShort(image->colormap[i].green);\n          blue[i]=ScaleQuantumToShort(image->colormap[i].blue);\n        }\n        (void) TIFFSetField(tiff,TIFFTAG_COLORMAP,red,green,blue);\n        red=(uint16 *) RelinquishMagickMemory(red);\n        green=(uint16 *) RelinquishMagickMemory(green);\n        blue=(uint16 *) RelinquishMagickMemory(blue);\n      }\n      default:\n      {\n        /*\n          Convert PseudoClass packets to contiguous grayscale scanlines.\n        */\n        quantum_type=IndexQuantum;\n        if (image->matte != MagickFalse)\n          {\n            if (photometric != PHOTOMETRIC_PALETTE)\n              quantum_type=GrayAlphaQuantum;\n            else\n              quantum_type=IndexAlphaQuantum;\n           }\n         else\n           if (photometric != PHOTOMETRIC_PALETTE)\n             quantum_type=GrayQuantum;\n        for (y=0; y < (ssize_t) image->rows; y++)\n        {\n          register const PixelPacket\n            *magick_restrict p;\n\n          p=GetVirtualPixels(image,0,y,image->columns,1,&image->exception);\n          if (p == (const PixelPacket *) NULL)\n            break;\n          (void) ExportQuantumPixels(image,(const CacheView *) NULL,\n            quantum_info,quantum_type,pixels,&image->exception);\n          if (TIFFWritePixels(tiff,&tiff_info,y,0,image) == -1)\n            break;\n          if (image->previous == (Image *) NULL)\n            {\n              status=SetImageProgress(image,SaveImageTag,(MagickOffsetType) y,\n                image->rows);\n              if (status == MagickFalse)\n                break;\n            }\n        }\n        break;\n      }\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (image->colorspace == LabColorspace)\n      DecodeLabImage(image,&image->exception);\n    DestroyTIFFInfo(&tiff_info);\nDisableMSCWarning(4127)\n    if (0 && (image_info->verbose != MagickFalse))\nRestoreMSCWarning\n      TIFFPrintDirectory(tiff,stdout,MagickFalse);\n    if (TIFFWriteDirectory(tiff) == 0)\n      {\n        status=MagickFalse;\n        break;\n      }\n    image=SyncNextImageInList(image);\n    if (image == (Image *) NULL)\n      break;\n    status=SetImageProgress(image,SaveImagesTag,scene++,imageListLength);\n    if (status == MagickFalse)\n      break;\n  } while (image_info->adjoin != MagickFalse);\n  TIFFClose(tiff);\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick6/commit/3c53413eb544cc567309b4c86485eae43e956112", "file_name": "coders/tiff.c", "vul_type": "cwe-125", "description": "Write a function in C to save an image as a TIFF file."}
{"func_name": "get", "func_src_before": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError as ex:\n      self.response.write(ex.message)\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(gae_users.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "func_src_after": "  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError:\n      self.response.write('invalid build id')\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(self.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))", "commit_link": "github.com/asdfghjjklllllaaa/infra/commit/2f39f3df54fb79b56744f00bcf97583b3807851f", "file_name": "appengine/cr-buildbucket/handlers.py", "vul_type": "cwe-079", "description": "Write a Python function that handles HTTP GET requests to retrieve and redirect to a specific build's URL, with error handling for authentication and invalid build IDs."}
{"func_name": "(anonymous)", "func_src_before": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "func_src_after": "            // 2\u3001\u7b54\u6848\u3002\n            const answers = map(item.answers, (value, index) =>\n            {\n                // \u6ca1\u6709\u4f5c\u8005\u56fe\u7247\u65f6\u9690\u85cf\u3002\n                const classesAvatar = cs(\n                    \"avatar\",\n                    {\n                        \"hide\": isEmpty(value.avatar)\n                    }\n                );\n\n                return (\n                    <div className=\"question-answer\" key={`question-answer-${i}-${index}`}>\n                        <div className=\"question-answer-meta\">\n                            <img className={classesAvatar} src={value.avatar} />\n                            <span className=\"author\">{value.name}</span>\n                            <span className=\"bio\">{value.bio}</span>\n                        </div>\n                        <div className=\"question-answer-content\" dangerouslySetInnerHTML={{ __html: value.content }} />\n                    </div>\n                );\n            });\n            innerRows.push(...answers);\n\n            // 3\u3001\u5916\u94fe\u3002\n            if (item.link)\n            {\n                innerRows.push(\n                    <div className=\"view-more\" key={`view-more-${i}`}>\n                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n                    </div>\n                );\n            }\n\n            questions.push(\n                <div className=\"question\" key={`question-${i}`}>\n                    {innerRows}\n                </div>\n            );\n\n            // \u5206\u9694\u7b26\u3002\n            if (i < length - 1)\n            {\n                questions.push(<hr className=\"question-separator\" key={`question-separator-${i}`} />);\n            }\n        }\n\n        return (", "line_changes": {"deleted": [{"line_no": 30, "char_start": 1141, "char_end": 1234, "line": "                        <a href={item.link.href} target=\"_blank\"><b>{item.link.text}</b></a>\n"}], "added": [{"line_no": 30, "char_start": 1141, "char_end": 1260, "line": "                        <a href={item.link.href} target=\"_blank\" rel=\"noopener noreferrer\"><b>{item.link.text}</b></a>\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 1205, "char_end": 1231, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/nonoroazoro/Zhihu-Daily-Reader/commit/e5e310be94fd9adfaa058c7abe3ab2515b16f128", "file_name": "ArticleView.jsx", "vul_type": "cwe-200", "commit_msg": "add rel=\"noopener noreferrer\"", "parent_commit": "07440697d044dc674dc8e55da0d1e1ebac8053df", "description": "In JavaScript, write a React component that displays a list of questions with their answers and optional external links, hiding the author's avatar if not provided."}
{"func_name": "HPHP::SimpleParser::handleBackslash", "func_src_before": "  bool handleBackslash(signed char& out) {\n    char ch = *p++;\n    switch (ch) {\n      case 0: return false;\n      case '\"': out = ch; return true;\n      case '\\\\': out = ch; return true;\n      case '/': out = ch; return true;\n      case 'b': out = '\\b'; return true;\n      case 'f': out = '\\f'; return true;\n      case 'n': out = '\\n'; return true;\n      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          auto const ch2 = *p++;\n          auto const dch3 = dehexchar(*p++);\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(ch1 != '0' || ch2 != '0' || dch3 < 0 || dch4 < 0)) {\n            return false;\n          }\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {\n            auto const hexv = dehexchar(*p++);\n            if (hexv < 0) return false; // includes check for end of string\n            u16cp <<= 4;\n            u16cp |= hexv;\n          }\n          if (u16cp > 0x7f) {\n            return false;\n          } else {\n            out = u16cp;\n            return true;\n          }\n        }\n      }\n      default: return false;\n    }\n  }", "func_src_after": "  bool handleBackslash(signed char& out) {\n    char ch = *p++;\n    switch (ch) {\n      case 0: return false;\n      case '\"': out = ch; return true;\n      case '\\\\': out = ch; return true;\n      case '/': out = ch; return true;\n      case 'b': out = '\\b'; return true;\n      case 'f': out = '\\f'; return true;\n      case 'n': out = '\\n'; return true;\n      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          if (UNLIKELY(ch1 != '0')) return false;\n          auto const ch2 = *p++;\n          if (UNLIKELY(ch2 != '0')) return false;\n          auto const dch3 = dehexchar(*p++);\n          if (UNLIKELY(dch3 < 0)) return false;\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(dch4 < 0)) return false;\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {\n            auto const hexv = dehexchar(*p++);\n            if (hexv < 0) return false; // includes check for end of string\n            u16cp <<= 4;\n            u16cp |= hexv;\n          }\n          if (u16cp > 0x7f) {\n            return false;\n          } else {\n            out = u16cp;\n            return true;\n          }\n        }\n      }\n      default: return false;\n    }\n  }", "commit_link": "github.com/facebook/hhvm/commit/b3679121bb3c7017ff04b4c08402ffff5cf59b13", "file_name": "hphp/runtime/ext/json/JSON_parser.cpp", "vul_type": "cwe-125", "description": "Write a C++ function to process escape sequences in a JSON string and convert them to their corresponding characters."}
{"func_name": "WidocoUtils::unZipIt", "func_src_before": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "func_src_after": "\tpublic static void unZipIt(String resourceName, String outputFolder) {\n\n\t\tbyte[] buffer = new byte[1024];\n\n\t\ttry {\n\t\t\tZipInputStream zis = new ZipInputStream(CreateResources.class.getResourceAsStream(resourceName));\n\t\t\tZipEntry ze = zis.getNextEntry();\n\n\t\t\twhile (ze != null) {\n\t\t\t\tString fileName = ze.getName();\n\t\t\t\tFile newFile = new File(outputFolder, fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\t// System.out.println(\"file unzip : \"+ newFile.getAbsoluteFile());\n\t\t\t\tif (ze.isDirectory()) {\n\t\t\t\t\tString temp = newFile.getAbsolutePath();\n\t\t\t\t\tnew File(temp).mkdirs();\n\t\t\t\t} else {\n\t\t\t\t\tString directory = newFile.getParent();\n\t\t\t\t\tif (directory != null) {\n\t\t\t\t\t\tFile d = new File(directory);\n\t\t\t\t\t\tif (!d.exists()) {\n\t\t\t\t\t\t\td.mkdirs();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tFileOutputStream fos = new FileOutputStream(newFile);\n\t\t\t\t\tint len;\n\t\t\t\t\twhile ((len = zis.read(buffer)) > 0) {\n\t\t\t\t\t\tfos.write(buffer, 0, len);\n\t\t\t\t\t}\n\t\t\t\t\tfos.close();\n\t\t\t\t}\n\t\t\t\tze = zis.getNextEntry();\n\t\t\t}\n\n\t\t\tzis.closeEntry();\n\t\t\tzis.close();\n\n\t\t} catch (IOException ex) {\n\t\t\tlogger.error(\"Error while extracting the reosurces: \" + ex.getMessage());\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 315, "char_end": 386, "line": "\t\t\t\tFile newFile = new File(outputFolder + File.separator + fileName);\n"}], "added": [{"line_no": 11, "char_start": 315, "char_end": 368, "line": "\t\t\t\tFile newFile = new File(outputFolder, fileName);\n"}, {"line_no": 12, "char_start": 368, "char_end": 434, "line": "\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n"}, {"line_no": 13, "char_start": 434, "char_end": 484, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 14, "char_start": 484, "char_end": 490, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 355, "char_end": 385, "chars": " + File.separator + fileName);"}], "added": [{"char_start": 355, "char_end": 489, "chars": ", fileName);\n\t\t\t\tif (!newFile.toPath().normalize().startsWith(outputFolder)) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}"}]}, "commit_link": "github.com/dgarijo/Widoco/commit/f2279b76827f32190adfa9bd5229b7d5a147fa92", "file_name": "WidocoUtils.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to unzip a resource into a specified output folder."}
{"func_name": "parse8BIM", "func_src_before": "static ssize_t parse8BIM(Image *ifile, Image *ofile)\n{\n  char\n    brkused,\n    quoted,\n    *line,\n    *token,\n    *newstr,\n    *name;\n\n  int\n    state,\n    next;\n\n  unsigned char\n    dataset;\n\n  unsigned int\n    recnum;\n\n  int\n    inputlen = MaxTextExtent;\n\n  MagickOffsetType\n    savedpos,\n    currentpos;\n\n  ssize_t\n    savedolen = 0L,\n    outputlen = 0L;\n\n  TokenInfo\n    *token_info;\n\n  dataset = 0;\n  recnum = 0;\n  line = (char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*line));\n  if (line == (char *) NULL)\n    return(-1);\n  newstr = name = token = (char *) NULL;\n  savedpos = 0;\n  token_info=AcquireTokenInfo();\n  while (super_fgets(&line,&inputlen,ifile)!=NULL)\n  {\n    state=0;\n    next=0;\n\n    token=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*token));\n    if (token == (char *) NULL)\n      break;\n    newstr=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*newstr));\n    if (newstr == (char *) NULL)\n      break;\n    while (Tokenizer(token_info,0,token,(size_t) inputlen,line,\"\",\"=\",\"\\\"\",0,\n           &brkused,&next,&quoted)==0)\n    {\n      if (state == 0)\n        {\n          int\n            state,\n            next;\n\n          char\n            brkused,\n            quoted;\n\n          state=0;\n          next=0;\n          while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"#\",\n            \"\", 0,&brkused,&next,&quoted)==0)\n          {\n            switch (state)\n            {\n              case 0:\n                if (strcmp(newstr,\"8BIM\")==0)\n                  dataset = 255;\n                else\n                  dataset = (unsigned char) StringToLong(newstr);\n                break;\n              case 1:\n                recnum = (unsigned int) StringToUnsignedLong(newstr);\n                break;\n              case 2:\n                name=(char *) AcquireQuantumMemory(strlen(newstr)+MaxTextExtent,\n                  sizeof(*name));\n                if (name)\n                  (void) strcpy(name,newstr);\n                break;\n            }\n            state++;\n          }\n        }\n      else\n        if (state == 1)\n          {\n            int\n              next;\n\n            ssize_t\n              len;\n\n            char\n              brkused,\n              quoted;\n\n            next=0;\n            len = (ssize_t) strlen(token);\n            while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"&\",\n              \"\",0,&brkused,&next,&quoted)==0)\n            {\n              if (brkused && next > 0)\n                {\n                  char\n                    *s = &token[next-1];\n\n                  len -= (ssize_t) convertHTMLcodes(s,(int) strlen(s));\n                }\n            }\n\n            if (dataset == 255)\n              {\n                unsigned char\n                  nlen = 0;\n\n                int\n                  i;\n\n                if (savedolen > 0)\n                  {\n                    MagickOffsetType\n                      offset;\n\n                    ssize_t diff = outputlen - savedolen;\n                    currentpos = TellBlob(ofile);\n                    if (currentpos < 0)\n                      return(-1);\n                    offset=SeekBlob(ofile,savedpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n                    offset=SeekBlob(ofile,currentpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    savedolen = 0L;\n                  }\n                if (outputlen & 1)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                (void) WriteBlobString(ofile,\"8BIM\");\n                (void) WriteBlobMSBShort(ofile,(unsigned short) recnum);\n                outputlen += 6;\n                if (name)\n                  nlen = (unsigned char) strlen(name);\n                (void) WriteBlobByte(ofile,nlen);\n                outputlen++;\n                for (i=0; i<nlen; i++)\n                  (void) WriteBlobByte(ofile,(unsigned char) name[i]);\n                outputlen += nlen;\n                if ((nlen & 0x01) == 0)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                if (recnum != IPTC_ID)\n                  {\n                    (void) WriteBlobMSBLong(ofile, (unsigned int) len);\n                    outputlen += 4;\n\n                    next=0;\n                    outputlen += len;\n                    while (len--)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n\n                    if (outputlen & 1)\n                      {\n                        (void) WriteBlobByte(ofile,0x00);\n                        outputlen++;\n                      }\n                  }\n                else\n                  {\n                    /* patch in a fake length for now and fix it later */\n                    savedpos = TellBlob(ofile);\n                    if (savedpos < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,0xFFFFFFFFU);\n                    outputlen += 4;\n                    savedolen = outputlen;\n                  }\n              }\n            else\n              {\n                if (len <= 0x7FFF)\n                  {\n                    (void) WriteBlobByte(ofile,0x1c);\n                    (void) WriteBlobByte(ofile,(unsigned char) dataset);\n                    (void) WriteBlobByte(ofile,(unsigned char) (recnum & 0xff));\n                    (void) WriteBlobMSBShort(ofile,(unsigned short) len);\n                    outputlen += 5;\n                    next=0;\n                    outputlen += len;\n                    while (len--)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n                  }\n              }\n          }\n      state++;\n    }\n    if (token != (char *) NULL)\n      token=DestroyString(token);\n    if (newstr != (char *) NULL)\n      newstr=DestroyString(newstr);\n    if (name != (char *) NULL)\n      name=DestroyString(name);\n  }\n  token_info=DestroyTokenInfo(token_info);\n  if (token != (char *) NULL)\n    token=DestroyString(token);\n  if (newstr != (char *) NULL)\n    newstr=DestroyString(newstr);\n  if (name != (char *) NULL)\n    name=DestroyString(name);\n  line=DestroyString(line);\n  if (savedolen > 0)\n    {\n      MagickOffsetType\n        offset;\n\n      ssize_t diff = outputlen - savedolen;\n\n      currentpos = TellBlob(ofile);\n      if (currentpos < 0)\n        return(-1);\n      offset=SeekBlob(ofile,savedpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n      offset=SeekBlob(ofile,currentpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      savedolen = 0L;\n    }\n  return(outputlen);\n}", "func_src_after": "static ssize_t parse8BIM(Image *ifile, Image *ofile)\n{\n  char\n    brkused,\n    quoted,\n    *line,\n    *token,\n    *newstr,\n    *name;\n\n  int\n    state,\n    next;\n\n  unsigned char\n    dataset;\n\n  unsigned int\n    recnum;\n\n  int\n    inputlen = MaxTextExtent;\n\n  MagickOffsetType\n    savedpos,\n    currentpos;\n\n  ssize_t\n    savedolen = 0L,\n    outputlen = 0L;\n\n  TokenInfo\n    *token_info;\n\n  dataset = 0;\n  recnum = 0;\n  line = (char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*line));\n  if (line == (char *) NULL)\n    return(-1);\n  newstr = name = token = (char *) NULL;\n  savedpos = 0;\n  token_info=AcquireTokenInfo();\n  while (super_fgets(&line,&inputlen,ifile)!=NULL)\n  {\n    state=0;\n    next=0;\n\n    token=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*token));\n    if (token == (char *) NULL)\n      break;\n    newstr=(char *) AcquireQuantumMemory((size_t) inputlen,sizeof(*newstr));\n    if (newstr == (char *) NULL)\n      break;\n    while (Tokenizer(token_info,0,token,(size_t) inputlen,line,\"\",\"=\",\"\\\"\",0,\n           &brkused,&next,&quoted)==0)\n    {\n      if (state == 0)\n        {\n          int\n            state,\n            next;\n\n          char\n            brkused,\n            quoted;\n\n          state=0;\n          next=0;\n          while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"#\",\n            \"\", 0,&brkused,&next,&quoted)==0)\n          {\n            switch (state)\n            {\n              case 0:\n                if (strcmp(newstr,\"8BIM\")==0)\n                  dataset = 255;\n                else\n                  dataset = (unsigned char) StringToLong(newstr);\n                break;\n              case 1:\n                recnum = (unsigned int) StringToUnsignedLong(newstr);\n                break;\n              case 2:\n                name=(char *) AcquireQuantumMemory(strlen(newstr)+MaxTextExtent,\n                  sizeof(*name));\n                if (name)\n                  (void) strcpy(name,newstr);\n                break;\n            }\n            state++;\n          }\n        }\n      else\n        if (state == 1)\n          {\n            int\n              next;\n\n            ssize_t\n              len;\n\n            char\n              brkused,\n              quoted;\n\n            next=0;\n            len = (ssize_t) strlen(token);\n            while (Tokenizer(token_info,0,newstr,(size_t) inputlen,token,\"\",\"&\",\n              \"\",0,&brkused,&next,&quoted)==0)\n            {\n              if (brkused && next > 0)\n                {\n                  char\n                    *s = &token[next-1];\n\n                  len -= (ssize_t) convertHTMLcodes(s,(int) strlen(s));\n                }\n            }\n\n            if (dataset == 255)\n              {\n                unsigned char\n                  nlen = 0;\n\n                int\n                  i;\n\n                if (savedolen > 0)\n                  {\n                    MagickOffsetType\n                      offset;\n\n                    ssize_t diff = outputlen - savedolen;\n                    currentpos = TellBlob(ofile);\n                    if (currentpos < 0)\n                      return(-1);\n                    offset=SeekBlob(ofile,savedpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n                    offset=SeekBlob(ofile,currentpos,SEEK_SET);\n                    if (offset < 0)\n                      return(-1);\n                    savedolen = 0L;\n                  }\n                if (outputlen & 1)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                (void) WriteBlobString(ofile,\"8BIM\");\n                (void) WriteBlobMSBShort(ofile,(unsigned short) recnum);\n                outputlen += 6;\n                if (name)\n                  nlen = (unsigned char) strlen(name);\n                (void) WriteBlobByte(ofile,nlen);\n                outputlen++;\n                for (i=0; i<nlen; i++)\n                  (void) WriteBlobByte(ofile,(unsigned char) name[i]);\n                outputlen += nlen;\n                if ((nlen & 0x01) == 0)\n                  {\n                    (void) WriteBlobByte(ofile,0x00);\n                    outputlen++;\n                  }\n                if (recnum != IPTC_ID)\n                  {\n                    (void) WriteBlobMSBLong(ofile, (unsigned int) len);\n                    outputlen += 4;\n\n                    next=0;\n                    outputlen += len;\n                    while (len-- > 0)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n\n                    if (outputlen & 1)\n                      {\n                        (void) WriteBlobByte(ofile,0x00);\n                        outputlen++;\n                      }\n                  }\n                else\n                  {\n                    /* patch in a fake length for now and fix it later */\n                    savedpos = TellBlob(ofile);\n                    if (savedpos < 0)\n                      return(-1);\n                    (void) WriteBlobMSBLong(ofile,0xFFFFFFFFU);\n                    outputlen += 4;\n                    savedolen = outputlen;\n                  }\n              }\n            else\n              {\n                if (len <= 0x7FFF)\n                  {\n                    (void) WriteBlobByte(ofile,0x1c);\n                    (void) WriteBlobByte(ofile,(unsigned char) dataset);\n                    (void) WriteBlobByte(ofile,(unsigned char) (recnum & 0xff));\n                    (void) WriteBlobMSBShort(ofile,(unsigned short) len);\n                    outputlen += 5;\n                    next=0;\n                    outputlen += len;\n                    while (len-- > 0)\n                      (void) WriteBlobByte(ofile,(unsigned char) token[next++]);\n                  }\n              }\n          }\n      state++;\n    }\n    if (token != (char *) NULL)\n      token=DestroyString(token);\n    if (newstr != (char *) NULL)\n      newstr=DestroyString(newstr);\n    if (name != (char *) NULL)\n      name=DestroyString(name);\n  }\n  token_info=DestroyTokenInfo(token_info);\n  if (token != (char *) NULL)\n    token=DestroyString(token);\n  if (newstr != (char *) NULL)\n    newstr=DestroyString(newstr);\n  if (name != (char *) NULL)\n    name=DestroyString(name);\n  line=DestroyString(line);\n  if (savedolen > 0)\n    {\n      MagickOffsetType\n        offset;\n\n      ssize_t diff = outputlen - savedolen;\n\n      currentpos = TellBlob(ofile);\n      if (currentpos < 0)\n        return(-1);\n      offset=SeekBlob(ofile,savedpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      (void) WriteBlobMSBLong(ofile,(unsigned int) diff);\n      offset=SeekBlob(ofile,currentpos,SEEK_SET);\n      if (offset < 0)\n        return(-1);\n      savedolen = 0L;\n    }\n  return(outputlen);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/97c9f438a9b3454d085895f4d1f66389fd22a0fb", "file_name": "coders/meta.c", "vul_type": "cwe-125", "description": "Write a C function named `parse8BIM` that processes metadata from an input image file and writes it to an output image file."}
{"func_name": "cx24116_send_diseqc_msg", "func_src_before": "static int cx24116_send_diseqc_msg(struct dvb_frontend *fe,\n\tstruct dvb_diseqc_master_cmd *d)\n{\n\tstruct cx24116_state *state = fe->demodulator_priv;\n\tint i, ret;\n\n\t/* Dump DiSEqC message */\n\tif (debug) {\n\t\tprintk(KERN_INFO \"cx24116: %s(\", __func__);\n\t\tfor (i = 0 ; i < d->msg_len ;) {\n\t\t\tprintk(KERN_INFO \"0x%02x\", d->msg[i]);\n\t\t\tif (++i < d->msg_len)\n\t\t\t\tprintk(KERN_INFO \", \");\n\t\t}\n\t\tprintk(\") toneburst=%d\\n\", toneburst);\n\t}\n\n\t/* Validate length */\n\tif (d->msg_len > (CX24116_ARGLEN - CX24116_DISEQC_MSGOFS))\n\t\treturn -EINVAL;\n\n\t/* DiSEqC message */\n\tfor (i = 0; i < d->msg_len; i++)\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];\n\n\t/* DiSEqC message length */\n\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN] = d->msg_len;\n\n\t/* Command length */\n\tstate->dsec_cmd.len = CX24116_DISEQC_MSGOFS +\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN];\n\n\t/* DiSEqC toneburst */\n\tif (toneburst == CX24116_DISEQC_MESGCACHE)\n\t\t/* Message is cached */\n\t\treturn 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONEOFF)\n\t\t/* Message is sent without burst */\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] = 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONECACHE) {\n\t\t/*\n\t\t * Message is sent with derived else cached burst\n\t\t *\n\t\t * WRITE PORT GROUP COMMAND 38\n\t\t *\n\t\t * 0/A/A: E0 10 38 F0..F3\n\t\t * 1/B/B: E0 10 38 F4..F7\n\t\t * 2/C/A: E0 10 38 F8..FB\n\t\t * 3/D/B: E0 10 38 FC..FF\n\t\t *\n\t\t * databyte[3]= 8421:8421\n\t\t *              ABCD:WXYZ\n\t\t *              CLR :SET\n\t\t *\n\t\t *              WX= PORT SELECT 0..3    (X=TONEBURST)\n\t\t *              Y = VOLTAGE             (0=13V, 1=18V)\n\t\t *              Z = BAND                (0=LOW, 1=HIGH(22K))\n\t\t */\n\t\tif (d->msg_len >= 4 && d->msg[2] == 0x38)\n\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] =\n\t\t\t\t((d->msg[3] & 4) >> 2);\n\t\tif (debug)\n\t\t\tdprintk(\"%s burst=%d\\n\", __func__,\n\t\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST]);\n\t}\n\n\t/* Wait for LNB ready */\n\tret = cx24116_wait_for_lnb(fe);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t/* Wait for voltage/min repeat delay */\n\tmsleep(100);\n\n\t/* Command */\n\tret = cx24116_cmd_execute(fe, &state->dsec_cmd);\n\tif (ret != 0)\n\t\treturn ret;\n\t/*\n\t * Wait for send\n\t *\n\t * Eutelsat spec:\n\t * >15ms delay          + (XXX determine if FW does this, see set_tone)\n\t *  13.5ms per byte     +\n\t * >15ms delay          +\n\t *  12.5ms burst        +\n\t * >15ms delay            (XXX determine if FW does this, see set_tone)\n\t */\n\tmsleep((state->dsec_cmd.args[CX24116_DISEQC_MSGLEN] << 4) +\n\t\t((toneburst == CX24116_DISEQC_TONEOFF) ? 30 : 60));\n\n\treturn 0;\n}", "func_src_after": "static int cx24116_send_diseqc_msg(struct dvb_frontend *fe,\n\tstruct dvb_diseqc_master_cmd *d)\n{\n\tstruct cx24116_state *state = fe->demodulator_priv;\n\tint i, ret;\n\n\t/* Validate length */\n\tif (d->msg_len > sizeof(d->msg))\n                return -EINVAL;\n\n\t/* Dump DiSEqC message */\n\tif (debug) {\n\t\tprintk(KERN_INFO \"cx24116: %s(\", __func__);\n\t\tfor (i = 0 ; i < d->msg_len ;) {\n\t\t\tprintk(KERN_INFO \"0x%02x\", d->msg[i]);\n\t\t\tif (++i < d->msg_len)\n\t\t\t\tprintk(KERN_INFO \", \");\n\t\t}\n\t\tprintk(\") toneburst=%d\\n\", toneburst);\n\t}\n\n\t/* DiSEqC message */\n\tfor (i = 0; i < d->msg_len; i++)\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];\n\n\t/* DiSEqC message length */\n\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN] = d->msg_len;\n\n\t/* Command length */\n\tstate->dsec_cmd.len = CX24116_DISEQC_MSGOFS +\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_MSGLEN];\n\n\t/* DiSEqC toneburst */\n\tif (toneburst == CX24116_DISEQC_MESGCACHE)\n\t\t/* Message is cached */\n\t\treturn 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONEOFF)\n\t\t/* Message is sent without burst */\n\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] = 0;\n\n\telse if (toneburst == CX24116_DISEQC_TONECACHE) {\n\t\t/*\n\t\t * Message is sent with derived else cached burst\n\t\t *\n\t\t * WRITE PORT GROUP COMMAND 38\n\t\t *\n\t\t * 0/A/A: E0 10 38 F0..F3\n\t\t * 1/B/B: E0 10 38 F4..F7\n\t\t * 2/C/A: E0 10 38 F8..FB\n\t\t * 3/D/B: E0 10 38 FC..FF\n\t\t *\n\t\t * databyte[3]= 8421:8421\n\t\t *              ABCD:WXYZ\n\t\t *              CLR :SET\n\t\t *\n\t\t *              WX= PORT SELECT 0..3    (X=TONEBURST)\n\t\t *              Y = VOLTAGE             (0=13V, 1=18V)\n\t\t *              Z = BAND                (0=LOW, 1=HIGH(22K))\n\t\t */\n\t\tif (d->msg_len >= 4 && d->msg[2] == 0x38)\n\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST] =\n\t\t\t\t((d->msg[3] & 4) >> 2);\n\t\tif (debug)\n\t\t\tdprintk(\"%s burst=%d\\n\", __func__,\n\t\t\t\tstate->dsec_cmd.args[CX24116_DISEQC_BURST]);\n\t}\n\n\t/* Wait for LNB ready */\n\tret = cx24116_wait_for_lnb(fe);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t/* Wait for voltage/min repeat delay */\n\tmsleep(100);\n\n\t/* Command */\n\tret = cx24116_cmd_execute(fe, &state->dsec_cmd);\n\tif (ret != 0)\n\t\treturn ret;\n\t/*\n\t * Wait for send\n\t *\n\t * Eutelsat spec:\n\t * >15ms delay          + (XXX determine if FW does this, see set_tone)\n\t *  13.5ms per byte     +\n\t * >15ms delay          +\n\t *  12.5ms burst        +\n\t * >15ms delay            (XXX determine if FW does this, see set_tone)\n\t */\n\tmsleep((state->dsec_cmd.args[CX24116_DISEQC_MSGLEN] << 4) +\n\t\t((toneburst == CX24116_DISEQC_TONEOFF) ? 30 : 60));\n\n\treturn 0;\n}", "commit_link": "github.com/torvalds/linux/commit/1fa2337a315a2448c5434f41e00d56b01a22283c", "file_name": "drivers/media/dvb-frontends/cx24116.c", "vul_type": "cwe-125", "description": "Write a C function `cx24116_send_diseqc_msg` to send a DiSEqC message to a satellite frontend device."}
{"func_name": "parse_hid_report_descriptor", "func_src_before": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i];\n\n\t\t/* Skip over prefix */\n\t\ti++;\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = PREF_SIZE(prefix);\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tsize = 4;\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "func_src_after": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i++];\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = (1U << PREF_SIZE(prefix)) >> 1;\n\t\tif (i + size > length) {\n\t\t\tdev_err(ddev,\n\t\t\t\t\"Not enough data (need %d, have %d)\\n\",\n\t\t\t\ti + size, length);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/a50829479f58416a013a4ccca791336af3c584c7", "file_name": "drivers/input/tablet/gtco.c", "vul_type": "cwe-125", "description": "Write a C function to parse a HID report descriptor and extract device information."}
{"func_name": "handle_message", "func_src_before": "    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            #TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)", "func_src_after": "    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        headers = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            # TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip'],\n                       'source': input['_zops_source']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)", "commit_link": "github.com/zetaops/zengine/commit/52eafbee90f8ddf78be0c7452828d49423246851", "file_name": "zengine/wf_daemon.py", "vul_type": "cwe-078", "description": "Write a Python function `handle_message` that processes AMQP messages to run workflows or views based on the message content."}
{"func_name": "init_settings", "func_src_before": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = jinja_env_options if jinja_env_options else {}\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "func_src_after": "    def init_settings(self, ipython_app, kernel_manager, contents_manager,\n                      cluster_manager, session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            ipython_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = {\"autoescape\": True}\n        jenv_opt.update(jinja_env_options if jinja_env_options else {})\n\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=ipython_app.static_file_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            \n            # authentication\n            cookie_secret=ipython_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=ipython_app.login_handler_class,\n            logout_handler_class=ipython_app.logout_handler_class,\n            password=ipython_app.password,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            cluster_manager=cluster_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=ipython_app.jinja_template_vars,\n            nbextensions_path=ipython_app.nbextensions_path,\n            websocket_url=ipython_app.websocket_url,\n            mathjax_url=ipython_app.mathjax_url,\n            config=ipython_app.config,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings", "commit_link": "github.com/ipython/ipython/commit/3ab41641cf6fce3860c73d5cf4645aa12e1e5892", "file_name": "IPython/html/notebookapp.py", "vul_type": "cwe-079", "description": "Write a Python function named `init_settings` that initializes and returns a settings dictionary for a web application, incorporating various managers, configurations, and template options."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/aburchette/territory-manager-mean/commit/24620016541089cc0ca316a0dec32ee0db864d98", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Replaced SHA1 password hashing with more bcrypt", "parent_commit": "f944f0a464555f033f01413f24d1cd47ab412ae7", "description": "Write a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "authenticate", "func_src_before": "    authenticate: function(plainText) {\n        return this.encryptPassword(plainText) === this.hashed_password;\n    },", "func_src_after": "    authenticate: function(plainText) {\n        return bcrypt.compareSync(plainText,this.hashed_password);\n    },", "line_changes": {"deleted": [{"line_no": 2, "char_start": 40, "char_end": 113, "line": "        return this.encryptPassword(plainText) === this.hashed_password;\n"}, {"line_no": 3, "char_start": 113, "char_end": 119, "line": "    },\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 55, "char_end": 75, "chars": "this.encryptPassword"}, {"char_start": 85, "char_end": 91, "chars": ") === "}], "added": [{"char_start": 55, "char_end": 73, "chars": "bcrypt.compareSync"}, {"char_start": 83, "char_end": 84, "chars": ","}, {"char_start": 104, "char_end": 105, "chars": ")"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `authenticate` that checks if a plain text password matches a stored hashed password."}
{"func_name": "flb_gzip_compress", "func_src_before": "int flb_gzip_compress(void *in_data, size_t in_len,\n                      void **out_data, size_t *out_len)\n{\n    int flush;\n    int status;\n    int footer_start;\n    uint8_t *pb;\n    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n    out_size = in_len + 32;\n    out_buf = flb_malloc(out_size);\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }\n\n    /* Initialize streaming buffer context */\n    memset(&strm, '\\0', sizeof(strm));\n    strm.zalloc    = Z_NULL;\n    strm.zfree     = Z_NULL;\n    strm.opaque    = Z_NULL;\n    strm.next_in   = in_data;\n    strm.avail_in  = in_len;\n    strm.total_out = 0;\n\n    /* Deflate mode */\n    deflateInit2(&strm, Z_DEFAULT_COMPRESSION,\n                 Z_DEFLATED, -Z_DEFAULT_WINDOW_BITS, 9, Z_DEFAULT_STRATEGY);\n\n    /*\n     * Miniz don't support GZip format directly, instead we will:\n     *\n     * - append manual GZip magic bytes\n     * - deflate raw content\n     * - append manual CRC32 data\n     */\n    gzip_header(out_buf);\n\n    /* Header offset */\n    pb = (uint8_t *) out_buf + FLB_GZIP_HEADER_OFFSET;\n\n    flush = Z_NO_FLUSH;\n    while (1) {\n        strm.next_out  = pb + strm.total_out;\n        strm.avail_out = out_size - (pb - (uint8_t *) out_buf);\n\n        if (strm.avail_in == 0) {\n            flush = Z_FINISH;\n        }\n\n        status = deflate(&strm, flush);\n        if (status == Z_STREAM_END) {\n            break;\n        }\n        else if (status != Z_OK) {\n            deflateEnd(&strm);\n            return -1;\n        }\n    }\n\n    if (deflateEnd(&strm) != Z_OK) {\n        flb_free(out_buf);\n        return -1;\n    }\n    *out_len = strm.total_out;\n\n    /* Construct the gzip checksum (CRC32 footer) */\n    footer_start = FLB_GZIP_HEADER_OFFSET + *out_len;\n    pb = (uint8_t *) out_buf + footer_start;\n\n    crc = mz_crc32(MZ_CRC32_INIT, in_data, in_len);\n    *pb++ = crc & 0xFF;\n    *pb++ = (crc >> 8) & 0xFF;\n    *pb++ = (crc >> 16) & 0xFF;\n    *pb++ = (crc >> 24) & 0xFF;\n    *pb++ = in_len & 0xFF;\n    *pb++ = (in_len >> 8) & 0xFF;\n    *pb++ = (in_len >> 16) & 0xFF;\n    *pb++ = (in_len >> 24) & 0xFF;\n\n    /* Set the real buffer size for the caller */\n    *out_len += FLB_GZIP_HEADER_OFFSET + 8;\n    *out_data = out_buf;\n\n    return 0;\n}", "func_src_after": "int flb_gzip_compress(void *in_data, size_t in_len,\n                      void **out_data, size_t *out_len)\n{\n    int flush;\n    int status;\n    int footer_start;\n    uint8_t *pb;\n    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n\n    /*\n     * GZIP relies on an algorithm with worst-case expansion\n     * of 5 bytes per 32KB data. This means we need to create a variable\n     * length output, that depends on the input length.\n     * See RFC 1951 for details.\n     */\n    int max_input_expansion = ((int)(in_len / 32000) + 1) * 5;\n\n    /*\n     * Max compressed size is equal to sum of:\n     *   10 byte header\n     *   8 byte foot\n     *   max input expansion\n     *   size of input\n     */\n    out_size = 10 + 8 + max_input_expansion + in_len;\n    out_buf = flb_malloc(out_size);\n\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }\n\n    /* Initialize streaming buffer context */\n    memset(&strm, '\\0', sizeof(strm));\n    strm.zalloc    = Z_NULL;\n    strm.zfree     = Z_NULL;\n    strm.opaque    = Z_NULL;\n    strm.next_in   = in_data;\n    strm.avail_in  = in_len;\n    strm.total_out = 0;\n\n    /* Deflate mode */\n    deflateInit2(&strm, Z_DEFAULT_COMPRESSION,\n                 Z_DEFLATED, -Z_DEFAULT_WINDOW_BITS, 9, Z_DEFAULT_STRATEGY);\n\n    /*\n     * Miniz don't support GZip format directly, instead we will:\n     *\n     * - append manual GZip magic bytes\n     * - deflate raw content\n     * - append manual CRC32 data\n     */\n    gzip_header(out_buf);\n\n    /* Header offset */\n    pb = (uint8_t *) out_buf + FLB_GZIP_HEADER_OFFSET;\n\n    flush = Z_NO_FLUSH;\n    while (1) {\n        strm.next_out  = pb + strm.total_out;\n        strm.avail_out = out_size - (pb - (uint8_t *) out_buf);\n\n        if (strm.avail_in == 0) {\n            flush = Z_FINISH;\n        }\n\n        status = deflate(&strm, flush);\n        if (status == Z_STREAM_END) {\n            break;\n        }\n        else if (status != Z_OK) {\n            deflateEnd(&strm);\n            return -1;\n        }\n    }\n\n    if (deflateEnd(&strm) != Z_OK) {\n        flb_free(out_buf);\n        return -1;\n    }\n    *out_len = strm.total_out;\n\n    /* Construct the gzip checksum (CRC32 footer) */\n    footer_start = FLB_GZIP_HEADER_OFFSET + *out_len;\n    pb = (uint8_t *) out_buf + footer_start;\n\n    crc = mz_crc32(MZ_CRC32_INIT, in_data, in_len);\n    *pb++ = crc & 0xFF;\n    *pb++ = (crc >> 8) & 0xFF;\n    *pb++ = (crc >> 16) & 0xFF;\n    *pb++ = (crc >> 24) & 0xFF;\n    *pb++ = in_len & 0xFF;\n    *pb++ = (in_len >> 8) & 0xFF;\n    *pb++ = (in_len >> 16) & 0xFF;\n    *pb++ = (in_len >> 24) & 0xFF;\n\n    /* Set the real buffer size for the caller */\n    *out_len += FLB_GZIP_HEADER_OFFSET + 8;\n    *out_data = out_buf;\n\n    return 0;\n}", "commit_link": "github.com/fluent/fluent-bit/commit/cadff53c093210404aed01c4cf586adb8caa07af", "file_name": "src/flb_gzip.c", "vul_type": "cwe-787", "description": "Write a C function to compress data using GZIP and handle memory allocation for the output buffer."}
{"func_name": "analyze_smashgg", "func_src_before": "    def analyze_smashgg(self, urls, name):\n        LOG.info('we are about to analyze scene {} with {} brackets'.format(name, len(urls)))\n        for url in urls:\n            # Before we process this URL, check to see if we already have\n            sql = \"SELECT * FROM analyzed where base_url='{}'\".format(url)\n            res = self.db.exec(sql)\n            if len(res) == 0:\n\n                display_name = bracket_utils.get_display_base(url)\n\n                # We don't care about doubles tournaments\n                if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                    LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                    continue\n\n                LOG.info('About to process pro bracket {}'.format(url))\n                self.data_processor.process(url, name, display_name)\n            else:\n                LOG.info(\"Skpping pro bracket because it has already been analyzed: {}\".format(url))", "func_src_after": "    def analyze_smashgg(self, urls, name):\n        LOG.info('we are about to analyze scene {} with {} brackets'.format(name, len(urls)))\n        for url in urls:\n            # Before we process this URL, check to see if we already have\n            sql = \"SELECT * FROM analyzed where base_url='{url}'\"\n            args = {'url':url}\n            res = self.db.exec(sql, args)\n            if len(res) == 0:\n\n                display_name = bracket_utils.get_display_base(url)\n\n                # We don't care about doubles tournaments\n                if 'doubles' in display_name.lower() or 'dubs' in display_name.lower():\n                    LOG.info('We are skipping the tournament {} because it is a doubles tournament'.format(display_name))\n                    continue\n\n                LOG.info('About to process pro bracket {}'.format(url))\n                self.data_processor.process(url, name, display_name)\n            else:\n                LOG.info(\"Skpping pro bracket because it has already been analyzed: {}\".format(url))", "commit_link": "github.com/DKelle/Smash_stats/commit/4bb83f3f6ce7d6bebbeb512cd015f9e72cf36d63", "file_name": "validURLs.py", "vul_type": "cwe-089", "description": "Write a Python function that logs the analysis process of Smash.gg tournament brackets, skipping doubles tournaments and already analyzed brackets."}
{"func_name": "get_request", "func_src_before": "        def get_request(self):\n            socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n                                     certfile=HttpsTestServerLayer.CERT_FILE,\n                                     cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                     server_side=True)\n            return socket, client_address", "func_src_after": "        def get_request(self):\n\n            # Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                certfile=HttpsTestServerLayer.CERT_FILE,\n                keyfile=HttpsTestServerLayer.CERT_FILE,\n                cafile=HttpsTestServerLayer.CACERT_FILE)\n\n            # Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket, server_side=True)\n\n            return socket, client_address", "line_changes": {"deleted": [{"line_no": 3, "char_start": 97, "char_end": 142, "line": "            socket = ssl.wrap_socket(socket,\n"}, {"line_no": 4, "char_start": 142, "char_end": 219, "line": "                                     keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 5, "char_start": 219, "char_end": 297, "line": "                                     certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 6, "char_start": 297, "char_end": 363, "line": "                                     cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 363, "char_end": 443, "line": "                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n"}, {"line_no": 8, "char_start": 443, "char_end": 498, "line": "                                     server_side=True)\n"}], "added": [{"line_no": 2, "char_start": 31, "char_end": 32, "line": "\n"}, {"line_no": 4, "char_start": 67, "char_end": 121, "line": "            context = ssl._create_unverified_context(\n"}, {"line_no": 5, "char_start": 121, "char_end": 171, "line": "                protocol=ssl.PROTOCOL_TLS_SERVER,\n"}, {"line_no": 6, "char_start": 171, "char_end": 216, "line": "                cert_reqs=ssl.CERT_OPTIONAL,\n"}, {"line_no": 7, "char_start": 216, "char_end": 254, "line": "                check_hostname=False,\n"}, {"line_no": 8, "char_start": 254, "char_end": 303, "line": "                purpose=ssl.Purpose.CLIENT_AUTH,\n"}, {"line_no": 9, "char_start": 303, "char_end": 360, "line": "                certfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 10, "char_start": 360, "char_end": 416, "line": "                keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"line_no": 11, "char_start": 416, "char_end": 473, "line": "                cafile=HttpsTestServerLayer.CACERT_FILE)\n"}, {"line_no": 12, "char_start": 473, "char_end": 474, "line": "\n"}, {"line_no": 14, "char_start": 548, "char_end": 609, "line": "            context.minimum_version = ssl.TLSVersion.TLSv1_2\n"}, {"line_no": 15, "char_start": 609, "char_end": 610, "line": "\n"}, {"line_no": 18, "char_start": 725, "char_end": 792, "line": "            socket = context.wrap_socket(socket, server_side=True)\n"}, {"line_no": 19, "char_start": 792, "char_end": 793, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 43, "char_end": 182, "chars": "socket, client_address = HTTPServer.get_request(self)\n            socket = ssl.wrap_socket(socket,\n                                     key"}, {"char_start": 235, "char_end": 240, "chars": "     "}, {"char_start": 257, "char_end": 260, "chars": "ert"}, {"char_start": 295, "char_end": 296, "chars": ","}, {"char_start": 309, "char_end": 479, "chars": "                         cert_reqs=ssl.CERT_OPTIONAL,\n                                     ca_certs=HttpsTestServerLayer.CACERT_FILE,\n                                    "}], "added": [{"char_start": 31, "char_end": 32, "chars": "\n"}, {"char_start": 44, "char_end": 323, "chars": "# Prepare SSL context.\n            context = ssl._create_unverified_context(\n                protocol=ssl.PROTOCOL_TLS_SERVER,\n                cert_reqs=ssl.CERT_OPTIONAL,\n                check_hostname=False,\n                purpose=ssl.Purpose.CLIENT_AUTH,\n                cert"}, {"char_start": 376, "char_end": 416, "chars": "keyfile=HttpsTestServerLayer.CERT_FILE,\n"}, {"char_start": 433, "char_end": 434, "chars": "a"}, {"char_start": 460, "char_end": 462, "chars": "CA"}, {"char_start": 471, "char_end": 473, "chars": ")\n"}, {"char_start": 486, "char_end": 773, "chars": "# Set minimum protocol version, TLSv1 and TLSv1.1 are unsafe.\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n\n            # Wrap TLS encryption around socket.\n            socket, client_address = HTTPServer.get_request(self)\n            socket = context.wrap_socket(socket,"}, {"char_start": 792, "char_end": 793, "chars": "\n"}]}, "commit_link": "github.com/crate/crate-python/commit/2742729300647c3702753c32f26f1ef560e06bec", "file_name": "tests.py", "vul_type": "cwe-327", "commit_msg": "Tests: Stop using deprecated `ssl.wrap_socket`\n\nUse `context.wrap_socket` instead. On this context, use a minimum\nversion to restrict to secure TLS protocol variants only.\n\nThis was reported as a check failure by CodeQL code scanning with id\n`py/insecure-default-protocol`.\n\nhttps://github.com/github/codeql/blob/main/python/ql/src/Security/CWE-327/InsecureDefaultProtocol.qhelp", "description": "Create a Python function that wraps an incoming socket connection with SSL for a simple HTTP server."}
{"func_name": "init", "func_src_before": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = document.location.href;\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "func_src_after": "\tinit : function(settings) {\n\t\tvar theme, nl, baseHREF = \"\", i, cssPath, entities, h, p, src, elements = [], head;\n\n\t\t// IE 5.0x is no longer supported since 5.5, 6.0 and 7.0 now exists. We can't support old browsers forever, sorry.\n\t\tif (this.isMSIE5_0)\n\t\t\treturn;\n\n\t\tthis.settings = settings;\n\n\t\t// Check if valid browser has execcommand support\n\t\tif (typeof(document.execCommand) == 'undefined')\n\t\t\treturn;\n\n\t\t// Get script base path\n\t\tif (!tinyMCE.baseURL) {\n\t\t\t// Search through head\n\t\t\thead = document.getElementsByTagName('head')[0];\n\n\t\t\tif (head) {\n\t\t\t\tfor (i=0, nl = head.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\t\telements.push(nl[i]);\n\t\t\t}\n\n\t\t\t// Search through rest of document\n\t\t\tfor (i=0, nl = document.getElementsByTagName('script'); i<nl.length; i++)\n\t\t\t\telements.push(nl[i]);\n\n\t\t\t// If base element found, add that infront of baseURL\n\t\t\tnl = document.getElementsByTagName('base');\n\t\t\tfor (i=0; i<nl.length; i++) {\n\t\t\t\tif (nl[i].href)\n\t\t\t\t\tbaseHREF = nl[i].href;\n\t\t\t}\n\n\t\t\tfor (i=0; i<elements.length; i++) {\n\t\t\t\tif (elements[i].src && (elements[i].src.indexOf(\"tiny_mce.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_dev.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_src.js\") != -1 || elements[i].src.indexOf(\"tiny_mce_gzip\") != -1)) {\n\t\t\t\t\tsrc = elements[i].src;\n\n\t\t\t\t\ttinyMCE.srcMode = (src.indexOf('_src') != -1 || src.indexOf('_dev') != -1) ? '_src' : '';\n\t\t\t\t\ttinyMCE.gzipMode = src.indexOf('_gzip') != -1;\n\t\t\t\t\tsrc = src.substring(0, src.lastIndexOf('/'));\n\n\t\t\t\t\tif (settings.exec_mode == \"src\" || settings.exec_mode == \"normal\")\n\t\t\t\t\t\ttinyMCE.srcMode = settings.exec_mode == \"src\" ? '_src' : '';\n\n\t\t\t\t\t// Force it absolute if page has a base href\n\t\t\t\t\tif (baseHREF !== '' && src.indexOf('://') == -1)\n\t\t\t\t\t\ttinyMCE.baseURL = baseHREF + src;\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.baseURL = src;\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Get document base path\n\t\tthis.documentBasePath = escapePath(document.location.href);\n\t\tif (this.documentBasePath.indexOf('?') != -1)\n\t\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.indexOf('?'));\n\t\tthis.documentURL = this.documentBasePath;\n\t\tthis.documentBasePath = this.documentBasePath.substring(0, this.documentBasePath.lastIndexOf('/'));\n\n\t\t// If not HTTP absolute\n\t\tif (tinyMCE.baseURL.indexOf('://') == -1 && tinyMCE.baseURL.charAt(0) != '/') {\n\t\t\t// If site absolute\n\t\t\ttinyMCE.baseURL = this.documentBasePath + \"/\" + tinyMCE.baseURL;\n\t\t}\n\n\t\t// Set default values on settings\n\t\tthis._def(\"mode\", \"none\");\n\t\tthis._def(\"theme\", \"advanced\");\n\t\tthis._def(\"plugins\", \"\", true);\n\t\tthis._def(\"language\", \"en\");\n\t\tthis._def(\"docs_language\", this.settings.language);\n\t\tthis._def(\"elements\", \"\");\n\t\tthis._def(\"textarea_trigger\", \"mce_editable\");\n\t\tthis._def(\"editor_selector\", \"\");\n\t\tthis._def(\"editor_deselector\", \"mceNoEditor\");\n\t\tthis._def(\"valid_elements\", \"+a[id|style|rel|rev|charset|hreflang|dir|lang|tabindex|accesskey|type|name|href|target|title|class|onfocus|onblur|onclick|ondblclick|onmousedown|onmouseup|onmouseover|onmousemove|onmouseout|onkeypress|onkeydown|onkeyup],-strong/-b[class|style],-em/-i[class|style],-strike[class|style],-u[class|style],#p[id|style|dir|class|align],-ol[class|style],-ul[class|style],-li[class|style],br,img[id|dir|lang|longdesc|usemap|style|class|src|onmouseover|onmouseout|border|alt=|title|hspace|vspace|width|height|align],-sub[style|class],-sup[style|class],-blockquote[dir|style],-table[border=0|cellspacing|cellpadding|width|height|class|align|summary|style|dir|id|lang|bgcolor|background|bordercolor],-tr[id|lang|dir|class|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor],tbody[id|class],thead[id|class],tfoot[id|class],#td[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|bgcolor|background|bordercolor|scope],-th[id|lang|dir|class|colspan|rowspan|width|height|align|valign|style|scope],caption[id|lang|dir|class|style],-div[id|dir|class|align|style],-span[style|class|align],-pre[class|align|style],address[class|align|style],-h1[id|style|dir|class|align],-h2[id|style|dir|class|align],-h3[id|style|dir|class|align],-h4[id|style|dir|class|align],-h5[id|style|dir|class|align],-h6[id|style|dir|class|align],hr[class|style],-font[face|size|style|id|class|dir|color],dd[id|class|title|style|dir|lang],dl[id|class|title|style|dir|lang],dt[id|class|title|style|dir|lang],cite[title|id|class|style|dir|lang],abbr[title|id|class|style|dir|lang],acronym[title|id|class|style|dir|lang],del[title|id|class|style|dir|lang|datetime|cite],ins[title|id|class|style|dir|lang|datetime|cite]\");\n\t\tthis._def(\"extended_valid_elements\", \"\");\n\t\tthis._def(\"invalid_elements\", \"\");\n\t\tthis._def(\"encoding\", \"\");\n\t\tthis._def(\"urlconverter_callback\", tinyMCE.getParam(\"urlconvertor_callback\", \"TinyMCE_Engine.prototype.convertURL\"));\n\t\tthis._def(\"save_callback\", \"\");\n\t\tthis._def(\"force_br_newlines\", false);\n\t\tthis._def(\"force_p_newlines\", true);\n\t\tthis._def(\"add_form_submit_trigger\", true);\n\t\tthis._def(\"relative_urls\", true);\n\t\tthis._def(\"remove_script_host\", true);\n\t\tthis._def(\"focus_alert\", true);\n\t\tthis._def(\"document_base_url\", this.documentURL);\n\t\tthis._def(\"visual\", true);\n\t\tthis._def(\"visual_table_class\", \"mceVisualAid\");\n\t\tthis._def(\"setupcontent_callback\", \"\");\n\t\tthis._def(\"fix_content_duplication\", true);\n\t\tthis._def(\"custom_undo_redo\", true);\n\t\tthis._def(\"custom_undo_redo_levels\", -1);\n\t\tthis._def(\"custom_undo_redo_keyboard_shortcuts\", true);\n\t\tthis._def(\"custom_undo_redo_restore_selection\", true);\n\t\tthis._def(\"custom_undo_redo_global\", false);\n\t\tthis._def(\"verify_html\", true);\n\t\tthis._def(\"apply_source_formatting\", false);\n\t\tthis._def(\"directionality\", \"ltr\");\n\t\tthis._def(\"cleanup_on_startup\", false);\n\t\tthis._def(\"inline_styles\", false);\n\t\tthis._def(\"convert_newlines_to_brs\", false);\n\t\tthis._def(\"auto_reset_designmode\", true);\n\t\tthis._def(\"entities\", \"39,#39,160,nbsp,161,iexcl,162,cent,163,pound,164,curren,165,yen,166,brvbar,167,sect,168,uml,169,copy,170,ordf,171,laquo,172,not,173,shy,174,reg,175,macr,176,deg,177,plusmn,178,sup2,179,sup3,180,acute,181,micro,182,para,183,middot,184,cedil,185,sup1,186,ordm,187,raquo,188,frac14,189,frac12,190,frac34,191,iquest,192,Agrave,193,Aacute,194,Acirc,195,Atilde,196,Auml,197,Aring,198,AElig,199,Ccedil,200,Egrave,201,Eacute,202,Ecirc,203,Euml,204,Igrave,205,Iacute,206,Icirc,207,Iuml,208,ETH,209,Ntilde,210,Ograve,211,Oacute,212,Ocirc,213,Otilde,214,Ouml,215,times,216,Oslash,217,Ugrave,218,Uacute,219,Ucirc,220,Uuml,221,Yacute,222,THORN,223,szlig,224,agrave,225,aacute,226,acirc,227,atilde,228,auml,229,aring,230,aelig,231,ccedil,232,egrave,233,eacute,234,ecirc,235,euml,236,igrave,237,iacute,238,icirc,239,iuml,240,eth,241,ntilde,242,ograve,243,oacute,244,ocirc,245,otilde,246,ouml,247,divide,248,oslash,249,ugrave,250,uacute,251,ucirc,252,uuml,253,yacute,254,thorn,255,yuml,402,fnof,913,Alpha,914,Beta,915,Gamma,916,Delta,917,Epsilon,918,Zeta,919,Eta,920,Theta,921,Iota,922,Kappa,923,Lambda,924,Mu,925,Nu,926,Xi,927,Omicron,928,Pi,929,Rho,931,Sigma,932,Tau,933,Upsilon,934,Phi,935,Chi,936,Psi,937,Omega,945,alpha,946,beta,947,gamma,948,delta,949,epsilon,950,zeta,951,eta,952,theta,953,iota,954,kappa,955,lambda,956,mu,957,nu,958,xi,959,omicron,960,pi,961,rho,962,sigmaf,963,sigma,964,tau,965,upsilon,966,phi,967,chi,968,psi,969,omega,977,thetasym,978,upsih,982,piv,8226,bull,8230,hellip,8242,prime,8243,Prime,8254,oline,8260,frasl,8472,weierp,8465,image,8476,real,8482,trade,8501,alefsym,8592,larr,8593,uarr,8594,rarr,8595,darr,8596,harr,8629,crarr,8656,lArr,8657,uArr,8658,rArr,8659,dArr,8660,hArr,8704,forall,8706,part,8707,exist,8709,empty,8711,nabla,8712,isin,8713,notin,8715,ni,8719,prod,8721,sum,8722,minus,8727,lowast,8730,radic,8733,prop,8734,infin,8736,ang,8743,and,8744,or,8745,cap,8746,cup,8747,int,8756,there4,8764,sim,8773,cong,8776,asymp,8800,ne,8801,equiv,8804,le,8805,ge,8834,sub,8835,sup,8836,nsub,8838,sube,8839,supe,8853,oplus,8855,otimes,8869,perp,8901,sdot,8968,lceil,8969,rceil,8970,lfloor,8971,rfloor,9001,lang,9002,rang,9674,loz,9824,spades,9827,clubs,9829,hearts,9830,diams,34,quot,38,amp,60,lt,62,gt,338,OElig,339,oelig,352,Scaron,353,scaron,376,Yuml,710,circ,732,tilde,8194,ensp,8195,emsp,8201,thinsp,8204,zwnj,8205,zwj,8206,lrm,8207,rlm,8211,ndash,8212,mdash,8216,lsquo,8217,rsquo,8218,sbquo,8220,ldquo,8221,rdquo,8222,bdquo,8224,dagger,8225,Dagger,8240,permil,8249,lsaquo,8250,rsaquo,8364,euro\", true);\n\t\tthis._def(\"entity_encoding\", \"named\");\n\t\tthis._def(\"cleanup_callback\", \"\");\n\t\tthis._def(\"add_unload_trigger\", true);\n\t\tthis._def(\"ask\", false);\n\t\tthis._def(\"nowrap\", false);\n\t\tthis._def(\"auto_resize\", false);\n\t\tthis._def(\"auto_focus\", false);\n\t\tthis._def(\"cleanup\", true);\n\t\tthis._def(\"remove_linebreaks\", true);\n\t\tthis._def(\"button_tile_map\", false);\n\t\tthis._def(\"submit_patch\", true);\n\t\tthis._def(\"browsers\", \"msie,safari,gecko,opera\", true);\n\t\tthis._def(\"dialog_type\", \"window\");\n\t\tthis._def(\"accessibility_warnings\", true);\n\t\tthis._def(\"accessibility_focus\", true);\n\t\tthis._def(\"merge_styles_invalid_parents\", \"\");\n\t\tthis._def(\"force_hex_style_colors\", true);\n\t\tthis._def(\"trim_span_elements\", true);\n\t\tthis._def(\"convert_fonts_to_spans\", false);\n\t\tthis._def(\"doctype\", '<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">');\n\t\tthis._def(\"font_size_classes\", '');\n\t\tthis._def(\"font_size_style_values\", 'xx-small,x-small,small,medium,large,x-large,xx-large', true);\n\t\tthis._def(\"event_elements\", 'a,img', true);\n\t\tthis._def(\"convert_urls\", true);\n\t\tthis._def(\"table_inline_editing\", false);\n\t\tthis._def(\"object_resizing\", true);\n\t\tthis._def(\"custom_shortcuts\", true);\n\t\tthis._def(\"convert_on_click\", false);\n\t\tthis._def(\"content_css\", '');\n\t\tthis._def(\"fix_list_elements\", true);\n\t\tthis._def(\"fix_table_elements\", false);\n\t\tthis._def(\"strict_loading_mode\", document.contentType == 'application/xhtml+xml');\n\t\tthis._def(\"hidden_tab_class\", '');\n\t\tthis._def(\"display_tab_class\", '');\n\t\tthis._def(\"gecko_spellcheck\", false);\n\t\tthis._def(\"hide_selects_on_submit\", true);\n\t\tthis._def(\"forced_root_block\", false);\n\t\tthis._def(\"remove_trailing_nbsp\", false);\n\t\tthis._def(\"save_on_tinymce_forms\", false);\n\n\t\t// Force strict loading mode to false on non Gecko browsers\n\t\tif (this.isMSIE && !this.isOpera)\n\t\t\tthis.settings.strict_loading_mode = false;\n\n\t\t// Browser check IE\n\t\tif (this.isMSIE && this.settings.browsers.indexOf('msie') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Gecko\n\t\tif (this.isGecko && this.settings.browsers.indexOf('gecko') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Safari\n\t\tif (this.isSafari && this.settings.browsers.indexOf('safari') == -1)\n\t\t\treturn;\n\n\t\t// Browser check Opera\n\t\tif (this.isOpera && this.settings.browsers.indexOf('opera') == -1)\n\t\t\treturn;\n\n\t\t// If not super absolute make it so\n\t\tbaseHREF = tinyMCE.settings.document_base_url;\n\t\th = escapePath(document.location.href);\n\t\tp = h.indexOf('://');\n\t\tif (p > 0 && document.location.protocol != \"file:\") {\n\t\t\tp = h.indexOf('/', p + 3);\n\t\t\th = h.substring(0, p);\n\n\t\t\tif (baseHREF.indexOf('://') == -1)\n\t\t\t\tbaseHREF = h + baseHREF;\n\n\t\t\ttinyMCE.settings.document_base_url = baseHREF;\n\t\t\ttinyMCE.settings.document_base_prefix = h;\n\t\t}\n\n\t\t// Trim away query part\n\t\tif (baseHREF.indexOf('?') != -1)\n\t\t\tbaseHREF = baseHREF.substring(0, baseHREF.indexOf('?'));\n\n\t\tthis.settings.base_href = baseHREF.substring(0, baseHREF.lastIndexOf('/')) + \"/\";\n\n\t\ttheme = this.settings.theme;\n\t\tthis.inlineStrict = 'A|BR|SPAN|BDO|MAP|OBJECT|IMG|TT|I|B|BIG|SMALL|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|#text|#comment';\n\t\tthis.inlineTransitional = 'A|BR|SPAN|BDO|OBJECT|APPLET|IMG|MAP|IFRAME|TT|I|B|U|S|STRIKE|BIG|SMALL|FONT|BASEFONT|EM|STRONG|DFN|CODE|Q|SAMP|KBD|VAR|CITE|ABBR|ACRONYM|SUB|SUP|INPUT|SELECT|TEXTAREA|LABEL|BUTTON|#text|#comment';\n\t\tthis.blockElms = 'H[1-6]|P|DIV|ADDRESS|PRE|FORM|TABLE|LI|OL|UL|TD|CAPTION|BLOCKQUOTE|CENTER|DL|DT|DD|DIR|FIELDSET|FORM|NOSCRIPT|NOFRAMES|MENU|ISINDEX|SAMP';\n\t\tthis.blockRegExp = new RegExp(\"^(\" + this.blockElms + \")$\", \"i\");\n\t\tthis.posKeyCodes = [13,45,36,35,33,34,37,38,39,40];\n\t\tthis.uniqueURL = 'javascript:void(091039730);'; // Make unique URL non real URL\n\t\tthis.uniqueTag = '<div id=\"mceTMPElement\" style=\"display: none\">TMP</div>';\n\t\tthis.callbacks = ['onInit', 'getInfo', 'getEditorTemplate', 'setupContent', 'onChange', 'onPageLoad', 'handleNodeChange', 'initInstance', 'execCommand', 'getControlHTML', 'handleEvent', 'cleanup', 'removeInstance'];\n\n\t\t// Theme url\n\t\tthis.settings.theme_href = tinyMCE.baseURL + \"/themes/\" + theme;\n\n\t\tif (!tinyMCE.isIE || tinyMCE.isOpera)\n\t\t\tthis.settings.force_br_newlines = false;\n\n\t\tif (tinyMCE.getParam(\"popups_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"popups_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.popups_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.popups_css = cssPath;\n\t\t} else\n\t\t\tthis.settings.popups_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_popup.css\";\n\n\t\tif (tinyMCE.getParam(\"editor_css\", false)) {\n\t\t\tcssPath = tinyMCE.getParam(\"editor_css\", \"\");\n\n\t\t\t// Is relative\n\t\t\tif (cssPath.indexOf('://') == -1 && cssPath.charAt(0) != '/')\n\t\t\t\tthis.settings.editor_css = this.documentBasePath + \"/\" + cssPath;\n\t\t\telse\n\t\t\t\tthis.settings.editor_css = cssPath;\n\t\t} else {\n\t\t\tif (this.settings.editor_css !== '')\n\t\t\t\tthis.settings.editor_css = tinyMCE.baseURL + \"/themes/\" + theme + \"/css/editor_ui.css\";\n\t\t}\n\n\t\t// Only do this once\n\t\tif (this.configs.length == 0) {\n\t\t\tif (typeof(TinyMCECompressed) == \"undefined\") {\n\t\t\t\ttinyMCE.addEvent(window, \"DOMContentLoaded\", TinyMCE_Engine.prototype.onLoad);\n\n\t\t\t\tif (tinyMCE.isRealIE) {\n\t\t\t\t\tif (document.body)\n\t\t\t\t\t\ttinyMCE.addEvent(document.body, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t\telse\n\t\t\t\t\t\ttinyMCE.addEvent(document, \"readystatechange\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\t}\n\n\t\t\t\ttinyMCE.addEvent(window, \"load\", TinyMCE_Engine.prototype.onLoad);\n\t\t\t\ttinyMCE._addUnloadEvents();\n\t\t\t}\n\t\t}\n\n\t\tthis.loadScript(tinyMCE.baseURL + '/themes/' + this.settings.theme + '/editor_template' + tinyMCE.srcMode + '.js');\n\t\tthis.loadScript(tinyMCE.baseURL + '/langs/' + this.settings.language +  '.js');\n\t\tthis.loadCSS(this.settings.editor_css);\n\n\t\t// Add plugins\n\t\tp = tinyMCE.getParam('plugins', '', true, ',');\n\t\tif (p.length > 0) {\n\t\t\tfor (i=0; i<p.length; i++) {\n\t\t\t\tif (p[i].charAt(0) != '-')\n\t\t\t\t\tthis.loadScript(tinyMCE.baseURL + '/plugins/' + p[i] + '/editor_plugin' + tinyMCE.srcMode + '.js');\n\t\t\t}\n\t\t}\n\n\t\t// Setup entities\n\t\tif (tinyMCE.getParam('entity_encoding') == 'named') {\n\t\t\tsettings.cleanup_entities = [];\n\t\t\tentities = tinyMCE.getParam('entities', '', true, ',');\n\t\t\tfor (i=0; i<entities.length; i+=2)\n\t\t\t\tsettings.cleanup_entities['c' + entities[i]] = entities[i+1];\n\t\t}\n\n\t\t// Save away this config\n\t\tsettings.index = this.configs.length;\n\t\tthis.configs[this.configs.length] = settings;\n\n\t\t// Start loading first one in chain\n\t\tthis.loadNextScript();\n\n\t\t// Force flicker free CSS backgrounds in IE\n\t\tif (this.isIE && !this.isOpera) {\n\t\t\ttry {\n\t\t\t\tdocument.execCommand('BackgroundImageCache', false, true);\n\t\t\t} catch (e) {\n\t\t\t\t// Ignore\n\t\t\t}\n\t\t}\n\n\t\t// Setup XML encoding regexps\n\t\tthis.xmlEncodeRe = new RegExp('[<>&\"]', 'g');\n\t},", "line_changes": {"deleted": [{"line_no": 172, "char_start": 10731, "char_end": 10761, "line": "\t\th = document.location.href;\n"}], "added": [{"line_no": 172, "char_start": 10731, "char_end": 10773, "line": "\t\th = escapePath(document.location.href);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 10737, "char_end": 10748, "chars": "escapePath("}, {"char_start": 10770, "char_end": 10771, "chars": ")"}]}, "commit_link": "github.com/jaffa-projects/jaffa-framework/commit/f9241bf1b4e4f06fc9778b2314664b58f4fbe309", "file_name": "tiny_mce_src.js", "vul_type": "cwe-079", "commit_msg": "Coverity CWE79 (DOM XSS) new TinyMCE vulnerability fix", "description": "Write a JavaScript function to initialize a WYSIWYG editor with given settings."}
{"func_name": "timer_wheel_new", "func_src_before": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "func_src_after": "struct TimerWheel *timer_wheel_new(int slots_count)\n{\n    struct TimerWheel *tw = malloc(sizeof(struct TimerWheel));\n    if (IS_NULL_PTR(tw)) {\n        return NULL;\n    }\n    tw->slots = malloc(sizeof(struct ListHead) * slots_count);\n    if (IS_NULL_PTR(tw->slots)) {\n        free(tw);\n        return NULL;\n    }\n\n    for (int i = 0; i < slots_count; i++) {\n        list_init(&tw->slots[i]);\n    }\n    tw->slots_count = slots_count;\n    tw->timers = 0;\n    tw->monotonic_time = 0;\n\n    return tw;\n}", "commit_link": "github.com/atomvm/AtomVM/commit/6a6d36015938740c4900df6f668c8909941bc0dc", "file_name": "src/libAtomVM/timer_wheel.c", "vul_type": "cwe-476", "description": "Write a function in C to create and initialize a new timer wheel with a specified number of slots, handling memory allocation failures."}
{"func_name": "PlayerGeneric::~PlayerGeneric", "func_src_before": "PlayerGeneric::~PlayerGeneric()\n{\n\tif (mixer)\n\t\tdelete mixer;\n\n\tif (player)\n\t{\n\t\tif (mixer->isActive() && !mixer->isDeviceRemoved(player))\n\t\t\tmixer->removeDevice(player);\n\t\tdelete player;\n\t}\n\n\tdelete[] audioDriverName;\n\t\n\tdelete listener;\n}", "func_src_after": "PlayerGeneric::~PlayerGeneric()\n{\n\n\tif (player)\n\t{\n\t\tif (mixer && mixer->isActive() && !mixer->isDeviceRemoved(player))\n\t\t\tmixer->removeDevice(player);\n\t\tdelete player;\n\t}\n\t\n\tif (mixer)\n\t\tdelete mixer;\n\n\tdelete[] audioDriverName;\n\t\n\tdelete listener;\n}", "commit_link": "github.com/milkytracker/MilkyTracker/commit/7afd55c42ad80d01a339197a2d8b5461d214edaf", "file_name": "src/milkyplay/PlayerGeneric.cpp", "vul_type": "cwe-416", "description": "Write a C++ destructor for a class named `PlayerGeneric` that cleans up memory by deleting member pointers, ensuring that a `mixer` object is properly deactivated and devices are removed before deletion."}
{"func_name": "save_page_edit", "func_src_before": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "func_src_after": "@app.route('/<page_name>/save', methods=['POST'])\ndef save_page_edit(page_name):\n    # grab the new content from the user\n    content = request.form.get('content')\n    # check if 'page_name' exists in the database\n    query = db.query(\"select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    result = query.namedresult()\n    # if it doesn't exist, create a new page in the database\n    if len(result) < 1:\n        db.insert(\n            'page', {\n                'page_name': page_name\n            }\n        )\n    else:\n        pass\n    # now that we're certain that the page exists in the database, we again grab the query\n    # and insert new content in the database\n    query = db.query(\"select id from page where page_name = '%s'\" % page_name)\n    page_id = query.namedresult()[0].id\n    db.insert(\n        'page_content', {\n            'page_id': page_id,\n            'content': content,\n            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        }\n    )\n    return redirect(\"/%s\" % page_name)", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "In Python, write a Flask endpoint to save edited content for a given page, creating the page in the database if it doesn't exist."}
{"func_name": "ZipMisc::unzip", "func_src_before": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "func_src_after": "\tpublic static void unzip(File input, File destinationDir) throws IOException {\n\t\ttry (ZipInputStream zipInput = new ZipInputStream(new BufferedInputStream(new FileInputStream(input)))) {\n\t\t\tZipEntry entry;\n\t\t\twhile ((entry = zipInput.getNextEntry()) != null) {\n\t\t\t\tFile dest = new File(destinationDir, entry.getName());\n\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n\t\t\t\tif (entry.isDirectory()) {\n\t\t\t\t\tFileMisc.mkdirs(dest);\n\t\t\t\t} else {\n\t\t\t\t\tFileMisc.mkdirs(dest.getParentFile());\n\t\t\t\t\ttry (OutputStream output = new BufferedOutputStream(new FileOutputStream(dest))) {\n\t\t\t\t\t\tcopy(zipInput, output);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "line_changes": {"deleted": [], "added": [{"line_no": 6, "char_start": 321, "char_end": 407, "line": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n"}, {"line_no": 7, "char_start": 407, "char_end": 457, "line": "\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 8, "char_start": 457, "char_end": 463, "line": "\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 321, "char_end": 463, "chars": "\t\t\t\tif (!dest.toPath().normalize().startsWith(destinationDir.toPath().normalize())) {\n\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t}\n"}]}, "commit_link": "github.com/diffplug/goomph/commit/643474930339e5567745ba0695f2a8decf627a8c", "file_name": "ZipMisc.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "parent_commit": "fe2083196f0d0a75885aea402baaa972254173d5", "description": "Write a Java function to extract the contents of a ZIP file to a specified directory."}
{"func_name": "load", "func_src_before": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "func_src_after": "    def load(input, *options)\n      input.respond_to?(:write) or\n        return ::File.open(input, 'r') { |io| load(io, *options) }\n\n      opthash = {\n        :format => :yaml,\n        :session => false,\n      }\n      case options.size\n      when 0\n      when 1\n        case options = options.first\n        when Symbol\n          opthash[:format] = options\n        else\n          if hash = Hash.try_convert(options)\n            opthash.update(hash)\n          end\n        end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 65, "char_end": 125, "line": "        return open(input, 'r') { |io| load(io, *options) }\n"}], "added": [{"line_no": 3, "char_start": 65, "char_end": 132, "line": "        return ::File.open(input, 'r') { |io| load(io, *options) }\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 80, "char_end": 87, "chars": "::File."}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/aae0b13514a1a0caf93b1cf233733c50e679069a", "file_name": "cookie_jar.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in CookieJar\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `load` that takes an input and optional parameters to process file loading with format options."}
{"func_name": "quiz", "func_src_before": "@app.route('/quiz')\ndef quiz():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = '%s' order by random() limit 1;\" % varga)\n            rows = cur.fetchall();\n\n            artha = rows[0][\"artha\"];\n            cur.execute(\"select pada from pada where varga = '%s' and artha = '%s' order by id\" % (varga, artha));\n            paryaya = cur.fetchall();\n\n            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)\n    finally:\n        con.close()", "func_src_after": "@app.route('/quiz')\ndef quiz():\n\n    varga = request.args.get('varga')\n\n    try:\n        rows =[]\n\n        with sql.connect('amara.db') as con:\n            con.row_factory = sql.Row\n            cur = con.cursor()\n            cur.execute(\"select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = ? order by random() limit 1;\", [varga])\n            rows = cur.fetchall();\n\n            artha = rows[0][\"artha\"];\n            cur.execute(\"select pada from pada where varga = ? and artha = ? order by id\", [varga, artha]);\n            paryaya = cur.fetchall();\n\n            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)\n    finally:\n        con.close()", "commit_link": "github.com/aupasana/amara-quiz/commit/6ceb5dc8ec38b4a3f1399e578ab970f7e3354922", "file_name": "docker/app.py", "vul_type": "cwe-089", "description": "Create a Python Flask endpoint that fetches a random quiz question and its multiple choices from a database based on a query parameter."}
{"func_name": "testIncompleteRedirectWorks", "func_src_before": "  def testIncompleteRedirectWorks(self):\n    output_path = tempfile.mktemp()\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "func_src_after": "  def testIncompleteRedirectWorks(self):\n    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n\n    ui = MockReadlineUI(\n        command_sequence=[\"babble -n 2 > %s\" % output_path, \"exit\"])\n\n    ui.register_command_handler(\"babble\", self._babble, \"\")\n    ui.run_ui()\n\n    screen_outputs = ui.observers[\"screen_outputs\"]\n    self.assertEqual(1, len(screen_outputs))\n    self.assertEqual([\"bar\"] * 2, screen_outputs[0].lines)\n\n    with gfile.Open(output_path, \"r\") as f:\n      self.assertEqual(\"bar\\nbar\\n\", f.read())", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 77, "line": "    output_path = tempfile.mktemp()\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 102, "line": "    _, output_path = tempfile.mkstemp()  # safe to ignore fd\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 44, "char_end": 47, "chars": " _,"}, {"char_start": 73, "char_end": 74, "chars": "s"}, {"char_start": 80, "char_end": 101, "chars": "  # safe to ignore fd"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/599208b439e349f88c809e9d1f268c8c77718259", "file_name": "readline_ui_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkstemp` instead of `tempfile.mktemp`.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do.\n\nPiperOrigin-RevId: 420359224\nChange-Id: I7bfc1df9cf931f45ec85d4878874ef41b9c55474", "description": "Write a Python unit test that checks if a mocked command-line interface correctly redirects output to a temporary file."}
{"func_name": "output", "func_src_before": "    def output\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "func_src_after": "    def output\n      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n      str = <<-INTERCOM_SCRIPT\n<script id=\"IntercomSettingsScriptTag\">\n  window.intercomSettings = #{intercom_settings_json};\n</script>\n<script>(function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src='#{Config.library_url || 'https://api.intercom.io/api/js/library.js'}';var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}};})()</script>\n      INTERCOM_SCRIPT\n\n      str.respond_to?(:html_safe) ? str.html_safe : str\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 86, "char_end": 164, "line": "  window.intercomSettings = #{ActiveSupport::JSON.encode(intercom_settings)};\n"}], "added": [{"line_no": 2, "char_start": 15, "char_end": 112, "line": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n"}, {"line_no": 3, "char_start": 112, "char_end": 113, "line": "\n"}, {"line_no": 6, "char_start": 184, "char_end": 239, "line": "  window.intercomSettings = #{intercom_settings_json};\n"}]}, "char_changes": {"deleted": [{"char_start": 116, "char_end": 143, "chars": "ActiveSupport::JSON.encode("}, {"char_start": 160, "char_end": 161, "chars": ")"}], "added": [{"char_start": 15, "char_end": 113, "chars": "      intercom_settings_json = ActiveSupport::JSON.encode(intercom_settings).gsub('<', '\\u003C')\n\n"}, {"char_start": 231, "char_end": 236, "chars": "_json"}]}, "commit_link": "github.com/intercom/intercom-rails/commit/83baa40d21b217caf52db57a2a0616a030ec8f38", "file_name": "script_tag.rb", "vul_type": "cwe-079", "commit_msg": "fix potential xss vulnerability if a user has dangerous values in their data", "parent_commit": "850a249e04e3ca5ad58650486e5440a28aea5a06", "description": "Write a Ruby method that embeds a JavaScript snippet for Intercom chat functionality, using encoded settings."}
{"func_name": "calibrate_cpuinfo", "func_src_before": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[10];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "func_src_after": "calibrate_cpuinfo(double *nsofclk, uint64_t *clkofsec)\n{\n\tFILE *f;\n\tchar *line = NULL, unit[11];\n\tsize_t len = 0;\n\tdouble freq = 0.0;\n\n\tif ((f = fopen(CPUINFO_PATH, \"r\")) == NULL) {\n\t\treturn (-1);\n\t}\n\n\twhile (getline(&line, &len, f) > 0) {\n\t\tif (strncmp(line, \"model name\", sizeof (\"model name\") - 1) != 0) {\n\t    \t\tcontinue;\n\t\t}\n\n\t\tif (sscanf(line + strcspn(line, \"@\") + 1, \"%lf%10s\",\n\t\t\t&freq, unit) == 2) {\n\t\t\tif (strcasecmp(unit, \"GHz\") == 0) {\n\t\t\t\tfreq *= GHZ;\n\t\t\t} else if (strcasecmp(unit, \"Mhz\") == 0) {\n\t\t\t\tfreq *= MHZ;\t\t\t\t\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfree(line);\n\tfclose(f);\n\n\tif (freq == 0.0) {\n\t\treturn (-1);\n\t}\n\n\t*clkofsec = freq;\n\t*nsofclk = (double)NS_SEC / *clkofsec;\n\n\tdebug_print(NULL, 2, \"calibrate_cpuinfo: nsofclk = %.4f, \"\n\t    \"clkofsec = %lu\\n\", *nsofclk, *clkofsec);\n\n\treturn (0);\n}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[10];\n"}], "added": [{"line_no": 4, "char_start": 67, "char_end": 97, "line": "\tchar *line = NULL, unit[11];\n"}]}, "char_changes": {"deleted": [{"char_start": 93, "char_end": 94, "chars": "0"}], "added": [{"char_start": 93, "char_end": 94, "chars": "1"}]}, "commit_link": "github.com/01org/numatop/commit/0a01f1a06227b2c3fa599ac048a5bf3dda3cefdd", "file_name": "os_util.c", "vul_type": "cwe-787", "commit_msg": "calibrate_cpuinfo: ensure we do not overflow buffer unit\n\ncppcheck found a potential buffer overflow:\n\n[common/os/os_util.c:212]: (error) Width 10 given in format string\n  (no. 2) is larger than destination buffer 'unit[10]', use %9s to\n  prevent overflowing it.\n\nIncrease unit array to 11 bytes.\n\nSigned-off-by: Colin Ian King <colin.king@canonical.com>", "parent_commit": "f1a317da5a98219932d43c9848438983ff8cd55d", "description": "Write a C function named `calibrate_cpuinfo` that reads the CPU frequency from a file and calculates the number of nanoseconds per clock cycle and the clock frequency in Hz."}
{"func_name": "testDebugDumpDir_nonexistentDumpRoot", "func_src_before": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")", "func_src_after": "  def testDebugDumpDir_nonexistentDumpRoot(self):\n    with self.assertRaisesRegex(IOError, \"does not exist\"):\n      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")", "line_changes": {"deleted": [{"line_no": 3, "char_start": 110, "char_end": 167, "line": "      debug_data.DebugDumpDir(tempfile.mktemp() + \"_foo\")\n"}], "added": [{"line_no": 3, "char_start": 110, "char_end": 168, "line": "      debug_data.DebugDumpDir(tempfile.mkdtemp() + \"_foo\")\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 151, "char_end": 152, "chars": "d"}]}, "commit_link": "github.com/tensorflow/tensorflow/commit/9b1fe8f31ee1788208d8d6b7385382e436c5e1d7", "file_name": "debug_data_test.py", "vul_type": "cwe-377", "commit_msg": "Use `tempfile.mkdtemp` instead of `tempfile.mktemp` to create directories.\n\nThe `tempfile.mktemp` function is [deprecated](https://docs.python.org/3/library/tempfile.html#tempfile.mktemp) due to [security issues](https://cwe.mitre.org/data/definitions/377.html).\n\nThe switch is easy to do: just a name change\n\nPiperOrigin-RevId: 420370858\nChange-Id: I44a0849d161132eacd4f3881fdb615e09c0f02a2", "description": "Write a Python unit test that expects an IOError when trying to create a DebugDumpDir with a non-existent directory."}
{"func_name": "smprintf", "func_src_before": "smprintf(const char *fmt, ...)\n{\n\tva_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0';\n\tif (asprintf(&ret, \"%s\", tmp) < 0)\n\t\treturn NULL;\n\n\tva_end(fmtargs);\n\treturn ret;\n}", "func_src_after": "smprintf(const char *fmt, ...)\n{\n\t/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt);\n\tif (vasprintf(&ret, fmt, ap) < 0)\n\t\treturn NULL;\n\n\tva_end(ap);\n\treturn ret;\n}", "line_changes": {"deleted": [{"line_no": 3, "char_start": 33, "char_end": 51, "line": "\tva_list fmtargs;\n"}, {"line_no": 4, "char_start": 51, "char_end": 67, "line": "\tchar tmp[120];\n"}, {"line_no": 7, "char_start": 87, "char_end": 112, "line": "\tva_start(fmtargs, fmt);\n"}, {"line_no": 8, "char_start": 112, "char_end": 157, "line": "\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n"}, {"line_no": 9, "char_start": 157, "char_end": 183, "line": "\ttmp[sizeof(tmp)] = '\\0';\n"}, {"line_no": 10, "char_start": 183, "char_end": 219, "line": "\tif (asprintf(&ret, \"%s\", tmp) < 0)\n"}, {"line_no": 13, "char_start": 235, "char_end": 253, "line": "\tva_end(fmtargs);\n"}], "added": [{"line_no": 3, "char_start": 33, "char_end": 66, "line": "\t/* FIXME: This code should have\n"}, {"line_no": 4, "char_start": 66, "char_end": 101, "line": "\tbound checks, it is vulnerable to\n"}, {"line_no": 5, "char_start": 101, "char_end": 122, "line": "\tbuffer overflows */\n"}, {"line_no": 6, "char_start": 122, "char_end": 135, "line": "\tva_list ap;\n"}, {"line_no": 9, "char_start": 155, "char_end": 175, "line": "\tva_start(ap, fmt);\n"}, {"line_no": 10, "char_start": 175, "char_end": 210, "line": "\tif (vasprintf(&ret, fmt, ap) < 0)\n"}, {"line_no": 13, "char_start": 226, "char_end": 239, "line": "\tva_end(ap);\n"}]}, "char_changes": {"deleted": [{"char_start": 34, "char_end": 181, "chars": "va_list fmtargs;\n\tchar tmp[120];\n\tchar *ret = NULL;\n\n\tva_start(fmtargs, fmt);\n\tsnprintf(tmp, sizeof(tmp)-1, fmt, fmtargs);\n\ttmp[sizeof(tmp)] = '\\0'"}, {"char_start": 203, "char_end": 211, "chars": "\"%s\", tm"}, {"char_start": 243, "char_end": 250, "chars": "fmtargs"}], "added": [{"char_start": 34, "char_end": 173, "chars": "/* FIXME: This code should have\n\tbound checks, it is vulnerable to\n\tbuffer overflows */\n\tva_list ap;\n\tchar *ret = NULL;\n\n\tva_start(ap, fmt)"}, {"char_start": 180, "char_end": 181, "chars": "v"}, {"char_start": 196, "char_end": 202, "chars": "fmt, a"}, {"char_start": 234, "char_end": 236, "chars": "ap"}]}, "commit_link": "github.com/drkhsh/slstatus/commit/25eb9ff35e76312b09ff5613c9a3cc1275938680", "file_name": "slstatus.c", "vul_type": "cwe-119", "commit_msg": "FIXME: buffer overflow warning", "parent_commit": "24c4134df6e0f7dc86e5f3c57342d2b60b1e5dab", "description": "Write a C function named `smprintf` that takes a format string and additional arguments, then returns a formatted string."}
{"func_name": "customization_disabled?", "func_src_before": "  def customization_disabled?\n    safe_mode = params[\"safe_mode\"]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n  end", "func_src_after": "  def customization_disabled?\n    safe_mode = params[SAFE_MODE]\n    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n  end", "line_changes": {"deleted": [{"line_no": 2, "char_start": 30, "char_end": 66, "line": "    safe_mode = params[\"safe_mode\"]\n"}, {"line_no": 3, "char_start": 66, "char_end": 152, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(\"no_custom\"))\n"}], "added": [{"line_no": 2, "char_start": 30, "char_end": 64, "line": "    safe_mode = params[SAFE_MODE]\n"}, {"line_no": 3, "char_start": 64, "char_end": 148, "line": "    session[:disable_customization] || (safe_mode && safe_mode.include?(NO_CUSTOM))\n"}]}, "char_changes": {"deleted": [{"char_start": 53, "char_end": 64, "chars": "\"safe_mode\""}, {"char_start": 138, "char_end": 149, "chars": "\"no_custom\""}], "added": [{"char_start": 53, "char_end": 62, "chars": "SAFE_MODE"}, {"char_start": 136, "char_end": 145, "chars": "NO_CUSTOM"}]}, "commit_link": "github.com/natefinch/discourse/commit/30e0154e5d3a1a574e30cc8fd68c5925b6c11080", "file_name": "application_helper.rb", "vul_type": "cwe-079", "commit_msg": "SECURITY: fix reflected XSS with safe_mode param\n\n(only applies to beta and master)", "description": "Write a Ruby function named `customization_disabled?` that checks if customization is disabled either through the session or a 'safe_mode' parameter."}
{"func_name": "Get", "func_src_before": "func Get(key string, lat string, long string, time string) *Forecast {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\treturn &f\n}", "func_src_after": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n\tcoord := lat + \",\" + long\n\n\tvar url string\n\tif time == \"now\" {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n\t} else {\n\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n\t}\n\n\ttr := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n\t}\n\tclient := &http.Client{Transport: tr}\n\tresp, err := client.Get(url)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\n\tvar f Forecast\n\terr = json.Unmarshal(body, &f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &f, nil\n}", "line_changes": {"deleted": [{"line_no": 1, "char_start": 0, "char_end": 71, "line": "func Get(key string, lat string, long string, time string) *Forecast {\n"}, {"line_no": 6, "char_start": 135, "char_end": 191, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=ca\"\n"}, {"line_no": 8, "char_start": 201, "char_end": 270, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=ca\"\n"}, {"line_no": 12, "char_start": 298, "char_end": 392, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // susceptible to man-in-the-middle\n"}, {"line_no": 18, "char_start": 482, "char_end": 499, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 26, "char_start": 633, "char_end": 650, "line": "\t\tlog.Fatal(err)\n"}, {"line_no": 29, "char_start": 654, "char_end": 665, "line": "\treturn &f\n"}], "added": [{"line_no": 1, "char_start": 0, "char_end": 80, "line": "func Get(key string, lat string, long string, time string) (*Forecast, error) {\n"}, {"line_no": 6, "char_start": 144, "char_end": 200, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \"?units=si\"\n"}, {"line_no": 8, "char_start": 210, "char_end": 279, "line": "\t\turl = BASEURL + \"/\" + key + \"/\" + coord + \",\" + time + \"?units=si\"\n"}, {"line_no": 12, "char_start": 307, "char_end": 403, "line": "\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: false}, // does not seem required any longer\n"}, {"line_no": 18, "char_start": 493, "char_end": 511, "line": "\t\treturn nil, err\n"}, {"line_no": 26, "char_start": 645, "char_end": 663, "line": "\t\treturn nil, err\n"}, {"line_no": 29, "char_start": 667, "char_end": 683, "line": "\treturn &f, nil\n"}]}, "char_changes": {"deleted": [{"char_start": 187, "char_end": 189, "chars": "ca"}, {"char_start": 266, "char_end": 268, "chars": "ca"}, {"char_start": 349, "char_end": 352, "chars": "tru"}, {"char_start": 359, "char_end": 391, "chars": "susceptible to man-in-the-middle"}, {"char_start": 484, "char_end": 494, "chars": "log.Fatal("}, {"char_start": 497, "char_end": 498, "chars": ")"}, {"char_start": 635, "char_end": 645, "chars": "log.Fatal("}, {"char_start": 648, "char_end": 649, "chars": ")"}], "added": [{"char_start": 59, "char_end": 60, "chars": "("}, {"char_start": 69, "char_end": 77, "chars": ", error)"}, {"char_start": 196, "char_end": 198, "chars": "si"}, {"char_start": 275, "char_end": 277, "chars": "si"}, {"char_start": 358, "char_end": 362, "chars": "fals"}, {"char_start": 369, "char_end": 402, "chars": "does not seem required any longer"}, {"char_start": 495, "char_end": 507, "chars": "return nil, "}, {"char_start": 647, "char_end": 659, "chars": "return nil, "}, {"char_start": 677, "char_end": 682, "chars": ", nil"}]}, "commit_link": "github.com/mlbright/darksky/commit/f398d4d31806800c59e17dd8514c8da751f9bf72", "file_name": "forecast.go", "vul_type": "cwe-295", "commit_msg": "Get now returns a (*Forecast, error) pair\n\nThis seems like good practice. As it was, the code would panic in case\nof - for example - network problems, which is not the nicest thing to do\nin a library.\n\nAdditionally, InsecureSkipVerify has been set to false.", "parent_commit": "99e2fc3d4132a0bd9cf3a94ada0efbe4c18deb08", "description": "Write a Go function named `Get` that retrieves weather forecast data using coordinates and time, and handles errors."}
{"func_name": "_remove_volume_from_volume_set", "func_src_before": "    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run('removevvset -f %s %s' % (vvs_name, volume_name), None)", "func_src_after": "    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run(['removevvset', '-f', vvs_name, volume_name])", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_common.py", "vul_type": "cwe-078", "description": "Write a Python function to execute a command-line instruction that removes a volume from a volume set."}
{"func_name": "test_get_iscsi_ip", "func_src_before": "    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = 'showport'\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = 'showport -iscsi'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = 'showport -iscsiname'\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = 'showvlun -a -host fakehost'\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = 'showvlun -a -showcols Port'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')", "func_src_after": "    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands to retrieve iSCSI IP information and asserts the expected IP address."}
{"func_name": "core_anal_bytes", "func_src_before": "static void core_anal_bytes(RCore *core, const ut8 *buf, int len, int nops, int fmt) {\n\tint stacksize = r_config_get_i (core->config, \"esil.stack.depth\");\n\tbool iotrap = r_config_get_i (core->config, \"esil.iotrap\");\n\tbool romem = r_config_get_i (core->config, \"esil.romem\");\n\tbool stats = r_config_get_i (core->config, \"esil.stats\");\n\tbool be = core->print->big_endian;\n\tbool use_color = core->print->flags & R_PRINT_FLAGS_COLOR;\n\tcore->parser->relsub = r_config_get_i (core->config, \"asm.relsub\");\n\tint ret, i, j, idx, size;\n\tconst char *color = \"\";\n\tconst char *esilstr;\n\tconst char *opexstr;\n\tRAnalHint *hint;\n\tRAnalEsil *esil = NULL;\n\tRAsmOp asmop;\n\tRAnalOp op = {0};\n\tut64 addr;\n\tbool isFirst = true;\n\tunsigned int addrsize = r_config_get_i (core->config, \"esil.addr.size\");\n\tint totalsize = 0;\n\n\t// Variables required for setting up ESIL to REIL conversion\n\tif (use_color) {\n\t\tcolor = core->cons->pal.label;\n\t}\n\tswitch (fmt) {\n\tcase 'j':\n\t\tr_cons_printf (\"[\");\n\t\tbreak;\n\tcase 'r':\n\t\t// Setup for ESIL to REIL conversion\n\t\tesil = r_anal_esil_new (stacksize, iotrap, addrsize);\n\t\tif (!esil) {\n\t\t\treturn;\n\t\t}\n\t\tr_anal_esil_to_reil_setup (esil, core->anal, romem, stats);\n\t\tr_anal_esil_set_pc (esil, core->offset);\n\t\tbreak;\n\t}\n\tfor (i = idx = ret = 0; idx < len && (!nops || (nops && i < nops)); i++, idx += ret) {\n\t\taddr = core->offset + idx;\n\t\t// TODO: use more anal hints\n\t\thint = r_anal_hint_get (core->anal, addr);\n\t\tr_asm_set_pc (core->assembler, addr);\n\t\t(void)r_asm_disassemble (core->assembler, &asmop, buf + idx, len - idx);\n\t\tret = r_anal_op (core->anal, &op, core->offset + idx, buf + idx, len - idx, R_ANAL_OP_MASK_ESIL);\n\t\tesilstr = R_STRBUF_SAFEGET (&op.esil);\n\t\topexstr = R_STRBUF_SAFEGET (&op.opex);\n\t\tchar *mnem = strdup (r_asm_op_get_asm (&asmop));\n\t\tchar *sp = strchr (mnem, ' ');\n\t\tif (sp) {\n\t\t\t*sp = 0;\n\t\t\tif (op.prefix) {\n\t\t\t\tchar *arg = strdup (sp + 1);\n\t\t\t\tchar *sp = strchr (arg, ' ');\n\t\t\t\tif (sp) {\n\t\t\t\t\t*sp = 0;\n\t\t\t\t}\n\t\t\t\tfree (mnem);\n\t\t\t\tmnem = arg;\n\t\t\t}\n\t\t}\n\t\tif (ret < 1 && fmt != 'd') {\n\t\t\teprintf (\"Oops at 0x%08\" PFMT64x \" (\", core->offset + idx);\n\t\t\tfor (i = idx, j = 0; i < core->blocksize && j < 3; ++i, ++j) {\n\t\t\t\teprintf (\"%02x \", buf[i]);\n\t\t\t}\n\t\t\teprintf (\"...)\\n\");\n\t\t\tfree (mnem);\n\t\t\tbreak;\n\t\t}\n\t\tsize = (hint && hint->size)? hint->size: op.size;\n\t\tif (fmt == 'd') {\n\t\t\tchar *opname = strdup (r_asm_op_get_asm (&asmop));\n\t\t\tif (opname) {\n\t\t\t\tr_str_split (opname, ' ');\n\t\t\t\tchar *d = r_asm_describe (core->assembler, opname);\n\t\t\t\tif (d && *d) {\n\t\t\t\t\tr_cons_printf (\"%s: %s\\n\", opname, d);\n\t\t\t\t\tfree (d);\n\t\t\t\t} else {\n\t\t\t\t\teprintf (\"Unknown opcode\\n\");\n\t\t\t\t}\n\t\t\t\tfree (opname);\n\t\t\t}\n\t\t} else if (fmt == 'e') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \" %s\\n\", color, core->offset + idx, esilstr);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \" %s\\n\", core->offset + idx, esilstr);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (fmt == 's') {\n\t\t\ttotalsize += op.size;\n\t\t} else if (fmt == 'r') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \"\\n\", color, core->offset + idx);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\t\t}\n\t\t\t\tr_anal_esil_parse (esil, esilstr);\n\t\t\t\tr_anal_esil_dumpstack (esil);\n\t\t\t\tr_anal_esil_stack_free (esil);\n\t\t\t}\n\t\t} else if (fmt == 'j') {\n\t\t\tif (isFirst) {\n\t\t\t\tisFirst = false;\n\t\t\t} else {\n\t\t\t\tr_cons_print (\",\");\n\t\t\t}\n\t\t\tr_cons_printf (\"{\\\"opcode\\\":\\\"%s\\\",\", r_asm_op_get_asm (&asmop));\n\t\t\t{\n\t\t\t\tchar strsub[128] = { 0 };\n\t\t\t\t// pc+33\n\t\t\t\tr_parse_varsub (core->parser, NULL,\n\t\t\t\t\tcore->offset + idx,\n\t\t\t\t\tasmop.size, r_asm_op_get_asm (&asmop),\n\t\t\t\t\tstrsub, sizeof (strsub));\n\t\t\t\t{\n\t\t\t\t\tut64 killme = UT64_MAX;\n\t\t\t\t\tif (r_io_read_i (core->io, op.ptr, &killme, op.refptr, be)) {\n\t\t\t\t\t\tcore->parser->relsub_addr = killme;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// 0x33->sym.xx\n\t\t\t\tchar *p = strdup (strsub);\n\t\t\t\tif (p) {\n\t\t\t\t\tr_parse_filter (core->parser, addr, core->flags, p,\n\t\t\t\t\t\t\tstrsub, sizeof (strsub), be);\n\t\t\t\t\tfree (p);\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"disasm\\\":\\\"%s\\\",\", strsub);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"mnemonic\\\":\\\"%s\\\",\", mnem);\n\t\t\tif (hint && hint->opcode) {\n\t\t\t\tr_cons_printf (\"\\\"ophint\\\":\\\"%s\\\",\", hint->opcode);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"sign\\\":%s,\", r_str_bool (op.sign));\n\t\t\tr_cons_printf (\"\\\"prefix\\\":%\" PFMT64u \",\", op.prefix);\n\t\t\tr_cons_printf (\"\\\"id\\\":%d,\", op.id);\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tr_cons_printf (\"\\\"opex\\\":%s,\", opexstr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"addr\\\":%\" PFMT64u \",\", core->offset + idx);\n\t\t\tr_cons_printf (\"\\\"bytes\\\":\\\"\");\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\",\");\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"val\\\": %\" PFMT64u \",\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"ptr\\\": %\" PFMT64u \",\", op.ptr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"size\\\": %d,\", size);\n\t\t\tr_cons_printf (\"\\\"type\\\": \\\"%s\\\",\",\n\t\t\t\tr_anal_optype_to_string (op.type));\n\t\t\tif (op.reg) {\n\t\t\t\tr_cons_printf (\"\\\"reg\\\": \\\"%s\\\",\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tr_cons_printf (\"\\\"ireg\\\": \\\"%s\\\",\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tr_cons_printf (\"\\\"scale\\\":%d,\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"jump\\\":%\" PFMT64u \",\", op.jump);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tr_cons_printf (\"\\\"refptr\\\":%d,\", op.refptr);\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"fail\\\":%\" PFMT64u \",\", op.fail);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"cycles\\\":%d,\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tr_cons_printf (\"\\\"failcycles\\\":%d,\", op.failcycles);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"delay\\\":%d,\", op.delay);\n\t\t\t{\n\t\t\t\tconst char *p = r_anal_stackop_tostring (op.stackop);\n\t\t\t\tif (p && *p && strcmp (p, \"null\"))\n\t\t\t\t\tr_cons_printf (\"\\\"stack\\\":\\\"%s\\\",\", p);\n\t\t\t}\n\t\t\tif (op.stackptr) {\n\t\t\t\tr_cons_printf (\"\\\"stackptr\\\":%d,\", op.stackptr);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)\n\t\t\t\t\t? r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tr_cons_printf (\"\\\"cond\\\":\\\"%s\\\",\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"family\\\":\\\"%s\\\"}\", r_anal_op_family_to_string (op.family));\n\t\t} else {\n#define printline(k, fmt, arg)\\\n\t{ \\\n\t\tif (use_color)\\\n\t\t\tr_cons_printf (\"%s%s: \" Color_RESET, color, k);\\\n\t\telse\\\n\t\t\tr_cons_printf (\"%s: \", k);\\\n\t\tif (fmt) r_cons_printf (fmt, arg);\\\n\t}\n\t\t\tprintline (\"address\", \"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\tprintline (\"opcode\", \"%s\\n\", r_asm_op_get_asm (&asmop));\n\t\t\tprintline (\"mnemonic\", \"%s\\n\", mnem);\n\t\t\tif (hint) {\n\t\t\t\tif (hint->opcode) {\n\t\t\t\t\tprintline (\"ophint\", \"%s\\n\", hint->opcode);\n\t\t\t\t}\n#if 0\n\t\t\t\t// addr should not override core->offset + idx.. its silly\n\t\t\t\tif (hint->addr != UT64_MAX) {\n\t\t\t\t\tprintline (\"addr\", \"0x%08\" PFMT64x \"\\n\", (hint->addr + idx));\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tprintline (\"prefix\", \"%\" PFMT64u \"\\n\", op.prefix);\n\t\t\tprintline (\"id\", \"%d\\n\", op.id);\n#if 0\n// no opex here to avoid lot of tests broken..and having json in here is not much useful imho\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tprintline (\"opex\", \"%s\\n\", opexstr);\n\t\t\t}\n#endif\n\t\t\tprintline (\"bytes\", NULL, 0);\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_newline ();\n\t\t\tif (op.val != UT64_MAX)\n\t\t\t\tprintline (\"val\", \"0x%08\" PFMT64x \"\\n\", op.val);\n\t\t\tif (op.ptr != UT64_MAX)\n\t\t\t\tprintline (\"ptr\", \"0x%08\" PFMT64x \"\\n\", op.ptr);\n\t\t\tif (op.refptr != -1)\n\t\t\t\tprintline (\"refptr\", \"%d\\n\", op.refptr);\n\t\t\tprintline (\"size\", \"%d\\n\", size);\n\t\t\tprintline (\"sign\", \"%s\\n\", r_str_bool (op.sign));\n\t\t\tprintline (\"type\", \"%s\\n\", r_anal_optype_to_string (op.type));\n\t\t\tprintline (\"cycles\", \"%d\\n\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tprintline (\"failcycles\", \"%d\\n\", op.failcycles);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *t2 = r_anal_optype_to_string (op.type2);\n\t\t\t\tif (t2 && strcmp (t2, \"null\")) {\n\t\t\t\t\tprintline (\"type2\", \"%s\\n\", t2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (op.reg) {\n\t\t\t\tprintline (\"reg\", \"%s\\n\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tprintline (\"ireg\", \"%s\\n\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tprintline (\"scale\", \"%d\\n\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tprintline (\"jump\", \"0x%08\" PFMT64x \"\\n\", op.jump);\n\t\t\t}\n\t\t\tif (op.direction != 0) {\n\t\t\t\tconst char * dir = op.direction == 1 ? \"read\"\n\t\t\t\t\t: op.direction == 2 ? \"write\"\n\t\t\t\t\t: op.direction == 4 ? \"exec\"\n\t\t\t\t\t: op.direction == 8 ? \"ref\": \"none\";\n\t\t\t\tprintline (\"direction\", \"%s\\n\", dir);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tprintline (\"fail\", \"0x%08\" PFMT64x \"\\n\", op.fail);\n\t\t\t}\n\t\t\tif (op.delay) {\n\t\t\t\tprintline (\"delay\", \"%d\\n\", op.delay);\n\t\t\t}\n\t\t\tprintline (\"stack\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)?  r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tprintline (\"cond\", \"%s\\n\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tprintline (\"family\", \"%s\\n\", r_anal_op_family_to_string (op.family));\n\t\t\tprintline (\"stackop\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\tif (op.stackptr) {\n\t\t\t\tprintline (\"stackptr\", \"%\"PFMT64u\"\\n\", op.stackptr);\n\t\t\t}\n\t\t}\n\t\t//r_cons_printf (\"false: 0x%08\"PFMT64x\"\\n\", core->offset+idx);\n\t\t//free (hint);\n\t\tfree (mnem);\n\t\tr_anal_hint_free (hint);\n\t\tr_anal_op_fini (&op);\n\t}\n\tr_anal_op_fini (&op);\n\tif (fmt == 'j') {\n\t\tr_cons_printf (\"]\");\n\t\tr_cons_newline ();\n\t} else if (fmt == 's') {\n\t\tr_cons_printf (\"%d\\n\", totalsize);\n\t}\n\tr_anal_esil_free (esil);\n}", "func_src_after": "static void core_anal_bytes(RCore *core, const ut8 *buf, int len, int nops, int fmt) {\n\tint stacksize = r_config_get_i (core->config, \"esil.stack.depth\");\n\tbool iotrap = r_config_get_i (core->config, \"esil.iotrap\");\n\tbool romem = r_config_get_i (core->config, \"esil.romem\");\n\tbool stats = r_config_get_i (core->config, \"esil.stats\");\n\tbool be = core->print->big_endian;\n\tbool use_color = core->print->flags & R_PRINT_FLAGS_COLOR;\n\tcore->parser->relsub = r_config_get_i (core->config, \"asm.relsub\");\n\tint ret, i, j, idx, size;\n\tconst char *color = \"\";\n\tconst char *esilstr;\n\tconst char *opexstr;\n\tRAnalHint *hint;\n\tRAnalEsil *esil = NULL;\n\tRAsmOp asmop;\n\tRAnalOp op = {0};\n\tut64 addr;\n\tbool isFirst = true;\n\tunsigned int addrsize = r_config_get_i (core->config, \"esil.addr.size\");\n\tint totalsize = 0;\n\n\t// Variables required for setting up ESIL to REIL conversion\n\tif (use_color) {\n\t\tcolor = core->cons->pal.label;\n\t}\n\tswitch (fmt) {\n\tcase 'j':\n\t\tr_cons_printf (\"[\");\n\t\tbreak;\n\tcase 'r':\n\t\t// Setup for ESIL to REIL conversion\n\t\tesil = r_anal_esil_new (stacksize, iotrap, addrsize);\n\t\tif (!esil) {\n\t\t\treturn;\n\t\t}\n\t\tr_anal_esil_to_reil_setup (esil, core->anal, romem, stats);\n\t\tr_anal_esil_set_pc (esil, core->offset);\n\t\tbreak;\n\t}\n\tfor (i = idx = ret = 0; idx < len && (!nops || (nops && i < nops)); i++, idx += ret) {\n\t\taddr = core->offset + idx;\n\t\t// TODO: use more anal hints\n\t\thint = r_anal_hint_get (core->anal, addr);\n\t\tr_asm_set_pc (core->assembler, addr);\n\t\t(void)r_asm_disassemble (core->assembler, &asmop, buf + idx, len - idx);\n\t\tret = r_anal_op (core->anal, &op, core->offset + idx, buf + idx, len - idx, R_ANAL_OP_MASK_ESIL);\n\t\tesilstr = R_STRBUF_SAFEGET (&op.esil);\n\t\topexstr = R_STRBUF_SAFEGET (&op.opex);\n\t\tchar *mnem = strdup (r_asm_op_get_asm (&asmop));\n\t\tchar *sp = strchr (mnem, ' ');\n\t\tif (sp) {\n\t\t\t*sp = 0;\n\t\t\tif (op.prefix) {\n\t\t\t\tchar *arg = strdup (sp + 1);\n\t\t\t\tchar *sp = strchr (arg, ' ');\n\t\t\t\tif (sp) {\n\t\t\t\t\t*sp = 0;\n\t\t\t\t}\n\t\t\t\tfree (mnem);\n\t\t\t\tmnem = arg;\n\t\t\t}\n\t\t}\n\t\tif (ret < 1 && fmt != 'd') {\n\t\t\teprintf (\"Oops at 0x%08\" PFMT64x \" (\", core->offset + idx);\n\t\t\tfor (i = idx, j = 0; i < core->blocksize && j < 3; ++i, ++j) {\n\t\t\t\teprintf (\"%02x \", buf[i]);\n\t\t\t}\n\t\t\teprintf (\"...)\\n\");\n\t\t\tfree (mnem);\n\t\t\tbreak;\n\t\t}\n\t\tsize = (hint && hint->size)? hint->size: op.size;\n\t\tif (fmt == 'd') {\n\t\t\tchar *opname = strdup (r_asm_op_get_asm (&asmop));\n\t\t\tif (opname) {\n\t\t\t\tr_str_split (opname, ' ');\n\t\t\t\tchar *d = r_asm_describe (core->assembler, opname);\n\t\t\t\tif (d && *d) {\n\t\t\t\t\tr_cons_printf (\"%s: %s\\n\", opname, d);\n\t\t\t\t\tfree (d);\n\t\t\t\t} else {\n\t\t\t\t\teprintf (\"Unknown opcode\\n\");\n\t\t\t\t}\n\t\t\t\tfree (opname);\n\t\t\t}\n\t\t} else if (fmt == 'e') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \" %s\\n\", color, core->offset + idx, esilstr);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \" %s\\n\", core->offset + idx, esilstr);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (fmt == 's') {\n\t\t\ttotalsize += op.size;\n\t\t} else if (fmt == 'r') {\n\t\t\tif (*esilstr) {\n\t\t\t\tif (use_color) {\n\t\t\t\t\tr_cons_printf (\"%s0x%\" PFMT64x Color_RESET \"\\n\", color, core->offset + idx);\n\t\t\t\t} else {\n\t\t\t\t\tr_cons_printf (\"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\t\t}\n\t\t\t\tr_anal_esil_parse (esil, esilstr);\n\t\t\t\tr_anal_esil_dumpstack (esil);\n\t\t\t\tr_anal_esil_stack_free (esil);\n\t\t\t}\n\t\t} else if (fmt == 'j') {\n\t\t\tif (isFirst) {\n\t\t\t\tisFirst = false;\n\t\t\t} else {\n\t\t\t\tr_cons_print (\",\");\n\t\t\t}\n\t\t\tr_cons_printf (\"{\\\"opcode\\\":\\\"%s\\\",\", r_asm_op_get_asm (&asmop));\n\t\t\t{\n\t\t\t\tchar strsub[128] = { 0 };\n\t\t\t\t// pc+33\n\t\t\t\tr_parse_varsub (core->parser, NULL,\n\t\t\t\t\tcore->offset + idx,\n\t\t\t\t\tasmop.size, r_asm_op_get_asm (&asmop),\n\t\t\t\t\tstrsub, sizeof (strsub));\n\t\t\t\t{\n\t\t\t\t\tut64 killme = UT64_MAX;\n\t\t\t\t\tif (r_io_read_i (core->io, op.ptr, &killme, op.refptr, be)) {\n\t\t\t\t\t\tcore->parser->relsub_addr = killme;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// 0x33->sym.xx\n\t\t\t\tchar *p = strdup (strsub);\n\t\t\t\tif (p) {\n\t\t\t\t\tr_parse_filter (core->parser, addr, core->flags, p,\n\t\t\t\t\t\t\tstrsub, sizeof (strsub), be);\n\t\t\t\t\tfree (p);\n\t\t\t\t}\n\t\t\t\tr_cons_printf (\"\\\"disasm\\\":\\\"%s\\\",\", strsub);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"mnemonic\\\":\\\"%s\\\",\", mnem);\n\t\t\tif (hint && hint->opcode) {\n\t\t\t\tr_cons_printf (\"\\\"ophint\\\":\\\"%s\\\",\", hint->opcode);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"sign\\\":%s,\", r_str_bool (op.sign));\n\t\t\tr_cons_printf (\"\\\"prefix\\\":%\" PFMT64u \",\", op.prefix);\n\t\t\tr_cons_printf (\"\\\"id\\\":%d,\", op.id);\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tr_cons_printf (\"\\\"opex\\\":%s,\", opexstr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"addr\\\":%\" PFMT64u \",\", core->offset + idx);\n\t\t\tr_cons_printf (\"\\\"bytes\\\":\\\"\");\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tr_cons_printf (\"%02x\", buf[j + idx]);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\",\");\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"val\\\": %\" PFMT64u \",\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"ptr\\\": %\" PFMT64u \",\", op.ptr);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"size\\\": %d,\", size);\n\t\t\tr_cons_printf (\"\\\"type\\\": \\\"%s\\\",\",\n\t\t\t\tr_anal_optype_to_string (op.type));\n\t\t\tif (op.reg) {\n\t\t\t\tr_cons_printf (\"\\\"reg\\\": \\\"%s\\\",\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tr_cons_printf (\"\\\"ireg\\\": \\\"%s\\\",\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tr_cons_printf (\"\\\"scale\\\":%d,\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tr_cons_printf (\"\\\"esil\\\": \\\"%s\\\",\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"jump\\\":%\" PFMT64u \",\", op.jump);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tr_cons_printf (\"\\\"refptr\\\":%d,\", op.refptr);\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tr_cons_printf (\"\\\"fail\\\":%\" PFMT64u \",\", op.fail);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"cycles\\\":%d,\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tr_cons_printf (\"\\\"failcycles\\\":%d,\", op.failcycles);\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"delay\\\":%d,\", op.delay);\n\t\t\t{\n\t\t\t\tconst char *p = r_anal_stackop_tostring (op.stackop);\n\t\t\t\tif (p && *p && strcmp (p, \"null\"))\n\t\t\t\t\tr_cons_printf (\"\\\"stack\\\":\\\"%s\\\",\", p);\n\t\t\t}\n\t\t\tif (op.stackptr) {\n\t\t\t\tr_cons_printf (\"\\\"stackptr\\\":%d,\", op.stackptr);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)\n\t\t\t\t\t? r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tr_cons_printf (\"\\\"cond\\\":\\\"%s\\\",\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tr_cons_printf (\"\\\"family\\\":\\\"%s\\\"}\", r_anal_op_family_to_string (op.family));\n\t\t} else {\n#define printline(k, fmt, arg)\\\n\t{ \\\n\t\tif (use_color)\\\n\t\t\tr_cons_printf (\"%s%s: \" Color_RESET, color, k);\\\n\t\telse\\\n\t\t\tr_cons_printf (\"%s: \", k);\\\n\t\tif (fmt) r_cons_printf (fmt, arg);\\\n\t}\n\t\t\tprintline (\"address\", \"0x%\" PFMT64x \"\\n\", core->offset + idx);\n\t\t\tprintline (\"opcode\", \"%s\\n\", r_asm_op_get_asm (&asmop));\n\t\t\tprintline (\"mnemonic\", \"%s\\n\", mnem);\n\t\t\tif (hint) {\n\t\t\t\tif (hint->opcode) {\n\t\t\t\t\tprintline (\"ophint\", \"%s\\n\", hint->opcode);\n\t\t\t\t}\n#if 0\n\t\t\t\t// addr should not override core->offset + idx.. its silly\n\t\t\t\tif (hint->addr != UT64_MAX) {\n\t\t\t\t\tprintline (\"addr\", \"0x%08\" PFMT64x \"\\n\", (hint->addr + idx));\n\t\t\t\t}\n#endif\n\t\t\t}\n\t\t\tprintline (\"prefix\", \"%\" PFMT64u \"\\n\", op.prefix);\n\t\t\tprintline (\"id\", \"%d\\n\", op.id);\n#if 0\n// no opex here to avoid lot of tests broken..and having json in here is not much useful imho\n\t\t\tif (opexstr && *opexstr) {\n\t\t\t\tprintline (\"opex\", \"%s\\n\", opexstr);\n\t\t\t}\n#endif\n\t\t\tprintline (\"bytes\", NULL, 0);\n\t\t\tint minsz = R_MIN (len, size);\n\t\t\tminsz = R_MAX (minsz, 0);\n\t\t\tfor (j = 0; j < minsz; j++) {\n\t\t\t\tut8 ch = ((j + idx - 1) > minsz)? 0xff: buf[j + idx];\n\t\t\t\tr_cons_printf (\"%02x\", ch);\n\t\t\t}\n\t\t\tr_cons_newline ();\n\t\t\tif (op.val != UT64_MAX) {\n\t\t\t\tprintline (\"val\", \"0x%08\" PFMT64x \"\\n\", op.val);\n\t\t\t}\n\t\t\tif (op.ptr != UT64_MAX) {\n\t\t\t\tprintline (\"ptr\", \"0x%08\" PFMT64x \"\\n\", op.ptr);\n\t\t\t}\n\t\t\tif (op.refptr != -1) {\n\t\t\t\tprintline (\"refptr\", \"%d\\n\", op.refptr);\n\t\t\t}\n\t\t\tprintline (\"size\", \"%d\\n\", size);\n\t\t\tprintline (\"sign\", \"%s\\n\", r_str_bool (op.sign));\n\t\t\tprintline (\"type\", \"%s\\n\", r_anal_optype_to_string (op.type));\n\t\t\tprintline (\"cycles\", \"%d\\n\", op.cycles);\n\t\t\tif (op.failcycles) {\n\t\t\t\tprintline (\"failcycles\", \"%d\\n\", op.failcycles);\n\t\t\t}\n\t\t\t{\n\t\t\t\tconst char *t2 = r_anal_optype_to_string (op.type2);\n\t\t\t\tif (t2 && strcmp (t2, \"null\")) {\n\t\t\t\t\tprintline (\"type2\", \"%s\\n\", t2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (op.reg) {\n\t\t\t\tprintline (\"reg\", \"%s\\n\", op.reg);\n\t\t\t}\n\t\t\tif (op.ireg) {\n\t\t\t\tprintline (\"ireg\", \"%s\\n\", op.ireg);\n\t\t\t}\n\t\t\tif (op.scale) {\n\t\t\t\tprintline (\"scale\", \"%d\\n\", op.scale);\n\t\t\t}\n\t\t\tif (hint && hint->esil) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", hint->esil);\n\t\t\t} else if (*esilstr) {\n\t\t\t\tprintline (\"esil\", \"%s\\n\", esilstr);\n\t\t\t}\n\t\t\tif (hint && hint->jump != UT64_MAX) {\n\t\t\t\top.jump = hint->jump;\n\t\t\t}\n\t\t\tif (op.jump != UT64_MAX) {\n\t\t\t\tprintline (\"jump\", \"0x%08\" PFMT64x \"\\n\", op.jump);\n\t\t\t}\n\t\t\tif (op.direction != 0) {\n\t\t\t\tconst char * dir = op.direction == 1 ? \"read\"\n\t\t\t\t\t: op.direction == 2 ? \"write\"\n\t\t\t\t\t: op.direction == 4 ? \"exec\"\n\t\t\t\t\t: op.direction == 8 ? \"ref\": \"none\";\n\t\t\t\tprintline (\"direction\", \"%s\\n\", dir);\n\t\t\t}\n\t\t\tif (hint && hint->fail != UT64_MAX) {\n\t\t\t\top.fail = hint->fail;\n\t\t\t}\n\t\t\tif (op.fail != UT64_MAX) {\n\t\t\t\tprintline (\"fail\", \"0x%08\" PFMT64x \"\\n\", op.fail);\n\t\t\t}\n\t\t\tif (op.delay) {\n\t\t\t\tprintline (\"delay\", \"%d\\n\", op.delay);\n\t\t\t}\n\t\t\tprintline (\"stack\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\t{\n\t\t\t\tconst char *arg = (op.type & R_ANAL_OP_TYPE_COND)?  r_anal_cond_tostring (op.cond): NULL;\n\t\t\t\tif (arg) {\n\t\t\t\t\tprintline (\"cond\", \"%s\\n\", arg);\n\t\t\t\t}\n\t\t\t}\n\t\t\tprintline (\"family\", \"%s\\n\", r_anal_op_family_to_string (op.family));\n\t\t\tprintline (\"stackop\", \"%s\\n\", r_anal_stackop_tostring (op.stackop));\n\t\t\tif (op.stackptr) {\n\t\t\t\tprintline (\"stackptr\", \"%\"PFMT64u\"\\n\", op.stackptr);\n\t\t\t}\n\t\t}\n\t\t//r_cons_printf (\"false: 0x%08\"PFMT64x\"\\n\", core->offset+idx);\n\t\t//free (hint);\n\t\tfree (mnem);\n\t\tr_anal_hint_free (hint);\n\t\tr_anal_op_fini (&op);\n\t}\n\tr_anal_op_fini (&op);\n\tif (fmt == 'j') {\n\t\tr_cons_printf (\"]\");\n\t\tr_cons_newline ();\n\t} else if (fmt == 's') {\n\t\tr_cons_printf (\"%d\\n\", totalsize);\n\t}\n\tr_anal_esil_free (esil);\n}", "commit_link": "github.com/radare/radare2/commit/a1bc65c3db593530775823d6d7506a457ed95267", "file_name": "libr/core/cmd_anal.c", "vul_type": "cwe-125", "description": "Write a C function named `core_anal_bytes` that analyzes a buffer of bytes for disassembly and ESIL (Evaluable Strings Intermediate Language) conversion in Radare2."}
{"func_name": "save", "func_src_before": "async def save(request):\n    # TODO csrf\n    data = await request.post()\n    item = Item(data['src'])\n\n    # Update name\n    new_src = data.get('new_src')\n    if new_src and new_src != data['src']:\n        # don't need to worry about html unquote\n        shutil.move(item.abspath, settings.STORAGE_DIR + new_src)\n        old_backup_abspath = item.backup_abspath\n        item = Item(new_src)\n        if os.path.isfile(old_backup_abspath):\n            shutil.move(old_backup_abspath, item.backup_abspath)\n\n    # Update meta\n    for field in item.FORM:\n        # TODO handle .repeatable (keywords)\n        item.meta[field] = [data.get(field, '')]\n\n    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):\n        shutil.copyfile(item.abspath, item.backup_abspath)\n\n    # WISHLIST don't write() if nothing changed\n    item.meta.write()\n\n    return web.Response(\n        status=200,\n        body=json.dumps(item.get_form_fields()).encode('utf8'),\n        content_type='application/json',\n    )", "func_src_after": "async def save(request):\n    # TODO csrf\n    data = await request.post()\n    item = Item(data['src'])\n\n    # Update name\n    new_src = data.get('new_src')\n    if new_src:\n        new_abspath = os.path.abspath(settings.STORAGE_DIR + new_src)\n        if not new_abspath.startswith(settings.STORAGE_DIR):\n            return web.Response(status=400, body=b'Invalid Request')\n\n        if new_abspath != item.abspath:\n            shutil.move(item.abspath, new_abspath)\n            old_backup_abspath = item.backup_abspath\n            item = Item(new_src)\n            if os.path.isfile(old_backup_abspath):\n                shutil.move(old_backup_abspath, item.backup_abspath)\n\n    # Update meta\n    for field in item.FORM:\n        # TODO handle .repeatable (keywords)\n        item.meta[field] = [data.get(field, '')]\n\n    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):\n        shutil.copyfile(item.abspath, item.backup_abspath)\n\n    # WISHLIST don't write() if nothing changed\n    item.meta.write()\n\n    return web.Response(\n        status=200,\n        body=json.dumps(item.get_form_fields()).encode('utf8'),\n        content_type='application/json',\n    )", "commit_link": "github.com/crccheck/gallery-cms/commit/60dec5c580a779ae27824ed54cb113eca25afdc0", "file_name": "gallery/gallery.py", "vul_type": "cwe-022", "description": "Write a Python function to update an item's name and metadata from a POST request."}
{"func_name": "get", "func_src_before": "    def get(self, path):\n        return static_file(path, self.get_base_path())", "func_src_after": "    def get(self, path):\n        path = self.sanitize_path(path)\n        base_paths = self.get_base_paths()\n        if hasattr(base_paths, 'split'):\n            # String, so go simple\n            base_path = base_paths\n        else:\n            base_path = self.get_first_base(base_paths, path)\n        return static_file(path, base_path)", "commit_link": "github.com/foxbunny/seagull/commit/1fb790712fe0c1d1957b31e34a8e0e6593af87a7", "file_name": "seagull/routes/app.py", "vul_type": "cwe-022", "description": "Write a Python function named `get` that retrieves a static file from a base path, which may involve sanitizing the path and handling multiple base paths."}
{"func_name": "add_post", "func_src_before": "def add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"insert into posts values ('%s')\" % content)\n  conn.commit()\n  conn.close()", "func_src_after": "def add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  one_post = content\n  cursor.execute(\"insert into posts values (%s)\", (one_post,))\n  conn.commit()\n  conn.close()", "commit_link": "github.com/paulc1600/DB-API-Forum/commit/069700fb4beec79182fff3c556e9cccce3230d6f", "file_name": "forumdb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new post into a forum database using psycopg2."}
{"func_name": "__init__", "func_src_before": "    def __init__(self,p):\n        p = pickle.loads(p)\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "func_src_after": "    def __init__(self,p):\n        try:\n            self.tokens = np.array([symbolToIndex[\"START\"]] + [ symbolToIndex[s] for s in serializeProgram(p) ] + [symbolToIndex[\"END\"]])\n        except KeyError:\n            print \"Key error in tokenization\",serializeProgram(p)\n            assert False\n        \n        self.image = p.convertToSequence().draw()\n        self.program = p\n\n        if str(parseOutput(serializeProgram(p))) != str(p):\n            print \"Serialization failure for program\",p\n            print serializeProgram(p)\n            print parseOutput(serializeProgram(p))\n            assert False", "line_changes": {"deleted": [{"line_no": 2, "char_start": 26, "char_end": 54, "line": "        p = pickle.loads(p)\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 26, "char_end": 54, "chars": "        p = pickle.loads(p)\n"}], "added": []}, "commit_link": "github.com/ellisk42/TikZ/commit/66ab87a1b9a4129fe6f2bc7645a17899f35c9c8b", "file_name": "noTraceBaseline.py", "vul_type": "cwe-502", "commit_msg": "fixed bug in pickle loading", "parent_commit": "c5bb3ad10611b779342b1866cad8961b4a3287ba", "description": "Write a Python class initializer that tokenizes a serialized program, generates an image from it, and checks for serialization consistency."}
{"func_name": "fiber_switch", "func_src_before": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  if (resume && c->status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (c->status == MRB_FIBER_RUNNING || c->status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (c->status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  mrb->c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  if (c->status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    if (len >= c->stend - c->stack) {\n      mrb_raise(mrb, E_FIBER_ERROR, \"too many arguments to fiber\");\n    }\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n  fiber_switch_context(mrb, c);\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "func_src_after": "fiber_switch(mrb_state *mrb, mrb_value self, mrb_int len, const mrb_value *a, mrb_bool resume, mrb_bool vmexec)\n{\n  struct mrb_context *c = fiber_check(mrb, self);\n  struct mrb_context *old_c = mrb->c;\n  enum mrb_fiber_state status;\n  mrb_value value;\n\n  fiber_check_cfunc(mrb, c);\n  status = c->status;\n  if (resume && status == MRB_FIBER_TRANSFERRED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming transferred fiber\");\n  }\n  if (status == MRB_FIBER_RUNNING || status == MRB_FIBER_RESUMED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"double resume (fib)\");\n  }\n  if (status == MRB_FIBER_TERMINATED) {\n    mrb_raise(mrb, E_FIBER_ERROR, \"resuming dead fiber\");\n  }\n  old_c->status = resume ? MRB_FIBER_RESUMED : MRB_FIBER_TRANSFERRED;\n  c->prev = resume ? mrb->c : (c->prev ? c->prev : mrb->root_c);\n  fiber_switch_context(mrb, c);\n  if (status == MRB_FIBER_CREATED) {\n    mrb_value *b, *e;\n\n    mrb_stack_extend(mrb, len+2); /* for receiver and (optional) block */\n    b = c->stack+1;\n    e = b + len;\n    while (b<e) {\n      *b++ = *a++;\n    }\n    c->cibase->argc = (int)len;\n    value = c->stack[0] = MRB_PROC_ENV(c->ci->proc)->stack[0];\n  }\n  else {\n    value = fiber_result(mrb, a, len);\n  }\n\n  if (vmexec) {\n    c->vmexec = TRUE;\n    value = mrb_vm_exec(mrb, c->ci[-1].proc, c->ci->pc);\n    mrb->c = old_c;\n  }\n  else {\n    MARK_CONTEXT_MODIFY(c);\n  }\n  return value;\n}", "commit_link": "github.com/mruby/mruby/commit/778500563a9f7ceba996937dc886bd8cde29b42b", "file_name": "mrbgems/mruby-fiber/src/fiber.c", "vul_type": "cwe-125", "description": "Write a C function `fiber_switch` for the MRuby language that handles fiber resumption, argument passing, and optional VM execution."}
{"func_name": "WebSecurityConfig::configure", "func_src_before": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "func_src_after": "\t@Override\r\n\tprotected void configure(HttpSecurity http) throws Exception {\r\n\t\thttp.authorizeRequests().antMatchers(\"/webjars/**\", \"/login\", \"/logout\", \"/static/**\", \"/console/*\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/user/**\").hasAnyAuthority(FredBetPermission.PERM_USER_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/admin/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\thttp.authorizeRequests().antMatchers(\"/buildinfo/**\").hasAnyAuthority(FredBetPermission.PERM_SYSTEM_INFO);\r\n\t\thttp.authorizeRequests().antMatchers(\"/administration/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\r\n\t\t// Spring Boot Actuator\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/health\").permitAll();\r\n\t\thttp.authorizeRequests().antMatchers(\"/actuator/**\").hasAnyAuthority(FredBetPermission.PERM_ADMINISTRATION);\r\n\t\t\r\n\t\thttp.authorizeRequests().anyRequest().authenticated();\r\n\t\t\r\n\t\thttp.formLogin().loginPage(\"/login\").defaultSuccessUrl(\"/matches/upcoming\").permitAll();\r\n\t\thttp.logout().logoutRequestMatcher(new AntPathRequestMatcher(\"/logout\")).logoutSuccessUrl(\"/login\").invalidateHttpSession(true)\r\n\t\t\t\t.deleteCookies(\"JSESSIONID\", \"remember-me\").permitAll();\r\n\t\thttp.rememberMe().tokenRepository(persistentTokenRepository()).tokenValiditySeconds(REMEMBER_ME_TOKEN_VALIDITY_SECONDS);\r\n\r\n\t\t// disable cache control to allow usage of ETAG headers (no image reload\r\n\t\t// if the image has not been changed)\r\n\t\thttp.headers().cacheControl().disable();\r\n\r\n\t\tif (environment.acceptsProfiles(FredBetProfile.DEV)) {\r\n\t\t\t// this is for the embedded h2 console\r\n\t\t\thttp.headers().frameOptions().disable();\r\n\t\t}\r\n\t}", "line_changes": {"deleted": [{"line_no": 21, "char_start": 1381, "char_end": 1407, "line": "\t\thttp.csrf().disable();\r\n"}, {"line_no": 22, "char_start": 1407, "char_end": 1409, "line": "\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1335, "char_end": 1409, "chars": "\t\t// we do not use CSRF in this app (by now)\r\n\t\thttp.csrf().disable();\r\n\r\n"}], "added": []}, "commit_link": "github.com/fred4jupiter/fredbet/commit/0470f1a9250316ca637ca55faea70fa0a4f2680a", "file_name": "WebSecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "enabled CSRF protection, because thymeleaf adds a form param for that\nautomatically", "parent_commit": "966f4c3fa53f7be32da840a85587565b82cb0948", "description": "Write a Java method using Spring Security to configure HTTP security, including authorization for specific paths and user roles, form login, logout, and remember-me functionality."}
{"func_name": "ReadPSDChannel", "func_src_before": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      mask->matte=MagickFalse;\n      channel_image=mask;\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "func_src_after": "static MagickBooleanType ReadPSDChannel(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,LayerInfo* layer_info,\n  const size_t channel,const PSDCompressionType compression,\n  ExceptionInfo *exception)\n{\n  Image\n    *channel_image,\n    *mask;\n\n  MagickOffsetType\n    offset;\n\n  MagickBooleanType\n    status;\n\n  channel_image=image;\n  mask=(Image *) NULL;\n  if (layer_info->channel_info[channel].type < -1)\n    {\n      const char\n        *option;\n      /*\n        Ignore mask that is not a user supplied layer mask, if the mask is\n        disabled or if the flags have unsupported values.\n      */\n      option=GetImageOption(image_info,\"psd:preserve-opacity-mask\");\n      if ((layer_info->channel_info[channel].type != -2) ||\n          (layer_info->mask.flags > 2) || ((layer_info->mask.flags & 0x02) &&\n           (IsStringTrue(option) == MagickFalse)))\n      {\n        SeekBlob(image,layer_info->channel_info[channel].size-2,SEEK_CUR);\n        return(MagickTrue);\n      }\n      mask=CloneImage(image,layer_info->mask.page.width,\n        layer_info->mask.page.height,MagickFalse,exception);\n      if (mask != (Image *) NULL)\n        {\n          mask->matte=MagickFalse;\n          channel_image=mask;\n        }\n    }\n\n  offset=TellBlob(image);\n  status=MagickTrue;\n  switch(compression)\n  {\n    case Raw:\n      status=ReadPSDChannelRaw(channel_image,psd_info->channels,\n        layer_info->channel_info[channel].type,exception);\n      break;\n    case RLE:\n      {\n        MagickOffsetType\n          *sizes;\n\n        sizes=ReadPSDRLESizes(channel_image,psd_info,channel_image->rows);\n        if (sizes == (MagickOffsetType *) NULL)\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        status=ReadPSDChannelRLE(channel_image,psd_info,\n          layer_info->channel_info[channel].type,sizes,exception);\n        sizes=(MagickOffsetType *) RelinquishMagickMemory(sizes);\n      }\n      break;\n    case ZipWithPrediction:\n    case ZipWithoutPrediction:\n#ifdef MAGICKCORE_ZLIB_DELEGATE\n      status=ReadPSDChannelZip(channel_image,layer_info->channels,\n        layer_info->channel_info[channel].type,compression,\n        layer_info->channel_info[channel].size-2,exception);\n#else\n      (void) ThrowMagickException(exception,GetMagickModule(),\n          MissingDelegateWarning,\"DelegateLibrarySupportNotBuiltIn\",\n            \"'%s' (ZLIB)\",image->filename);\n#endif\n      break;\n    default:\n      (void) ThrowMagickException(exception,GetMagickModule(),TypeWarning,\n        \"CompressionNotSupported\",\"'%.20g'\",(double) compression);\n      break;\n  }\n\n  SeekBlob(image,offset+layer_info->channel_info[channel].size-2,SEEK_SET);\n  if (status == MagickFalse)\n    {\n      if (mask != (Image *) NULL)\n        DestroyImage(mask);\n      ThrowBinaryException(CoderError,\"UnableToDecompressImage\",\n        image->filename);\n    }\n  layer_info->mask.image=mask;\n  return(status);\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/7f2dc7a1afc067d0c89f12c82bcdec0445fb1b94", "file_name": "coders/psd.c", "vul_type": "cwe-476", "description": "In C, write a function to read a channel from a PSD file, handling different compression types and optional masks."}
{"func_name": "delete", "func_src_before": "    @jwt_required\n    def delete(self, user_id):\n        \"\"\" Deletes user with the corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from users where user_id = '{user_id}'\"\"\")", "func_src_after": "    @jwt_required\n    def delete(self, user_id):\n        \"\"\" Deletes user with the corresponding user_id \"\"\"\n        return database_utilities.execute_query(f\"\"\"delete from users where user_id = %s\"\"\", (user_id, ))", "commit_link": "github.com/sgosal2/tiger-boards-backend/commit/4670109dd613df2f2fe7e8403ebd149df2b55485", "file_name": "apis/users.py", "vul_type": "cwe-089", "description": "Create a Python function with JWT authentication that deletes a user from the database by their user_id."}
{"func_name": "create_basename_core", "func_src_before": "def create_basename_core(basename):\n    try:\n        basename = basename.casefold()\n    except Exception:\n        basename = basename.lower()\n\n    basename = basename.replace(' ', '-')\n    basename = re.sub(r'<[^>]*>', r'', basename)\n    basename = re.sub(r'[^a-z0-9\\-]', r'', basename)\n    basename = re.sub(r'\\-\\-', r'-', basename)\n    basename = urllib.parse.quote_plus(basename)\n\n    return basename", "func_src_after": "def create_basename_core(basename):\n    try:\n        basename = basename.casefold()\n    except Exception:\n        basename = basename.lower()\n\n    basename = re.sub(r'[ \\./]', r'-', basename)\n    basename = re.sub(r'<[^>]*>', r'', basename)\n    basename = re.sub(r'[^a-z0-9\\-]', r'', basename)\n    basename = re.sub(r'\\-\\-', r'-', basename)\n    basename = urllib.parse.quote_plus(basename)\n\n    return basename", "commit_link": "github.com/syegulalp/mercury/commit/3f7c7442fa49aec37577dbdb47ce11a848e7bd03", "file_name": "MeTal/core/utils.py", "vul_type": "cwe-022", "description": "Write a Python function to sanitize and encode a string for use as a URL basename."}
{"func_name": "SessionAttributesManager::doGet", "func_src_before": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n    response.getWriter().append(message);\n  }", "func_src_after": "  @Override\n  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n    response.setHeader(\"Cache-Control\", \"no-cache\");\n    response.setHeader(\"Pragma\", \"no-cache\");\n    response.setDateHeader(\"Expires\", -1);\n\n    String name = request.getParameter(\"name\");\n    String value = request.getParameter(\"value\");\n    String message = \"The parameter 'name' is empty\";\n\n    if (name != null && name.length() > 0) {\n      if (allowedAttributes.contains(name)) {\n\n        // add bcdBeanUser.name entries as HashMap session variable \"bcdBeanUser\" where the key/value pairs\n        // are kept without the prefix.\n        boolean error = true;\n        Subject subject = SecurityUtils.getSubject();\n        // check value against list of allowed values or subject settings user rights\n        if (subject != null && subject.getSession() != null && ((allowedValues.get(name) != null && allowedValues.get(name).contains(\" \" + value + \" \")) || subject.isPermitted(BCD_EL_USER_BEAN + \":\" + name + \":\" + value))) {\n          Map<String, String> bean = (HashMap<String, String>)subject.getSession().getAttribute(BCD_EL_USER_BEAN);\n          if (bean == null)\n            bean = new HashMap<>();\n          bean.put(name, value);\n          subject.getSession().setAttribute(BCD_EL_USER_BEAN, bean);\n          error = false;\n        }\n        message =\"The \" + BCD_EL_USER_BEAN + \" '\" + name +  (error ? \"' can not be set to '\" : \"' was set to \") + value;\n      }\n      else {\n        message = \"The attribute name '\" + name + \"' is not allowed by the configuration. Please set the init param ALLOWED_ATTRIBUTES in your web.xml\";\n      }\n    }\n    log.debug(message);\n  }", "line_changes": {"deleted": [{"line_no": 34, "char_start": 1712, "char_end": 1754, "line": "    response.getWriter().append(message);\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1712, "char_end": 1754, "chars": "    response.getWriter().append(message);\n"}], "added": []}, "commit_link": "github.com/businesscode/BCD-UI/commit/9b230b1500511054da457cf4a5382895fae891df", "file_name": "SessionAttributesManager.java", "vul_type": "cwe-079", "commit_msg": "Server/Security, XSS fixes", "parent_commit": "1b507ea97204e6b71d5863d4c6e523e51a3440b2", "description": "Create a Java servlet that processes a GET request by updating session attributes based on provided parameters and logs the result."}
{"func_name": "(anonymous)", "func_src_before": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n    });", "func_src_after": "    $(document.body).on(\"keyup change\", \".sr-input\", function(foo) {\n        value = $(this).val();\n        var modalBody = $(this).closest(\"form#streamrule-form\").find(\".modal-body\");\n\n        if (value != undefined && value != \"\") {\n            // Selectbox options can have a custom replace string.\n            s = $(\"option:selected\", this);\n            if (s != undefined && s.attr(\"data-reflect-string\") != undefined && s.attr(\"data-reflect-string\") != \"\") {\n                value = s.attr(\"data-reflect-string\");\n\n                // Inverted?\n                if ($(\"#sr-inverted\", modalBody).is(':checked')) {\n                    value = \"not \" + value;\n                }\n            }\n        } else {\n            value = $(this).attr(\"placeholder\");\n        }\n\n        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n    });", "line_changes": {"deleted": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).html(value);\n"}], "added": [{"line_no": 20, "char_start": 770, "char_end": 834, "line": "        $($(this).attr(\"data-reflect\"), modalBody).text(value);\n"}]}, "char_changes": {"deleted": [{"char_start": 821, "char_end": 825, "chars": "html"}], "added": [{"char_start": 821, "char_end": 825, "chars": "text"}]}, "commit_link": "github.com/edmundoa/graylog2-server/commit/a88cae99955cd0ccdd5d99a1c6d506029eb15c60", "file_name": "streamrules.js", "vul_type": "cwe-079", "commit_msg": "use .text() not .html() in stream rule editor to prevent DOM XSS\n\nfixes #543", "description": "In JavaScript, write a jQuery event handler that updates text in a modal based on user input and selection changes, with special handling for an \"inverted\" checkbox state."}
{"func_name": "self.open", "func_src_before": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url, \"rb\", options) do |file|\n        read(file, ext)\n      end\n    end", "func_src_after": "    def self.open(path_or_url, ext = nil, options = {})\n      options, ext = ext, nil if ext.is_a?(Hash)\n\n      uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n      end\n    end", "line_changes": {"deleted": [{"line_no": 4, "char_start": 106, "char_end": 120, "line": "      ext ||=\n"}, {"line_no": 5, "char_start": 120, "char_end": 156, "line": "        if File.exist?(path_or_url)\n"}, {"line_no": 6, "char_start": 156, "char_end": 192, "line": "          File.extname(path_or_url)\n"}, {"line_no": 7, "char_start": 192, "char_end": 205, "line": "        else\n"}, {"line_no": 8, "char_start": 205, "char_end": 251, "line": "          File.extname(URI(path_or_url).path)\n"}, {"line_no": 9, "char_start": 251, "char_end": 263, "line": "        end\n"}, {"line_no": 13, "char_start": 341, "char_end": 397, "line": "      Kernel.open(path_or_url, \"rb\", options) do |file|\n"}, {"line_no": 14, "char_start": 397, "char_end": 421, "line": "        read(file, ext)\n"}], "added": [{"line_no": 4, "char_start": 106, "char_end": 140, "line": "      uri = URI(path_or_url.to_s)\n"}, {"line_no": 6, "char_start": 141, "char_end": 178, "line": "      ext ||= File.extname(uri.path)\n"}, {"line_no": 9, "char_start": 255, "char_end": 308, "line": "      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n"}, {"line_no": 10, "char_start": 308, "char_end": 361, "line": "        uri.open(options) { |file| read(file, ext) }\n"}, {"line_no": 11, "char_start": 361, "char_end": 372, "line": "      else\n"}, {"line_no": 12, "char_start": 372, "char_end": 442, "line": "        File.open(uri.to_s, \"rb\", options) { |file| read(file, ext) }\n"}]}, "char_changes": {"deleted": [{"char_start": 112, "char_end": 370, "chars": "ext ||=\n        if File.exist?(path_or_url)\n          File.extname(path_or_url)\n        else\n          File.extname(URI(path_or_url).path)\n        end\n\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      Kernel.open(path_or_url"}, {"char_start": 387, "char_end": 389, "chars": "do"}, {"char_start": 396, "char_end": 404, "chars": "\n       "}], "added": [{"char_start": 112, "char_end": 398, "chars": "uri = URI(path_or_url.to_s)\n\n      ext ||= File.extname(uri.path)\n      ext.sub!(/:.*/, '') # hack for filenames or URLs that include a colon\n\n      if uri.is_a?(URI::HTTP) || uri.is_a?(URI::FTP)\n        uri.open(options) { |file| read(file, ext) }\n      else\n        File.open(uri.to_s"}, {"char_start": 415, "char_end": 416, "chars": "{"}, {"char_start": 439, "char_end": 441, "chars": " }"}]}, "commit_link": "github.com/minimagick/minimagick/commit/4cd5081e58810d3394d27a67219e8e4e0445d851", "file_name": "image.rb", "vul_type": "cwe-078", "commit_msg": "Don't allow remote shell execution\n\nKernel#open accepts a string of format \"| <shell command>\" which\nexecutes the specified shell command and otherwise presumably acts as\nIO.popen. The open-uri standard library overrides Kernel#open to also\naccept URLs.\n\nHowever, the overridden Kernel#open just delegates to URI#open, so we\nswitch to using that directly and avoid the remote shell execution\nvulnerability. For files we just use File.open, which should have the\nsame behaviour as Kernel#open.", "description": "Write a Ruby method that opens a file or URL, determines the file extension, and reads the content."}
{"func_name": "PropertiesWidget::loadTorrentInfos", "func_src_before": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(m_torrent->comment()));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(m_torrent->creator());\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "func_src_after": "void PropertiesWidget::loadTorrentInfos(BitTorrent::TorrentHandle *const torrent)\n{\n    clear();\n    m_torrent = torrent;\n    downloaded_pieces->setTorrent(m_torrent);\n    pieces_availability->setTorrent(m_torrent);\n    if (!m_torrent) return;\n\n    // Save path\n    updateSavePath(m_torrent);\n    // Hash\n    hash_lbl->setText(m_torrent->hash());\n    PropListModel->model()->clear();\n    if (m_torrent->hasMetadata()) {\n        // Creation date\n        lbl_creationDate->setText(m_torrent->creationDate().toString(Qt::DefaultLocaleShortDate));\n\n        label_total_size_val->setText(Utils::Misc::friendlyUnit(m_torrent->totalSize()));\n\n        // Comment\n        comment_text->setText(Utils::Misc::parseHtmlLinks(Utils::String::toHtmlEscaped(m_torrent->comment())));\n\n        // URL seeds\n        loadUrlSeeds();\n\n        label_created_by_val->setText(Utils::String::toHtmlEscaped(m_torrent->creator()));\n\n        // List files in torrent\n        PropListModel->model()->setupModelData(m_torrent->info());\n        filesList->setExpanded(PropListModel->index(0, 0), true);\n\n        // Load file priorities\n        PropListModel->model()->updateFilesPriorities(m_torrent->filePriorities());\n    }\n    // Load dynamic data\n    loadDynamicData();\n}", "commit_link": "github.com/qbittorrent/qBittorrent/commit/6ca3e4f094da0a0017cb2d483ec1db6176bb0b16", "file_name": "src/gui/properties/propertieswidget.cpp", "vul_type": "cwe-079", "description": "Write a C++ function in the PropertiesWidget class that loads and displays torrent information using the BitTorrent::TorrentHandle object."}
{"func_name": "generatePrivateKey", "func_src_before": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "func_src_after": "func generatePrivateKey(keyType string, keyBits int, container ParsedPrivateKeyContainer, entropyReader io.Reader) error {\n\tvar err error\n\tvar privateKeyType PrivateKeyType\n\tvar privateKeyBytes []byte\n\tvar privateKey crypto.Signer\n\n\tvar randReader io.Reader = rand.Reader\n\tif entropyReader != nil {\n\t\trandReader = entropyReader\n\t}\n\n\tswitch keyType {\n\tcase \"rsa\":\n\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n\t\tprivateKeyType = RSAPrivateKey\n\t\tprivateKey, err = rsa.GenerateKey(randReader, keyBits)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating RSA private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes = x509.MarshalPKCS1PrivateKey(privateKey.(*rsa.PrivateKey))\n\tcase \"ec\":\n\t\tprivateKeyType = ECPrivateKey\n\t\tvar curve elliptic.Curve\n\t\tswitch keyBits {\n\t\tcase 224:\n\t\t\tcurve = elliptic.P224()\n\t\tcase 256:\n\t\t\tcurve = elliptic.P256()\n\t\tcase 384:\n\t\t\tcurve = elliptic.P384()\n\t\tcase 521:\n\t\t\tcurve = elliptic.P521()\n\t\tdefault:\n\t\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unsupported bit length for EC key: %d\", keyBits)}\n\t\t}\n\t\tprivateKey, err = ecdsa.GenerateKey(curve, randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating EC private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalECPrivateKey(privateKey.(*ecdsa.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling EC private key: %v\", err)}\n\t\t}\n\tcase \"ed25519\":\n\t\tprivateKeyType = Ed25519PrivateKey\n\t\t_, privateKey, err = ed25519.GenerateKey(randReader)\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error generating ed25519 private key: %v\", err)}\n\t\t}\n\t\tprivateKeyBytes, err = x509.MarshalPKCS8PrivateKey(privateKey.(ed25519.PrivateKey))\n\t\tif err != nil {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"error marshalling Ed25519 private key: %v\", err)}\n\t\t}\n\tdefault:\n\t\treturn errutil.UserError{Err: fmt.Sprintf(\"unknown key type: %s\", keyType)}\n\t}\n\n\tcontainer.SetParsedPrivateKey(privateKey, privateKeyType, privateKeyBytes)\n\treturn nil\n}", "line_changes": {"deleted": [], "added": [{"line_no": 22, "char_start": 887, "char_end": 909, "line": "\t\tif keyBits < 2048 {\n"}, {"line_no": 23, "char_start": 909, "char_end": 1015, "line": "\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n"}, {"line_no": 24, "char_start": 1015, "char_end": 1019, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 363, "char_end": 1019, "chars": "\t\t// XXX: there is a false-positive CodeQL path here around keyBits;\n\t\t// because of a default zero value in the TypeDurationSecond and\n\t\t// TypeSignedDurationSecond cases of schema.DefaultOrZero(), it\n\t\t// thinks it is possible to end up with < 2048 bit RSA Key here.\n\t\t// While this is true for SSH keys, it isn't true for PKI keys\n\t\t// due to ValidateKeyTypeLength(...) below. While we could close\n\t\t// the report as a false-positive, enforcing a minimum keyBits size\n\t\t// here of 2048 would ensure no other paths exist.\n\t\tif keyBits < 2048 {\n\t\t\treturn errutil.InternalError{Err: fmt.Sprintf(\"insecure bit length for RSA private key: %d\", keyBits)}\n\t\t}\n"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "helpers.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to generate a private key of a specified type and size, optionally using a custom entropy source."}
{"func_name": "tile_by_id", "func_src_before": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "func_src_after": "def tile_by_id(id,path):\n    '''\n    ''' \n\n    conn = mysql_connection()\n    mysql = conn.cursor(cursor_class=MySQLCursorDict)\n    \n    tms_path = '.'.join(path.split('.')[:-1])\n    bucket = aws_prefix+'stuff'\n    opaque = False\n    \n    image = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n    \n    if request.endpoint == \"tilemap\": \n        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n    elif request.endpoint == \"tileatlas\":\n        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n\n    items = mysql.fetchdicts()\n\n    conn.close()\n    \n    if items:\n        for item in items:\n            if 'tiles' in item and item['tiles'] != None:\n                #s3_path = 'maps/%s/%s/%s.png' % (item.name, item['tiles'], tms_path)\n                s3_path = '%s/%s.png' % ( item['tiles'], tms_path)\n                url = 'http://%(bucket)s.s3.amazonaws.com/%(s3_path)s' % locals()\n        \n                try:\n                    tile_img = Image.open(StringIO(urlopen(url).read()))\n                except IOError: \n                    continue\n        \n                fresh_img = Image.new('RGBA', (256, 256), (0, 0, 0, 0))\n                fresh_img.paste(tile_img, (0, 0), tile_img)\n                fresh_img.paste(image, (0, 0), image)\n                image = fresh_img\n        \n            if Stat(image).extrema[3][0] > 0:\n                opaque = True         \n                break  \n    \n    if not opaque:\n        url = 'http://tile.stamen.com/toner-lite/%s.png' % tms_path\n        tile_img = Image.open(StringIO(urlopen(url).read()))\n        tile_img.paste(image, (0, 0), image)\n        image = tile_img\n        \n\n    bytes = StringIO()\n    image.save(bytes, 'JPEG')\n    \n    resp = make_response(bytes.getvalue(), 200)\n    resp.headers['Content-Type'] = 'image/jpeg'\n\n    return resp", "line_changes": {"deleted": [{"line_no": 15, "char_start": 334, "char_end": 403, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = '%s'\" % id)\n"}, {"line_no": 17, "char_start": 445, "char_end": 562, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = '%s' AND image IS NOT NULL ORDER BY image DESC\" % id)\n"}], "added": [{"line_no": 15, "char_start": 334, "char_end": 404, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE id = %s\", (id, ))\n"}, {"line_no": 17, "char_start": 446, "char_end": 564, "line": "        mysql.execute(\"SELECT tiles FROM maps WHERE atlas_id = %s AND image IS NOT NULL ORDER BY image DESC\", (id, ))\n"}]}, "char_changes": {"deleted": [{"char_start": 391, "char_end": 392, "chars": "'"}, {"char_start": 394, "char_end": 401, "chars": "'\" % id"}, {"char_start": 508, "char_end": 509, "chars": "'"}, {"char_start": 511, "char_end": 512, "chars": "'"}, {"char_start": 555, "char_end": 560, "chars": " % id"}], "added": [{"char_start": 393, "char_end": 402, "chars": "\", (id, )"}, {"char_start": 554, "char_end": 562, "chars": ", (id, )"}]}, "commit_link": "github.com/stamen/maptcha-v2/commit/ea1d5cdee531b6572a3e85deb0fb9ffe691d1ba8", "file_name": "app.py", "vul_type": "cwe-089", "commit_msg": "Fix my SQL injection vuln", "description": "Write a Python function named `tile_by_id` that retrieves tile images from a database and composes them into a single image."}
{"func_name": "tid_to_tid_num", "func_src_before": "    def tid_to_tid_num(self, tid):\n        ''' Returns tid_num, given tid. '''\n\n        q = \"SELECT rowid FROM tids WHERE tid = '\" + tid + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]", "func_src_after": "    def tid_to_tid_num(self, tid):\n        ''' Returns tid_num, given tid. '''\n\n        q = \"SELECT rowid FROM tids WHERE tid = ?\"\n        self.query(q, tid)\n        return self.c.fetchone()[0]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves the numerical ID (rowid) for a given text identifier (tid) from a database table named 'tids'."}
{"func_name": "ResponseParser::parse", "func_src_before": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "func_src_after": "\tpublic Object parse(SerializerHandler serializerHandler, InputStream response, boolean debugMode) throws XMLRPCException {\n\n\t\ttry {\n\t\t\tDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n\n\t\t\tDocumentBuilder builder = factory.newDocumentBuilder();\n\t\t\tDocument dom = builder.parse(response);\n\t\t\tif (debugMode ){\n\t\t\t\tprintDocument(dom, System.out);\n\t\t\t}\n\t\t\tElement e = dom.getDocumentElement();\n\n\n\t\t\t// Check for root tag\n\t\t\tif(!e.getNodeName().equals(XMLRPCClient.METHOD_RESPONSE)) {\n\t\t\t\tthrow new XMLRPCException(\"MethodResponse root tag is missing.\");\n\t\t\t}\n\n\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\tif(e.getNodeName().equals(XMLRPCClient.PARAMS)) {\n\n\t\t\t\te = XMLUtil.getOnlyChildElement(e.getChildNodes());\n\n\t\t\t\tif(!e.getNodeName().equals(XMLRPCClient.PARAM)) {\n\t\t\t\t\tthrow new XMLRPCException(\"The params tag must contain a param tag.\");\n\t\t\t\t}\n\n\t\t\t\treturn getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t} else if(e.getNodeName().equals(XMLRPCClient.FAULT)) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tMap<String,Object> o = (Map<String,Object>)getReturnValueFromElement(serializerHandler, e);\n\n\t\t\t\tthrow new XMLRPCServerException((String)o.get(FAULT_STRING), (Integer)o.get(FAULT_CODE));\n\n\t\t\t}\n\n\t\t\tthrow new XMLRPCException(\"The methodResponse tag must contain a fault or params tag.\");\n\n\t\t} catch(XMLRPCServerException e) {\n\t\t\tthrow e;\n\t\t} catch (Exception ex) {\n\t\t\tthrow new XMLRPCException(\"Error getting result from server.\", ex);\n\t\t}\n\n\t}", "line_changes": {"deleted": [{"line_no": 4, "char_start": 133, "char_end": 134, "line": "\n"}], "added": [{"line_no": 5, "char_start": 207, "char_end": 208, "line": "\n"}, {"line_no": 8, "char_start": 351, "char_end": 436, "line": "\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n"}, {"line_no": 9, "char_start": 436, "char_end": 481, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 11, "char_start": 517, "char_end": 553, "line": "\t\t\tfactory.setXIncludeAware(false);\n"}, {"line_no": 12, "char_start": 553, "char_end": 598, "line": "\t\t\tfactory.setExpandEntityReferences(false);\n"}, {"line_no": 14, "char_start": 655, "char_end": 656, "line": "\n"}]}, "char_changes": {"deleted": [{"char_start": 133, "char_end": 134, "chars": "\n"}, {"char_start": 208, "char_end": 243, "chars": "\t\t\tfactory.setNamespaceAware(true);"}], "added": [{"char_start": 207, "char_end": 655, "chars": "\n\t\t\t// Ensure the xml parser won't allow exploitation of the vuln CWE-611\n\t\t\t// (described on https://cwe.mitre.org/data/definitions/611.html )\n\t\t\tfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\tfactory.setNamespaceAware(true);\n\t\t\tfactory.setXIncludeAware(false);\n\t\t\tfactory.setExpandEntityReferences(false);\n\t\t\t// End of the configuration of the parser for CWE-611\n"}]}, "commit_link": "github.com/timroes/aXMLRPC/commit/ad6615b3ec41353e614f6ea5fdd5b046442a832b", "file_name": "ResponseParser.java", "vul_type": "cwe-611", "commit_msg": "Fix CWE-611\n\nThis commit fixes the issue described on\nhttps://cwe.mitre.org/data/definitions/611.html\n\ntest", "parent_commit": "2e59d03c961607d32bc9dc5e7aba931338907603", "description": "Write a Java function to parse an XMLRPC response from an InputStream and handle debug mode."}
{"func_name": "mrb_obj_clone", "func_src_before": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags = mrb_obj_ptr(self)->flags;\n\n  return clone;\n}", "func_src_after": "mrb_obj_clone(mrb_state *mrb, mrb_value self)\n{\n  struct RObject *p;\n  mrb_value clone;\n\n  if (mrb_immediate_p(self)) {\n    mrb_raisef(mrb, E_TYPE_ERROR, \"can't clone %S\", self);\n  }\n  if (mrb_type(self) == MRB_TT_SCLASS) {\n    mrb_raise(mrb, E_TYPE_ERROR, \"can't clone singleton class\");\n  }\n  p = (struct RObject*)mrb_obj_alloc(mrb, mrb_type(self), mrb_obj_class(mrb, self));\n  p->c = mrb_singleton_class_clone(mrb, self);\n  mrb_field_write_barrier(mrb, (struct RBasic*)p, (struct RBasic*)p->c);\n  clone = mrb_obj_value(p);\n  init_copy(mrb, clone, self);\n  p->flags |= mrb_obj_ptr(self)->flags & MRB_FLAG_IS_FROZEN;\n\n  return clone;\n}", "commit_link": "github.com/mruby/mruby/commit/55edae0226409de25e59922807cb09acb45731a2", "file_name": "src/kernel.c", "vul_type": "cwe-476", "description": "Write a function in C for the MRuby engine that clones an object, handling immediate values and singleton classes, and preserving the original object's frozen state."}
{"func_name": "Cipher::blowfishECB", "func_src_before": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction)\n        temp2 = byteToB64(temp2);\n\n    return temp2;\n}", "func_src_after": "QByteArray Cipher::blowfishECB(QByteArray cipherText, bool direction)\n{\n    QCA::Initializer init;\n    QByteArray temp = cipherText;\n\n    //do padding ourselves\n    if (direction)\n    {\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n    else\n    {\n        // ECB Blowfish encodes in blocks of 12 chars, so anything else is malformed input\n        if ((temp.length() % 12) != 0)\n            return cipherText;\n\n        temp = b64ToByte(temp);\n        while ((temp.length() % 8) != 0) temp.append('\\0');\n    }\n\n    QCA::Direction dir = (direction) ? QCA::Encode : QCA::Decode;\n    QCA::Cipher cipher(m_type, QCA::Cipher::ECB, QCA::Cipher::NoPadding, dir, m_key);\n    QByteArray temp2 = cipher.update(QCA::MemoryRegion(temp)).toByteArray();\n    temp2 += cipher.final().toByteArray();\n\n    if (!cipher.ok())\n        return cipherText;\n\n    if (direction) {\n        // Sanity check\n        if ((temp2.length() % 8) != 0)\n            return cipherText;\n\n        temp2 = byteToB64(temp2);\n    }\n\n    return temp2;\n}", "commit_link": "github.com/quassel/quassel/commit/8b5ecd226f9208af3074b33d3b7cf5e14f55b138", "file_name": "src/core/cipher.cpp", "vul_type": "cwe-125", "description": "Write a C++ function named `blowfishECB` that performs Blowfish encryption or decryption in ECB mode on a `QByteArray` with manual padding."}
{"func_name": "_create_database", "func_src_before": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "func_src_after": "    @staticmethod\n    def _create_database(engine: \"Engine\", db: Text):\n        \"\"\"Create database `db` on `engine` if it does not exist.\"\"\"\n\n        import psycopg2\n\n        conn = engine.connect()\n\n        cursor = conn.connection.cursor()\n        cursor.execute(\"COMMIT\")\n        cursor.execute(\n            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n        )\n        exists = cursor.fetchone()\n        if not exists:\n            try:\n                cursor.execute(f\"CREATE DATABASE {db}\")\n            except psycopg2.IntegrityError as e:\n                logger.error(f\"Could not create database '{db}': {e}\")\n\n        cursor.close()\n        conn.close()", "line_changes": {"deleted": [{"line_no": 11, "char_start": 275, "char_end": 362, "line": "        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db}'\")\n"}], "added": [{"line_no": 11, "char_start": 275, "char_end": 299, "line": "        cursor.execute(\n"}, {"line_no": 12, "char_start": 299, "char_end": 385, "line": "            f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = ?\", (db,)  # nosec\n"}, {"line_no": 13, "char_start": 385, "char_end": 395, "line": "        )\n"}]}, "char_changes": {"deleted": [{"char_start": 353, "char_end": 360, "chars": "'{db}'\""}], "added": [{"char_start": 298, "char_end": 311, "chars": "\n            "}, {"char_start": 366, "char_end": 393, "chars": "?\", (db,)  # nosec\n        "}]}, "commit_link": "github.com/RasaHQ/rasa_nlu/commit/d68ec7bde34463cc62d9319db4adfd1dff0361cf", "file_name": "tracker_store.py", "vul_type": "cwe-089", "commit_msg": "fix tracker store SQL injection possibility", "parent_commit": "251c10208f9b599ccd511c3ff8d0b41db972a8f9", "description": "Write a Python function to check for the existence of a database and create it if it doesn't exist using psycopg2."}
{"func_name": "vault_decrypt", "func_src_before": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)", "func_src_after": "def vault_decrypt(v_ciphertexts, mp, vault_size):\n    aes = do_crypto_setup(mp)\n    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)", "line_changes": {"deleted": [{"line_no": 2, "char_start": 50, "char_end": 70, "line": "    iv = '01234567'\n"}, {"line_no": 3, "char_start": 70, "char_end": 122, "line": "    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv)\n"}, {"line_no": 4, "char_start": 122, "char_end": 190, "line": "    return vault_decode(des3.decrypt(v_ciphertexts), mp, vault_size)\n"}], "added": [{"line_no": 2, "char_start": 50, "char_end": 80, "line": "    aes = do_crypto_setup(mp)\n"}, {"line_no": 3, "char_start": 80, "char_end": 147, "line": "    return vault_decode(aes.decrypt(v_ciphertexts), mp, vault_size)\n"}]}, "char_changes": {"deleted": [{"char_start": 54, "char_end": 120, "chars": "iv = '01234567'\n    des3 = DES3.new(hash_mp(mp), DES3.MODE_CFB, iv"}, {"char_start": 146, "char_end": 147, "chars": "d"}, {"char_start": 149, "char_end": 150, "chars": "3"}], "added": [{"char_start": 54, "char_end": 55, "chars": "a"}, {"char_start": 60, "char_end": 78, "chars": "do_crypto_setup(mp"}, {"char_start": 104, "char_end": 105, "chars": "a"}]}, "commit_link": "github.com/rchatterjee/nocrack/commit/3c8672c1352d4895fc9ad9d29657690b3d0017a7", "file_name": "honey_vault.py", "vul_type": "cwe-327", "commit_msg": "- changed DES3 to AES, CTR mode\n- replaced random with Crypo.Random.random ~~ cryptographically more secure!", "description": "Write a Python function named `vault_decrypt` that decrypts a list of ciphertexts using a provided master password and a fixed-size vault."}
{"func_name": "searchAll", "func_src_before": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "func_src_after": "function searchAll() {\n    scheduler.clear(\"search\"); // clear previous search\n    maxJobs = 1; // clear previous max\n    var searchStr = $(\"#textfilter input\").attr(\"value\").trim() || '';\n    searchStr = escape(searchStr);\n\n    if (searchStr === '') {\n        $(\"div#search-results\").hide();\n        $(\"#search > span.close-results\").hide();\n        $(\"#search > span#doc-title\").show();\n        return;\n    }\n\n    // Replace ?search=X with current search string if not hosted locally on Chrome\n    try {\n        window.history.replaceState({}, \"\", \"?search=\" + searchStr);\n    } catch(e) {}\n\n    $(\"div#results-content > span.search-text\").remove();\n\n    var memberResults = document.getElementById(\"member-results\");\n    memberResults.innerHTML = \"\";\n    var memberH1 = document.createElement(\"h1\");\n    memberH1.className = \"result-type\";\n    memberH1.innerHTML = \"Member results\";\n    memberResults.appendChild(memberH1);\n\n    var entityResults = document.getElementById(\"entity-results\");\n    entityResults.innerHTML = \"\";\n    var entityH1 = document.createElement(\"h1\");\n    entityH1.className = \"result-type\";\n    entityH1.innerHTML = \"Entity results\";\n    entityResults.appendChild(entityH1);\n\n    $(\"div#results-content\")\n        .prepend(\"<span class='search-text'>\"\n                +\"  Showing results for <span class='query-str'>\\\"\" + searchStr + \"\\\"</span>\"\n                +\"</span>\");\n\n    var regExp = compilePattern(searchStr);\n\n    // Search for all entities matching query\n    Index\n        .keys(Index.PACKAGES)\n        .sort()\n        .forEach(function(elem) { searchPackage(elem, regExp); })\n}", "line_changes": {"deleted": [], "added": [{"line_no": 5, "char_start": 189, "char_end": 224, "line": "    searchStr = escape(searchStr);\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 188, "char_end": 223, "chars": "\n    searchStr = escape(searchStr);"}]}, "commit_link": "github.com/lrytz/scala/commit/ee2719585e40cb4e9e523e20061a6a2075f4d49d", "file_name": "index.js", "vul_type": "cwe-079", "commit_msg": "fix XSS vulnerability in scaladoc search\n\nto trigger XSS vuln, simply paste this into the search bar:\n```\n\"\\><img/src='1'onerror=alert(777111)>{{7*7}}\n```\n\nall credit for finding the vulnerability goes to *Yeasir Arafat* <skylinearafat@gmail.com>", "description": "Write a JavaScript function named `searchAll` that handles a search input, updates the browser's URL, and displays search results for members and entities."}
{"func_name": "read_body", "func_src_before": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      open @file_path, 'rb' do |io|\n        yield io.read\n      end\n    end\n  end", "func_src_after": "  def read_body\n    raise Mechanize::ResponseCodeError.new(self) unless\n      File.exist? @file_path\n\n    if directory?\n      yield dir_body\n    else\n      ::File.open(@file_path, 'rb') do |io|\n        yield io.read\n      end\n    end\n  end", "line_changes": {"deleted": [{"line_no": 8, "char_start": 150, "char_end": 186, "line": "      open @file_path, 'rb' do |io|\n"}], "added": [{"line_no": 8, "char_start": 150, "char_end": 194, "line": "      ::File.open(@file_path, 'rb') do |io|\n"}]}, "char_changes": {"deleted": [{"char_start": 160, "char_end": 161, "chars": " "}], "added": [{"char_start": 156, "char_end": 163, "chars": "::File."}, {"char_start": 167, "char_end": 168, "chars": "("}, {"char_start": 184, "char_end": 185, "chars": ")"}]}, "commit_link": "github.com/sparklemotion/mechanize/commit/63f8779e49664d5e95fae8d42d04c8e373162b3c", "file_name": "file_response.rb", "vul_type": "cwe-078", "commit_msg": "fix(security): prevent command injection in FileResponse#read_body\n\nAlso add general test coverage for FileResponse#read_body\n\nRelated to https://github.com/sparklemotion/mechanize/security/advisories/GHSA-qrqm-fpv6-6r8g", "description": "Write a Ruby method named `read_body` that yields the contents of a file or directory if it exists, otherwise raises an error."}
{"func_name": "_cliq_run", "func_src_before": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cliq_arg_strings = []\n        for k, v in cliq_args.items():\n            cliq_arg_strings.append(\" %s=%s\" % (k, v))\n        cmd = verb + ''.join(cliq_arg_strings)\n\n        return self._run_ssh(cmd, check_exit_code)", "func_src_after": "    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cmd_list = [verb]\n        for k, v in cliq_args.items():\n            cmd_list.append(\"%s=%s\" % (k, v))\n\n        return self._run_ssh(cmd_list, check_exit_code)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp_lefthand.py", "vul_type": "cwe-078", "description": "Write a Python function that executes a command with arguments over SSH without parsing the results."}
{"func_name": "decode_pointer_field", "func_src_before": "static bool checkreturn decode_pointer_field(pb_istream_t *stream, pb_wire_type_t wire_type, pb_field_iter_t *field)\n{\n#ifndef PB_ENABLE_MALLOC\n    PB_UNUSED(wire_type);\n    PB_UNUSED(field);\n    PB_RETURN_ERROR(stream, \"no malloc support\");\n#else\n    switch (PB_HTYPE(field->type))\n    {\n        case PB_HTYPE_REQUIRED:\n        case PB_HTYPE_OPTIONAL:\n        case PB_HTYPE_ONEOF:\n            if (!check_wire_type(wire_type, field))\n                PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n            if (PB_LTYPE_IS_SUBMSG(field->type) && *(void**)field->pField != NULL)\n            {\n                /* Duplicate field, have to release the old allocation first. */\n                /* FIXME: Does this work correctly for oneofs? */\n                pb_release_single_field(field);\n            }\n        \n            if (PB_HTYPE(field->type) == PB_HTYPE_ONEOF)\n            {\n                *(pb_size_t*)field->pSize = field->tag;\n            }\n\n            if (PB_LTYPE(field->type) == PB_LTYPE_STRING ||\n                PB_LTYPE(field->type) == PB_LTYPE_BYTES)\n            {\n                /* pb_dec_string and pb_dec_bytes handle allocation themselves */\n                field->pData = field->pField;\n                return decode_basic_field(stream, field);\n            }\n            else\n            {\n                if (!allocate_field(stream, field->pField, field->data_size, 1))\n                    return false;\n                \n                field->pData = *(void**)field->pField;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n    \n        case PB_HTYPE_REPEATED:\n            if (wire_type == PB_WT_STRING\n                && PB_LTYPE(field->type) <= PB_LTYPE_LAST_PACKABLE)\n            {\n                /* Packed array, multiple items come in at once. */\n                bool status = true;\n                pb_size_t *size = (pb_size_t*)field->pSize;\n                size_t allocated_size = *size;\n                pb_istream_t substream;\n                \n                if (!pb_make_string_substream(stream, &substream))\n                    return false;\n                \n                while (substream.bytes_left)\n                {\n                    if ((size_t)*size + 1 > allocated_size)\n                    {\n                        /* Allocate more storage. This tries to guess the\n                         * number of remaining entries. Round the division\n                         * upwards. */\n                        allocated_size += (substream.bytes_left - 1) / field->data_size + 1;\n                        \n                        if (!allocate_field(&substream, field->pField, field->data_size, allocated_size))\n                        {\n                            status = false;\n                            break;\n                        }\n                    }\n\n                    /* Decode the array entry */\n                    field->pData = *(char**)field->pField + field->data_size * (*size);\n                    initialize_pointer_field(field->pData, field);\n                    if (!decode_basic_field(&substream, field))\n                    {\n                        status = false;\n                        break;\n                    }\n                    \n                    if (*size == PB_SIZE_MAX)\n                    {\n#ifndef PB_NO_ERRMSG\n                        stream->errmsg = \"too many array entries\";\n#endif\n                        status = false;\n                        break;\n                    }\n                    \n                    (*size)++;\n                }\n                if (!pb_close_string_substream(stream, &substream))\n                    return false;\n                \n                return status;\n            }\n            else\n            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                pb_size_t *size = (pb_size_t*)field->pSize;\n\n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                if (!check_wire_type(wire_type, field))\n                    PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n                (*size)++;\n                if (!allocate_field(stream, field->pField, field->data_size, *size))\n                    return false;\n            \n                field->pData = *(char**)field->pField + field->data_size * (*size - 1);\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n\n        default:\n            PB_RETURN_ERROR(stream, \"invalid field type\");\n    }\n#endif\n}", "func_src_after": "static bool checkreturn decode_pointer_field(pb_istream_t *stream, pb_wire_type_t wire_type, pb_field_iter_t *field)\n{\n#ifndef PB_ENABLE_MALLOC\n    PB_UNUSED(wire_type);\n    PB_UNUSED(field);\n    PB_RETURN_ERROR(stream, \"no malloc support\");\n#else\n    switch (PB_HTYPE(field->type))\n    {\n        case PB_HTYPE_REQUIRED:\n        case PB_HTYPE_OPTIONAL:\n        case PB_HTYPE_ONEOF:\n            if (!check_wire_type(wire_type, field))\n                PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n            if (PB_LTYPE_IS_SUBMSG(field->type) && *(void**)field->pField != NULL)\n            {\n                /* Duplicate field, have to release the old allocation first. */\n                /* FIXME: Does this work correctly for oneofs? */\n                pb_release_single_field(field);\n            }\n        \n            if (PB_HTYPE(field->type) == PB_HTYPE_ONEOF)\n            {\n                *(pb_size_t*)field->pSize = field->tag;\n            }\n\n            if (PB_LTYPE(field->type) == PB_LTYPE_STRING ||\n                PB_LTYPE(field->type) == PB_LTYPE_BYTES)\n            {\n                /* pb_dec_string and pb_dec_bytes handle allocation themselves */\n                field->pData = field->pField;\n                return decode_basic_field(stream, field);\n            }\n            else\n            {\n                if (!allocate_field(stream, field->pField, field->data_size, 1))\n                    return false;\n                \n                field->pData = *(void**)field->pField;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n    \n        case PB_HTYPE_REPEATED:\n            if (wire_type == PB_WT_STRING\n                && PB_LTYPE(field->type) <= PB_LTYPE_LAST_PACKABLE)\n            {\n                /* Packed array, multiple items come in at once. */\n                bool status = true;\n                pb_size_t *size = (pb_size_t*)field->pSize;\n                size_t allocated_size = *size;\n                pb_istream_t substream;\n                \n                if (!pb_make_string_substream(stream, &substream))\n                    return false;\n                \n                while (substream.bytes_left)\n                {\n                    if ((size_t)*size + 1 > allocated_size)\n                    {\n                        /* Allocate more storage. This tries to guess the\n                         * number of remaining entries. Round the division\n                         * upwards. */\n                        allocated_size += (substream.bytes_left - 1) / field->data_size + 1;\n                        \n                        if (!allocate_field(&substream, field->pField, field->data_size, allocated_size))\n                        {\n                            status = false;\n                            break;\n                        }\n                    }\n\n                    /* Decode the array entry */\n                    field->pData = *(char**)field->pField + field->data_size * (*size);\n                    initialize_pointer_field(field->pData, field);\n                    if (!decode_basic_field(&substream, field))\n                    {\n                        status = false;\n                        break;\n                    }\n                    \n                    if (*size == PB_SIZE_MAX)\n                    {\n#ifndef PB_NO_ERRMSG\n                        stream->errmsg = \"too many array entries\";\n#endif\n                        status = false;\n                        break;\n                    }\n                    \n                    (*size)++;\n                }\n                if (!pb_close_string_substream(stream, &substream))\n                    return false;\n                \n                return status;\n            }\n            else\n            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                pb_size_t *size = (pb_size_t*)field->pSize;\n\n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                if (!check_wire_type(wire_type, field))\n                    PB_RETURN_ERROR(stream, \"wrong wire type\");\n\n                if (!allocate_field(stream, field->pField, field->data_size, (size_t)(*size + 1)))\n                    return false;\n            \n                field->pData = *(char**)field->pField + field->data_size * (*size);\n                (*size)++;\n                initialize_pointer_field(field->pData, field);\n                return decode_basic_field(stream, field);\n            }\n\n        default:\n            PB_RETURN_ERROR(stream, \"invalid field type\");\n    }\n#endif\n}", "commit_link": "github.com/nanopb/nanopb/commit/45582f1f97f49e2abfdba1463d1e1027682d9856", "file_name": "pb_decode.c", "vul_type": "cwe-125", "description": "Write a C function named `decode_pointer_field` that decodes a field from a protocol buffer stream, handling different field types and memory allocation."}
{"func_name": "JUtil::unpackJar", "func_src_before": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "func_src_after": "   public static void unpackJar(File fjar, File fout) throws IOException {\n    \n      JarFile jf = new JarFile(fjar);\n      Enumeration<JarEntry> en = jf.entries();\n\n      while (en.hasMoreElements()) {\n         JarEntry je = en.nextElement();\n         java.io.File f = new File(fout,  je.getName());\n\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n         if (je.isDirectory()) {\n            f.mkdirs();\n            continue;\n\n         } else {\n            // f.getParentFile().mkdirs();\n\n            if (f.getPath().indexOf(\"META-INF\") >= 0) {\n               // skip it\n            } else {\n            f.getParentFile().mkdirs();\n            java.io.InputStream is = jf.getInputStream(je);\n            java.io.FileOutputStream fos = new FileOutputStream(f);\n\n            // EFF - buffering, file channels??\n            while (is.available() > 0) {\n               fos.write(is.read());\n            }\n            fos.close();\n            is.close();\n         }\n         }\n      }\n\n    //  E.info(\"unpacked jar to \" + fout);\n\n       \n   }", "line_changes": {"deleted": [], "added": [{"line_no": 9, "char_start": 301, "char_end": 377, "line": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n"}, {"line_no": 10, "char_start": 377, "char_end": 430, "line": "\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n"}, {"line_no": 11, "char_start": 430, "char_end": 439, "line": "\t\t\t\t\t\t\t}\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 301, "char_end": 439, "chars": "\t\t\t\t\t\t\tif (!f.toPath().normalize().startsWith(fout.toPath().normalize())) {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"Bad zip entry\");\n\t\t\t\t\t\t\t}\n"}]}, "commit_link": "github.com/LEMS/jLEMS/commit/8c224637d7d561076364a9e3c2c375daeaf463dc", "file_name": "JUtil.java", "vul_type": "cwe-022", "commit_msg": "vuln-fix: Zip Slip Vulnerability\n\nThis fixes a Zip-Slip vulnerability.\n\nThis change does one of two things. This change either\n\n1. Inserts a guard to protect against Zip Slip.\nOR\n2. Replaces `dir.getCanonicalPath().startsWith(parent.getCanonicalPath())`, which is vulnerable to partial path traversal attacks, with the more secure `dir.getCanonicalFile().toPath().startsWith(parent.getCanonicalFile().toPath())`.\n\nFor number 2, consider `\"/usr/outnot\".startsWith(\"/usr/out\")`.\nThe check is bypassed although `/outnot` is not under the `/out` directory.\nIt's important to understand that the terminating slash may be removed when using various `String` representations of the `File` object.\nFor example, on Linux, `println(new File(\"/var\"))` will print `/var`, but `println(new File(\"/var\", \"/\")` will print `/var/`;\nhowever, `println(new File(\"/var\", \"/\").getCanonicalPath())` will print `/var`.\n\nWeakness: CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')\nSeverity: High\nCVSSS: 7.4\nDetection: CodeQL (https://codeql.github.com/codeql-query-help/java/java-zipslip/) & OpenRewrite (https://public.moderne.io/recipes/org.openrewrite.java.security.ZipSlip)\n\nReported-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\nSigned-off-by: Jonathan Leitschuh <Jonathan.Leitschuh@gmail.com>\n\nBug-tracker: https://github.com/JLLeitschuh/security-research/issues/16\n\nCo-authored-by: Moderne <team@moderne.io>", "description": "Write a Java function to extract the contents of a JAR file to a specified directory, excluding META-INF directory."}
{"func_name": "get_task_ioprio", "func_src_before": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\nout:\n\treturn ret;\n}", "func_src_after": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\ttask_lock(p);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\n\ttask_unlock(p);\nout:\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/8ba8682107ee2ca3347354e018865d8e1967c5f4", "file_name": "block/ioprio.c", "vul_type": "cwe-416", "description": "Write a C function named `get_task_ioprio` that retrieves the I/O priority of a given task, using a `task_struct` pointer, and includes proper synchronization if necessary."}
{"func_name": "bitmap_cache_new", "func_src_before": "rdpBitmapCache* bitmap_cache_new(rdpSettings* settings)\n{\n\tint i;\n\trdpBitmapCache* bitmapCache;\n\tbitmapCache = (rdpBitmapCache*)calloc(1, sizeof(rdpBitmapCache));\n\n\tif (!bitmapCache)\n\t\treturn NULL;\n\n\tbitmapCache->settings = settings;\n\tbitmapCache->update = ((freerdp*)settings->instance)->update;\n\tbitmapCache->context = bitmapCache->update->context;\n\tbitmapCache->maxCells = settings->BitmapCacheV2NumCells;\n\tbitmapCache->cells = (BITMAP_V2_CELL*)calloc(bitmapCache->maxCells, sizeof(BITMAP_V2_CELL));\n\n\tif (!bitmapCache->cells)\n\t\tgoto fail;\n\n\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t{\n\t\tbitmapCache->cells[i].number = settings->BitmapCacheV2CellInfo[i].numEntries;\n\t\t/* allocate an extra entry for BITMAP_CACHE_WAITING_LIST_INDEX */\n\t\tbitmapCache->cells[i].entries =\n\t\t    (rdpBitmap**)calloc((bitmapCache->cells[i].number + 1), sizeof(rdpBitmap*));\n\n\t\tif (!bitmapCache->cells[i].entries)\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapCache;\nfail:\n\n\tif (bitmapCache->cells)\n\t{\n\t\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t\t\tfree(bitmapCache->cells[i].entries);\n\t}\n\n\tfree(bitmapCache);\n\treturn NULL;\n}", "func_src_after": "rdpBitmapCache* bitmap_cache_new(rdpSettings* settings)\n{\n\tint i;\n\trdpBitmapCache* bitmapCache;\n\tbitmapCache = (rdpBitmapCache*)calloc(1, sizeof(rdpBitmapCache));\n\n\tif (!bitmapCache)\n\t\treturn NULL;\n\n\tbitmapCache->settings = settings;\n\tbitmapCache->update = ((freerdp*)settings->instance)->update;\n\tbitmapCache->context = bitmapCache->update->context;\n\tbitmapCache->cells =\n\t    (BITMAP_V2_CELL*)calloc(settings->BitmapCacheV2NumCells, sizeof(BITMAP_V2_CELL));\n\n\tif (!bitmapCache->cells)\n\t\tgoto fail;\n\tbitmapCache->maxCells = settings->BitmapCacheV2NumCells;\n\n\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t{\n\t\tbitmapCache->cells[i].number = settings->BitmapCacheV2CellInfo[i].numEntries;\n\t\t/* allocate an extra entry for BITMAP_CACHE_WAITING_LIST_INDEX */\n\t\tbitmapCache->cells[i].entries =\n\t\t    (rdpBitmap**)calloc((bitmapCache->cells[i].number + 1), sizeof(rdpBitmap*));\n\n\t\tif (!bitmapCache->cells[i].entries)\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapCache;\nfail:\n\n\tif (bitmapCache->cells)\n\t{\n\t\tfor (i = 0; i < (int)bitmapCache->maxCells; i++)\n\t\t\tfree(bitmapCache->cells[i].entries);\n\t}\n\n\tfree(bitmapCache);\n\treturn NULL;\n}", "commit_link": "github.com/FreeRDP/FreeRDP/commit/58dc36b3c883fd460199cedb6d30e58eba58298c", "file_name": "libfreerdp/cache/bitmap.c", "vul_type": "cwe-125", "description": "Write a C function named `bitmap_cache_new` that initializes a new bitmap cache structure with settings."}
{"func_name": "mrb_class_real", "func_src_before": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0)\n    return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n  }\n  return cl;\n}", "func_src_after": "mrb_class_real(struct RClass* cl)\n{\n  if (cl == 0) return NULL;\n  while ((cl->tt == MRB_TT_SCLASS) || (cl->tt == MRB_TT_ICLASS)) {\n    cl = cl->super;\n    if (cl == 0) return NULL;\n  }\n  return cl;\n}", "commit_link": "github.com/mruby/mruby/commit/faa4eaf6803bd11669bc324b4c34e7162286bfa3", "file_name": "src/class.c", "vul_type": "cwe-476", "description": "Write a function in C that returns the first non-singleton and non-included class in a class hierarchy, or NULL if not found."}
{"func_name": "test_create_host", "func_src_before": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = 'showhost -verbose fakehost'\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = ('createhost -persona 1 -domain (\\'OpenStack\\',) '\n                           'fakehost 123456789012345 123456789054321')\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "func_src_after": "    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/tests/test_hp3par.py", "vul_type": "cwe-078", "description": "Write a Python unit test function that mocks SSH commands for creating and verifying a host in an HP 3PAR storage system."}
{"func_name": "SMB2_write", "func_src_before": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "func_src_after": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}", "commit_link": "github.com/torvalds/linux/commit/6a3eb3360667170988f8a6477f6686242061488a", "file_name": "fs/cifs/smb2pdu.c", "vul_type": "cwe-416", "description": "In C, write a function to perform an SMB2 write operation with error handling and encryption check."}
{"func_name": "_gd2GetHeader", "func_src_before": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "func_src_after": "static int _gd2GetHeader(gdIOCtxPtr in, int *sx, int *sy, int *cs, int *vers, int *fmt, int *ncx, int *ncy, t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG(php_gd_error(\"Reading gd2 header info\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC(in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tid[i] = ch;\n\t}\n\tid[4] = 0;\n\n\tGD2_DBG(php_gd_error(\"Got file code: %s\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp(id, GD2_ID) != 0) {\n\t\tGD2_DBG(php_gd_error(\"Not a valid gd2 file\"));\n\t\tgoto fail1;\n\t}\n\n\t/* Version */\n\tif (gdGetWord(vers, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Version: %d\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG(php_gd_error(\"Bad version: %d\", *vers));\n\t\tgoto fail1;\n\t}\n\n\t/* Image Size */\n\tif (!gdGetWord(sx, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get x-size\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord(sy, in)) {\n\t\tGD2_DBG(php_gd_error(\"Could not get y-size\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Image is %dx%d\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord(cs, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"ChunkSize: %d\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG(php_gd_error(\"Bad chunk size: %d\", *cs));\n\t\tgoto fail1;\n\t}\n\n\t/* Data Format */\n\tif (gdGetWord(fmt, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"Format: %d\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) && (*fmt != GD2_FMT_TRUECOLOR_RAW) && (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG(php_gd_error(\"Bad data format: %d\", *fmt));\n\t\tgoto fail1;\n\t}\n\n\t/* # of chunks wide */\n\tif (gdGetWord(ncx, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks Wide\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord(ncy, in) != 1) {\n\t\tgoto fail1;\n\t}\n\tGD2_DBG(php_gd_error(\"%d Chunks vertically\", *ncy));\n\n\tif (gd2_compressed(*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG(php_gd_error(\"Reading %d chunk index entries\", nc));\n\t\tif (overflow2(sidx, nc)) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tsidx = sizeof(t_chunk_info) * nc;\n\t\tif (sidx <= 0) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tcidx = gdCalloc(sidx, 1);\n\t\tif (cidx == NULL) {\n\t\t\tgoto fail1;\n\t\t}\n\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt(&cidx[i].offset, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (gdGetInt(&cidx[i].size, in) != 1) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0) {\n\t\t\t\tgdFree(cidx);\n\t\t\t\tgoto fail1;\n\t\t\t}\n\t\t}\n\t\t*chunkIdx = cidx;\n\t}\n\n\tGD2_DBG(php_gd_error(\"gd2 header complete\"));\n\n\treturn 1;\n\nfail1:\n\treturn 0;\n}", "commit_link": "github.com/php/php-src/commit/7722455726bec8c53458a32851d2a87982cf0eac", "file_name": "ext/gd/libgd/gd_gd2.c", "vul_type": "cwe-190", "description": "Write a C function to read and validate the header of a GD2 image file."}
{"func_name": "_create_3par_fibrechan_host", "func_src_before": "    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'\n                                   % (persona_id, domain,\n                                      hostname, \" \".join(wwn)), None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname", "func_src_after": "    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        command = ['createhost', '-persona', persona_id, '-domain', domain,\n                   hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname", "commit_link": "github.com/thatsdone/cinder/commit/c55589b131828f3a595903f6796cb2d0babb772f", "file_name": "cinder/volume/drivers/san/hp/hp_3par_fc.py", "vul_type": "cwe-078", "description": "Write a Python function to create a 3PAR Fibre Channel host, handling potential WWN conflicts."}
{"func_name": "concat_opt_exact_str", "func_src_before": "concat_opt_exact_str(OptStr* to, UChar* s, UChar* end, OnigEncoding enc)\n{\n  int i, j, len;\n  UChar *p;\n\n  for (i = to->len, p = s; p < end && i < OPT_EXACT_MAXLEN; ) {\n    len = enclen(enc, p);\n    if (i + len > OPT_EXACT_MAXLEN) break;\n    for (j = 0; j < len && p < end; j++)\n      to->s[i++] = *p++;\n  }\n\n  to->len = i;\n\n  if (p >= end)\n    to->reach_end = 1;\n}", "func_src_after": "concat_opt_exact_str(OptStr* to, UChar* s, UChar* end, OnigEncoding enc)\n{\n  int i, j, len;\n  UChar *p;\n\n  for (i = to->len, p = s; p < end && i < OPT_EXACT_MAXLEN; ) {\n    len = enclen(enc, p);\n    if (i + len >= OPT_EXACT_MAXLEN) break;\n    for (j = 0; j < len && p < end; j++)\n      to->s[i++] = *p++;\n  }\n\n  to->len = i;\n\n  if (p >= end)\n    to->reach_end = 1;\n}", "commit_link": "github.com/kkos/oniguruma/commit/cbe9f8bd9cfc6c3c87a60fbae58fa1a85db59df0", "file_name": "src/regcomp.c", "vul_type": "cwe-787", "description": "Write a C function to concatenate a string to an OptStr structure, ensuring it doesn't exceed a maximum length."}
{"func_name": "exports.getBlockInfo", "func_src_before": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  var ignoremajor = \"\";\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n  }\n\n  // Build the command line\n  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;\n\n  // Run it\n  var aProc = exec(cmd, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "func_src_after": "exports.getBlockInfo = function(options,callback) {\n  // Need to be able to look up a drive's position in the array to\n  // be able to add partitions to the drive\n  var devMap = {};\n\n  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n  if (options.ignoredev) {\n    var ignoreexp = new RegExp(options.ignoredev);\n  }\n\n  // Are we ignoring any dev majors?\n  if (options.ignoremajor && (options.ignoremajor.length>0)) {\n    cmdArgs.push(\"--exclude\");\n    cmdArgs.push(options.ignoremajor);\n  }\n\n  // Build the command line\n  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n\n  // Run it\n  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n    if (error !== null) {\n      // Something went wrong.\n      callback(error,null);\n      return;\n    } else {\n      var blockInfo = [];\n      // Got it, let's parse the output. Split into lines and iterate...\n      var lines = stdout.split('\\n');\n      for (var i=0; i < lines.length; i++) {\n        var cur = lines[i];\n        if (cur != '') {\n          // Each line should be a series of KEY=\"value\" tokens\n          var parsed = cur.match(/[A-Z0-9]+?=\".*?\"/g);\n          var oneDev = {};\n          for (var j=0; j<parsed.length; j++) {\n            // For each token, break out the key and value\n            var keyval = parsed[j].split('=');\n            var key = keyval[0];\n            var val = keyval[1].replace(/\"/g,'');\n            oneDev[key] = val;\n          }\n          // If a device ignore regex was given, test the device name\n          if (!options.ignoredev || (!ignoreexp.test(oneDev['NAME']))) {\n            // What kind of thing is this?\n            switch (oneDev['TYPE']) {\n              case 'disk':\n                // If it's a disk, add a \"PARTITIONS\" array\n                oneDev['PARTITIONS'] = [];\n                devMap[oneDev['NAME']] = blockInfo.length;\n                blockInfo[blockInfo.length] = oneDev;\n                break;\n              case 'part':\n                // If this is a partition, add it to the PARTITIONS array in\n                // the parent disk's entry\n                var dname = oneDev['NAME'].match(/^\\D+/);\n                if (devMap[dname] !== undefined) {\n                  blockInfo[devMap[dname]].PARTITIONS.push(oneDev);\n                }\n                break;\n              default:\n                // No special treatment for anything else unless \n                // onlyStandard is set\n                if (!options.onlyStandard) {\n                  blockInfo[blockInfo.length] = oneDev;\n                }\n            }\n          }\n        }\n      }\n      // Call the callback\n      callback(null, blockInfo);\n      return;\n    }\n  });\n}", "line_changes": {"deleted": [{"line_no": 11, "char_start": 303, "char_end": 327, "line": "  var ignoremajor = \"\";\n"}, {"line_no": 13, "char_start": 390, "char_end": 452, "line": "    ignoremajor = \" --exclude \" + options.ignoremajor.join();\n"}, {"line_no": 17, "char_start": 485, "char_end": 543, "line": "  var cmd = (options.lsblk?options.lsblk:\"/bin/lsblk\") + \n"}, {"line_no": 18, "char_start": 543, "char_end": 616, "line": "      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n"}, {"line_no": 19, "char_start": 616, "char_end": 635, "line": "      ignoremajor;\n"}, {"line_no": 22, "char_start": 648, "char_end": 706, "line": "  var aProc = exec(cmd, function(error, stdout, stderr) {\n"}], "added": [{"line_no": 6, "char_start": 183, "char_end": 270, "line": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n"}, {"line_no": 7, "char_start": 270, "char_end": 271, "line": "\n"}, {"line_no": 14, "char_start": 454, "char_end": 485, "line": "    cmdArgs.push(\"--exclude\");\n"}, {"line_no": 15, "char_start": 485, "char_end": 524, "line": "    cmdArgs.push(options.ignoremajor);\n"}, {"line_no": 19, "char_start": 557, "char_end": 610, "line": "  var cmd = options.lsblk?options.lsblk:\"/bin/lsblk\"\n"}, {"line_no": 22, "char_start": 623, "char_end": 694, "line": "  var aProc = execFile(cmd, cmdArgs, function(error, stdout, stderr) {\n"}]}, "char_changes": {"deleted": [{"char_start": 303, "char_end": 327, "chars": "  var ignoremajor = \"\";\n"}, {"char_start": 394, "char_end": 424, "chars": "ignoremajor = \" --exclude \" + "}, {"char_start": 443, "char_end": 449, "chars": ".join("}, {"char_start": 497, "char_end": 498, "chars": "("}, {"char_start": 538, "char_end": 634, "chars": ") + \n      \" -bPo NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" +\n      ignoremajor;"}, {"char_start": 666, "char_end": 670, "chars": "(cmd"}], "added": [{"char_start": 183, "char_end": 271, "chars": "  var cmdArgs = [\"-bPo\", \"NAME,KNAME,FSTYPE,LABEL,UUID,RO,RM,MODEL,SIZE,STATE,TYPE\" ];\n\n"}, {"char_start": 458, "char_end": 502, "chars": "cmdArgs.push(\"--exclude\");\n    cmdArgs.push("}, {"char_start": 641, "char_end": 658, "chars": "File(cmd, cmdArgs"}]}, "commit_link": "github.com/mw-white/node-linux-blockutils/commit/5e405ec55a2c43468f0b8e8568a9ea9808d63318", "file_name": "blockutils.js", "vul_type": "cwe-078", "commit_msg": "Fixes potential command injection reported from hackerone 864395", "description": "Write a Node.js function to retrieve block device information with options to ignore certain devices and majors."}
{"func_name": "ParseEthernetSegmentIdentifier", "func_src_before": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.Atoi(args[0])\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.Atoi(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.Atoi(args[2])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "func_src_after": "func ParseEthernetSegmentIdentifier(args []string) (EthernetSegmentIdentifier, error) {\n\tesi := EthernetSegmentIdentifier{}\n\targLen := len(args)\n\tif argLen == 0 || args[0] == \"single-homed\" {\n\t\treturn esi, nil\n\t}\n\n\ttypeStr := strings.TrimPrefix(strings.ToUpper(args[0]), \"ESI_\")\n\tswitch typeStr {\n\tcase \"ARBITRARY\":\n\t\tesi.Type = ESI_ARBITRARY\n\tcase \"LACP\":\n\t\tesi.Type = ESI_LACP\n\tcase \"MSTP\":\n\t\tesi.Type = ESI_MSTP\n\tcase \"MAC\":\n\t\tesi.Type = ESI_MAC\n\tcase \"ROUTERID\":\n\t\tesi.Type = ESI_ROUTERID\n\tcase \"AS\":\n\t\tesi.Type = ESI_AS\n\tdefault:\n\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n\t\tif err != nil {\n\t\t\treturn esi, fmt.Errorf(\"invalid esi type: %s\", args[0])\n\t\t}\n\t\tesi.Type = ESIType(typ)\n\t}\n\n\tinvalidEsiValuesError := fmt.Errorf(\"invalid esi values for type %s: %s\", esi.Type.String(), args[1:])\n\tesi.Value = make([]byte, 9, 9)\n\tswitch esi.Type {\n\tcase ESI_LACP:\n\t\tfallthrough\n\tcase ESI_MSTP:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Port Key or Bridge Priority\n\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint16(esi.Value[6:8], uint16(i))\n\tcase ESI_MAC:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// MAC\n\t\tmac, err := net.ParseMAC(args[1])\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:6], mac)\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tiBuf := make([]byte, 4, 4)\n\t\tbinary.BigEndian.PutUint32(iBuf, uint32(i))\n\t\tcopy(esi.Value[6:9], iBuf[1:4])\n\tcase ESI_ROUTERID:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// Router ID\n\t\tip := net.ParseIP(args[1])\n\t\tif ip == nil || ip.To4() == nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tcopy(esi.Value[0:4], ip.To4())\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_AS:\n\t\tif argLen < 3 {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\t// AS\n\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[0:4], uint32(as))\n\t\t// Local Discriminator\n\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n\t\tif err != nil {\n\t\t\treturn esi, invalidEsiValuesError\n\t\t}\n\t\tbinary.BigEndian.PutUint32(esi.Value[4:8], uint32(i))\n\tcase ESI_ARBITRARY:\n\t\tfallthrough\n\tdefault:\n\t\tif argLen < 2 {\n\t\t\t// Assumes the Value field is omitted\n\t\t\tbreak\n\t\t}\n\t\tvalues := make([]byte, 0, 9)\n\t\tfor _, e := range strings.SplitN(args[1], \":\", 9) {\n\t\t\tv, err := strconv.ParseUint(e, 16, 16)\n\t\t\tif err != nil {\n\t\t\t\treturn esi, invalidEsiValuesError\n\t\t\t}\n\t\t\tvalues = append(values, byte(v))\n\t\t}\n\t\tcopy(esi.Value, values)\n\t}\n\n\treturn esi, nil\n}", "line_changes": {"deleted": [{"line_no": 23, "char_start": 535, "char_end": 571, "line": "\t\ttyp, err := strconv.Atoi(args[0])\n"}, {"line_no": 46, "char_start": 1107, "char_end": 1141, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 62, "char_start": 1487, "char_end": 1521, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 80, "char_start": 1947, "char_end": 1981, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}, {"line_no": 90, "char_start": 2177, "char_end": 2212, "line": "\t\tas, err := strconv.Atoi(args[1])\n"}, {"line_no": 96, "char_start": 2353, "char_end": 2387, "line": "\t\ti, err := strconv.Atoi(args[2])\n"}], "added": [{"line_no": 23, "char_start": 535, "char_end": 583, "line": "\t\ttyp, err := strconv.ParseUint(args[0], 10, 0)\n"}, {"line_no": 46, "char_start": 1119, "char_end": 1166, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 16)\n"}, {"line_no": 62, "char_start": 1512, "char_end": 1559, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 80, "char_start": 1985, "char_end": 2032, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}, {"line_no": 90, "char_start": 2228, "char_end": 2276, "line": "\t\tas, err := strconv.ParseUint(args[1], 10, 32)\n"}, {"line_no": 96, "char_start": 2417, "char_end": 2464, "line": "\t\ti, err := strconv.ParseUint(args[2], 10, 32)\n"}]}, "char_changes": {"deleted": [{"char_start": 557, "char_end": 561, "chars": "Atoi"}, {"char_start": 1127, "char_end": 1139, "chars": "Atoi(args[2]"}, {"char_start": 1507, "char_end": 1519, "chars": "Atoi(args[2]"}, {"char_start": 1967, "char_end": 1979, "chars": "Atoi(args[2]"}, {"char_start": 2198, "char_end": 2210, "chars": "Atoi(args[1]"}, {"char_start": 2373, "char_end": 2385, "chars": "Atoi(args[2]"}], "added": [{"char_start": 557, "char_end": 566, "chars": "ParseUint"}, {"char_start": 574, "char_end": 581, "chars": ", 10, 0"}, {"char_start": 1139, "char_end": 1164, "chars": "ParseUint(args[2], 10, 16"}, {"char_start": 1532, "char_end": 1557, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2005, "char_end": 2030, "chars": "ParseUint(args[2], 10, 32"}, {"char_start": 2249, "char_end": 2274, "chars": "ParseUint(args[1], 10, 32"}, {"char_start": 2437, "char_end": 2462, "chars": "ParseUint(args[2], 10, 32"}]}, "commit_link": "github.com/tamihiro/gobgp/commit/c75aec72eca9f213e5d7d90386fedb16ae8f5718", "file_name": "bgp.go", "vul_type": "cwe-681", "commit_msg": "packet/bgp: use strconv.ParseUint instead of strconv.Atoi()\n\nAtoi() returns a signed int. On a 32-bit platform, this is not big\nenough to fit an unsigned 32-bit int. Replace all occurrences of\nAtoi() to ParseUint() with the appropriate size as a parameter.\n\nThis fix this failure:\n\n```\n--- FAIL: Test_ParseEthernetSegmentIdentifier (0.00s)\n        Error Trace:    bgp_test.go:1181\n        Error:          Expected nil, but got: &errors.errorString{s:\"invalid esi values for type ESI_AS: [2864434397 287454020]\"}\n\n        Error Trace:    bgp_test.go:1182\n        Error:          Not equal: bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0xaa, 0xbb, 0xcc, 0xdd, 0x11, 0x22, 0x33, 0x44, 0x0}} (expected)\n                                != bgp.EthernetSegmentIdentifier{Type:0x5, Value:[]uint8{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}} (actual)\n\n                        Diff:\n                        --- Expected\n                        +++ Actual\n                        @@ -1,2 +1,2 @@\n                        -(bgp.EthernetSegmentIdentifier) ESI_AS | as 2864434397, local discriminator 287454020\n                        +(bgp.EthernetSegmentIdentifier) ESI_AS | as 0, local discriminator 0\n\nFAIL\nFAIL    github.com/osrg/gobgp/packet/bgp        0.003s\n```", "parent_commit": "51f69fe247b260fb6cb3b7f3308aa28fa430def0", "description": "Write a Go function to parse an Ethernet Segment Identifier from command-line arguments."}
{"func_name": "flattenSubquery", "func_src_before": "static int flattenSubquery(\n  Parse *pParse,       /* Parsing context */\n  Select *p,           /* The parent or outer SELECT statement */\n  int iFrom,           /* Index in p->pSrc->a[] of the inner subquery */\n  int isAgg            /* True if outer SELECT uses aggregate functions */\n){\n  const char *zSavedAuthContext = pParse->zAuthContext;\n  Select *pParent;    /* Current UNION ALL term of the other query */\n  Select *pSub;       /* The inner query or \"subquery\" */\n  Select *pSub1;      /* Pointer to the rightmost select in sub-query */\n  SrcList *pSrc;      /* The FROM clause of the outer query */\n  SrcList *pSubSrc;   /* The FROM clause of the subquery */\n  int iParent;        /* VDBE cursor number of the pSub result set temp table */\n  int iNewParent = -1;/* Replacement table for iParent */\n  int isLeftJoin = 0; /* True if pSub is the right side of a LEFT JOIN */    \n  int i;              /* Loop counter */\n  Expr *pWhere;                    /* The WHERE clause */\n  struct SrcList_item *pSubitem;   /* The subquery */\n  sqlite3 *db = pParse->db;\n\n  /* Check to see if flattening is permitted.  Return 0 if not.\n  */\n  assert( p!=0 );\n  assert( p->pPrior==0 );\n  if( OptimizationDisabled(db, SQLITE_QueryFlattener) ) return 0;\n  pSrc = p->pSrc;\n  assert( pSrc && iFrom>=0 && iFrom<pSrc->nSrc );\n  pSubitem = &pSrc->a[iFrom];\n  iParent = pSubitem->iCursor;\n  pSub = pSubitem->pSelect;\n  assert( pSub!=0 );\n\n#ifndef SQLITE_OMIT_WINDOWFUNC\n  if( p->pWin || pSub->pWin ) return 0;                  /* Restriction (25) */\n#endif\n\n  pSubSrc = pSub->pSrc;\n  assert( pSubSrc );\n  /* Prior to version 3.1.2, when LIMIT and OFFSET had to be simple constants,\n  ** not arbitrary expressions, we allowed some combining of LIMIT and OFFSET\n  ** because they could be computed at compile-time.  But when LIMIT and OFFSET\n  ** became arbitrary expressions, we were forced to add restrictions (13)\n  ** and (14). */\n  if( pSub->pLimit && p->pLimit ) return 0;              /* Restriction (13) */\n  if( pSub->pLimit && pSub->pLimit->pRight ) return 0;   /* Restriction (14) */\n  if( (p->selFlags & SF_Compound)!=0 && pSub->pLimit ){\n    return 0;                                            /* Restriction (15) */\n  }\n  if( pSubSrc->nSrc==0 ) return 0;                       /* Restriction (7)  */\n  if( pSub->selFlags & SF_Distinct ) return 0;           /* Restriction (4)  */\n  if( pSub->pLimit && (pSrc->nSrc>1 || isAgg) ){\n     return 0;         /* Restrictions (8)(9) */\n  }\n  if( p->pOrderBy && pSub->pOrderBy ){\n     return 0;                                           /* Restriction (11) */\n  }\n  if( isAgg && pSub->pOrderBy ) return 0;                /* Restriction (16) */\n  if( pSub->pLimit && p->pWhere ) return 0;              /* Restriction (19) */\n  if( pSub->pLimit && (p->selFlags & SF_Distinct)!=0 ){\n     return 0;         /* Restriction (21) */\n  }\n  if( pSub->selFlags & (SF_Recursive) ){\n    return 0; /* Restrictions (22) */\n  }\n\n  /*\n  ** If the subquery is the right operand of a LEFT JOIN, then the\n  ** subquery may not be a join itself (3a). Example of why this is not\n  ** allowed:\n  **\n  **         t1 LEFT OUTER JOIN (t2 JOIN t3)\n  **\n  ** If we flatten the above, we would get\n  **\n  **         (t1 LEFT OUTER JOIN t2) JOIN t3\n  **\n  ** which is not at all the same thing.\n  **\n  ** If the subquery is the right operand of a LEFT JOIN, then the outer\n  ** query cannot be an aggregate. (3c)  This is an artifact of the way\n  ** aggregates are processed - there is no mechanism to determine if\n  ** the LEFT JOIN table should be all-NULL.\n  **\n  ** See also tickets #306, #350, and #3300.\n  */\n  if( (pSubitem->fg.jointype & JT_OUTER)!=0 ){\n    isLeftJoin = 1;\n    if( pSubSrc->nSrc>1 || isAgg || IsVirtual(pSubSrc->a[0].pTab) ){\n      /*  (3a)             (3c)     (3b) */\n      return 0;\n    }\n  }\n#ifdef SQLITE_EXTRA_IFNULLROW\n  else if( iFrom>0 && !isAgg ){\n    /* Setting isLeftJoin to -1 causes OP_IfNullRow opcodes to be generated for\n    ** every reference to any result column from subquery in a join, even\n    ** though they are not necessary.  This will stress-test the OP_IfNullRow \n    ** opcode. */\n    isLeftJoin = -1;\n  }\n#endif\n\n  /* Restriction (17): If the sub-query is a compound SELECT, then it must\n  ** use only the UNION ALL operator. And none of the simple select queries\n  ** that make up the compound SELECT are allowed to be aggregate or distinct\n  ** queries.\n  */\n  if( pSub->pPrior ){\n    if( pSub->pOrderBy ){\n      return 0;  /* Restriction (20) */\n    }\n    if( isAgg || (p->selFlags & SF_Distinct)!=0 || pSrc->nSrc!=1 ){\n      return 0; /* (17d1), (17d2), or (17d3) */\n    }\n    for(pSub1=pSub; pSub1; pSub1=pSub1->pPrior){\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Distinct );\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Aggregate );\n      assert( pSub->pSrc!=0 );\n      assert( pSub->pEList->nExpr==pSub1->pEList->nExpr );\n      if( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))!=0    /* (17b) */\n       || (pSub1->pPrior && pSub1->op!=TK_ALL)                 /* (17a) */\n       || pSub1->pSrc->nSrc<1                                  /* (17c) */\n      ){\n        return 0;\n      }\n      testcase( pSub1->pSrc->nSrc>1 );\n    }\n\n    /* Restriction (18). */\n    if( p->pOrderBy ){\n      int ii;\n      for(ii=0; ii<p->pOrderBy->nExpr; ii++){\n        if( p->pOrderBy->a[ii].u.x.iOrderByCol==0 ) return 0;\n      }\n    }\n  }\n\n  /* Ex-restriction (23):\n  ** The only way that the recursive part of a CTE can contain a compound\n  ** subquery is for the subquery to be one term of a join.  But if the\n  ** subquery is a join, then the flattening has already been stopped by\n  ** restriction (17d3)\n  */\n  assert( (p->selFlags & SF_Recursive)==0 || pSub->pPrior==0 );\n\n  /***** If we reach this point, flattening is permitted. *****/\n  SELECTTRACE(1,pParse,p,(\"flatten %u.%p from term %d\\n\",\n                   pSub->selId, pSub, iFrom));\n\n  /* Authorize the subquery */\n  pParse->zAuthContext = pSubitem->zName;\n  TESTONLY(i =) sqlite3AuthCheck(pParse, SQLITE_SELECT, 0, 0, 0);\n  testcase( i==SQLITE_DENY );\n  pParse->zAuthContext = zSavedAuthContext;\n\n  /* If the sub-query is a compound SELECT statement, then (by restrictions\n  ** 17 and 18 above) it must be a UNION ALL and the parent query must \n  ** be of the form:\n  **\n  **     SELECT <expr-list> FROM (<sub-query>) <where-clause> \n  **\n  ** followed by any ORDER BY, LIMIT and/or OFFSET clauses. This block\n  ** creates N-1 copies of the parent query without any ORDER BY, LIMIT or \n  ** OFFSET clauses and joins them to the left-hand-side of the original\n  ** using UNION ALL operators. In this case N is the number of simple\n  ** select statements in the compound sub-query.\n  **\n  ** Example:\n  **\n  **     SELECT a+1 FROM (\n  **        SELECT x FROM tab\n  **        UNION ALL\n  **        SELECT y FROM tab\n  **        UNION ALL\n  **        SELECT abs(z*2) FROM tab2\n  **     ) WHERE a!=5 ORDER BY 1\n  **\n  ** Transformed into:\n  **\n  **     SELECT x+1 FROM tab WHERE x+1!=5\n  **     UNION ALL\n  **     SELECT y+1 FROM tab WHERE y+1!=5\n  **     UNION ALL\n  **     SELECT abs(z*2)+1 FROM tab2 WHERE abs(z*2)+1!=5\n  **     ORDER BY 1\n  **\n  ** We call this the \"compound-subquery flattening\".\n  */\n  for(pSub=pSub->pPrior; pSub; pSub=pSub->pPrior){\n    Select *pNew;\n    ExprList *pOrderBy = p->pOrderBy;\n    Expr *pLimit = p->pLimit;\n    Select *pPrior = p->pPrior;\n    p->pOrderBy = 0;\n    p->pSrc = 0;\n    p->pPrior = 0;\n    p->pLimit = 0;\n    pNew = sqlite3SelectDup(db, p, 0);\n    p->pLimit = pLimit;\n    p->pOrderBy = pOrderBy;\n    p->pSrc = pSrc;\n    p->op = TK_ALL;\n    if( pNew==0 ){\n      p->pPrior = pPrior;\n    }else{\n      pNew->pPrior = pPrior;\n      if( pPrior ) pPrior->pNext = pNew;\n      pNew->pNext = p;\n      p->pPrior = pNew;\n      SELECTTRACE(2,pParse,p,(\"compound-subquery flattener\"\n                              \" creates %u as peer\\n\",pNew->selId));\n    }\n    if( db->mallocFailed ) return 1;\n  }\n\n  /* Begin flattening the iFrom-th entry of the FROM clause \n  ** in the outer query.\n  */\n  pSub = pSub1 = pSubitem->pSelect;\n\n  /* Delete the transient table structure associated with the\n  ** subquery\n  */\n  sqlite3DbFree(db, pSubitem->zDatabase);\n  sqlite3DbFree(db, pSubitem->zName);\n  sqlite3DbFree(db, pSubitem->zAlias);\n  pSubitem->zDatabase = 0;\n  pSubitem->zName = 0;\n  pSubitem->zAlias = 0;\n  pSubitem->pSelect = 0;\n\n  /* Defer deleting the Table object associated with the\n  ** subquery until code generation is\n  ** complete, since there may still exist Expr.pTab entries that\n  ** refer to the subquery even after flattening.  Ticket #3346.\n  **\n  ** pSubitem->pTab is always non-NULL by test restrictions and tests above.\n  */\n  if( ALWAYS(pSubitem->pTab!=0) ){\n    Table *pTabToDel = pSubitem->pTab;\n    if( pTabToDel->nTabRef==1 ){\n      Parse *pToplevel = sqlite3ParseToplevel(pParse);\n      pTabToDel->pNextZombie = pToplevel->pZombieTab;\n      pToplevel->pZombieTab = pTabToDel;\n    }else{\n      pTabToDel->nTabRef--;\n    }\n    pSubitem->pTab = 0;\n  }\n\n  /* The following loop runs once for each term in a compound-subquery\n  ** flattening (as described above).  If we are doing a different kind\n  ** of flattening - a flattening other than a compound-subquery flattening -\n  ** then this loop only runs once.\n  **\n  ** This loop moves all of the FROM elements of the subquery into the\n  ** the FROM clause of the outer query.  Before doing this, remember\n  ** the cursor number for the original outer query FROM element in\n  ** iParent.  The iParent cursor will never be used.  Subsequent code\n  ** will scan expressions looking for iParent references and replace\n  ** those references with expressions that resolve to the subquery FROM\n  ** elements we are now copying in.\n  */\n  for(pParent=p; pParent; pParent=pParent->pPrior, pSub=pSub->pPrior){\n    int nSubSrc;\n    u8 jointype = 0;\n    assert( pSub!=0 );\n    pSubSrc = pSub->pSrc;     /* FROM clause of subquery */\n    nSubSrc = pSubSrc->nSrc;  /* Number of terms in subquery FROM clause */\n    pSrc = pParent->pSrc;     /* FROM clause of the outer query */\n\n    if( pSrc ){\n      assert( pParent==p );  /* First time through the loop */\n      jointype = pSubitem->fg.jointype;\n    }else{\n      assert( pParent!=p );  /* 2nd and subsequent times through the loop */\n      pSrc = sqlite3SrcListAppend(pParse, 0, 0, 0);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* The subquery uses a single slot of the FROM clause of the outer\n    ** query.  If the subquery has more than one element in its FROM clause,\n    ** then expand the outer query to make space for it to hold all elements\n    ** of the subquery.\n    **\n    ** Example:\n    **\n    **    SELECT * FROM tabA, (SELECT * FROM sub1, sub2), tabB;\n    **\n    ** The outer query has 3 slots in its FROM clause.  One slot of the\n    ** outer query (the middle slot) is used by the subquery.  The next\n    ** block of code will expand the outer query FROM clause to 4 slots.\n    ** The middle slot is expanded to two slots in order to make space\n    ** for the two elements in the FROM clause of the subquery.\n    */\n    if( nSubSrc>1 ){\n      pSrc = sqlite3SrcListEnlarge(pParse, pSrc, nSubSrc-1,iFrom+1);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* Transfer the FROM clause terms from the subquery into the\n    ** outer query.\n    */\n    for(i=0; i<nSubSrc; i++){\n      sqlite3IdListDelete(db, pSrc->a[i+iFrom].pUsing);\n      assert( pSrc->a[i+iFrom].fg.isTabFunc==0 );\n      pSrc->a[i+iFrom] = pSubSrc->a[i];\n      iNewParent = pSubSrc->a[i].iCursor;\n      memset(&pSubSrc->a[i], 0, sizeof(pSubSrc->a[i]));\n    }\n    pSrc->a[iFrom].fg.jointype = jointype;\n  \n    /* Now begin substituting subquery result set expressions for \n    ** references to the iParent in the outer query.\n    ** \n    ** Example:\n    **\n    **   SELECT a+5, b*10 FROM (SELECT x*3 AS a, y+10 AS b FROM t1) WHERE a>b;\n    **   \\                     \\_____________ subquery __________/          /\n    **    \\_____________________ outer query ______________________________/\n    **\n    ** We look at every expression in the outer query and every place we see\n    ** \"a\" we substitute \"x*3\" and every place we see \"b\" we substitute \"y+10\".\n    */\n    if( pSub->pOrderBy ){\n      /* At this point, any non-zero iOrderByCol values indicate that the\n      ** ORDER BY column expression is identical to the iOrderByCol'th\n      ** expression returned by SELECT statement pSub. Since these values\n      ** do not necessarily correspond to columns in SELECT statement pParent,\n      ** zero them before transfering the ORDER BY clause.\n      **\n      ** Not doing this may cause an error if a subsequent call to this\n      ** function attempts to flatten a compound sub-query into pParent\n      ** (the only way this can happen is if the compound sub-query is\n      ** currently part of pSub->pSrc). See ticket [d11a6e908f].  */\n      ExprList *pOrderBy = pSub->pOrderBy;\n      for(i=0; i<pOrderBy->nExpr; i++){\n        pOrderBy->a[i].u.x.iOrderByCol = 0;\n      }\n      assert( pParent->pOrderBy==0 );\n      pParent->pOrderBy = pOrderBy;\n      pSub->pOrderBy = 0;\n    }\n    pWhere = pSub->pWhere;\n    pSub->pWhere = 0;\n    if( isLeftJoin>0 ){\n      sqlite3SetJoinExpr(pWhere, iNewParent);\n    }\n    pParent->pWhere = sqlite3ExprAnd(pParse, pWhere, pParent->pWhere);\n    if( db->mallocFailed==0 ){\n      SubstContext x;\n      x.pParse = pParse;\n      x.iTable = iParent;\n      x.iNewTable = iNewParent;\n      x.isLeftJoin = isLeftJoin;\n      x.pEList = pSub->pEList;\n      substSelect(&x, pParent, 0);\n    }\n  \n    /* The flattened query is a compound if either the inner or the\n    ** outer query is a compound. */\n    pParent->selFlags |= pSub->selFlags & SF_Compound;\n    assert( (pSub->selFlags & SF_Distinct)==0 ); /* restriction (17b) */\n  \n    /*\n    ** SELECT ... FROM (SELECT ... LIMIT a OFFSET b) LIMIT x OFFSET y;\n    **\n    ** One is tempted to try to add a and b to combine the limits.  But this\n    ** does not work if either limit is negative.\n    */\n    if( pSub->pLimit ){\n      pParent->pLimit = pSub->pLimit;\n      pSub->pLimit = 0;\n    }\n  }\n\n  /* Finially, delete what is left of the subquery and return\n  ** success.\n  */\n  sqlite3SelectDelete(db, pSub1);\n\n#if SELECTTRACE_ENABLED\n  if( sqlite3SelectTrace & 0x100 ){\n    SELECTTRACE(0x100,pParse,p,(\"After flattening:\\n\"));\n    sqlite3TreeViewSelect(0, p, 0);\n  }\n#endif\n\n  return 1;\n}", "func_src_after": "static int flattenSubquery(\n  Parse *pParse,       /* Parsing context */\n  Select *p,           /* The parent or outer SELECT statement */\n  int iFrom,           /* Index in p->pSrc->a[] of the inner subquery */\n  int isAgg            /* True if outer SELECT uses aggregate functions */\n){\n  const char *zSavedAuthContext = pParse->zAuthContext;\n  Select *pParent;    /* Current UNION ALL term of the other query */\n  Select *pSub;       /* The inner query or \"subquery\" */\n  Select *pSub1;      /* Pointer to the rightmost select in sub-query */\n  SrcList *pSrc;      /* The FROM clause of the outer query */\n  SrcList *pSubSrc;   /* The FROM clause of the subquery */\n  int iParent;        /* VDBE cursor number of the pSub result set temp table */\n  int iNewParent = -1;/* Replacement table for iParent */\n  int isLeftJoin = 0; /* True if pSub is the right side of a LEFT JOIN */    \n  int i;              /* Loop counter */\n  Expr *pWhere;                    /* The WHERE clause */\n  struct SrcList_item *pSubitem;   /* The subquery */\n  sqlite3 *db = pParse->db;\n\n  /* Check to see if flattening is permitted.  Return 0 if not.\n  */\n  assert( p!=0 );\n  assert( p->pPrior==0 );\n  if( OptimizationDisabled(db, SQLITE_QueryFlattener) ) return 0;\n  pSrc = p->pSrc;\n  assert( pSrc && iFrom>=0 && iFrom<pSrc->nSrc );\n  pSubitem = &pSrc->a[iFrom];\n  iParent = pSubitem->iCursor;\n  pSub = pSubitem->pSelect;\n  assert( pSub!=0 );\n\n#ifndef SQLITE_OMIT_WINDOWFUNC\n  if( p->pWin || pSub->pWin ) return 0;                  /* Restriction (25) */\n#endif\n\n  pSubSrc = pSub->pSrc;\n  assert( pSubSrc );\n  /* Prior to version 3.1.2, when LIMIT and OFFSET had to be simple constants,\n  ** not arbitrary expressions, we allowed some combining of LIMIT and OFFSET\n  ** because they could be computed at compile-time.  But when LIMIT and OFFSET\n  ** became arbitrary expressions, we were forced to add restrictions (13)\n  ** and (14). */\n  if( pSub->pLimit && p->pLimit ) return 0;              /* Restriction (13) */\n  if( pSub->pLimit && pSub->pLimit->pRight ) return 0;   /* Restriction (14) */\n  if( (p->selFlags & SF_Compound)!=0 && pSub->pLimit ){\n    return 0;                                            /* Restriction (15) */\n  }\n  if( pSubSrc->nSrc==0 ) return 0;                       /* Restriction (7)  */\n  if( pSub->selFlags & SF_Distinct ) return 0;           /* Restriction (4)  */\n  if( pSub->pLimit && (pSrc->nSrc>1 || isAgg) ){\n     return 0;         /* Restrictions (8)(9) */\n  }\n  if( p->pOrderBy && pSub->pOrderBy ){\n     return 0;                                           /* Restriction (11) */\n  }\n  if( isAgg && pSub->pOrderBy ) return 0;                /* Restriction (16) */\n  if( pSub->pLimit && p->pWhere ) return 0;              /* Restriction (19) */\n  if( pSub->pLimit && (p->selFlags & SF_Distinct)!=0 ){\n     return 0;         /* Restriction (21) */\n  }\n  if( pSub->selFlags & (SF_Recursive) ){\n    return 0; /* Restrictions (22) */\n  }\n\n  /*\n  ** If the subquery is the right operand of a LEFT JOIN, then the\n  ** subquery may not be a join itself (3a). Example of why this is not\n  ** allowed:\n  **\n  **         t1 LEFT OUTER JOIN (t2 JOIN t3)\n  **\n  ** If we flatten the above, we would get\n  **\n  **         (t1 LEFT OUTER JOIN t2) JOIN t3\n  **\n  ** which is not at all the same thing.\n  **\n  ** If the subquery is the right operand of a LEFT JOIN, then the outer\n  ** query cannot be an aggregate. (3c)  This is an artifact of the way\n  ** aggregates are processed - there is no mechanism to determine if\n  ** the LEFT JOIN table should be all-NULL.\n  **\n  ** See also tickets #306, #350, and #3300.\n  */\n  if( (pSubitem->fg.jointype & JT_OUTER)!=0 ){\n    isLeftJoin = 1;\n    if( pSubSrc->nSrc>1                   /* (3a) */\n     || isAgg                             /* (3b) */\n     || IsVirtual(pSubSrc->a[0].pTab)     /* (3c) */\n     || (p->selFlags & SF_Distinct)!=0    /* (3d) */\n    ){\n      return 0;\n    }\n  }\n#ifdef SQLITE_EXTRA_IFNULLROW\n  else if( iFrom>0 && !isAgg ){\n    /* Setting isLeftJoin to -1 causes OP_IfNullRow opcodes to be generated for\n    ** every reference to any result column from subquery in a join, even\n    ** though they are not necessary.  This will stress-test the OP_IfNullRow \n    ** opcode. */\n    isLeftJoin = -1;\n  }\n#endif\n\n  /* Restriction (17): If the sub-query is a compound SELECT, then it must\n  ** use only the UNION ALL operator. And none of the simple select queries\n  ** that make up the compound SELECT are allowed to be aggregate or distinct\n  ** queries.\n  */\n  if( pSub->pPrior ){\n    if( pSub->pOrderBy ){\n      return 0;  /* Restriction (20) */\n    }\n    if( isAgg || (p->selFlags & SF_Distinct)!=0 || pSrc->nSrc!=1 ){\n      return 0; /* (17d1), (17d2), or (17d3) */\n    }\n    for(pSub1=pSub; pSub1; pSub1=pSub1->pPrior){\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Distinct );\n      testcase( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))==SF_Aggregate );\n      assert( pSub->pSrc!=0 );\n      assert( pSub->pEList->nExpr==pSub1->pEList->nExpr );\n      if( (pSub1->selFlags & (SF_Distinct|SF_Aggregate))!=0    /* (17b) */\n       || (pSub1->pPrior && pSub1->op!=TK_ALL)                 /* (17a) */\n       || pSub1->pSrc->nSrc<1                                  /* (17c) */\n      ){\n        return 0;\n      }\n      testcase( pSub1->pSrc->nSrc>1 );\n    }\n\n    /* Restriction (18). */\n    if( p->pOrderBy ){\n      int ii;\n      for(ii=0; ii<p->pOrderBy->nExpr; ii++){\n        if( p->pOrderBy->a[ii].u.x.iOrderByCol==0 ) return 0;\n      }\n    }\n  }\n\n  /* Ex-restriction (23):\n  ** The only way that the recursive part of a CTE can contain a compound\n  ** subquery is for the subquery to be one term of a join.  But if the\n  ** subquery is a join, then the flattening has already been stopped by\n  ** restriction (17d3)\n  */\n  assert( (p->selFlags & SF_Recursive)==0 || pSub->pPrior==0 );\n\n  /***** If we reach this point, flattening is permitted. *****/\n  SELECTTRACE(1,pParse,p,(\"flatten %u.%p from term %d\\n\",\n                   pSub->selId, pSub, iFrom));\n\n  /* Authorize the subquery */\n  pParse->zAuthContext = pSubitem->zName;\n  TESTONLY(i =) sqlite3AuthCheck(pParse, SQLITE_SELECT, 0, 0, 0);\n  testcase( i==SQLITE_DENY );\n  pParse->zAuthContext = zSavedAuthContext;\n\n  /* If the sub-query is a compound SELECT statement, then (by restrictions\n  ** 17 and 18 above) it must be a UNION ALL and the parent query must \n  ** be of the form:\n  **\n  **     SELECT <expr-list> FROM (<sub-query>) <where-clause> \n  **\n  ** followed by any ORDER BY, LIMIT and/or OFFSET clauses. This block\n  ** creates N-1 copies of the parent query without any ORDER BY, LIMIT or \n  ** OFFSET clauses and joins them to the left-hand-side of the original\n  ** using UNION ALL operators. In this case N is the number of simple\n  ** select statements in the compound sub-query.\n  **\n  ** Example:\n  **\n  **     SELECT a+1 FROM (\n  **        SELECT x FROM tab\n  **        UNION ALL\n  **        SELECT y FROM tab\n  **        UNION ALL\n  **        SELECT abs(z*2) FROM tab2\n  **     ) WHERE a!=5 ORDER BY 1\n  **\n  ** Transformed into:\n  **\n  **     SELECT x+1 FROM tab WHERE x+1!=5\n  **     UNION ALL\n  **     SELECT y+1 FROM tab WHERE y+1!=5\n  **     UNION ALL\n  **     SELECT abs(z*2)+1 FROM tab2 WHERE abs(z*2)+1!=5\n  **     ORDER BY 1\n  **\n  ** We call this the \"compound-subquery flattening\".\n  */\n  for(pSub=pSub->pPrior; pSub; pSub=pSub->pPrior){\n    Select *pNew;\n    ExprList *pOrderBy = p->pOrderBy;\n    Expr *pLimit = p->pLimit;\n    Select *pPrior = p->pPrior;\n    p->pOrderBy = 0;\n    p->pSrc = 0;\n    p->pPrior = 0;\n    p->pLimit = 0;\n    pNew = sqlite3SelectDup(db, p, 0);\n    p->pLimit = pLimit;\n    p->pOrderBy = pOrderBy;\n    p->pSrc = pSrc;\n    p->op = TK_ALL;\n    if( pNew==0 ){\n      p->pPrior = pPrior;\n    }else{\n      pNew->pPrior = pPrior;\n      if( pPrior ) pPrior->pNext = pNew;\n      pNew->pNext = p;\n      p->pPrior = pNew;\n      SELECTTRACE(2,pParse,p,(\"compound-subquery flattener\"\n                              \" creates %u as peer\\n\",pNew->selId));\n    }\n    if( db->mallocFailed ) return 1;\n  }\n\n  /* Begin flattening the iFrom-th entry of the FROM clause \n  ** in the outer query.\n  */\n  pSub = pSub1 = pSubitem->pSelect;\n\n  /* Delete the transient table structure associated with the\n  ** subquery\n  */\n  sqlite3DbFree(db, pSubitem->zDatabase);\n  sqlite3DbFree(db, pSubitem->zName);\n  sqlite3DbFree(db, pSubitem->zAlias);\n  pSubitem->zDatabase = 0;\n  pSubitem->zName = 0;\n  pSubitem->zAlias = 0;\n  pSubitem->pSelect = 0;\n\n  /* Defer deleting the Table object associated with the\n  ** subquery until code generation is\n  ** complete, since there may still exist Expr.pTab entries that\n  ** refer to the subquery even after flattening.  Ticket #3346.\n  **\n  ** pSubitem->pTab is always non-NULL by test restrictions and tests above.\n  */\n  if( ALWAYS(pSubitem->pTab!=0) ){\n    Table *pTabToDel = pSubitem->pTab;\n    if( pTabToDel->nTabRef==1 ){\n      Parse *pToplevel = sqlite3ParseToplevel(pParse);\n      pTabToDel->pNextZombie = pToplevel->pZombieTab;\n      pToplevel->pZombieTab = pTabToDel;\n    }else{\n      pTabToDel->nTabRef--;\n    }\n    pSubitem->pTab = 0;\n  }\n\n  /* The following loop runs once for each term in a compound-subquery\n  ** flattening (as described above).  If we are doing a different kind\n  ** of flattening - a flattening other than a compound-subquery flattening -\n  ** then this loop only runs once.\n  **\n  ** This loop moves all of the FROM elements of the subquery into the\n  ** the FROM clause of the outer query.  Before doing this, remember\n  ** the cursor number for the original outer query FROM element in\n  ** iParent.  The iParent cursor will never be used.  Subsequent code\n  ** will scan expressions looking for iParent references and replace\n  ** those references with expressions that resolve to the subquery FROM\n  ** elements we are now copying in.\n  */\n  for(pParent=p; pParent; pParent=pParent->pPrior, pSub=pSub->pPrior){\n    int nSubSrc;\n    u8 jointype = 0;\n    assert( pSub!=0 );\n    pSubSrc = pSub->pSrc;     /* FROM clause of subquery */\n    nSubSrc = pSubSrc->nSrc;  /* Number of terms in subquery FROM clause */\n    pSrc = pParent->pSrc;     /* FROM clause of the outer query */\n\n    if( pSrc ){\n      assert( pParent==p );  /* First time through the loop */\n      jointype = pSubitem->fg.jointype;\n    }else{\n      assert( pParent!=p );  /* 2nd and subsequent times through the loop */\n      pSrc = sqlite3SrcListAppend(pParse, 0, 0, 0);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* The subquery uses a single slot of the FROM clause of the outer\n    ** query.  If the subquery has more than one element in its FROM clause,\n    ** then expand the outer query to make space for it to hold all elements\n    ** of the subquery.\n    **\n    ** Example:\n    **\n    **    SELECT * FROM tabA, (SELECT * FROM sub1, sub2), tabB;\n    **\n    ** The outer query has 3 slots in its FROM clause.  One slot of the\n    ** outer query (the middle slot) is used by the subquery.  The next\n    ** block of code will expand the outer query FROM clause to 4 slots.\n    ** The middle slot is expanded to two slots in order to make space\n    ** for the two elements in the FROM clause of the subquery.\n    */\n    if( nSubSrc>1 ){\n      pSrc = sqlite3SrcListEnlarge(pParse, pSrc, nSubSrc-1,iFrom+1);\n      if( pSrc==0 ) break;\n      pParent->pSrc = pSrc;\n    }\n\n    /* Transfer the FROM clause terms from the subquery into the\n    ** outer query.\n    */\n    for(i=0; i<nSubSrc; i++){\n      sqlite3IdListDelete(db, pSrc->a[i+iFrom].pUsing);\n      assert( pSrc->a[i+iFrom].fg.isTabFunc==0 );\n      pSrc->a[i+iFrom] = pSubSrc->a[i];\n      iNewParent = pSubSrc->a[i].iCursor;\n      memset(&pSubSrc->a[i], 0, sizeof(pSubSrc->a[i]));\n    }\n    pSrc->a[iFrom].fg.jointype = jointype;\n  \n    /* Now begin substituting subquery result set expressions for \n    ** references to the iParent in the outer query.\n    ** \n    ** Example:\n    **\n    **   SELECT a+5, b*10 FROM (SELECT x*3 AS a, y+10 AS b FROM t1) WHERE a>b;\n    **   \\                     \\_____________ subquery __________/          /\n    **    \\_____________________ outer query ______________________________/\n    **\n    ** We look at every expression in the outer query and every place we see\n    ** \"a\" we substitute \"x*3\" and every place we see \"b\" we substitute \"y+10\".\n    */\n    if( pSub->pOrderBy ){\n      /* At this point, any non-zero iOrderByCol values indicate that the\n      ** ORDER BY column expression is identical to the iOrderByCol'th\n      ** expression returned by SELECT statement pSub. Since these values\n      ** do not necessarily correspond to columns in SELECT statement pParent,\n      ** zero them before transfering the ORDER BY clause.\n      **\n      ** Not doing this may cause an error if a subsequent call to this\n      ** function attempts to flatten a compound sub-query into pParent\n      ** (the only way this can happen is if the compound sub-query is\n      ** currently part of pSub->pSrc). See ticket [d11a6e908f].  */\n      ExprList *pOrderBy = pSub->pOrderBy;\n      for(i=0; i<pOrderBy->nExpr; i++){\n        pOrderBy->a[i].u.x.iOrderByCol = 0;\n      }\n      assert( pParent->pOrderBy==0 );\n      pParent->pOrderBy = pOrderBy;\n      pSub->pOrderBy = 0;\n    }\n    pWhere = pSub->pWhere;\n    pSub->pWhere = 0;\n    if( isLeftJoin>0 ){\n      sqlite3SetJoinExpr(pWhere, iNewParent);\n    }\n    pParent->pWhere = sqlite3ExprAnd(pParse, pWhere, pParent->pWhere);\n    if( db->mallocFailed==0 ){\n      SubstContext x;\n      x.pParse = pParse;\n      x.iTable = iParent;\n      x.iNewTable = iNewParent;\n      x.isLeftJoin = isLeftJoin;\n      x.pEList = pSub->pEList;\n      substSelect(&x, pParent, 0);\n    }\n  \n    /* The flattened query is a compound if either the inner or the\n    ** outer query is a compound. */\n    pParent->selFlags |= pSub->selFlags & SF_Compound;\n    assert( (pSub->selFlags & SF_Distinct)==0 ); /* restriction (17b) */\n  \n    /*\n    ** SELECT ... FROM (SELECT ... LIMIT a OFFSET b) LIMIT x OFFSET y;\n    **\n    ** One is tempted to try to add a and b to combine the limits.  But this\n    ** does not work if either limit is negative.\n    */\n    if( pSub->pLimit ){\n      pParent->pLimit = pSub->pLimit;\n      pSub->pLimit = 0;\n    }\n  }\n\n  /* Finially, delete what is left of the subquery and return\n  ** success.\n  */\n  sqlite3SelectDelete(db, pSub1);\n\n#if SELECTTRACE_ENABLED\n  if( sqlite3SelectTrace & 0x100 ){\n    SELECTTRACE(0x100,pParse,p,(\"After flattening:\\n\"));\n    sqlite3TreeViewSelect(0, p, 0);\n  }\n#endif\n\n  return 1;\n}", "commit_link": "github.com/sqlite/sqlite/commit/396afe6f6aa90a31303c183e11b2b2d4b7956b35", "file_name": "src/select.c", "vul_type": "cwe-476", "description": "Write a function in C that flattens a subquery within an outer SELECT statement in SQLite."}
{"func_name": "write", "func_src_before": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "func_src_after": "    def write(self, bib_data, filename):\n        def process_person_roles(entry):\n            for role, persons in entry.persons.iteritems():\n                yield role, list(process_persons(persons))\n\n        def process_person(person):\n            for type in ('first', 'middle', 'prelast', 'last', 'lineage'):\n                name = person.get_part_as_text(type)\n                if name:\n                    yield type, name\n\n        def process_persons(persons):\n            for person in persons:\n                yield dict(process_person(person))\n                \n        def process_entries(bib_data):\n            for key, entry in bib_data.iteritems():\n                fields = dict(entry.fields)\n                fields['type'] = entry.type\n                fields.update(process_person_roles(entry))\n                yield key, fields\n\n        data = {'data': dict(process_entries(bib_data))}\n        f = open(filename, 'w')\n        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n        f.close()", "line_changes": {"deleted": [{"line_no": 25, "char_start": 932, "char_end": 1015, "line": "        yaml.dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}], "added": [{"line_no": 25, "char_start": 932, "char_end": 1020, "line": "        yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, indent=4)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 945, "char_end": 950, "chars": "safe_"}]}, "commit_link": "github.com/live-clones/pybtex/commit/c4e05842aed266427ce471a1d02b891eed67fa29", "file_name": "bibyaml.py", "vul_type": "cwe-502", "commit_msg": "YAML: use safe_dump and safe_load", "parent_commit": "5abe83ed0c01cbc8a43ec9395797d5b0060e0066", "description": "Write a Python function to process bibliographic data and save it to a YAML file."}
{"func_name": "also_add", "func_src_before": "def also_add(name, also):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            INSERT INTO isalso(name,also) VALUES('{}','{}')\n            '''.format(name, also))\n        db.commit()\n        logger.debug('added to isalso name {} with value {}'.format(\n            name, also))\n        db.close()\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "func_src_after": "def also_add(name, also):\n    db = db_connect()\n    cursor = db.cursor()\n    try:\n        cursor.execute('''\n            INSERT INTO isalso(name,also) VALUES(%(name)s,%(also)s)\n            ''', (\n            name,\n            also,\n        ))\n        db.commit()\n        logger.debug('added to isalso name {} with value {}'.format(\n            name, also))\n        db.close()\n    except Exception as e:\n        logger.error('Execution failed with error: {}'.format(e))\n        raise", "commit_link": "github.com/tylarb/KarmaBoi-PCF/commit/c1d00a27d7f6b7eb6f15a3dacd4269654a32c10a", "file_name": "KarmaBoi/dbopts.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a name and an associated value into an 'isalso' database table and log the action."}
{"func_name": "multiSelect", "func_src_before": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "func_src_after": "static int multiSelect(\n  Parse *pParse,        /* Parsing context */\n  Select *p,            /* The right-most of SELECTs to be coded */\n  SelectDest *pDest     /* What to do with query results */\n){\n  int rc = SQLITE_OK;   /* Success code from a subroutine */\n  Select *pPrior;       /* Another SELECT immediately to our left */\n  Vdbe *v;              /* Generate code to this VDBE */\n  SelectDest dest;      /* Alternative data destination */\n  Select *pDelete = 0;  /* Chain of simple selects to delete */\n  sqlite3 *db;          /* Database connection */\n\n  /* Make sure there is no ORDER BY or LIMIT clause on prior SELECTs.  Only\n  ** the last (right-most) SELECT in the series may have an ORDER BY or LIMIT.\n  */\n  assert( p && p->pPrior );  /* Calling function guarantees this much */\n  assert( (p->selFlags & SF_Recursive)==0 || p->op==TK_ALL || p->op==TK_UNION );\n  assert( p->selFlags & SF_Compound );\n  db = pParse->db;\n  pPrior = p->pPrior;\n  dest = *pDest;\n  if( pPrior->pOrderBy || pPrior->pLimit ){\n    sqlite3ErrorMsg(pParse,\"%s clause should come after %s not before\",\n      pPrior->pOrderBy!=0 ? \"ORDER BY\" : \"LIMIT\", selectOpName(p->op));\n    rc = 1;\n    goto multi_select_end;\n  }\n\n  v = sqlite3GetVdbe(pParse);\n  assert( v!=0 );  /* The VDBE already created by calling function */\n\n  /* Create the destination temporary table if necessary\n  */\n  if( dest.eDest==SRT_EphemTab ){\n    assert( p->pEList );\n    sqlite3VdbeAddOp2(v, OP_OpenEphemeral, dest.iSDParm, p->pEList->nExpr);\n    dest.eDest = SRT_Table;\n  }\n\n  /* Special handling for a compound-select that originates as a VALUES clause.\n  */\n  if( p->selFlags & SF_MultiValue ){\n    rc = multiSelectValues(pParse, p, &dest);\n    if( rc>=0 ) goto multi_select_end;\n    rc = SQLITE_OK;\n  }\n\n  /* Make sure all SELECTs in the statement have the same number of elements\n  ** in their result sets.\n  */\n  assert( p->pEList && pPrior->pEList );\n  assert( p->pEList->nExpr==pPrior->pEList->nExpr );\n\n#ifndef SQLITE_OMIT_CTE\n  if( p->selFlags & SF_Recursive ){\n    generateWithRecursiveQuery(pParse, p, &dest);\n  }else\n#endif\n\n  /* Compound SELECTs that have an ORDER BY clause are handled separately.\n  */\n  if( p->pOrderBy ){\n    return multiSelectOrderBy(pParse, p, pDest);\n  }else{\n\n#ifndef SQLITE_OMIT_EXPLAIN\n    if( pPrior->pPrior==0 ){\n      ExplainQueryPlan((pParse, 1, \"COMPOUND QUERY\"));\n      ExplainQueryPlan((pParse, 1, \"LEFT-MOST SUBQUERY\"));\n    }\n#endif\n\n    /* Generate code for the left and right SELECT statements.\n    */\n    switch( p->op ){\n      case TK_ALL: {\n        int addr = 0;\n        int nLimit;\n        assert( !pPrior->pLimit );\n        pPrior->iLimit = p->iLimit;\n        pPrior->iOffset = p->iOffset;\n        pPrior->pLimit = p->pLimit;\n        rc = sqlite3Select(pParse, pPrior, &dest);\n        p->pLimit = 0;\n        if( rc ){\n          goto multi_select_end;\n        }\n        p->pPrior = 0;\n        p->iLimit = pPrior->iLimit;\n        p->iOffset = pPrior->iOffset;\n        if( p->iLimit ){\n          addr = sqlite3VdbeAddOp1(v, OP_IfNot, p->iLimit); VdbeCoverage(v);\n          VdbeComment((v, \"Jump ahead if LIMIT reached\"));\n          if( p->iOffset ){\n            sqlite3VdbeAddOp3(v, OP_OffsetLimit,\n                              p->iLimit, p->iOffset+1, p->iOffset);\n          }\n        }\n        ExplainQueryPlan((pParse, 1, \"UNION ALL\"));\n        rc = sqlite3Select(pParse, p, &dest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        if( pPrior->pLimit\n         && sqlite3ExprIsInteger(pPrior->pLimit->pLeft, &nLimit)\n         && nLimit>0 && p->nSelectRow > sqlite3LogEst((u64)nLimit) \n        ){\n          p->nSelectRow = sqlite3LogEst((u64)nLimit);\n        }\n        if( addr ){\n          sqlite3VdbeJumpHere(v, addr);\n        }\n        break;\n      }\n      case TK_EXCEPT:\n      case TK_UNION: {\n        int unionTab;    /* Cursor number of the temp table holding result */\n        u8 op = 0;       /* One of the SRT_ operations to apply to self */\n        int priorOp;     /* The SRT_ operation to apply to prior selects */\n        Expr *pLimit;    /* Saved values of p->nLimit  */\n        int addr;\n        SelectDest uniondest;\n  \n        testcase( p->op==TK_EXCEPT );\n        testcase( p->op==TK_UNION );\n        priorOp = SRT_Union;\n        if( dest.eDest==priorOp ){\n          /* We can reuse a temporary table generated by a SELECT to our\n          ** right.\n          */\n          assert( p->pLimit==0 );      /* Not allowed on leftward elements */\n          unionTab = dest.iSDParm;\n        }else{\n          /* We will need to create our own temporary table to hold the\n          ** intermediate results.\n          */\n          unionTab = pParse->nTab++;\n          assert( p->pOrderBy==0 );\n          addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, unionTab, 0);\n          assert( p->addrOpenEphm[0] == -1 );\n          p->addrOpenEphm[0] = addr;\n          findRightmost(p)->selFlags |= SF_UsesEphemeral;\n          assert( p->pEList );\n        }\n  \n        /* Code the SELECT statements to our left\n        */\n        assert( !pPrior->pOrderBy );\n        sqlite3SelectDestInit(&uniondest, priorOp, unionTab);\n        rc = sqlite3Select(pParse, pPrior, &uniondest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT statement\n        */\n        if( p->op==TK_EXCEPT ){\n          op = SRT_Except;\n        }else{\n          assert( p->op==TK_UNION );\n          op = SRT_Union;\n        }\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        uniondest.eDest = op;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &uniondest);\n        testcase( rc!=SQLITE_OK );\n        /* Query flattening in sqlite3Select() might refill p->pOrderBy.\n        ** Be sure to delete p->pOrderBy, therefore, to avoid a memory leak. */\n        sqlite3ExprListDelete(db, p->pOrderBy);\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        p->pOrderBy = 0;\n        if( p->op==TK_UNION ){\n          p->nSelectRow = sqlite3LogEstAdd(p->nSelectRow, pPrior->nSelectRow);\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n        p->iLimit = 0;\n        p->iOffset = 0;\n  \n        /* Convert the data in the temporary table into whatever form\n        ** it is that we currently need.\n        */\n        assert( unionTab==dest.iSDParm || dest.eDest!=priorOp );\n        if( dest.eDest!=priorOp ){\n          int iCont, iBreak, iStart;\n          assert( p->pEList );\n          iBreak = sqlite3VdbeMakeLabel(pParse);\n          iCont = sqlite3VdbeMakeLabel(pParse);\n          computeLimitRegisters(pParse, p, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Rewind, unionTab, iBreak); VdbeCoverage(v);\n          iStart = sqlite3VdbeCurrentAddr(v);\n          selectInnerLoop(pParse, p, unionTab,\n                          0, 0, &dest, iCont, iBreak);\n          sqlite3VdbeResolveLabel(v, iCont);\n          sqlite3VdbeAddOp2(v, OP_Next, unionTab, iStart); VdbeCoverage(v);\n          sqlite3VdbeResolveLabel(v, iBreak);\n          sqlite3VdbeAddOp2(v, OP_Close, unionTab, 0);\n        }\n        break;\n      }\n      default: assert( p->op==TK_INTERSECT ); {\n        int tab1, tab2;\n        int iCont, iBreak, iStart;\n        Expr *pLimit;\n        int addr;\n        SelectDest intersectdest;\n        int r1;\n  \n        /* INTERSECT is different from the others since it requires\n        ** two temporary tables.  Hence it has its own case.  Begin\n        ** by allocating the tables we will need.\n        */\n        tab1 = pParse->nTab++;\n        tab2 = pParse->nTab++;\n        assert( p->pOrderBy==0 );\n  \n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab1, 0);\n        assert( p->addrOpenEphm[0] == -1 );\n        p->addrOpenEphm[0] = addr;\n        findRightmost(p)->selFlags |= SF_UsesEphemeral;\n        assert( p->pEList );\n  \n        /* Code the SELECTs to our left into temporary table \"tab1\".\n        */\n        sqlite3SelectDestInit(&intersectdest, SRT_Union, tab1);\n        rc = sqlite3Select(pParse, pPrior, &intersectdest);\n        if( rc ){\n          goto multi_select_end;\n        }\n  \n        /* Code the current SELECT into temporary table \"tab2\"\n        */\n        addr = sqlite3VdbeAddOp2(v, OP_OpenEphemeral, tab2, 0);\n        assert( p->addrOpenEphm[1] == -1 );\n        p->addrOpenEphm[1] = addr;\n        p->pPrior = 0;\n        pLimit = p->pLimit;\n        p->pLimit = 0;\n        intersectdest.iSDParm = tab2;\n        ExplainQueryPlan((pParse, 1, \"%s USING TEMP B-TREE\",\n                          selectOpName(p->op)));\n        rc = sqlite3Select(pParse, p, &intersectdest);\n        testcase( rc!=SQLITE_OK );\n        pDelete = p->pPrior;\n        p->pPrior = pPrior;\n        if( p->nSelectRow>pPrior->nSelectRow ){\n          p->nSelectRow = pPrior->nSelectRow;\n        }\n        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);\n        r1 = sqlite3GetTempReg(pParse);\n        iStart = sqlite3VdbeAddOp2(v, OP_RowData, tab1, r1);\n        sqlite3VdbeAddOp4Int(v, OP_NotFound, tab2, iCont, r1, 0);\n        VdbeCoverage(v);\n        sqlite3ReleaseTempReg(pParse, r1);\n        selectInnerLoop(pParse, p, tab1,\n                        0, 0, &dest, iCont, iBreak);\n        sqlite3VdbeResolveLabel(v, iCont);\n        sqlite3VdbeAddOp2(v, OP_Next, tab1, iStart); VdbeCoverage(v);\n        sqlite3VdbeResolveLabel(v, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Close, tab2, 0);\n        sqlite3VdbeAddOp2(v, OP_Close, tab1, 0);\n        break;\n      }\n    }\n  \n  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  if( pParse->nErr ) goto multi_select_end;\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **\n  ** This section is run by the right-most SELECT statement only.\n  ** SELECT statements to the left always skip this part.  The right-most\n  ** SELECT might also skip this part if it has no ORDER BY clause and\n  ** no temp tables are required.\n  */\n  if( p->selFlags & SF_UsesEphemeral ){\n    int i;                        /* Loop counter */\n    KeyInfo *pKeyInfo;            /* Collating sequence for the result set */\n    Select *pLoop;                /* For looping through SELECT statements */\n    CollSeq **apColl;             /* For looping through pKeyInfo->aColl[] */\n    int nCol;                     /* Number of columns in result set */\n\n    assert( p->pNext==0 );\n    nCol = p->pEList->nExpr;\n    pKeyInfo = sqlite3KeyInfoAlloc(db, nCol, 1);\n    if( !pKeyInfo ){\n      rc = SQLITE_NOMEM_BKPT;\n      goto multi_select_end;\n    }\n    for(i=0, apColl=pKeyInfo->aColl; i<nCol; i++, apColl++){\n      *apColl = multiSelectCollSeq(pParse, p, i);\n      if( 0==*apColl ){\n        *apColl = db->pDfltColl;\n      }\n    }\n\n    for(pLoop=p; pLoop; pLoop=pLoop->pPrior){\n      for(i=0; i<2; i++){\n        int addr = pLoop->addrOpenEphm[i];\n        if( addr<0 ){\n          /* If [0] is unused then [1] is also unused.  So we can\n          ** always safely abort as soon as the first unused slot is found */\n          assert( pLoop->addrOpenEphm[1]<0 );\n          break;\n        }\n        sqlite3VdbeChangeP2(v, addr, nCol);\n        sqlite3VdbeChangeP4(v, addr, (char*)sqlite3KeyInfoRef(pKeyInfo),\n                            P4_KEYINFO);\n        pLoop->addrOpenEphm[i] = -1;\n      }\n    }\n    sqlite3KeyInfoUnref(pKeyInfo);\n  }\n\nmulti_select_end:\n  pDest->iSdst = dest.iSdst;\n  pDest->nSdst = dest.nSdst;\n  sqlite3SelectDelete(db, pDelete);\n  return rc;\n}", "commit_link": "github.com/sqlite/sqlite/commit/8428b3b437569338a9d1e10c4cd8154acbe33089", "file_name": "src/select.c", "vul_type": "cwe-476", "description": "Write a function in C for SQLite that handles the execution of compound SELECT queries with specific handling for UNION, INTERSECT, and EXCEPT operations."}
{"func_name": "lookup_assets", "func_src_before": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE '\"+start+\"%'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "func_src_after": "@app.route('/lookup_assets')\ndef lookup_assets():\n    start = request.args.get('start')\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT aname FROM assets WHERE aname LIKE %s\"\n    cur.execute(query, (start+'%',))\n    results = cur.fetchall()\n    con.close()\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to search for asset names starting with a given string using psycopg2 for database access."}
{"func_name": "make_canonical", "func_src_before": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "func_src_after": "make_canonical(struct ly_ctx *ctx, int type, const char **value, void *data1, void *data2)\n{\n    const uint16_t buf_len = 511;\n    char buf[buf_len + 1];\n    struct lys_type_bit **bits = NULL;\n    struct lyxp_expr *exp;\n    const char *module_name, *cur_expr, *end;\n    int i, j, count;\n    int64_t num;\n    uint64_t unum;\n    uint8_t c;\n\n#define LOGBUF(str) LOGERR(ctx, LY_EINVAL, \"Value \\\"%s\\\" is too long.\", str)\n\n    switch (type) {\n    case LY_TYPE_BITS:\n        bits = (struct lys_type_bit **)data1;\n        count = *((int *)data2);\n        /* in canonical form, the bits are ordered by their position */\n        buf[0] = '\\0';\n        for (i = 0; i < count; i++) {\n            if (!bits[i]) {\n                /* bit not set */\n                continue;\n            }\n            if (buf[0]) {\n                LY_CHECK_ERR_RETURN(strlen(buf) + 1 + strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                sprintf(buf + strlen(buf), \" %s\", bits[i]->name);\n            } else {\n                LY_CHECK_ERR_RETURN(strlen(bits[i]->name) > buf_len, LOGBUF(bits[i]->name), -1);\n                strcpy(buf, bits[i]->name);\n            }\n        }\n        break;\n\n    case LY_TYPE_IDENT:\n        module_name = (const char *)data1;\n        /* identity must always have a prefix */\n        if (!strchr(*value, ':')) {\n            LY_CHECK_ERR_RETURN(strlen(module_name) + 1 + strlen(*value) > buf_len, LOGBUF(*value), -1);\n            sprintf(buf, \"%s:%s\", module_name, *value);\n        } else {\n            LY_CHECK_ERR_RETURN(strlen(*value) > buf_len, LOGBUF(*value), -1);\n            strcpy(buf, *value);\n        }\n        break;\n\n    case LY_TYPE_INST:\n        exp = lyxp_parse_expr(ctx, *value);\n        LY_CHECK_ERR_RETURN(!exp, LOGINT(ctx), -1);\n\n        module_name = NULL;\n        count = 0;\n        for (i = 0; (unsigned)i < exp->used; ++i) {\n            cur_expr = &exp->expr[exp->expr_pos[i]];\n\n            /* copy WS */\n            if (i && ((end = exp->expr + exp->expr_pos[i - 1] + exp->tok_len[i - 1]) != cur_expr)) {\n                if (count + (cur_expr - end) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, cur_expr - end);\n                count += cur_expr - end;\n            }\n\n            if ((exp->tokens[i] == LYXP_TOKEN_NAMETEST) && (end = strnchr(cur_expr, ':', exp->tok_len[i]))) {\n                /* get the module name with \":\" */\n                ++end;\n                j = end - cur_expr;\n\n                if (!module_name || strncmp(cur_expr, module_name, j)) {\n                    /* print module name with colon, it does not equal to the parent one */\n                    if (count + j > buf_len) {\n                        lyxp_expr_free(exp);\n                        LOGBUF(cur_expr);\n                        return -1;\n                    }\n                    strncpy(&buf[count], cur_expr, j);\n                    count += j;\n                }\n                module_name = cur_expr;\n\n                /* copy the rest */\n                if (count + (exp->tok_len[i] - j) > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(end);\n                    return -1;\n                }\n                strncpy(&buf[count], end, exp->tok_len[i] - j);\n                count += exp->tok_len[i] - j;\n            } else {\n                if (count + exp->tok_len[i] > buf_len) {\n                    lyxp_expr_free(exp);\n                    LOGBUF(&exp->expr[exp->expr_pos[i]]);\n                    return -1;\n                }\n                strncpy(&buf[count], &exp->expr[exp->expr_pos[i]], exp->tok_len[i]);\n                count += exp->tok_len[i];\n            }\n        }\n        if (count > buf_len) {\n            LOGINT(ctx);\n            lyxp_expr_free(exp);\n            return -1;\n        }\n        buf[count] = '\\0';\n\n        lyxp_expr_free(exp);\n        break;\n\n    case LY_TYPE_DEC64:\n        num = *((int64_t *)data1);\n        c = *((uint8_t *)data2);\n        if (num) {\n            count = sprintf(buf, \"%\"PRId64\" \", num);\n            if ( (num > 0 && (count - 1) <= c)\n                 || (count - 2) <= c ) {\n                /* we have 0. value, print the value with the leading zeros\n                 * (one for 0. and also keep the correct with of num according\n                 * to fraction-digits value)\n                 * for (num<0) - extra character for '-' sign */\n                count = sprintf(buf, \"%0*\"PRId64\" \", (num > 0) ? (c + 1) : (c + 2), num);\n            }\n            for (i = c, j = 1; i > 0 ; i--) {\n                if (j && i > 1 && buf[count - 2] == '0') {\n                    /* we have trailing zero to skip */\n                    buf[count - 1] = '\\0';\n                } else {\n                    j = 0;\n                    buf[count - 1] = buf[count - 2];\n                }\n                count--;\n            }\n            buf[count - 1] = '.';\n        } else {\n            /* zero */\n            sprintf(buf, \"0.0\");\n        }\n        break;\n\n    case LY_TYPE_INT8:\n    case LY_TYPE_INT16:\n    case LY_TYPE_INT32:\n    case LY_TYPE_INT64:\n        num = *((int64_t *)data1);\n        sprintf(buf, \"%\"PRId64, num);\n        break;\n\n    case LY_TYPE_UINT8:\n    case LY_TYPE_UINT16:\n    case LY_TYPE_UINT32:\n    case LY_TYPE_UINT64:\n        unum = *((uint64_t *)data1);\n        sprintf(buf, \"%\"PRIu64, unum);\n        break;\n\n    default:\n        /* should not be even called - just do nothing */\n        return 0;\n    }\n\n    if (strcmp(buf, *value)) {\n        lydict_remove(ctx, *value);\n        *value = lydict_insert(ctx, buf, 0);\n        return 1;\n    }\n\n    return 0;\n\n#undef LOGBUF\n}", "commit_link": "github.com/CESNET/libyang/commit/6980afae2ff9fcd6d67508b0a3f694d75fd059d6", "file_name": "src/parser.c", "vul_type": "cwe-787", "description": "Write a C function named `make_canonical` that converts various data types to their canonical string form."}
{"func_name": "referrer_count", "func_src_before": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer='\"+account_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchone()\n\n    return jsonify(results)", "func_src_after": "@app.route('/referrer_count')\ndef referrer_count():\n    account_id = request.args.get('account_id')\n\n    if not isObject(account_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_account_names\",[[\"' + account_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n\n        account_id = j_l[\"result\"][0][\"id\"]\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"select count(*) from referrers where referrer=%s\"\n    cur.execute(query, (account_id,))\n    results = cur.fetchone()\n\n    return jsonify(results)", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "Write a Python Flask endpoint to count referrer entries in a PostgreSQL database using an account ID parameter."}
{"func_name": "test_settings_path_skip_issue_909", "func_src_before": "def test_settings_path_skip_issue_909(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'skip =\\n'\n                                        '    file_to_be_skipped.py\\n'\n                                        'skip_glob =\\n'\n                                        '    *glob_skip*\\n')\n\n    base_dir.join('file_glob_skip.py').write('import os\\n'\n                                             '\\n'\n                                             'print(\"Hello World\")\\n'\n                                             '\\n'\n                                             'import sys\\n')\n    base_dir.join('file_to_be_skipped.py').write('import os\\n'\n                                                 '\\n'\n                                                 'print(\"Hello World\")'\n                                                 '\\n'\n                                                 'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors\n        check_output(['isort', '--check-only'])\n    results = check_output(['isort', '--check-only', '--settings-path=conf/.isort.cfg'])\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 2' in results.lower()", "func_src_after": "def test_settings_path_skip_issue_909(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'skip =\\n'\n                                        '    file_to_be_skipped.py\\n'\n                                        'skip_glob =\\n'\n                                        '    *glob_skip*\\n')\n\n    base_dir.join('file_glob_skip.py').write('import os\\n'\n                                             '\\n'\n                                             'print(\"Hello World\")\\n'\n                                             '\\n'\n                                             'import sys\\n')\n    base_dir.join('file_to_be_skipped.py').write('import os\\n'\n                                                 '\\n'\n                                                 'print(\"Hello World\")'\n                                                 '\\n'\n                                                 'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors\n        subprocess.run(['isort', '--check-only'], check=True)\n    result = subprocess.run(\n        ['isort', '--check-only', '--settings-path=conf/.isort.cfg'],\n        stdout=subprocess.PIPE,\n        check=True\n    )\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 2' in result.stdout.lower()", "commit_link": "github.com/timothycrosley/isort/commit/1ab38f4f7840a3c19bf961a24630a992a8373a76", "file_name": "test_isort.py", "vul_type": "cwe-078", "description": "Write a Python test function that creates a temporary directory structure to simulate skipping files with isort configuration and verifies the behavior."}
{"func_name": "usb_get_bos_descriptor", "func_src_before": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\t\tlength = cap->bLength;\n\n\t\tif (total_len < length)\n\t\t\tbreak;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "func_src_after": "int usb_get_bos_descriptor(struct usb_device *dev)\n{\n\tstruct device *ddev = &dev->dev;\n\tstruct usb_bos_descriptor *bos;\n\tstruct usb_dev_cap_header *cap;\n\tunsigned char *buffer;\n\tint length, total_len, num, i;\n\tint ret;\n\n\tbos = kzalloc(sizeof(struct usb_bos_descriptor), GFP_KERNEL);\n\tif (!bos)\n\t\treturn -ENOMEM;\n\n\t/* Get BOS descriptor */\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, bos, USB_DT_BOS_SIZE);\n\tif (ret < USB_DT_BOS_SIZE) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tkfree(bos);\n\t\treturn ret;\n\t}\n\n\tlength = bos->bLength;\n\ttotal_len = le16_to_cpu(bos->wTotalLength);\n\tnum = bos->bNumDeviceCaps;\n\tkfree(bos);\n\tif (total_len < length)\n\t\treturn -EINVAL;\n\n\tdev->bos = kzalloc(sizeof(struct usb_host_bos), GFP_KERNEL);\n\tif (!dev->bos)\n\t\treturn -ENOMEM;\n\n\t/* Now let's get the whole BOS descriptor set */\n\tbuffer = kzalloc(total_len, GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdev->bos->desc = (struct usb_bos_descriptor *)buffer;\n\n\tret = usb_get_descriptor(dev, USB_DT_BOS, 0, buffer, total_len);\n\tif (ret < total_len) {\n\t\tdev_err(ddev, \"unable to get BOS descriptor set\\n\");\n\t\tif (ret >= 0)\n\t\t\tret = -ENOMSG;\n\t\tgoto err;\n\t}\n\ttotal_len -= length;\n\n\tfor (i = 0; i < num; i++) {\n\t\tbuffer += length;\n\t\tcap = (struct usb_dev_cap_header *)buffer;\n\n\t\tif (total_len < sizeof(*cap) || total_len < cap->bLength) {\n\t\t\tdev->bos->desc->bNumDeviceCaps = i;\n\t\t\tbreak;\n\t\t}\n\t\tlength = cap->bLength;\n\t\ttotal_len -= length;\n\n\t\tif (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {\n\t\t\tdev_warn(ddev, \"descriptor type invalid, skip\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (cap->bDevCapabilityType) {\n\t\tcase USB_CAP_TYPE_WIRELESS_USB:\n\t\t\t/* Wireless USB cap descriptor is handled by wusb */\n\t\t\tbreak;\n\t\tcase USB_CAP_TYPE_EXT:\n\t\t\tdev->bos->ext_cap =\n\t\t\t\t(struct usb_ext_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SS_CAP_TYPE:\n\t\t\tdev->bos->ss_cap =\n\t\t\t\t(struct usb_ss_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_SSP_CAP_TYPE:\n\t\t\tdev->bos->ssp_cap =\n\t\t\t\t(struct usb_ssp_cap_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase CONTAINER_ID_TYPE:\n\t\t\tdev->bos->ss_id =\n\t\t\t\t(struct usb_ss_container_id_descriptor *)buffer;\n\t\t\tbreak;\n\t\tcase USB_PTM_CAP_TYPE:\n\t\t\tdev->bos->ptm_cap =\n\t\t\t\t(struct usb_ptm_cap_descriptor *)buffer;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tusb_release_bos_descriptor(dev);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/1c0edc3633b56000e18d82fc241e3995ca18a69e", "file_name": "drivers/usb/core/config.c", "vul_type": "cwe-125", "description": "Write a C function named `usb_get_bos_descriptor` that retrieves the Binary Object Store (BOS) descriptor for a USB device."}
{"func_name": "add_translationname", "func_src_before": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (\"%s\", \"%s\", \"%s\")' % (item[0], trname[1], trname[2]))\n            self.connection.commit()", "func_src_after": "    def add_translationname(self, trname):\n        \"\"\"Add new translation by item name for an item.\"\"\"\n        if self.connection:\n            for item in self.find_item_name([trname[0], '0']):\n                t = (item[0], trname[1], trname[2], )\n                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values (?, ?, ?)', t)\n            self.connection.commit()", "commit_link": "github.com/ecosl-developers/ecosl/commit/8af050a513338bf68ff2a243e4a2482d24e9aa3a", "file_name": "ecosldb/ecosldb.py", "vul_type": "cwe-089", "description": "Write a Python function to insert a new translation into a database given an item name and translation details."}
{"func_name": "_bson_iter_next_internal", "func_src_before": "_bson_iter_next_internal (bson_iter_t *iter,    /* INOUT */\n                          uint32_t next_keylen, /* IN */\n                          const char **key,     /* OUT */\n                          uint32_t *bson_type,  /* OUT */\n                          bool *unsupported)    /* OUT */\n{\n   const uint8_t *data;\n   uint32_t o;\n   unsigned int len;\n\n   BSON_ASSERT (iter);\n\n   *unsupported = false;\n\n   if (!iter->raw) {\n      *key = NULL;\n      *bson_type = BSON_TYPE_EOD;\n      return false;\n   }\n\n   data = iter->raw;\n   len = iter->len;\n\n   iter->off = iter->next_off;\n   iter->type = iter->off;\n   iter->key = iter->off + 1;\n   iter->d1 = 0;\n   iter->d2 = 0;\n   iter->d3 = 0;\n   iter->d4 = 0;\n\n   if (next_keylen == 0) {\n      /* iterate from start to end of NULL-terminated key string */\n      for (o = iter->key; o < len; o++) {\n         if (!data[o]) {\n            iter->d1 = ++o;\n            goto fill_data_fields;\n         }\n      }\n   } else {\n      o = iter->key + next_keylen + 1;\n      iter->d1 = o;\n      goto fill_data_fields;\n   }\n\n   goto mark_invalid;\n\nfill_data_fields:\n\n   *key = bson_iter_key_unsafe (iter);\n   *bson_type = ITER_TYPE (iter);\n\n   switch (*bson_type) {\n   case BSON_TYPE_DATE_TIME:\n   case BSON_TYPE_DOUBLE:\n   case BSON_TYPE_INT64:\n   case BSON_TYPE_TIMESTAMP:\n      iter->next_off = o + 8;\n      break;\n   case BSON_TYPE_CODE:\n   case BSON_TYPE_SYMBOL:\n   case BSON_TYPE_UTF8: {\n      uint32_t l;\n\n      if ((o + 4) >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l > (len - (o + 4))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 4 + l;\n\n      /*\n       * Make sure the string length includes the NUL byte.\n       */\n      if (BSON_UNLIKELY ((l == 0) || (iter->next_off >= len))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      /*\n       * Make sure the last byte is a NUL byte.\n       */\n      if (BSON_UNLIKELY ((iter->raw + iter->d2)[l - 1] != '\\0')) {\n         iter->err_off = o + 4 + l - 1;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_BINARY: {\n      bson_subtype_t subtype;\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 5;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l >= (len - o)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      subtype = *(iter->raw + iter->d2);\n\n      if (subtype == BSON_SUBTYPE_BINARY_DEPRECATED) {\n         int32_t binary_len;\n\n         if (l < 4) {\n            iter->err_off = o;\n            goto mark_invalid;\n         }\n\n         /* subtype 2 has a redundant length header in the data */\n         memcpy (&binary_len, (iter->raw + iter->d3), sizeof (binary_len));\n         binary_len = BSON_UINT32_FROM_LE (binary_len);\n         if (binary_len + 4 != l) {\n            iter->err_off = iter->d3;\n            goto mark_invalid;\n         }\n      }\n\n      iter->next_off = o + 5 + l;\n   } break;\n   case BSON_TYPE_ARRAY:\n   case BSON_TYPE_DOCUMENT: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l > len) || (l > (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n   } break;\n   case BSON_TYPE_OID:\n      iter->next_off = o + 12;\n      break;\n   case BSON_TYPE_BOOL: {\n      char val;\n\n      if (iter->d1 >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&val, iter->raw + iter->d1, 1);\n      if (val != 0x00 && val != 0x01) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_REGEX: {\n      bool eor = false;\n      bool eoo = false;\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            iter->d2 = ++o;\n            eor = true;\n            break;\n         }\n      }\n\n      if (!eor) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            eoo = true;\n            break;\n         }\n      }\n\n      if (!eoo) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_DBPOINTER: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      /* Check valid string length. l counts '\\0' but not 4 bytes for itself. */\n      if (l == 0 || l > (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if (*(iter->raw + o + l + 3)) {\n         /* not null terminated */\n         iter->err_off = o + l + 3;\n         goto mark_invalid;\n      }\n\n      iter->d3 = o + 4 + l;\n      iter->next_off = o + 4 + l + 12;\n   } break;\n   case BSON_TYPE_CODEWSCOPE: {\n      uint32_t l;\n      uint32_t doclen;\n\n      if ((len < 19) || (o >= (len - 14))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 8;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l < 14) || (l >= (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n\n      if (iter->next_off >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d2, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l == 0 || l >= (len - o - 4 - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if ((o + 4 + 4 + l + 4) >= iter->next_off) {\n         iter->err_off = o + 4;\n         goto mark_invalid;\n      }\n\n      iter->d4 = o + 4 + 4 + l;\n      memcpy (&doclen, iter->raw + iter->d4, sizeof (doclen));\n      doclen = BSON_UINT32_FROM_LE (doclen);\n\n      if ((o + 4 + 4 + l + doclen) != iter->next_off) {\n         iter->err_off = o + 4 + 4 + l;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_INT32:\n      iter->next_off = o + 4;\n      break;\n   case BSON_TYPE_DECIMAL128:\n      iter->next_off = o + 16;\n      break;\n   case BSON_TYPE_MAXKEY:\n   case BSON_TYPE_MINKEY:\n   case BSON_TYPE_NULL:\n   case BSON_TYPE_UNDEFINED:\n      iter->next_off = o;\n      break;\n   default:\n      *unsupported = true;\n   /* FALL THROUGH */\n   case BSON_TYPE_EOD:\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   /*\n    * Check to see if any of the field locations would overflow the\n    * current BSON buffer. If so, set the error location to the offset\n    * of where the field starts.\n    */\n   if (iter->next_off >= len) {\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   iter->err_off = 0;\n\n   return true;\n\nmark_invalid:\n   iter->raw = NULL;\n   iter->len = 0;\n   iter->next_off = 0;\n\n   return false;\n}", "func_src_after": "_bson_iter_next_internal (bson_iter_t *iter,    /* INOUT */\n                          uint32_t next_keylen, /* IN */\n                          const char **key,     /* OUT */\n                          uint32_t *bson_type,  /* OUT */\n                          bool *unsupported)    /* OUT */\n{\n   const uint8_t *data;\n   uint32_t o;\n   unsigned int len;\n\n   BSON_ASSERT (iter);\n\n   *unsupported = false;\n\n   if (!iter->raw) {\n      *key = NULL;\n      *bson_type = BSON_TYPE_EOD;\n      return false;\n   }\n\n   data = iter->raw;\n   len = iter->len;\n\n   iter->off = iter->next_off;\n   iter->type = iter->off;\n   iter->key = iter->off + 1;\n   iter->d1 = 0;\n   iter->d2 = 0;\n   iter->d3 = 0;\n   iter->d4 = 0;\n\n   if (next_keylen == 0) {\n      /* iterate from start to end of NULL-terminated key string */\n      for (o = iter->key; o < len; o++) {\n         if (!data[o]) {\n            iter->d1 = ++o;\n            goto fill_data_fields;\n         }\n      }\n   } else {\n      o = iter->key + next_keylen + 1;\n      iter->d1 = o;\n      goto fill_data_fields;\n   }\n\n   goto mark_invalid;\n\nfill_data_fields:\n\n   *key = bson_iter_key_unsafe (iter);\n   *bson_type = ITER_TYPE (iter);\n\n   switch (*bson_type) {\n   case BSON_TYPE_DATE_TIME:\n   case BSON_TYPE_DOUBLE:\n   case BSON_TYPE_INT64:\n   case BSON_TYPE_TIMESTAMP:\n      iter->next_off = o + 8;\n      break;\n   case BSON_TYPE_CODE:\n   case BSON_TYPE_SYMBOL:\n   case BSON_TYPE_UTF8: {\n      uint32_t l;\n\n      if ((o + 4) >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l > (len - (o + 4))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 4 + l;\n\n      /*\n       * Make sure the string length includes the NUL byte.\n       */\n      if (BSON_UNLIKELY ((l == 0) || (iter->next_off >= len))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      /*\n       * Make sure the last byte is a NUL byte.\n       */\n      if (BSON_UNLIKELY ((iter->raw + iter->d2)[l - 1] != '\\0')) {\n         iter->err_off = o + 4 + l - 1;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_BINARY: {\n      bson_subtype_t subtype;\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 5;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l >= (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      subtype = *(iter->raw + iter->d2);\n\n      if (subtype == BSON_SUBTYPE_BINARY_DEPRECATED) {\n         int32_t binary_len;\n\n         if (l < 4) {\n            iter->err_off = o;\n            goto mark_invalid;\n         }\n\n         /* subtype 2 has a redundant length header in the data */\n         memcpy (&binary_len, (iter->raw + iter->d3), sizeof (binary_len));\n         binary_len = BSON_UINT32_FROM_LE (binary_len);\n         if (binary_len + 4 != l) {\n            iter->err_off = iter->d3;\n            goto mark_invalid;\n         }\n      }\n\n      iter->next_off = o + 5 + l;\n   } break;\n   case BSON_TYPE_ARRAY:\n   case BSON_TYPE_DOCUMENT: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l > len) || (l > (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n   } break;\n   case BSON_TYPE_OID:\n      iter->next_off = o + 12;\n      break;\n   case BSON_TYPE_BOOL: {\n      char val;\n\n      if (iter->d1 >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&val, iter->raw + iter->d1, 1);\n      if (val != 0x00 && val != 0x01) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_REGEX: {\n      bool eor = false;\n      bool eoo = false;\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            iter->d2 = ++o;\n            eor = true;\n            break;\n         }\n      }\n\n      if (!eor) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      for (; o < len; o++) {\n         if (!data[o]) {\n            eoo = true;\n            break;\n         }\n      }\n\n      if (!eoo) {\n         iter->err_off = iter->next_off;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + 1;\n   } break;\n   case BSON_TYPE_DBPOINTER: {\n      uint32_t l;\n\n      if (o >= (len - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      /* Check valid string length. l counts '\\0' but not 4 bytes for itself. */\n      if (l == 0 || l > (len - o - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if (*(iter->raw + o + l + 3)) {\n         /* not null terminated */\n         iter->err_off = o + l + 3;\n         goto mark_invalid;\n      }\n\n      iter->d3 = o + 4 + l;\n      iter->next_off = o + 4 + l + 12;\n   } break;\n   case BSON_TYPE_CODEWSCOPE: {\n      uint32_t l;\n      uint32_t doclen;\n\n      if ((len < 19) || (o >= (len - 14))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->d2 = o + 4;\n      iter->d3 = o + 8;\n\n      memcpy (&l, iter->raw + iter->d1, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if ((l < 14) || (l >= (len - o))) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      iter->next_off = o + l;\n\n      if (iter->next_off >= len) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      memcpy (&l, iter->raw + iter->d2, sizeof (l));\n      l = BSON_UINT32_FROM_LE (l);\n\n      if (l == 0 || l >= (len - o - 4 - 4)) {\n         iter->err_off = o;\n         goto mark_invalid;\n      }\n\n      if ((o + 4 + 4 + l + 4) >= iter->next_off) {\n         iter->err_off = o + 4;\n         goto mark_invalid;\n      }\n\n      iter->d4 = o + 4 + 4 + l;\n      memcpy (&doclen, iter->raw + iter->d4, sizeof (doclen));\n      doclen = BSON_UINT32_FROM_LE (doclen);\n\n      if ((o + 4 + 4 + l + doclen) != iter->next_off) {\n         iter->err_off = o + 4 + 4 + l;\n         goto mark_invalid;\n      }\n   } break;\n   case BSON_TYPE_INT32:\n      iter->next_off = o + 4;\n      break;\n   case BSON_TYPE_DECIMAL128:\n      iter->next_off = o + 16;\n      break;\n   case BSON_TYPE_MAXKEY:\n   case BSON_TYPE_MINKEY:\n   case BSON_TYPE_NULL:\n   case BSON_TYPE_UNDEFINED:\n      iter->next_off = o;\n      break;\n   default:\n      *unsupported = true;\n   /* FALL THROUGH */\n   case BSON_TYPE_EOD:\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   /*\n    * Check to see if any of the field locations would overflow the\n    * current BSON buffer. If so, set the error location to the offset\n    * of where the field starts.\n    */\n   if (iter->next_off >= len) {\n      iter->err_off = o;\n      goto mark_invalid;\n   }\n\n   iter->err_off = 0;\n\n   return true;\n\nmark_invalid:\n   iter->raw = NULL;\n   iter->len = 0;\n   iter->next_off = 0;\n\n   return false;\n}", "commit_link": "github.com/mongodb/mongo-c-driver/commit/0d9a4d98bfdf4acd2c0138d4aaeb4e2e0934bd84", "file_name": "src/libbson/src/bson/bson-iter.c", "vul_type": "cwe-125", "description": "Write a C function to iterate over BSON document fields, updating the iterator state and handling various BSON types."}
{"func_name": "create_token", "func_src_before": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise HTTPForbidden()", "func_src_after": "    async def create_token(self, uid):\n        \"\"\" get session token by user id \"\"\"\n        try:\n            token = hashtoken().decode()\n            async with aiopg.create_pool(self.dsn) as pool:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        query = f\"\"\"\n                INSERT INTO aio.tokens (token, user_id)\n                VALUES ('{token}', '{uid}') \"\"\"\n                        await cur.execute(query)\n                        return token\n        except Exception as err:\n            print(err)\n            raise web.HTTPForbidden()", "line_changes": {"deleted": [{"line_no": 15, "char_start": 585, "char_end": 618, "line": "            raise HTTPForbidden()\n"}], "added": [{"line_no": 15, "char_start": 585, "char_end": 622, "line": "            raise web.HTTPForbidden()\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 603, "char_end": 607, "chars": "web."}]}, "commit_link": "github.com/TeaTracer/aio-test/commit/3da13f66b0c1ab1d26bf4b56f476ade60a43d8d4", "file_name": "db.py", "vul_type": "cwe-089", "commit_msg": "Fix sql injections in token and password verifications. Fix HTTTPForbidden exception.", "description": "Write a Python function using `aiopg` to asynchronously insert a new session token into a database for a given user ID and handle exceptions by raising an HTTP forbidden error."}
{"func_name": "index", "func_src_before": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "func_src_after": "  def index\n    authorize! :posts, @post_type\n    per_page = current_site.admin_per_page\n    posts_all = @post_type.posts.eager_load(:parent, :post_type)\n    if params[:taxonomy].present? && params[:taxonomy_id].present?\n      if params[:taxonomy] == \"category\"\n        cat_owner = current_site.full_categories.find(params[:taxonomy_id]).decorate\n        posts_all = cat_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.category\"), @post_type.the_admin_url(\"category\")\n        add_breadcrumb cat_owner.the_title, cat_owner.the_edit_url\n      end\n\n      if params[:taxonomy] == \"post_tag\"\n        tag_owner = current_site.post_tags.find(params[:taxonomy_id]).decorate\n        posts_all = tag_owner.posts\n        add_breadcrumb t(\"camaleon_cms.admin.post_type.tags\"), @post_type.the_admin_url(\"tag\")\n        add_breadcrumb tag_owner.the_title, tag_owner.the_edit_url\n      end\n    end\n\n    if params[:q].present?\n      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n    end\n\n    @posts = posts_all\n    params[:s] = 'published' unless params[:s].present?\n    @lists_tab = params[:s]\n    add_breadcrumb I18n.t(\"camaleon_cms.admin.post_type.#{params[:s]}\") if params[:s].present?\n    case params[:s]\n      when \"published\", \"pending\", \"draft\", \"trash\"\n        @posts = @posts.where(status:  params[:s])\n\n      when \"all\"\n        @posts = @posts.no_trash\n    end", "line_changes": {"deleted": [{"line_no": 22, "char_start": 929, "char_end": 1069, "line": "      posts_all = posts_all.where(params[:q].split(\" \").map{|text| \"#{CamaleonCms::Post.table_name}.title LIKE '%#{text}%'\" }.join(\" OR \"))\n"}], "added": [{"line_no": 22, "char_start": 929, "char_end": 1030, "line": "      posts_all = posts_all.where(\"#{CamaleonCms::Post.table_name}.title LIKE ?\", \"%#{params[:q]}%\")\n"}, {"line_no": 23, "char_start": 1030, "char_end": 1089, "line": "      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\"\n"}]}, "char_changes": {"deleted": [{"char_start": 963, "char_end": 996, "chars": "params[:q].split(\" \").map{|text| "}, {"char_start": 1040, "char_end": 1068, "chars": "'%#{text}%'\" }.join(\" OR \"))"}], "added": [{"char_start": 1007, "char_end": 1088, "chars": "?\", \"%#{params[:q]}%\")\n      puts \"@@@@@@@@@@@@@@@@@@@@@@@@: #{posts_all.to_sql}\""}]}, "commit_link": "github.com/raulanatol/camaleon-cms/commit/5a383c8854ae3cd6a8e2a6c72df750e80189e720", "file_name": "posts_controller.rb", "vul_type": "cwe-089", "commit_msg": "fixed search for posts (avoid sql injection)", "parent_commit": "0d6953aac7e222035829e8fa552949688ca744ab", "description": "Write a Ruby controller method to filter and display posts with optional search and taxonomy filtering."}
{"func_name": "tag_to_tag_num", "func_src_before": "    def tag_to_tag_num(self, tag):\n        ''' Returns tag_num given tag. '''\n\n        q = \"SELECT rowid FROM tags WHERE tag = '\" + tag + \"'\"\n        self.query(q)\n        return self.c.fetchone()[0]", "func_src_after": "    def tag_to_tag_num(self, tag):\n        ''' Returns tag_num given tag. '''\n\n        q = \"SELECT rowid FROM tags WHERE tag = ?\"\n        self.query(q, tag)\n        return self.c.fetchone()[0]", "commit_link": "github.com/pukkapies/urop2019/commit/3ca2e2c291d2d5fe262d20a8e0520bdfb622432b", "file_name": "modules/query_lastfm.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves the numerical ID of a tag from a database."}
{"func_name": "deleteKey", "func_src_before": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal BAD_REQUEST\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\n\tif re.search('[^a-zA-Z0-9]', token_data['key']):\n\t\traise FoxlockError(BAD_REQUEST, 'Invalid key requested')\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "func_src_after": "def deleteKey(client):\n\t\"\"\"Deletes the specified key.\n\tReturns an error if the key doesn't exist\n\t\"\"\"\n\tglobal NOT_FOUND\n\n\tvalidateClient(client)\n\tclient_pub_key = loadClientRSAKey(client)\n\ttoken_data = decodeRequestToken(request.data, client_pub_key)\n\tvalidateKeyName(token_data['key'])\n\n\ttry:\n\t\tos.remove('keys/%s/%s.key' % (client, token_data['key']))\n\texcept FileNotFoundError:\n\t\traise FoxlockError(NOT_FOUND, \"Key '%s' not found\" % token_data['key'])\n\n\treturn \"Key '%s' successfully deleted\" % token_data['key']", "commit_link": "github.com/Mimickal/FoxLock/commit/7c665e556987f4e2c1a75e143a1e80ae066ad833", "file_name": "impl.py", "vul_type": "cwe-022", "description": "Write a Python function named `deleteKey` that removes a client's key file and handles the case where the key does not exist."}
{"func_name": "hi3660_stub_clk_probe", "func_src_before": "static int hi3660_stub_clk_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *res;\n\tunsigned int i;\n\tint ret;\n\n\t/* Use mailbox client without blocking */\n\tstub_clk_chan.cl.dev = dev;\n\tstub_clk_chan.cl.tx_done = NULL;\n\tstub_clk_chan.cl.tx_block = false;\n\tstub_clk_chan.cl.knows_txdone = false;\n\n\t/* Allocate mailbox channel */\n\tstub_clk_chan.mbox = mbox_request_channel(&stub_clk_chan.cl, 0);\n\tif (IS_ERR(stub_clk_chan.mbox))\n\t\treturn PTR_ERR(stub_clk_chan.mbox);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tfreq_reg = devm_ioremap(dev, res->start, resource_size(res));\n\tif (!freq_reg)\n\t\treturn -ENOMEM;\n\n\tfreq_reg += HI3660_STUB_CLOCK_DATA;\n\n\tfor (i = 0; i < HI3660_CLK_STUB_NUM; i++) {\n\t\tret = devm_clk_hw_register(&pdev->dev, &hi3660_stub_clks[i].hw);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn devm_of_clk_add_hw_provider(&pdev->dev, hi3660_stub_clk_hw_get,\n\t\t\t\t\t   hi3660_stub_clks);\n}", "func_src_after": "static int hi3660_stub_clk_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *res;\n\tunsigned int i;\n\tint ret;\n\n\t/* Use mailbox client without blocking */\n\tstub_clk_chan.cl.dev = dev;\n\tstub_clk_chan.cl.tx_done = NULL;\n\tstub_clk_chan.cl.tx_block = false;\n\tstub_clk_chan.cl.knows_txdone = false;\n\n\t/* Allocate mailbox channel */\n\tstub_clk_chan.mbox = mbox_request_channel(&stub_clk_chan.cl, 0);\n\tif (IS_ERR(stub_clk_chan.mbox))\n\t\treturn PTR_ERR(stub_clk_chan.mbox);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res)\n\t\treturn -EINVAL;\n\tfreq_reg = devm_ioremap(dev, res->start, resource_size(res));\n\tif (!freq_reg)\n\t\treturn -ENOMEM;\n\n\tfreq_reg += HI3660_STUB_CLOCK_DATA;\n\n\tfor (i = 0; i < HI3660_CLK_STUB_NUM; i++) {\n\t\tret = devm_clk_hw_register(&pdev->dev, &hi3660_stub_clks[i].hw);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn devm_of_clk_add_hw_provider(&pdev->dev, hi3660_stub_clk_hw_get,\n\t\t\t\t\t   hi3660_stub_clks);\n}", "commit_link": "github.com/torvalds/linux/commit/9903e41ae1f5d50c93f268ca3304d4d7c64b9311", "file_name": "drivers/clk/hisilicon/clk-hi3660-stub.c", "vul_type": "cwe-476", "description": "Write a C function for initializing a clock in the hi3660 platform driver."}
{"func_name": "changeValue", "func_src_before": "\tchangeValue:function(val){ \n\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n\t\tthis.editValue(val)\n\t},", "func_src_after": "\tchangeValue:function(val){ \n\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n\t\tthis.editValue(val)\n\t},", "line_changes": {"deleted": [{"line_no": 2, "char_start": 29, "char_end": 107, "line": "\t\t$('#akSelectValueStatic_'+val).html( $('#akSelectValueField_'+val).val() );\n"}], "added": [{"line_no": 2, "char_start": 29, "char_end": 110, "line": "\t\tvar txtValue = $('<div/>').text($('#akSelectValueField_'+val).val()).html();\t\t\n"}, {"line_no": 3, "char_start": 110, "char_end": 161, "line": "\t\t$('#akSelectValueStatic_'+val).html( txtValue );\n"}]}, "char_changes": {"deleted": [{"char_start": 48, "char_end": 54, "chars": "Static"}, {"char_start": 62, "char_end": 68, "chars": "html( "}, {"char_start": 85, "char_end": 90, "chars": "Field"}, {"char_start": 98, "char_end": 103, "chars": "val()"}], "added": [{"char_start": 31, "char_end": 63, "chars": "var txtValue = $('<div/>').text("}, {"char_start": 80, "char_end": 85, "chars": "Field"}, {"char_start": 93, "char_end": 112, "chars": "val()).html();\t\t\n\t\t"}, {"char_start": 129, "char_end": 135, "chars": "Static"}, {"char_start": 143, "char_end": 157, "chars": "html( txtValue"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function named `changeValue` that updates the HTML content of an element and calls another function to edit the value."}
{"func_name": "dbd_st_fetch", "func_src_before": "dbd_st_fetch(SV *sth, imp_sth_t* imp_sth)\n{\n  dTHX;\n  int num_fields, ChopBlanks, i, rc;\n  unsigned long *lengths;\n  AV *av;\n  int av_length, av_readonly;\n  MYSQL_ROW cols;\n  D_imp_dbh_from_sth;\n  MYSQL* svsock= imp_dbh->pmysql;\n  imp_sth_fbh_t *fbh;\n  D_imp_xxh(sth);\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  MYSQL_BIND *buffer;\n#endif\n  MYSQL_FIELD *fields;\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t-> dbd_st_fetch\\n\");\n\n#if MYSQL_ASYNC\n  if(imp_dbh->async_query_in_flight) {\n      if(mysql_db_async_result(sth, &imp_sth->result) <= 0) {\n        return Nullav;\n      }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (!DBIc_ACTIVE(imp_sth) )\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"no statement executing\\n\",NULL);\n      return Nullav;\n    }\n\n    if (imp_sth->fetch_done)\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"fetch() but fetch already done\",NULL);\n      return Nullav;\n    }\n\n    if (!imp_sth->done_desc)\n    {\n      if (!dbd_describe(sth, imp_sth))\n      {\n        do_error(sth, JW_ERR_SEQUENCE, \"Error while describe result set.\",\n                 NULL);\n        return Nullav;\n      }\n    }\n  }\n#endif\n\n  ChopBlanks = DBIc_is(imp_sth, DBIcf_ChopBlanks);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                  \"\\t\\tdbd_st_fetch for %p, chopblanks %d\\n\",\n                  sth, ChopBlanks);\n\n  if (!imp_sth->result)\n  {\n    do_error(sth, JW_ERR_SEQUENCE, \"fetch() without execute()\" ,NULL);\n    return Nullav;\n  }\n\n  /* fix from 2.9008 */\n  imp_dbh->pmysql->net.last_errno = 0;\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch calling mysql_fetch\\n\");\n\n    if ((rc= mysql_stmt_fetch(imp_sth->stmt)))\n    {\n      if (rc == 1)\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_stmt_sqlstate(imp_sth->stmt));\n\n#if MYSQL_VERSION_ID >= MYSQL_VERSION_5_0 \n      if (rc == MYSQL_DATA_TRUNCATED) {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch data truncated\\n\");\n        goto process;\n      }\n#endif\n\n      if (rc == MYSQL_NO_DATA)\n      {\n        /* Update row_num to affected_rows value */\n        imp_sth->row_num= mysql_stmt_affected_rows(imp_sth->stmt);\n        imp_sth->fetch_done=1;\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch no data\\n\");\n      }\n\n      dbd_st_finish(sth, imp_sth);\n\n      return Nullav;\n    }\n\nprocess:\n    imp_sth->currow++;\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n    num_fields=mysql_stmt_field_count(imp_sth->stmt);\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tdbd_st_fetch called mysql_fetch, rc %d num_fields %d\\n\",\n                    rc, num_fields);\n\n    for (\n         buffer= imp_sth->buffer,\n         fbh= imp_sth->fbh,\n         i= 0;\n         i < num_fields;\n         i++,\n         fbh++,\n         buffer++\n        )\n    {\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n      STRLEN len;\n\n      /* This is wrong, null is not being set correctly\n       * This is not the way to determine length (this would break blobs!)\n       */\n      if (fbh->is_null)\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n      else\n      {\n        /* In case of BLOB/TEXT fields we allocate only 8192 bytes\n           in dbd_describe() for data. Here we know real size of field\n           so we should increase buffer size and refetch column value\n        */\n        if (fbh->length > buffer->buffer_length || fbh->error)\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n              \"\\t\\tRefetch BLOB/TEXT column: %d, length: %lu, error: %d\\n\",\n              i, fbh->length, fbh->error);\n\n          Renew(fbh->data, fbh->length, char);\n          buffer->buffer_length= fbh->length;\n          buffer->buffer= (char *) fbh->data;\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tbefore buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n\n          /*TODO: Use offset instead of 0 to fetch only remain part of data*/\n          if (mysql_stmt_fetch_column(imp_sth->stmt, buffer , i, 0))\n            do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                     mysql_stmt_error(imp_sth->stmt),\n                     mysql_stmt_sqlstate(imp_sth->stmt));\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tafter buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n        }\n\n        /* This does look a lot like Georg's PHP driver doesn't it?  --Brian */\n        /* Credit due to Georg - mysqli_api.c  ;) --PMG */\n        switch (buffer->buffer_type) {\n        case MYSQL_TYPE_DOUBLE:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch double data %f\\n\", fbh->ddata);\n          sv_setnv(sv, fbh->ddata);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch int data %\"IVdf\", unsigned? %d\\n\",\n                          fbh->ldata, buffer->is_unsigned);\n          if (buffer->is_unsigned)\n            sv_setuv(sv, fbh->ldata);\n          else\n            sv_setiv(sv, fbh->ldata);\n\n          break;\n\n        case MYSQL_TYPE_BIT:\n          sv_setpvn(sv, fbh->data, fbh->length);\n\n          break;\n\n        default:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tERROR IN st_fetch_string\");\n          len= fbh->length;\n\t  /* ChopBlanks server-side prepared statement */\n          if (ChopBlanks)\n          {\n            /* \n              see bottom of:\n              http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html\n            */\n            if (fbh->charsetnr != 63)\n              while (len && fbh->data[len-1] == ' ') { --len; }\n          }\n\t  /* END OF ChopBlanks */\n\n          sv_setpvn(sv, fbh->data, len);\n\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n#if MYSQL_VERSION_ID >= FIELD_CHARSETNR_VERSION \n  /* SHOW COLLATION WHERE Id = 63; -- 63 == charset binary, collation binary */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fbh->charsetnr != 63)\n#else\n\tif ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && !(fbh->flags & BINARY_FLAG))\n#endif\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n\n        }\n\n      }\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n\n    return av;\n  }\n  else\n  {\n#endif\n\n    imp_sth->currow++;\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    {\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch result set details\\n\");\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\timp_sth->result=%p\\n\", imp_sth->result);\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_fields=%u\\n\",\n                    mysql_num_fields(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_rows=%llu\\n\",\n                    mysql_num_rows(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_affected_rows=%llu\\n\",\n                    mysql_affected_rows(imp_dbh->pmysql));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch for %p, currow= %d\\n\",\n                    sth,imp_sth->currow);\n    }\n\n    if (!(cols= mysql_fetch_row(imp_sth->result)))\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      {\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch, no more rows to fetch\");\n      }\n      if (mysql_errno(imp_dbh->pmysql))\n        do_error(sth, mysql_errno(imp_dbh->pmysql),\n                 mysql_error(imp_dbh->pmysql),\n                 mysql_sqlstate(imp_dbh->pmysql));\n\n\n#if MYSQL_VERSION_ID >= MULTIPLE_RESULT_SET_VERSION\n      if (!mysql_more_results(svsock))\n#endif\n        dbd_st_finish(sth, imp_sth);\n      return Nullav;\n    }\n\n    num_fields= mysql_num_fields(imp_sth->result);\n    fields= mysql_fetch_fields(imp_sth->result);\n    lengths= mysql_fetch_lengths(imp_sth->result);\n\n    if ((av= DBIc_FIELDS_AV(imp_sth)) != Nullav)\n    {\n      av_length= av_len(av)+1;\n\n      if (av_length != num_fields)              /* Resize array if necessary */\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, size of results array(%d) != num_fields(%d)\\n\",\n                                   av_length, num_fields);\n\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, result fields(%d)\\n\",\n                                   DBIc_NUM_FIELDS(imp_sth));\n\n        av_readonly = SvREADONLY(av);\n\n        if (av_readonly)\n          SvREADONLY_off( av );              /* DBI sets this readonly */\n\n        while (av_length < num_fields)\n        {\n          av_store(av, av_length++, newSV(0));\n        }\n\n        while (av_length > num_fields)\n        {\n          SvREFCNT_dec(av_pop(av));\n          av_length--;\n        }\n        if (av_readonly)\n          SvREADONLY_on(av);\n      }\n    }\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n\n    for (i= 0;  i < num_fields; ++i)\n    {\n      char *col= cols[i];\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n\n      if (col)\n      {\n        STRLEN len= lengths[i];\n        if (ChopBlanks)\n        {\n          while (len && col[len-1] == ' ')\n          {\t--len; }\n        }\n\n        /* Set string value returned from mysql server */\n        sv_setpvn(sv, col, len);\n\n        switch (mysql_to_perl_type(fields[i].type)) {\n        case MYSQL_TYPE_DOUBLE:\n          /* Coerce to dobule and set scalar as NV */\n          (void) SvNV(sv);\n          SvNOK_only(sv);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          /* Coerce to integer and set scalar as UV resp. IV */\n          if (fields[i].flags & UNSIGNED_FLAG)\n          {\n            (void) SvUV(sv);\n            SvIOK_only_UV(sv);\n          }\n          else\n          {\n            (void) SvIV(sv);\n            SvIOK_only(sv);\n          }\n          break;\n\n#if MYSQL_VERSION_ID > NEW_DATATYPE_VERSION\n        case MYSQL_TYPE_BIT:\n          /* Let it as binary string */\n          break;\n#endif\n\n        default:\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n  /* see bottom of: http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fields[i].charsetnr != 63)\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n        }\n      }\n      else\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n    return av;\n\n#if MYSQL_VERSION_ID  >= SERVER_PREPARE_VERSION\n  }\n#endif\n\n}", "func_src_after": "dbd_st_fetch(SV *sth, imp_sth_t* imp_sth)\n{\n  dTHX;\n  int num_fields, ChopBlanks, i, rc;\n  unsigned long *lengths;\n  AV *av;\n  int av_length, av_readonly;\n  MYSQL_ROW cols;\n  D_imp_dbh_from_sth;\n  MYSQL* svsock= imp_dbh->pmysql;\n  imp_sth_fbh_t *fbh;\n  D_imp_xxh(sth);\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  MYSQL_BIND *buffer;\n#endif\n  MYSQL_FIELD *fields;\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t-> dbd_st_fetch\\n\");\n\n#if MYSQL_ASYNC\n  if(imp_dbh->async_query_in_flight) {\n      if(mysql_db_async_result(sth, &imp_sth->result) <= 0) {\n        return Nullav;\n      }\n  }\n#endif\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (!DBIc_ACTIVE(imp_sth) )\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"no statement executing\\n\",NULL);\n      return Nullav;\n    }\n\n    if (imp_sth->fetch_done)\n    {\n      do_error(sth, JW_ERR_SEQUENCE, \"fetch() but fetch already done\",NULL);\n      return Nullav;\n    }\n\n    if (!imp_sth->done_desc)\n    {\n      if (!dbd_describe(sth, imp_sth))\n      {\n        do_error(sth, JW_ERR_SEQUENCE, \"Error while describe result set.\",\n                 NULL);\n        return Nullav;\n      }\n    }\n  }\n#endif\n\n  ChopBlanks = DBIc_is(imp_sth, DBIcf_ChopBlanks);\n\n  if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                  \"\\t\\tdbd_st_fetch for %p, chopblanks %d\\n\",\n                  sth, ChopBlanks);\n\n  if (!imp_sth->result)\n  {\n    do_error(sth, JW_ERR_SEQUENCE, \"fetch() without execute()\" ,NULL);\n    return Nullav;\n  }\n\n  /* fix from 2.9008 */\n  imp_dbh->pmysql->net.last_errno = 0;\n\n#if MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n  if (imp_sth->use_server_side_prepare)\n  {\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch calling mysql_fetch\\n\");\n\n    if ((rc= mysql_stmt_fetch(imp_sth->stmt)))\n    {\n      if (rc == 1)\n        do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                 mysql_stmt_error(imp_sth->stmt),\n                mysql_stmt_sqlstate(imp_sth->stmt));\n\n#if MYSQL_VERSION_ID >= MYSQL_VERSION_5_0 \n      if (rc == MYSQL_DATA_TRUNCATED) {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch data truncated\\n\");\n        goto process;\n      }\n#endif\n\n      if (rc == MYSQL_NO_DATA)\n      {\n        /* Update row_num to affected_rows value */\n        imp_sth->row_num= mysql_stmt_affected_rows(imp_sth->stmt);\n        imp_sth->fetch_done=1;\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tdbd_st_fetch no data\\n\");\n      }\n\n      dbd_st_finish(sth, imp_sth);\n\n      return Nullav;\n    }\n\nprocess:\n    imp_sth->currow++;\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n    num_fields=mysql_stmt_field_count(imp_sth->stmt);\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n                    \"\\t\\tdbd_st_fetch called mysql_fetch, rc %d num_fields %d\\n\",\n                    rc, num_fields);\n\n    for (\n         buffer= imp_sth->buffer,\n         fbh= imp_sth->fbh,\n         i= 0;\n         i < num_fields;\n         i++,\n         fbh++,\n         buffer++\n        )\n    {\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n      STRLEN len;\n\n      /* This is wrong, null is not being set correctly\n       * This is not the way to determine length (this would break blobs!)\n       */\n      if (fbh->is_null)\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n      else\n      {\n        /* In case of BLOB/TEXT fields we allocate only 8192 bytes\n           in dbd_describe() for data. Here we know real size of field\n           so we should increase buffer size and refetch column value\n        */\n        if (fbh->length > buffer->buffer_length || fbh->error)\n        {\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\n              \"\\t\\tRefetch BLOB/TEXT column: %d, length: %lu, error: %d\\n\",\n              i, fbh->length, fbh->error);\n\n          Renew(fbh->data, fbh->length, char);\n          buffer->buffer_length= fbh->length;\n          buffer->buffer= (char *) fbh->data;\n          imp_sth->stmt->bind[i].buffer_length = fbh->length;\n          imp_sth->stmt->bind[i].buffer = (char *)fbh->data;\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tbefore buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n\n          /*TODO: Use offset instead of 0 to fetch only remain part of data*/\n          if (mysql_stmt_fetch_column(imp_sth->stmt, buffer , i, 0))\n            do_error(sth, mysql_stmt_errno(imp_sth->stmt),\n                     mysql_stmt_error(imp_sth->stmt),\n                     mysql_stmt_sqlstate(imp_sth->stmt));\n\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2) {\n            int j;\n            int m = MIN(*buffer->length, buffer->buffer_length);\n            char *ptr = (char*)buffer->buffer;\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\t\\tafter buffer->buffer: \");\n            for (j = 0; j < m; j++) {\n              PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"%c\", *ptr++);\n            }\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh),\"\\n\");\n          }\n        }\n\n        /* This does look a lot like Georg's PHP driver doesn't it?  --Brian */\n        /* Credit due to Georg - mysqli_api.c  ;) --PMG */\n        switch (buffer->buffer_type) {\n        case MYSQL_TYPE_DOUBLE:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch double data %f\\n\", fbh->ddata);\n          sv_setnv(sv, fbh->ddata);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tst_fetch int data %\"IVdf\", unsigned? %d\\n\",\n                          fbh->ldata, buffer->is_unsigned);\n          if (buffer->is_unsigned)\n            sv_setuv(sv, fbh->ldata);\n          else\n            sv_setiv(sv, fbh->ldata);\n\n          break;\n\n        case MYSQL_TYPE_BIT:\n          sv_setpvn(sv, fbh->data, fbh->length);\n\n          break;\n\n        default:\n          if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n            PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t\\tERROR IN st_fetch_string\");\n          len= fbh->length;\n\t  /* ChopBlanks server-side prepared statement */\n          if (ChopBlanks)\n          {\n            /* \n              see bottom of:\n              http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html\n            */\n            if (fbh->charsetnr != 63)\n              while (len && fbh->data[len-1] == ' ') { --len; }\n          }\n\t  /* END OF ChopBlanks */\n\n          sv_setpvn(sv, fbh->data, len);\n\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n#if MYSQL_VERSION_ID >= FIELD_CHARSETNR_VERSION \n  /* SHOW COLLATION WHERE Id = 63; -- 63 == charset binary, collation binary */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fbh->charsetnr != 63)\n#else\n\tif ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && !(fbh->flags & BINARY_FLAG))\n#endif\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n\n        }\n\n      }\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n\n    return av;\n  }\n  else\n  {\n#endif\n\n    imp_sth->currow++;\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n    {\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch result set details\\n\");\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\timp_sth->result=%p\\n\", imp_sth->result);\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_fields=%u\\n\",\n                    mysql_num_fields(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_num_rows=%llu\\n\",\n                    mysql_num_rows(imp_sth->result));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tmysql_affected_rows=%llu\\n\",\n                    mysql_affected_rows(imp_dbh->pmysql));\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch for %p, currow= %d\\n\",\n                    sth,imp_sth->currow);\n    }\n\n    if (!(cols= mysql_fetch_row(imp_sth->result)))\n    {\n      if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      {\n        PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\tdbd_st_fetch, no more rows to fetch\");\n      }\n      if (mysql_errno(imp_dbh->pmysql))\n        do_error(sth, mysql_errno(imp_dbh->pmysql),\n                 mysql_error(imp_dbh->pmysql),\n                 mysql_sqlstate(imp_dbh->pmysql));\n\n\n#if MYSQL_VERSION_ID >= MULTIPLE_RESULT_SET_VERSION\n      if (!mysql_more_results(svsock))\n#endif\n        dbd_st_finish(sth, imp_sth);\n      return Nullav;\n    }\n\n    num_fields= mysql_num_fields(imp_sth->result);\n    fields= mysql_fetch_fields(imp_sth->result);\n    lengths= mysql_fetch_lengths(imp_sth->result);\n\n    if ((av= DBIc_FIELDS_AV(imp_sth)) != Nullav)\n    {\n      av_length= av_len(av)+1;\n\n      if (av_length != num_fields)              /* Resize array if necessary */\n      {\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, size of results array(%d) != num_fields(%d)\\n\",\n                                   av_length, num_fields);\n\n        if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n          PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, result fields(%d)\\n\",\n                                   DBIc_NUM_FIELDS(imp_sth));\n\n        av_readonly = SvREADONLY(av);\n\n        if (av_readonly)\n          SvREADONLY_off( av );              /* DBI sets this readonly */\n\n        while (av_length < num_fields)\n        {\n          av_store(av, av_length++, newSV(0));\n        }\n\n        while (av_length > num_fields)\n        {\n          SvREFCNT_dec(av_pop(av));\n          av_length--;\n        }\n        if (av_readonly)\n          SvREADONLY_on(av);\n      }\n    }\n\n    av= DBIc_DBISTATE(imp_sth)->get_fbav(imp_sth);\n\n    for (i= 0;  i < num_fields; ++i)\n    {\n      char *col= cols[i];\n      SV *sv= AvARRAY(av)[i]; /* Note: we (re)use the SV in the AV\t*/\n\n      if (col)\n      {\n        STRLEN len= lengths[i];\n        if (ChopBlanks)\n        {\n          while (len && col[len-1] == ' ')\n          {\t--len; }\n        }\n\n        /* Set string value returned from mysql server */\n        sv_setpvn(sv, col, len);\n\n        switch (mysql_to_perl_type(fields[i].type)) {\n        case MYSQL_TYPE_DOUBLE:\n          /* Coerce to dobule and set scalar as NV */\n          (void) SvNV(sv);\n          SvNOK_only(sv);\n          break;\n\n        case MYSQL_TYPE_LONG:\n        case MYSQL_TYPE_LONGLONG:\n          /* Coerce to integer and set scalar as UV resp. IV */\n          if (fields[i].flags & UNSIGNED_FLAG)\n          {\n            (void) SvUV(sv);\n            SvIOK_only_UV(sv);\n          }\n          else\n          {\n            (void) SvIV(sv);\n            SvIOK_only(sv);\n          }\n          break;\n\n#if MYSQL_VERSION_ID > NEW_DATATYPE_VERSION\n        case MYSQL_TYPE_BIT:\n          /* Let it as binary string */\n          break;\n#endif\n\n        default:\n\t/* UTF8 */\n        /*HELMUT*/\n#if defined(sv_utf8_decode) && MYSQL_VERSION_ID >=SERVER_PREPARE_VERSION\n\n  /* see bottom of: http://www.mysql.org/doc/refman/5.0/en/c-api-datatypes.html */\n        if ((imp_dbh->enable_utf8 || imp_dbh->enable_utf8mb4) && fields[i].charsetnr != 63)\n\t  sv_utf8_decode(sv);\n#endif\n\t/* END OF UTF8 */\n          break;\n        }\n      }\n      else\n        (void) SvOK_off(sv);  /*  Field is NULL, return undef  */\n    }\n\n    if (DBIc_TRACE_LEVEL(imp_xxh) >= 2)\n      PerlIO_printf(DBIc_LOGPIO(imp_xxh), \"\\t<- dbd_st_fetch, %d cols\\n\", num_fields);\n    return av;\n\n#if MYSQL_VERSION_ID  >= SERVER_PREPARE_VERSION\n  }\n#endif\n\n}", "commit_link": "github.com/perl5-dbi/DBD-mysql/commit/3619c170461a3107a258d1fd2d00ed4832adb1b1", "file_name": "dbdimp.c", "vul_type": "cwe-416", "description": "Write a Perl function to fetch a row of data from a MySQL database using the DBI module."}
{"func_name": "hfs_cat_traverse", "func_src_before": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                uint16_t keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "func_src_after": "hfs_cat_traverse(HFS_INFO * hfs,\n    TSK_HFS_BTREE_CB a_cb, void *ptr)\n{\n    TSK_FS_INFO *fs = &(hfs->fs_info);\n    uint32_t cur_node;          /* node id of the current node */\n    char *node;\n\n    uint16_t nodesize;\n    uint8_t is_done = 0;\n\n    tsk_error_reset();\n\n    nodesize = tsk_getu16(fs->endian, hfs->catalog_header.nodesize);\n    if ((node = (char *) tsk_malloc(nodesize)) == NULL)\n        return 1;\n\n    /* start at root node */\n    cur_node = tsk_getu32(fs->endian, hfs->catalog_header.rootNode);\n\n    /* if the root node is zero, then the extents btree is empty */\n    /* if no files have overflow extents, the Extents B-tree still\n       exists on disk, but is an empty B-tree containing only\n       the header node */\n    if (cur_node == 0) {\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: \"\n                \"empty extents btree\\n\");\n        free(node);\n        return 1;\n    }\n\n    if (tsk_verbose)\n        tsk_fprintf(stderr, \"hfs_cat_traverse: starting at \"\n            \"root node %\" PRIu32 \"; nodesize = %\"\n            PRIu16 \"\\n\", cur_node, nodesize);\n\n    /* Recurse down to the needed leaf nodes and then go forward */\n    is_done = 0;\n    while (is_done == 0) {\n        TSK_OFF_T cur_off;      /* start address of cur_node */\n        uint16_t num_rec;       /* number of records in this node */\n        ssize_t cnt;\n        hfs_btree_node *node_desc;\n\n        // sanity check\n        if (cur_node > tsk_getu32(fs->endian,\n                hfs->catalog_header.totalNodes)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n                (\"hfs_cat_traverse: Node %d too large for file\", cur_node);\n            free(node);\n            return 1;\n        }\n\n        // read the current node\n        cur_off = cur_node * nodesize;\n        cnt = tsk_fs_attr_read(hfs->catalog_attr, cur_off,\n            node, nodesize, 0);\n        if (cnt != nodesize) {\n            if (cnt >= 0) {\n                tsk_error_reset();\n                tsk_error_set_errno(TSK_ERR_FS_READ);\n            }\n            tsk_error_set_errstr2\n                (\"hfs_cat_traverse: Error reading node %d at offset %\"\n                PRIuOFF, cur_node, cur_off);\n            free(node);\n            return 1;\n        }\n\n        // process the header / descriptor\n        if (nodesize < sizeof(hfs_btree_node)) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr\n            (\"hfs_cat_traverse: Node size %d is too small to be valid\", nodesize);\n            free(node);\n            return 1;\n        }\n        node_desc = (hfs_btree_node *) node;\n        num_rec = tsk_getu16(fs->endian, node_desc->num_rec);\n\n        if (tsk_verbose)\n            tsk_fprintf(stderr, \"hfs_cat_traverse: node %\" PRIu32\n                \" @ %\" PRIu64 \" has %\" PRIu16 \" records\\n\",\n                cur_node, cur_off, num_rec);\n\n        if (num_rec == 0) {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: zero records in node %\"\n                PRIu32, cur_node);\n            free(node);\n            return 1;\n        }\n\n        /* With an index node, find the record with the largest key that is smaller\n         * to or equal to cnid */\n        if (node_desc->type == HFS_BT_NODE_TYPE_IDX) {\n            uint32_t next_node = 0;\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in index node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \" ; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n\n\n                /* save the info from this record unless it is too big */\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_IDX, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n                // record the closest entry\n                else if ((retval == HFS_BTREE_CB_IDX_LT)\n                    || (next_node == 0)) {\n                    hfs_btree_index_record *idx_rec;\n                    int keylen =\n                        2 + hfs_get_idxkeylen(hfs, tsk_getu16(fs->endian,\n                            key->key_len), &(hfs->catalog_header));\n                    if (rec_off + keylen > nodesize) {\n                        tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                        tsk_error_set_errstr\n                            (\"hfs_cat_traverse: offset of record and keylength %d in index node %d too large (%d vs %\"\n                            PRIu16 \")\", rec, cur_node,\n                            (int) rec_off + keylen, nodesize);\n                        free(node);\n                        return 1;\n                    }\n                    idx_rec =\n                        (hfs_btree_index_record *) & node[rec_off +\n                        keylen];\n                    next_node = tsk_getu32(fs->endian, idx_rec->childNode);\n                }\n                if (retval == HFS_BTREE_CB_IDX_EQGT) {\n                    // move down to the next node\n                    break;\n                }\n            }\n            // check if we found a relevant node\n            if (next_node == 0) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: did not find any keys in index node %d\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            // TODO: Handle multinode loops\n            if (next_node == cur_node) {\n                tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                tsk_error_set_errstr\n                    (\"hfs_cat_traverse: node %d references itself as next node\",\n                    cur_node);\n                is_done = 1;\n                break;\n            }\n            cur_node = next_node;\n        }\n\n        /* With a leaf, we look for the specific record. */\n        else if (node_desc->type == HFS_BT_NODE_TYPE_LEAF) {\n            int rec;\n\n            for (rec = 0; rec < num_rec; ++rec) {\n                size_t rec_off;\n                hfs_btree_key_cat *key;\n                uint8_t retval;\n                int keylen;\n\n                // get the record offset in the node\n                rec_off =\n                    tsk_getu16(fs->endian,\n                    &node[nodesize - (rec + 1) * 2]);\n                if (rec_off > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: offset of record %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, (int) rec_off,\n                        nodesize);\n                    free(node);\n                    return 1;\n                }\n                key = (hfs_btree_key_cat *) & node[rec_off];\n\n                keylen = 2 + tsk_getu16(hfs->fs_info.endian, key->key_len);\n                if ((keylen) > nodesize) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr\n                        (\"hfs_cat_traverse: length of key %d in leaf node %d too large (%d vs %\"\n                        PRIu16 \")\", rec, cur_node, keylen, nodesize);\n                    free(node);\n                    return 1;\n                }\n\n                /*\n                   if (tsk_verbose)\n                   tsk_fprintf(stderr,\n                   \"hfs_cat_traverse: record %\" PRIu16\n                   \"; keylen %\" PRIu16 \" (%\" PRIu32 \")\\n\", rec,\n                   tsk_getu16(fs->endian, key->key_len),\n                   tsk_getu32(fs->endian, key->parent_cnid));\n                 */\n                //                rec_cnid = tsk_getu32(fs->endian, key->file_id);\n\n                retval =\n                    a_cb(hfs, HFS_BT_NODE_TYPE_LEAF, key,\n                    cur_off + rec_off, ptr);\n                if (retval == HFS_BTREE_CB_LEAF_STOP) {\n                    is_done = 1;\n                    break;\n                }\n                else if (retval == HFS_BTREE_CB_ERR) {\n                    tsk_error_set_errno(TSK_ERR_FS_GENFS);\n                    tsk_error_set_errstr2\n                        (\"hfs_cat_traverse: Callback returned error\");\n                    free(node);\n                    return 1;\n                }\n            }\n\n            // move right to the next node if we got this far\n            if (is_done == 0) {\n                cur_node = tsk_getu32(fs->endian, node_desc->flink);\n                if (cur_node == 0) {\n                    is_done = 1;\n                }\n                if (tsk_verbose)\n                    tsk_fprintf(stderr,\n                        \"hfs_cat_traverse: moving forward to next leaf\");\n            }\n        }\n        else {\n            tsk_error_set_errno(TSK_ERR_FS_GENFS);\n            tsk_error_set_errstr(\"hfs_cat_traverse: btree node %\" PRIu32\n                \" (%\" PRIu64 \") is neither index nor leaf (%\" PRIu8 \")\",\n                cur_node, cur_off, node_desc->type);\n            free(node);\n            return 1;\n        }\n    }\n    free(node);\n    return 0;\n}", "commit_link": "github.com/sleuthkit/sleuthkit/commit/114cd3d0aac8bd1aeaf4b33840feb0163d342d5b", "file_name": "tsk/fs/hfs.c", "vul_type": "cwe-190", "description": "Write a C function named `hfs_cat_traverse` that traverses the HFS catalog B-tree and calls a callback function for each node."}
{"func_name": "wiki_handle_rest_call", "func_src_before": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t      file_write(page, wikitext);\t      \n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && (access(page, R_OK) == 0)) \n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "func_src_after": "wiki_handle_rest_call(HttpRequest  *req, \n\t\t      HttpResponse *res,\n\t\t      char         *func)\n{\n\n  if (func != NULL && *func != '\\0')\n    {\n      if (!strcmp(func, \"page/get\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"%s\", file_read(page));\n\t      http_response_send(res);\n\t      return;\n\t    }  \n\t}\n      else if (!strcmp(func, \"page/set\"))\n\t{\n\t  char *wikitext = NULL, *page = NULL;\n\t  if( ( (wikitext = http_request_param_get(req, \"text\")) != NULL)\n\t      && ( (page = http_request_param_get(req, \"page\")) != NULL))\n\t    {\n\t  if (page_name_is_good(page))\n\t    {\n\t      file_write(page, wikitext);\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;\n\t    }\n\t    }\n\t}\n      else if (!strcmp(func, \"page/delete\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (unlink(page) > 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"page/exists\"))\n\t{\n\t  char *page = http_request_param_get(req, \"page\");\n\n\t  if (page == NULL)\n\t    page = http_request_get_query_string(req);\n\n\t  if (page && page_name_is_good(page) && (access(page, R_OK) == 0))\n\t    {\n\t      http_response_printf(res, \"success\");\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n      else if (!strcmp(func, \"pages\") || !strcmp(func, \"search\"))\n\t{\n\t  WikiPageList **pages = NULL;\n\t  int            n_pages, i;\n\t  char          *expr = http_request_param_get(req, \"expr\");\n\n\t  if (expr == NULL)\n\t    expr = http_request_get_query_string(req);\n\t  \n\t  pages = wiki_get_pages(&n_pages, expr);\n\n\t  if (pages)\n\t    {\n\t      for (i=0; i<n_pages; i++)\n\t\t{\n\t\t  struct tm   *pTm;\n\t\t  char   datebuf[64];\n\t\t  \n\t\t  pTm = localtime(&pages[i]->mtime);\n\t\t  strftime(datebuf, sizeof(datebuf), \"%Y-%m-%d %H:%M\", pTm);\n\t\t  http_response_printf(res, \"%s\\t%s\\n\", pages[i]->name, datebuf);\n\t\t}\n\n\t      http_response_send(res);\n\t      return;  \n\t    }\n\t}\n    }\n\n  http_response_set_status(res, 500, \"Error\");\n  http_response_printf(res, \"<html><body>Failed</body></html>\\n\");\n  http_response_send(res);\n\n  return;  \n}", "commit_link": "github.com/yarolig/didiwiki/commit/5e5c796617e1712905dc5462b94bd5e6c08d15ea", "file_name": "src/wiki.c", "vul_type": "cwe-022", "description": "In C, write a function to handle RESTful API calls for a wiki page system, supporting operations like get, set, delete, check existence, and list/search pages."}
{"func_name": "FromkLinuxSockAddr", "func_src_before": "bool FromkLinuxSockAddr(const struct klinux_sockaddr *input,\n                        socklen_t input_len, struct sockaddr *output,\n                        socklen_t *output_len,\n                        void (*abort_handler)(const char *)) {\n  if (!input || !output || !output_len || input_len == 0) {\n    output = nullptr;\n    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n    sockaddr_un_out.sun_family = AF_UNIX;\n    InitializeToZeroArray(sockaddr_un_out.sun_path);\n    ReinterpretCopyArray(\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n    sockaddr_in_out.sin_family = AF_INET;\n    sockaddr_in_out.sin_port = klinux_sockaddr_in_in->klinux_sin_port;\n    InitializeToZeroSingle(&sockaddr_in_out.sin_addr);\n    ReinterpretCopySingle(&sockaddr_in_out.sin_addr,\n                          &klinux_sockaddr_in_in->klinux_sin_addr);\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;\n    sockaddr_in6_out.sin6_family = AF_INET6;\n    sockaddr_in6_out.sin6_port = klinux_sockaddr_in6_in->klinux_sin6_port;\n    sockaddr_in6_out.sin6_flowinfo =\n        klinux_sockaddr_in6_in->klinux_sin6_flowinfo;\n    sockaddr_in6_out.sin6_scope_id =\n        klinux_sockaddr_in6_in->klinux_sin6_scope_id;\n    InitializeToZeroSingle(&sockaddr_in6_out.sin6_addr);\n    ReinterpretCopySingle(&sockaddr_in6_out.sin6_addr,\n                          &klinux_sockaddr_in6_in->klinux_sin6_addr);\n    CopySockaddr(&sockaddr_in6_out, sizeof(sockaddr_in6_out), output,\n                 output_len);\n  } else if (klinux_family == kLinux_AF_UNSPEC) {\n    output = nullptr;\n    *output_len = 0;\n  } else {\n    if (abort_handler != nullptr) {\n      std::string message = absl::StrCat(\n          \"Type conversion error - Unsupported AF family: \", klinux_family);\n      abort_handler(message.c_str());\n    } else {\n      abort();\n    }\n  }\n  return true;\n}", "func_src_after": "bool FromkLinuxSockAddr(const struct klinux_sockaddr *input,\n                        socklen_t input_len, struct sockaddr *output,\n                        socklen_t *output_len,\n                        void (*abort_handler)(const char *)) {\n  if (!input || !output || !output_len || input_len == 0) {\n    output = nullptr;\n    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    if (input_len < sizeof(struct klinux_sockaddr_un)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n    sockaddr_un_out.sun_family = AF_UNIX;\n    InitializeToZeroArray(sockaddr_un_out.sun_path);\n    ReinterpretCopyArray(\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    if (input_len < sizeof(struct klinux_sockaddr_in)) {\n      return false;\n    }\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n    sockaddr_in_out.sin_family = AF_INET;\n    sockaddr_in_out.sin_port = klinux_sockaddr_in_in->klinux_sin_port;\n    InitializeToZeroSingle(&sockaddr_in_out.sin_addr);\n    ReinterpretCopySingle(&sockaddr_in_out.sin_addr,\n                          &klinux_sockaddr_in_in->klinux_sin_addr);\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    if (input_len < sizeof(struct klinux_sockaddr_in6)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;\n    sockaddr_in6_out.sin6_family = AF_INET6;\n    sockaddr_in6_out.sin6_port = klinux_sockaddr_in6_in->klinux_sin6_port;\n    sockaddr_in6_out.sin6_flowinfo =\n        klinux_sockaddr_in6_in->klinux_sin6_flowinfo;\n    sockaddr_in6_out.sin6_scope_id =\n        klinux_sockaddr_in6_in->klinux_sin6_scope_id;\n    InitializeToZeroSingle(&sockaddr_in6_out.sin6_addr);\n    ReinterpretCopySingle(&sockaddr_in6_out.sin6_addr,\n                          &klinux_sockaddr_in6_in->klinux_sin6_addr);\n    CopySockaddr(&sockaddr_in6_out, sizeof(sockaddr_in6_out), output,\n                 output_len);\n  } else if (klinux_family == kLinux_AF_UNSPEC) {\n    output = nullptr;\n    *output_len = 0;\n  } else {\n    if (abort_handler != nullptr) {\n      std::string message = absl::StrCat(\n          \"Type conversion error - Unsupported AF family: \", klinux_family);\n      abort_handler(message.c_str());\n    } else {\n      abort();\n    }\n  }\n  return true;\n}", "commit_link": "github.com/google/asylo/commit/bda9772e7872b0d2b9bee32930cf7a4983837b39", "file_name": "asylo/platform/system_call/type_conversions/manual_types_functions.cc", "vul_type": "cwe-787", "description": "Write a C++ function to convert a Linux-specific socket address structure to a generic socket address structure, handling different address families and providing an abort callback for errors."}
{"func_name": "_keyify", "func_src_before": "def _keyify(key):\n    return _key_pattern.sub(' ', key.lower())", "func_src_after": "def _keyify(key):\n    key = escape(key.lower(), quote=True)\n    return _key_pattern.sub(' ', key)", "commit_link": "github.com/lepture/mistune/commit/5f06d724bc05580e7f203db2d4a4905fc1127f98", "file_name": "mistune.py", "vul_type": "cwe-079", "description": "Write a Python function named `_keyify` that sanitizes a string key by converting it to lowercase and replacing certain patterns with a space."}
{"func_name": "_connect", "func_src_before": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "func_src_after": "    def _connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        if self.sock:\n            return\n        if self._proxy_host is not None:\n            logger.info('Connecting to http proxy %s:%s',\n                        self._proxy_host, self._proxy_port)\n            sock = socketutil.create_connection((self._proxy_host,\n                                                 self._proxy_port))\n            if self.ssl:\n                # TODO proxy header support\n                data = self._buildheaders('CONNECT', '%s:%d' % (self.host,\n                                                                self.port),\n                                          {}, HTTP_VER_1_0)\n                sock.send(data)\n                sock.setblocking(0)\n                r = self.response_class(sock, self.timeout, 'CONNECT')\n                timeout_exc = HTTPTimeoutException(\n                    'Timed out waiting for CONNECT response from proxy')\n                while not r.complete():\n                    try:\n                        # We're a friend of the response class, so let\n                        # us use the private attribute.\n                        # pylint: disable=W0212\n                        if not r._select():\n                            if not r.complete():\n                                raise timeout_exc\n                    except HTTPTimeoutException:\n                        # This raise/except pattern looks goofy, but\n                        # _select can raise the timeout as well as the\n                        # loop body. I wish it wasn't this convoluted,\n                        # but I don't have a better solution\n                        # immediately handy.\n                        raise timeout_exc\n                if r.status != 200:\n                    raise HTTPProxyConnectFailedException(\n                        'Proxy connection failed: %d %s' % (r.status,\n                                                            r.read()))\n                logger.info('CONNECT (for SSL) to %s:%s via proxy succeeded.',\n                            self.host, self.port)\n        else:\n            sock = socketutil.create_connection((self.host, self.port))\n        if self.ssl:\n            # This is the default, but in the case of proxied SSL\n            # requests the proxy logic above will have cleared\n            # blocking mode, so re-enable it just to be safe.\n            sock.setblocking(1)\n            logger.debug('wrapping socket for ssl with options %r',\n                         self.ssl_opts)\n            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n            if self._ssl_validator:\n                self._ssl_validator(sock)\n        sock.setblocking(0)\n        self.sock = sock", "line_changes": {"deleted": [{"line_no": 50, "char_start": 2563, "char_end": 2628, "line": "            sock = socketutil.wrap_socket(sock, **self.ssl_opts)\n"}], "added": [{"line_no": 50, "char_start": 2563, "char_end": 2627, "line": "            sock = self._ssl_wrap_socket(sock, **self.ssl_opts)\n"}]}, "char_changes": {"deleted": [{"char_start": 2583, "char_end": 2593, "chars": "ocketutil."}], "added": [{"char_start": 2583, "char_end": 2592, "chars": "elf._ssl_"}]}, "commit_link": "github.com/dscho/hg/commit/bfe415fca85c8dcfcdb91347c4fffb4cf43e302e", "file_name": "__init__.py", "vul_type": "cwe-327", "commit_msg": "httpclient: import 4bb625347d4a to provide SSL wrapper injection\n\nThis lets us inject our own ssl.wrap_socket equivalent into\nhttpclient, which means that any changes we make to our ssl handling\ncan be *entirely* on our side without having to muck with httpclient,\nwhich sounds appealing. For example, an extension could wrap\nsslutil.ssl_wrap_socket with an api-compatible wrapper and then tweak\nSSL settings more precisely or use GnuTLS instead of OpenSSL.", "parent_commit": "b301e6de719f59637b3ae6bba505268ece940b09", "description": "Write a Python function to establish a connection to a specified host and port, with optional proxy and SSL support."}
{"func_name": "render_page_edit", "func_src_before": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1\" % page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "func_src_after": "@app.route('/<page_name>/edit')\ndef render_page_edit(page_name):\n    query = db.query(\"select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = $1 order by page_content.id desc limit 1\", page_name)\n    wiki_page = query.namedresult()\n    if len(wiki_page) > 0:\n        content = wiki_page[0].content\n    else:\n        content = \"\"\n    return render_template(\n        'edit_page.html',\n        page_name = page_name,\n        content = content\n    )", "commit_link": "github.com/Pumala/python_wiki_app_redo/commit/65d60747cd8efb05970304234d3bd949d2088e8b", "file_name": "server.py", "vul_type": "cwe-089", "description": "Write a Python Flask function to display the latest content of a wiki page for editing, using a SQL query to retrieve the data."}
{"func_name": "login", "func_src_before": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                               password=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Not activated\" % username)\n    except NoResultFound:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username or Password wrong\" % username)\n    return None", "func_src_after": "def login(username, password):\n    \"\"\"Returns a `User` instance if the login does not fail with the\n    given login and password.\n\n    :username: username as String\n    :password: password as SHA1 encrypted string\n    :returns: `User` instance if login is OK else None.\n\n    \"\"\"\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n                if passwords_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n                log.info(\"Login failed for user '%s'. \"\n                         \"Reason: Not activated\" % username)\n        else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else:\n        log.info(\"Login failed for user '%s'. \"\n                 \"Reason: Username not known\" % username)\n    return None", "line_changes": {"deleted": [{"line_no": 10, "char_start": 279, "char_end": 306, "line": "    md5_pw = hashlib.md5()\n"}, {"line_no": 11, "char_start": 306, "char_end": 340, "line": "    md5_pw.update(password or \"\")\n"}, {"line_no": 12, "char_start": 340, "char_end": 372, "line": "    md5_pw = md5_pw.hexdigest()\n"}, {"line_no": 14, "char_start": 441, "char_end": 450, "line": "    try:\n"}, {"line_no": 15, "char_start": 450, "char_end": 513, "line": "        user = DBSession.query(User).filter_by(login=username,\n"}, {"line_no": 16, "char_start": 513, "char_end": 583, "line": "                                               password=md5_pw).one()\n"}, {"line_no": 17, "char_start": 583, "char_end": 610, "line": "        if user.activated:\n"}, {"line_no": 18, "char_start": 610, "char_end": 670, "line": "            log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 19, "char_start": 670, "char_end": 694, "line": "            return user\n"}, {"line_no": 20, "char_start": 694, "char_end": 742, "line": "        log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 21, "char_start": 742, "char_end": 795, "line": "                 \"Reason: Not activated\" % username)\n"}, {"line_no": 22, "char_start": 795, "char_end": 821, "line": "    except NoResultFound:\n"}, {"line_no": 24, "char_start": 869, "char_end": 935, "line": "                 \"Reason: Username or Password wrong\" % username)\n"}], "added": [{"line_no": 11, "char_start": 348, "char_end": 379, "line": "    user = load_user(username)\n"}, {"line_no": 12, "char_start": 379, "char_end": 392, "line": "    if user:\n"}, {"line_no": 13, "char_start": 392, "char_end": 445, "line": "        if verify_password(password, user.password):\n"}, {"line_no": 14, "char_start": 445, "char_end": 476, "line": "            if user.activated:\n"}, {"line_no": 15, "char_start": 476, "char_end": 540, "line": "                log.info(\"Login successfull '%s'\" % (username))\n"}, {"line_no": 16, "char_start": 540, "char_end": 598, "line": "                if passwords_needs_update(user.password):\n"}, {"line_no": 17, "char_start": 598, "char_end": 675, "line": "                    log.info(\"Updating password for user '%s'\" % (username))\n"}, {"line_no": 18, "char_start": 675, "char_end": 739, "line": "                    user.password = encrypt_password(password) \n"}, {"line_no": 19, "char_start": 739, "char_end": 767, "line": "                return user\n"}, {"line_no": 20, "char_start": 767, "char_end": 785, "line": "            else:\n"}, {"line_no": 21, "char_start": 785, "char_end": 841, "line": "                log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 22, "char_start": 841, "char_end": 902, "line": "                         \"Reason: Not activated\" % username)\n"}, {"line_no": 23, "char_start": 902, "char_end": 916, "line": "        else:\n"}, {"line_no": 24, "char_start": 916, "char_end": 968, "line": "            log.info(\"Login failed for user '%s'. \"\n"}, {"line_no": 25, "char_start": 968, "char_end": 1026, "line": "                     \"Reason: Wrong password\" % username)\n"}, {"line_no": 26, "char_start": 1026, "char_end": 1036, "line": "    else:\n"}, {"line_no": 28, "char_start": 1084, "char_end": 1142, "line": "                 \"Reason: Username not known\" % username)\n"}]}, "char_changes": {"deleted": [{"char_start": 283, "char_end": 545, "chars": "md5_pw = hashlib.md5()\n    md5_pw.update(password or \"\")\n    md5_pw = md5_pw.hexdigest()\n    log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    try:\n        user = DBSession.query(User).filter_by(login=username,\n                                "}, {"char_start": 568, "char_end": 694, "chars": "=md5_pw).one()\n        if user.activated:\n            log.info(\"Login successfull '%s'\" % (username))\n            return user\n"}, {"char_start": 799, "char_end": 819, "chars": "except NoResultFound"}, {"char_start": 904, "char_end": 921, "chars": "or Password wrong"}], "added": [{"char_start": 283, "char_end": 540, "chars": "log.debug(\"Login user '%s' with pw '%s'\" % (username, password))\n    user = load_user(username)\n    if user:\n        if verify_password(password, user.password):\n            if user.activated:\n                log.info(\"Login successfull '%s'\" % (username))\n"}, {"char_start": 555, "char_end": 559, "chars": " if "}, {"char_start": 567, "char_end": 793, "chars": "s_needs_update(user.password):\n                    log.info(\"Updating password for user '%s'\" % (username))\n                    user.password = encrypt_password(password) \n                return user\n            else:\n        "}, {"char_start": 858, "char_end": 863, "chars": "     "}, {"char_start": 863, "char_end": 866, "chars": "   "}, {"char_start": 906, "char_end": 1034, "chars": "    else:\n            log.info(\"Login failed for user '%s'. \"\n                     \"Reason: Wrong password\" % username)\n    else"}, {"char_start": 1119, "char_end": 1128, "chars": "not known"}]}, "commit_link": "github.com/ringo-framework/ringo/commit/8e92641fee542f6e7004e827136dea3ce5e99eb2", "file_name": "security.py", "vul_type": "cwe-327", "commit_msg": "Replaced use of the old hashlib.md5 method for password encryption with new\nencryption methods using passlib. Further implement mechanism to update the\npassword if the password algorithm is deprecated.", "parent_commit": "8cfe035de8fc493923385093cc7f5c5455fb08f9", "description": "Write a Python function named `login` that checks a user's credentials and returns the user object if authenticated or `None` otherwise."}
{"func_name": "(anonymous)", "func_src_before": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "func_src_after": "\t\t\t\t{actionList.map((action) => {\n\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n\t\t\t\t})}", "line_changes": {"deleted": [{"line_no": 2, "char_start": 34, "char_end": 172, "line": "\t\t\t\t\treturn <a href={action.url} class={`btn btn-default btn-sm`} target=\"_blank\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}], "added": [{"line_no": 2, "char_start": 34, "char_end": 196, "line": "\t\t\t\t\treturn <a href={action.url} class=\"btn btn-default btn-sm\" target=\"_blank\" rel=\"noopener noreferrer\"><i class={`fa fa-${action.icon}`} /> {action.name}</a>;\n"}]}, "char_changes": {"deleted": [{"char_start": 73, "char_end": 75, "chars": "{`"}, {"char_start": 97, "char_end": 99, "chars": "`}"}], "added": [{"char_start": 73, "char_end": 74, "chars": "\""}, {"char_start": 96, "char_end": 97, "chars": "\""}, {"char_start": 113, "char_end": 139, "chars": " rel=\"noopener noreferrer\""}]}, "commit_link": "github.com/MyHomeworkSpace/client/commit/ec5d9dd6c12b12bb89de7ed96fadc37e45710396", "file_name": "CalendarEventPopover.jsx", "vul_type": "cwe-200", "commit_msg": "add noopener noreferrer to event actions", "parent_commit": "f248047d17897e99f2ae3a9a7b7bbb7a6f885e8f", "description": "Generate a React component in JavaScript that maps over an array of action objects to create a list of anchor elements with icons and names."}
{"func_name": "get_requested_day", "func_src_before": "    def get_requested_day(self, date):\n\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, SUM(Power) AS Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN %s AND %s \n            GROUP BY TimeStamp;\n        '''\n\n        data['data'] = list()\n        for row in self.c.execute(query % (day_start, day_end)):\n            data['data'].append({ 'time': row[0], 'power': row[1] })\n\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT SUM(EToday) as EToday\n                FROM Inverters;\n                '''\n        else:\n            query = '''\n                SELECT SUM(DayYield) AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN %s AND %s\n                GROUP BY TimeStamp\n                ''' % (day_start, day_end)\n        self.c.execute(query)\n        row = self.c.fetchone()\n        if row and row[0]: data['total'] = row[0]\n        else: data['total'] = 0\n\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data):  data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        #print(json.dumps(data, indent=4))\n        return data", "func_src_after": "    def get_requested_day(self, date):\n\n        data = dict()\n\n        day_start, day_end = self.get_epoch_day(date)\n        data['interval'] = {'from': self.convert_local_ts_to_utc(day_start, self.local_timezone), 'to': self.convert_local_ts_to_utc(day_end, self.local_timezone)}\n\n        query = '''\n            SELECT TimeStamp, SUM(Power) AS Power \n            FROM DayData \n            WHERE TimeStamp BETWEEN ? AND ?\n            GROUP BY TimeStamp;\n        '''\n\n        data['data'] = list()\n        for row in self.c.execute(query, (day_start, day_end)):\n            data['data'].append({ 'time': row[0], 'power': row[1] })\n\n\n        if self.get_datetime(date).date() == datetime.today().date():\n            query = '''\n                SELECT SUM(EToday) as EToday\n                FROM Inverters;\n                '''\n            self.c.execute(query)\n        else:\n            query = '''\n                SELECT SUM(DayYield) AS Power \n                FROM MonthData \n                WHERE TimeStamp BETWEEN ? AND ?\n                GROUP BY TimeStamp;\n                '''\n            self.c.execute(query, (day_start, day_end))\n\n        row = self.c.fetchone()\n        if row and row[0]: data['total'] = row[0]\n        else: data['total'] = 0\n\n\n        query = '''\n            SELECT MIN(TimeStamp) as Min, MAX(TimeStamp) as Max \n            FROM ( SELECT TimeStamp FROM DayData GROUP BY TimeStamp );\n            '''\n\n        self.c.execute(query)\n        first_data, last_data = self.c.fetchone()\n\n        if (first_data):  data['hasPrevious'] = (first_data < day_start)\n        else: data['hasPrevious'] = False\n\n        if (last_data): data['hasNext'] = (last_data > day_end)\n        else: data['hasNext'] = False\n\n        #print(json.dumps(data, indent=4))\n        return data", "commit_link": "github.com/philipptrenz/sunportal/commit/7eef493a168ed4e6731ff800713bfb8aee99a506", "file_name": "util/database.py", "vul_type": "cwe-089", "description": "Write a Python function that retrieves and summarizes power data for a given day, including checking for previous and next day data availability."}
{"func_name": "PHP_FUNCTION", "func_src_before": "PHPAPI PHP_FUNCTION(fread)\n{\n\tzval *arg1;\n\tlong len;\n\tphp_stream *stream;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rl\", &arg1, &len) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tPHP_STREAM_TO_ZVAL(stream, &arg1);\n\n\tif (len <= 0) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be greater than 0\");\n\t\tRETURN_FALSE;\n\t}\n\n\tZ_STRVAL_P(return_value) = emalloc(len + 1);\n\tZ_STRLEN_P(return_value) = php_stream_read(stream, Z_STRVAL_P(return_value), len);\n\n\t/* needed because recv/read/gzread doesnt put a null at the end*/\n\tZ_STRVAL_P(return_value)[Z_STRLEN_P(return_value)] = 0;\n\tZ_TYPE_P(return_value) = IS_STRING;\n}", "func_src_after": "PHPAPI PHP_FUNCTION(fread)\n{\n\tzval *arg1;\n\tlong len;\n\tphp_stream *stream;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rl\", &arg1, &len) == FAILURE) {\n\t\tRETURN_FALSE;\n\t}\n\n\tPHP_STREAM_TO_ZVAL(stream, &arg1);\n\n\tif (len <= 0) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be greater than 0\");\n\t\tRETURN_FALSE;\n\t}\n\n\tif (len > INT_MAX) {\n\t\t/* string length is int in 5.x so we can not read more than int */\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Length parameter must be no more than %d\", INT_MAX);\n\t\tRETURN_FALSE;\n\t}\n\n\tZ_STRVAL_P(return_value) = emalloc(len + 1);\n\tZ_STRLEN_P(return_value) = php_stream_read(stream, Z_STRVAL_P(return_value), len);\n\n\t/* needed because recv/read/gzread doesnt put a null at the end*/\n\tZ_STRVAL_P(return_value)[Z_STRLEN_P(return_value)] = 0;\n\tZ_TYPE_P(return_value) = IS_STRING;\n}", "commit_link": "github.com/php/php-src/commit/abd159cce48f3e34f08e4751c568e09677d5ec9c", "file_name": "ext/standard/file.c", "vul_type": "cwe-190", "description": "Write a PHP function to read a specified number of bytes from a stream resource."}
{"func_name": "(anonymous)", "func_src_before": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', url);\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "func_src_after": "        $('#modalExport .btn-primary').on('click', function (e) {\n            e.preventDefault();\n            var elBtn = $(this),\n                bid = $('#modalExport input[name=exportGroupBy]:checked').val(),\n                url = listId + '/export' + (bid ? '/' + bid : '');\n\n            elBtn.attr('href', eHtml(url));\n\n            handleDownloadBtnClick(elBtn);\n            $('#modalExport').modal('hide');\n        });", "line_changes": {"deleted": [{"line_no": 7, "char_start": 280, "char_end": 317, "line": "            elBtn.attr('href', url);\n"}], "added": [{"line_no": 7, "char_start": 280, "char_end": 324, "line": "            elBtn.attr('href', eHtml(url));\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 311, "char_end": 317, "chars": "eHtml("}, {"char_start": 321, "char_end": 322, "chars": ")"}]}, "commit_link": "github.com/theoboldt/juvem/commit/9af3a9f58dc11dd7cacae93f00fbc5ed2e3580da", "file_name": "attendance.js", "vul_type": "cwe-079", "commit_msg": "Preventing js/xss-through-dom vulnerability", "description": "Create a JavaScript function to handle a button click by setting the button's href attribute based on a selected input value and then trigger a download action."}
{"func_name": "NeXTDecode", "func_src_before": "NeXTDecode(TIFF* tif, uint8* buf, tmsize_t occ, uint16 s)\n{\n\tstatic const char module[] = \"NeXTDecode\";\n\tunsigned char *bp, *op;\n\ttmsize_t cc;\n\tuint8* row;\n\ttmsize_t scanline, n;\n\n\t(void) s;\n\t/*\n\t * Each scanline is assumed to start off as all\n\t * white (we assume a PhotometricInterpretation\n\t * of ``min-is-black'').\n\t */\n\tfor (op = (unsigned char*) buf, cc = occ; cc-- > 0;)\n\t\t*op++ = 0xff;\n\n\tbp = (unsigned char *)tif->tif_rawcp;\n\tcc = tif->tif_rawcc;\n\tscanline = tif->tif_scanlinesize;\n\tif (occ % scanline)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata, module, \"Fractional scanlines cannot be read\");\n\t\treturn (0);\n\t}\n\tfor (row = buf; cc > 0 && occ > 0; occ -= scanline, row += scanline) {\n\t\tn = *bp++, cc--;\n\t\tswitch (n) {\n\t\tcase LITERALROW:\n\t\t\t/*\n\t\t\t * The entire scanline is given as literal values.\n\t\t\t */\n\t\t\tif (cc < scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row, bp, scanline);\n\t\t\tbp += scanline;\n\t\t\tcc -= scanline;\n\t\t\tbreak;\n\t\tcase LITERALSPAN: {\n\t\t\ttmsize_t off;\n\t\t\t/*\n\t\t\t * The scanline has a literal span that begins at some\n\t\t\t * offset.\n\t\t\t */\n\t\t\tif( cc < 4 )\n\t\t\t\tgoto bad;\n\t\t\toff = (bp[0] * 256) + bp[1];\n\t\t\tn = (bp[2] * 256) + bp[3];\n\t\t\tif (cc < 4+n || off+n > scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row+off, bp+4, n);\n\t\t\tbp += 4+n;\n\t\t\tcc -= 4+n;\n\t\t\tbreak;\n\t\t}\n\t\tdefault: {\n\t\t\tuint32 npixels = 0, grey;\n\t\t\tuint32 imagewidth = tif->tif_dir.td_imagewidth;\n            if( isTiled(tif) )\n                imagewidth = tif->tif_dir.td_tilewidth;\n\n\t\t\t/*\n\t\t\t * The scanline is composed of a sequence of constant\n\t\t\t * color ``runs''.  We shift into ``run mode'' and\n\t\t\t * interpret bytes as codes of the form\n\t\t\t * <color><npixels> until we've filled the scanline.\n\t\t\t */\n\t\t\top = row;\n\t\t\tfor (;;) {\n\t\t\t\tgrey = (uint32)((n>>6) & 0x3);\n\t\t\t\tn &= 0x3f;\n\t\t\t\t/*\n\t\t\t\t * Ensure the run does not exceed the scanline\n\t\t\t\t * bounds, potentially resulting in a security\n\t\t\t\t * issue.\n\t\t\t\t */\n\t\t\t\twhile (n-- > 0 && npixels < imagewidth)\n\t\t\t\t\tSETPIXEL(op, grey);\n\t\t\t\tif (npixels >= imagewidth)\n\t\t\t\t\tbreak;\n\t\t\t\tif (cc == 0)\n\t\t\t\t\tgoto bad;\n\t\t\t\tn = *bp++, cc--;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t}\n\t}\n\ttif->tif_rawcp = (uint8*) bp;\n\ttif->tif_rawcc = cc;\n\treturn (1);\nbad:\n\tTIFFErrorExt(tif->tif_clientdata, module, \"Not enough data for scanline %ld\",\n\t    (long) tif->tif_row);\n\treturn (0);\n}", "func_src_after": "NeXTDecode(TIFF* tif, uint8* buf, tmsize_t occ, uint16 s)\n{\n\tstatic const char module[] = \"NeXTDecode\";\n\tunsigned char *bp, *op;\n\ttmsize_t cc;\n\tuint8* row;\n\ttmsize_t scanline, n;\n\n\t(void) s;\n\t/*\n\t * Each scanline is assumed to start off as all\n\t * white (we assume a PhotometricInterpretation\n\t * of ``min-is-black'').\n\t */\n\tfor (op = (unsigned char*) buf, cc = occ; cc-- > 0;)\n\t\t*op++ = 0xff;\n\n\tbp = (unsigned char *)tif->tif_rawcp;\n\tcc = tif->tif_rawcc;\n\tscanline = tif->tif_scanlinesize;\n\tif (occ % scanline)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata, module, \"Fractional scanlines cannot be read\");\n\t\treturn (0);\n\t}\n\tfor (row = buf; cc > 0 && occ > 0; occ -= scanline, row += scanline) {\n\t\tn = *bp++, cc--;\n\t\tswitch (n) {\n\t\tcase LITERALROW:\n\t\t\t/*\n\t\t\t * The entire scanline is given as literal values.\n\t\t\t */\n\t\t\tif (cc < scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row, bp, scanline);\n\t\t\tbp += scanline;\n\t\t\tcc -= scanline;\n\t\t\tbreak;\n\t\tcase LITERALSPAN: {\n\t\t\ttmsize_t off;\n\t\t\t/*\n\t\t\t * The scanline has a literal span that begins at some\n\t\t\t * offset.\n\t\t\t */\n\t\t\tif( cc < 4 )\n\t\t\t\tgoto bad;\n\t\t\toff = (bp[0] * 256) + bp[1];\n\t\t\tn = (bp[2] * 256) + bp[3];\n\t\t\tif (cc < 4+n || off+n > scanline)\n\t\t\t\tgoto bad;\n\t\t\t_TIFFmemcpy(row+off, bp+4, n);\n\t\t\tbp += 4+n;\n\t\t\tcc -= 4+n;\n\t\t\tbreak;\n\t\t}\n\t\tdefault: {\n\t\t\tuint32 npixels = 0, grey;\n\t\t\tuint32 imagewidth = tif->tif_dir.td_imagewidth;\n            if( isTiled(tif) )\n                imagewidth = tif->tif_dir.td_tilewidth;\n            tmsize_t op_offset = 0;\n\n\t\t\t/*\n\t\t\t * The scanline is composed of a sequence of constant\n\t\t\t * color ``runs''.  We shift into ``run mode'' and\n\t\t\t * interpret bytes as codes of the form\n\t\t\t * <color><npixels> until we've filled the scanline.\n\t\t\t */\n\t\t\top = row;\n\t\t\tfor (;;) {\n\t\t\t\tgrey = (uint32)((n>>6) & 0x3);\n\t\t\t\tn &= 0x3f;\n\t\t\t\t/*\n\t\t\t\t * Ensure the run does not exceed the scanline\n\t\t\t\t * bounds, potentially resulting in a security\n\t\t\t\t * issue.\n\t\t\t\t */\n\t\t\t\twhile (n-- > 0 && npixels < imagewidth && op_offset < scanline)\n\t\t\t\t\tSETPIXEL(op, grey);\n\t\t\t\tif (npixels >= imagewidth)\n\t\t\t\t\tbreak;\n                if (op_offset >= scanline ) {\n                    TIFFErrorExt(tif->tif_clientdata, module, \"Invalid data for scanline %ld\",\n                        (long) tif->tif_row);\n                    return (0);\n                }\n\t\t\t\tif (cc == 0)\n\t\t\t\t\tgoto bad;\n\t\t\t\tn = *bp++, cc--;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t}\n\t}\n\ttif->tif_rawcp = (uint8*) bp;\n\ttif->tif_rawcc = cc;\n\treturn (1);\nbad:\n\tTIFFErrorExt(tif->tif_clientdata, module, \"Not enough data for scanline %ld\",\n\t    (long) tif->tif_row);\n\treturn (0);\n}", "commit_link": "github.com/vadz/libtiff/commit/b18012dae552f85dcc5c57d3bf4e997a15b1cc1c", "file_name": "libtiff/tif_next.c", "vul_type": "cwe-787", "description": "Write a C function named `NeXTDecode` that decodes a scanline from a TIFF image."}
{"func_name": "(anonymous)", "func_src_before": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n\t\t\t\t\t +'</tr>');\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "func_src_after": "\t$(selectedFiles).each(function(i,elem){\n\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n\t\ttbody.append(newtr);\n\t\tif (elem.type === 'dir') {\n\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+OC.imagePath('core', 'filetypes/folder.png')+')');\n\t\t} else {\n\t\t\tgetMimeIcon(elem.mime,function(path){\n\t\t\t\tnewtr.find('td.filename').attr('style','background-image:url('+path+')');\n\t\t\t});\n\t\t}\n\t});", "line_changes": {"deleted": [{"line_no": 2, "char_start": 41, "char_end": 113, "line": "\t\tvar newtr = $('<tr data-dir=\"'+dir+'\" data-filename=\"'+elem.name+'\">'\n"}, {"line_no": 3, "char_start": 113, "char_end": 212, "line": "\t\t\t\t\t\t+'<td class=\"filename\">'+elem.name+'</td><td class=\"size\">'+humanFileSize(elem.size)+'</td>'\n"}, {"line_no": 4, "char_start": 212, "char_end": 229, "line": "\t\t\t\t\t +'</tr>');\n"}], "added": [{"line_no": 2, "char_start": 41, "char_end": 122, "line": "\t\tvar newtr = $('<tr/>').attr('data-dir', dir).attr('data-filename', elem.name);\n"}, {"line_no": 3, "char_start": 122, "char_end": 187, "line": "\t\tnewtr.append($('<td/>').addClass('filename').text(elem.name));\n"}, {"line_no": 4, "char_start": 187, "char_end": 263, "line": "\t\tnewtr.append($('<td/>').addClass('size').text(humanFileSize(elem.size)));\n"}]}, "char_changes": {"deleted": [{"char_start": 61, "char_end": 81, "chars": " data-dir=\"'+dir+'\" "}, {"char_start": 94, "char_end": 98, "chars": "=\"'+"}, {"char_start": 107, "char_end": 126, "chars": "+'\">'\n\t\t\t\t\t\t+'<td c"}, {"char_start": 130, "char_end": 132, "chars": "=\""}, {"char_start": 140, "char_end": 144, "chars": "\">'+"}, {"char_start": 153, "char_end": 154, "chars": "+"}, {"char_start": 156, "char_end": 157, "chars": "/"}, {"char_start": 159, "char_end": 165, "chars": "><td c"}, {"char_start": 169, "char_end": 171, "chars": "=\""}, {"char_start": 175, "char_end": 179, "chars": "\">'+"}, {"char_start": 203, "char_end": 226, "chars": "+'</td>'\n\t\t\t\t\t +'</tr>'"}], "added": [{"char_start": 61, "char_end": 94, "chars": "/>').attr('data-dir', dir).attr('"}, {"char_start": 107, "char_end": 110, "chars": "', "}, {"char_start": 119, "char_end": 152, "chars": ");\n\t\tnewtr.append($('<td/>').addC"}, {"char_start": 156, "char_end": 158, "chars": "('"}, {"char_start": 166, "char_end": 174, "chars": "').text("}, {"char_start": 183, "char_end": 204, "chars": "));\n\t\tnewtr.append($("}, {"char_start": 208, "char_end": 217, "chars": "/>').addC"}, {"char_start": 221, "char_end": 223, "chars": "('"}, {"char_start": 227, "char_end": 235, "chars": "').text("}, {"char_start": 259, "char_end": 260, "chars": ")"}]}, "commit_link": "github.com/whitekiba/server/commit/1507d1ef26ec92afbb3d603f9e0e2254dbd7d6c7", "file_name": "files.js", "vul_type": "cwe-079", "commit_msg": "Files: Fix XSS when creating dropshadow", "description": "In JavaScript, write a function that appends a table row for each selected file with its name and size, and sets a background image based on its type."}
{"func_name": "fpm_log_write", "func_src_before": "int fpm_log_write(char *log_format) /* {{{ */\n{\n\tchar *s, *b;\n\tchar buffer[FPM_LOG_BUFFER+1];\n\tint token, test;\n\tsize_t len, len2;\n\tstruct fpm_scoreboard_proc_s proc, *proc_p;\n\tstruct fpm_scoreboard_s *scoreboard;\n\tchar tmp[129];\n\tchar format[129];\n\ttime_t now_epoch;\n#ifdef HAVE_TIMES\n\tclock_t tms_total;\n#endif\n\n\tif (!log_format && (!fpm_log_format || fpm_log_fd == -1)) {\n\t\treturn -1;\n\t}\n\n\tif (!log_format) {\n\t\tlog_format = fpm_log_format;\n\t\ttest = 0;\n\t} else {\n\t\ttest = 1;\n\t}\n\n\tnow_epoch = time(NULL);\n\n\tif (!test) {\n\t\tscoreboard = fpm_scoreboard_get();\n\t\tif (!scoreboard) {\n\t\t\tzlog(ZLOG_WARNING, \"unable to get scoreboard while preparing the access log\");\n\t\t\treturn -1;\n\t\t}\n\t\tproc_p = fpm_scoreboard_proc_acquire(NULL, -1, 0);\n\t\tif (!proc_p) {\n\t\t\tzlog(ZLOG_WARNING, \"[pool %s] Unable to acquire shm slot while preparing the access log\", scoreboard->pool);\n\t\t\treturn -1;\n\t\t}\n\t\tproc = *proc_p;\n\t\tfpm_scoreboard_proc_release(proc_p);\n\t}\n\n\ttoken = 0;\n\n\tmemset(buffer, '\\0', sizeof(buffer));\n\tb = buffer;\n\tlen = 0;\n\n\n\ts = log_format;\n\n\twhile (*s != '\\0') {\n\t\t/* Test is we have place for 1 more char. */\n\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!token && *s == '%') {\n\t\t\ttoken = 1;\n\t\t\tmemset(format, '\\0', sizeof(format)); /* reset format */\n\t\t\ts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (token) {\n\t\t\ttoken = 0;\n\t\t\tlen2 = 0;\n\t\t\tswitch (*s) {\n\n\t\t\t\tcase '%': /* '%' */\n\t\t\t\t\t*b = '%';\n\t\t\t\t\tlen2 = 1;\n\t\t\t\t\tbreak;\n\n#ifdef HAVE_TIMES\n\t\t\t\tcase 'C': /* %CPU */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"total\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cutime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"user\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_cutime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"system\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'total', 'user' or 'system' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.2f\", tms_total / fpm_scoreboard_get_tick() / (proc.cpu_duration.tv_sec + proc.cpu_duration.tv_usec / 1000000.) * 100.);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n#endif\n\n\t\t\t\tcase 'd': /* duration \u00b5s */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"seconds\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec + proc.duration.tv_usec / 1000000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* miliseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"miliseconds\") || !strcasecmp(format, \"mili\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec * 1000. + proc.duration.tv_usec / 1000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* microseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"microseconds\") || !strcasecmp(format, \"micro\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.duration.tv_sec * 1000000UL + proc.duration.tv_usec);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'seconds', 'mili', 'miliseconds', 'micro' or 'microseconds' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'e': /* fastcgi env  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the environment variable must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tchar *env = fcgi_getenv((fcgi_request*) SG(server_context), format, strlen(format));\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", env ? env : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'f': /* script */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\",  *proc.script_filename ? proc.script_filename : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'l': /* content length */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.content_length);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'm': /* method */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.request_method ? proc.request_method : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'M': /* memory */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"bytes\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.memory);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* kilobytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"kilobytes\") || !strcasecmp(format, \"kilo\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* megabytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"megabytes\") || !strcasecmp(format, \"mega\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024 / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'bytes', 'kilo', 'kilobytes', 'mega' or 'megabytes' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'n': /* pool name */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", scoreboard->pool[0] ? scoreboard->pool : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'o': /* header output  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the header must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tsapi_header_struct *h;\n\t\t\t\t\t\tzend_llist_position pos;\n\t\t\t\t\t\tsapi_headers_struct *sapi_headers = &SG(sapi_headers);\n\t\t\t\t\t\tsize_t format_len = strlen(format);\n\n\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_first_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\twhile (h) {\n\t\t\t\t\t\t\tchar *header;\n\t\t\t\t\t\t\tif (!h->header_len) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (!strstr(h->header, format)) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t/* test if enought char after the header name + ': ' */\n\t\t\t\t\t\t\tif (h->header_len <= format_len + 2) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (h->header[format_len] != ':' || h->header[format_len + 1] != ' ') {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\theader = h->header + format_len + 2;\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", header && *header ? header : \"-\");\n\n\t\t\t\t\t\t\t/* found, done */\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!len2) {\n\t\t\t\t\t\t\tlen2 = 1;\n\t\t\t\t\t\t\t*b = '-';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'p': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getpid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'P': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getppid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'q': /* query_string */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.query_string);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'Q': /* '?' */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.query_string  ? \"?\" : \"\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'r': /* request URI */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.request_uri);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'R': /* remote IP address */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tconst char *tmp = fcgi_get_last_client_ip();\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp ? tmp : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 's': /* status */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%d\", SG(sapi_headers).http_response_code);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'T':\n\t\t\t\tcase 't': /* time */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\ttime_t *t;\n\t\t\t\t\t\tif (*s == 't') {\n\t\t\t\t\t\t\tt = &proc.accepted_epoch;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt = &now_epoch;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, \"%d/%b/%Y:%H:%M:%S %z\", localtime(t));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, format, localtime(t));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp);\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'u': /* remote user */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.auth_user);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase '{': /* complex var */\n\t\t\t\t\ttoken = 1;\n\t\t\t\t\t{\n\t\t\t\t\t\tchar *start;\n\t\t\t\t\t\tsize_t l;\n\n\t\t\t\t\t\tstart = ++s;\n\n\t\t\t\t\t\twhile (*s != '\\0') {\n\t\t\t\t\t\t\tif (*s == '}') {\n\t\t\t\t\t\t\t\tl = s - start;\n\n\t\t\t\t\t\t\t\tif (l >= sizeof(format) - 1) {\n\t\t\t\t\t\t\t\t\tl = sizeof(format) - 1;\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tmemcpy(format, start, l);\n\t\t\t\t\t\t\t\tformat[l] = '\\0';\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (s[1] == '\\0') {\n\t\t\t\t\t\t\tzlog(ZLOG_WARNING, \"missing closing embrace in the access.format\");\n\t\t\t\t\t\t\treturn -1;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tzlog(ZLOG_WARNING, \"Invalid token in the access.format (%%%c)\", *s);\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (*s != '}' && format[0] != '\\0') {\n\t\t\t\tzlog(ZLOG_WARNING, \"embrace is not allowed for modifier %%%c\", *s);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ts++;\n\t\t\tif (!test) {\n\t\t\t\tb += len2;\n\t\t\t\tlen += len2;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test) {\n\t\t\t// push the normal char to the output buffer\n\t\t\t*b = *s;\n\t\t\tb++;\n\t\t\tlen++;\n\t\t}\n\t\ts++;\n\t}\n\n\tif (!test && strlen(buffer) > 0) {\n\t\tbuffer[len] = '\\n';\n\t\twrite(fpm_log_fd, buffer, len + 1);\n\t}\n\n\treturn 0;\n}", "func_src_after": "int fpm_log_write(char *log_format) /* {{{ */\n{\n\tchar *s, *b;\n\tchar buffer[FPM_LOG_BUFFER+1];\n\tint token, test;\n\tsize_t len, len2;\n\tstruct fpm_scoreboard_proc_s proc, *proc_p;\n\tstruct fpm_scoreboard_s *scoreboard;\n\tchar tmp[129];\n\tchar format[129];\n\ttime_t now_epoch;\n#ifdef HAVE_TIMES\n\tclock_t tms_total;\n#endif\n\n\tif (!log_format && (!fpm_log_format || fpm_log_fd == -1)) {\n\t\treturn -1;\n\t}\n\n\tif (!log_format) {\n\t\tlog_format = fpm_log_format;\n\t\ttest = 0;\n\t} else {\n\t\ttest = 1;\n\t}\n\n\tnow_epoch = time(NULL);\n\n\tif (!test) {\n\t\tscoreboard = fpm_scoreboard_get();\n\t\tif (!scoreboard) {\n\t\t\tzlog(ZLOG_WARNING, \"unable to get scoreboard while preparing the access log\");\n\t\t\treturn -1;\n\t\t}\n\t\tproc_p = fpm_scoreboard_proc_acquire(NULL, -1, 0);\n\t\tif (!proc_p) {\n\t\t\tzlog(ZLOG_WARNING, \"[pool %s] Unable to acquire shm slot while preparing the access log\", scoreboard->pool);\n\t\t\treturn -1;\n\t\t}\n\t\tproc = *proc_p;\n\t\tfpm_scoreboard_proc_release(proc_p);\n\t}\n\n\ttoken = 0;\n\n\tmemset(buffer, '\\0', sizeof(buffer));\n\tb = buffer;\n\tlen = 0;\n\n\n\ts = log_format;\n\n\twhile (*s != '\\0') {\n\t\t/* Test is we have place for 1 more char. */\n\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!token && *s == '%') {\n\t\t\ttoken = 1;\n\t\t\tmemset(format, '\\0', sizeof(format)); /* reset format */\n\t\t\ts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (token) {\n\t\t\ttoken = 0;\n\t\t\tlen2 = 0;\n\t\t\tswitch (*s) {\n\n\t\t\t\tcase '%': /* '%' */\n\t\t\t\t\t*b = '%';\n\t\t\t\t\tlen2 = 1;\n\t\t\t\t\tbreak;\n\n#ifdef HAVE_TIMES\n\t\t\t\tcase 'C': /* %CPU */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"total\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cutime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"user\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_utime + proc.last_request_cpu.tms_cutime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (!strcasecmp(format, \"system\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\ttms_total = proc.last_request_cpu.tms_stime + proc.last_request_cpu.tms_cstime;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'total', 'user' or 'system' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.2f\", tms_total / fpm_scoreboard_get_tick() / (proc.cpu_duration.tv_sec + proc.cpu_duration.tv_usec / 1000000.) * 100.);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n#endif\n\n\t\t\t\tcase 'd': /* duration \u00b5s */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"seconds\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec + proc.duration.tv_usec / 1000000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* miliseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"miliseconds\") || !strcasecmp(format, \"mili\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%.3f\", proc.duration.tv_sec * 1000. + proc.duration.tv_usec / 1000.);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* microseconds */\n\t\t\t\t\t} else if (!strcasecmp(format, \"microseconds\") || !strcasecmp(format, \"micro\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.duration.tv_sec * 1000000UL + proc.duration.tv_usec);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'seconds', 'mili', 'miliseconds', 'micro' or 'microseconds' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'e': /* fastcgi env  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the environment variable must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tchar *env = fcgi_getenv((fcgi_request*) SG(server_context), format, strlen(format));\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", env ? env : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'f': /* script */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\",  *proc.script_filename ? proc.script_filename : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'l': /* content length */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.content_length);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'm': /* method */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.request_method ? proc.request_method : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'M': /* memory */\n\t\t\t\t\t/* seconds */\n\t\t\t\t\tif (format[0] == '\\0' || !strcasecmp(format, \"bytes\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%zu\", proc.memory);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* kilobytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"kilobytes\") || !strcasecmp(format, \"kilo\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t/* megabytes */\n\t\t\t\t\t} else if (!strcasecmp(format, \"megabytes\") || !strcasecmp(format, \"mega\")) {\n\t\t\t\t\t\tif (!test) {\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%lu\", proc.memory / 1024 / 1024);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"only 'bytes', 'kilo', 'kilobytes', 'mega' or 'megabytes' are allowed as a modifier for %%%c ('%s')\", *s, format);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'n': /* pool name */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", scoreboard->pool[0] ? scoreboard->pool : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'o': /* header output  */\n\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\tzlog(ZLOG_WARNING, \"the name of the header must be set between embraces for %%%c\", *s);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tsapi_header_struct *h;\n\t\t\t\t\t\tzend_llist_position pos;\n\t\t\t\t\t\tsapi_headers_struct *sapi_headers = &SG(sapi_headers);\n\t\t\t\t\t\tsize_t format_len = strlen(format);\n\n\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_first_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\twhile (h) {\n\t\t\t\t\t\t\tchar *header;\n\t\t\t\t\t\t\tif (!h->header_len) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (!strstr(h->header, format)) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t/* test if enought char after the header name + ': ' */\n\t\t\t\t\t\t\tif (h->header_len <= format_len + 2) {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (h->header[format_len] != ':' || h->header[format_len + 1] != ' ') {\n\t\t\t\t\t\t\t\th = (sapi_header_struct*)zend_llist_get_next_ex(&sapi_headers->headers, &pos);\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\theader = h->header + format_len + 2;\n\t\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", header && *header ? header : \"-\");\n\n\t\t\t\t\t\t\t/* found, done */\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!len2) {\n\t\t\t\t\t\t\tlen2 = 1;\n\t\t\t\t\t\t\t*b = '-';\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'p': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getpid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'P': /* PID */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%ld\", (long)getppid());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'q': /* query_string */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.query_string);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'Q': /* '?' */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", *proc.query_string  ? \"?\" : \"\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'r': /* request URI */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.request_uri);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'R': /* remote IP address */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tconst char *tmp = fcgi_get_last_client_ip();\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp ? tmp : \"-\");\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 's': /* status */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%d\", SG(sapi_headers).http_response_code);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'T':\n\t\t\t\tcase 't': /* time */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\ttime_t *t;\n\t\t\t\t\t\tif (*s == 't') {\n\t\t\t\t\t\t\tt = &proc.accepted_epoch;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt = &now_epoch;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (format[0] == '\\0') {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, \"%d/%b/%Y:%H:%M:%S %z\", localtime(t));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tstrftime(tmp, sizeof(tmp) - 1, format, localtime(t));\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", tmp);\n\t\t\t\t\t}\n\t\t\t\t\tformat[0] = '\\0';\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 'u': /* remote user */\n\t\t\t\t\tif (!test) {\n\t\t\t\t\t\tlen2 = snprintf(b, FPM_LOG_BUFFER - len, \"%s\", proc.auth_user);\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase '{': /* complex var */\n\t\t\t\t\ttoken = 1;\n\t\t\t\t\t{\n\t\t\t\t\t\tchar *start;\n\t\t\t\t\t\tsize_t l;\n\n\t\t\t\t\t\tstart = ++s;\n\n\t\t\t\t\t\twhile (*s != '\\0') {\n\t\t\t\t\t\t\tif (*s == '}') {\n\t\t\t\t\t\t\t\tl = s - start;\n\n\t\t\t\t\t\t\t\tif (l >= sizeof(format) - 1) {\n\t\t\t\t\t\t\t\t\tl = sizeof(format) - 1;\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tmemcpy(format, start, l);\n\t\t\t\t\t\t\t\tformat[l] = '\\0';\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ts++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (s[1] == '\\0') {\n\t\t\t\t\t\t\tzlog(ZLOG_WARNING, \"missing closing embrace in the access.format\");\n\t\t\t\t\t\t\treturn -1;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tzlog(ZLOG_WARNING, \"Invalid token in the access.format (%%%c)\", *s);\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (*s != '}' && format[0] != '\\0') {\n\t\t\t\tzlog(ZLOG_WARNING, \"embrace is not allowed for modifier %%%c\", *s);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ts++;\n\t\t\tif (!test) {\n\t\t\t\tb += len2;\n\t\t\t\tlen += len2;\n\t\t\t}\n\t\t\tif (len >= FPM_LOG_BUFFER) {\n\t\t\t\tzlog(ZLOG_NOTICE, \"the log buffer is full (%d). The access log request has been truncated.\", FPM_LOG_BUFFER);\n\t\t\t\tlen = FPM_LOG_BUFFER;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test) {\n\t\t\t// push the normal char to the output buffer\n\t\t\t*b = *s;\n\t\t\tb++;\n\t\t\tlen++;\n\t\t}\n\t\ts++;\n\t}\n\n\tif (!test && strlen(buffer) > 0) {\n\t\tbuffer[len] = '\\n';\n\t\twrite(fpm_log_fd, buffer, len + 1);\n\t}\n\n\treturn 0;\n}", "commit_link": "github.com/php/php-src/commit/2721a0148649e07ed74468f097a28899741eb58f", "file_name": "sapi/fpm/fpm/fpm_log.c", "vul_type": "cwe-125", "description": "Write a C function named `fpm_log_write` that processes a log format string and writes the formatted log entry to a file."}
{"func_name": "check_max_open_files", "func_src_before": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = len([\n        key for key in os.listdir(accepted_keys_dir) if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ])\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "func_src_after": "def check_max_open_files(opts):\n    '''\n    Check the number of max allowed open files and adjust if needed\n    '''\n    mof_c = opts.get('max_open_files', 100000)\n    if sys.platform.startswith('win'):\n        # Check the Windows API for more detail on this\n        # http://msdn.microsoft.com/en-us/library/xt874334(v=vs.71).aspx\n        # and the python binding http://timgolden.me.uk/pywin32-docs/win32file.html\n        mof_s = mof_h = win32file._getmaxstdio()\n    else:\n        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)\n\n    accepted_keys_dir = os.path.join(opts.get('pki_dir'), 'minions')\n    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n\n    log.debug(\n        'This salt-master instance has accepted {0} minion keys.'.format(\n            accepted_count\n        )\n    )\n\n    level = logging.INFO\n\n    if (accepted_count * 4) <= mof_s:\n        # We check for the soft value of max open files here because that's the\n        # value the user chose to raise to.\n        #\n        # The number of accepted keys multiplied by four(4) is lower than the\n        # soft value, everything should be OK\n        return\n\n    msg = (\n        'The number of accepted minion keys({0}) should be lower than 1/4 '\n        'of the max open files soft setting({1}). '.format(\n            accepted_count, mof_s\n        )\n    )\n\n    if accepted_count >= mof_s:\n        # This should never occur, it might have already crashed\n        msg += 'salt-master will crash pretty soon! '\n        level = logging.CRITICAL\n    elif (accepted_count * 2) >= mof_s:\n        # This is way too low, CRITICAL\n        level = logging.CRITICAL\n    elif (accepted_count * 3) >= mof_s:\n        level = logging.WARNING\n        # The accepted count is more than 3 time, WARN\n    elif (accepted_count * 4) >= mof_s:\n        level = logging.INFO\n\n    if mof_c < mof_h:\n        msg += ('According to the system\\'s hard limit, there\\'s still a '\n                'margin of {0} to raise the salt\\'s max_open_files '\n                'setting. ').format(mof_h - mof_c)\n\n    msg += 'Please consider raising this value.'\n    log.log(level=level, msg=msg)", "line_changes": {"deleted": [{"line_no": 15, "char_start": 610, "char_end": 637, "line": "    accepted_count = len([\n"}, {"line_no": 16, "char_start": 637, "char_end": 693, "line": "        key for key in os.listdir(accepted_keys_dir) if\n"}, {"line_no": 17, "char_start": 693, "char_end": 754, "line": "        os.path.isfile(os.path.join(accepted_keys_dir, key))\n"}, {"line_no": 18, "char_start": 754, "char_end": 761, "line": "    ])\n"}], "added": [{"line_no": 15, "char_start": 610, "char_end": 677, "line": "    accepted_count = sum(1 for _ in os.listdir(accepted_keys_dir))\n"}]}, "char_changes": {"deleted": [{"char_start": 631, "char_end": 648, "chars": "len([\n        key"}, {"char_start": 653, "char_end": 656, "chars": "key"}, {"char_start": 689, "char_end": 759, "chars": " if\n        os.path.isfile(os.path.join(accepted_keys_dir, key))\n    ]"}], "added": [{"char_start": 631, "char_end": 636, "chars": "sum(1"}, {"char_start": 641, "char_end": 642, "chars": "_"}]}, "commit_link": "github.com/saltstack/salt/commit/887017fa0a5b691b40dbc1831cd4472e88157733", "file_name": "verify.py", "vul_type": "cwe-022", "commit_msg": "Do not waste CPU cycles on stat(2) on each _auth\n\nCurrently servers that handle many thousands of minions spend measurable time\ndoing only stat(2) calls.\n\nIn strace it looks like::\n\n     0.000093 stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0\n     0.000236 open(\"/etc/salt/pki/master/minions\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 154\n     0.011412 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000102 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     ...many thousands lines...\n     0.000064 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000062 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=796, ...}) = 0\n     0.000485 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000065 stat(\"/export/apps/salt/log/master\", {st_mode=S_IFREG|0644, st_size=37769598988, ...}) = 0\n     0.000184 stat(\"/etc/salt/pki/master/minions_rejected/hostXXXX.linkedin.com\", 0x7fff28209f40) = -1 ENOENT (No such file or directory)\n     0.000071 stat(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", {st_mode=S_IFREG|0644, st_size=800, ...}) = 0\n     0.000074 open(\"/etc/salt/pki/master/minions/hostXXXX.linkedin.com\", O_RDONLY) = 154\n\nHappens that on each _auth() call salt is counting files in pki/master/minions\nand checks whenever those are regular files by applying os.path.isfile()\nfunction to each which considerably slows things down.\n\nThis patch removes isfile() call that effectively makes check_max_open_files()\ncheck less precise (but since it's already subject to external race conditions\nlike creation/removal of files by another process it should be OK) in exchange\nfor making it much faster.\n\nPS. Note that calling salt.utils.verify.check_max_open_files() on each _auth is\nnot really efficient, it probably should be done periodically in a background\nthread.\n\nSponsored by: LinkedIn\nSigned-off-by: Alexey Ivanov <SaveTheRbtz@GMail.com>", "parent_commit": "26edc398f706b4266fd9cfdf886d15ab0e32049b", "description": "Write a Python function to evaluate and log system resource limits against the number of accepted keys for a service."}
{"func_name": "handle_client_startup", "func_src_before": "static bool handle_client_startup(PgSocket *client, PktHdr *pkt)\n{\n\tconst char *passwd;\n\tconst uint8_t *key;\n\tbool ok;\n\n\tSBuf *sbuf = &client->sbuf;\n\n\t/* don't tolerate partial packets */\n\tif (incomplete_pkt(pkt)) {\n\t\tdisconnect_client(client, true, \"client sent partial pkt in startup phase\");\n\t\treturn false;\n\t}\n\n\tif (client->wait_for_welcome) {\n\t\tif  (finish_client_login(client)) {\n\t\t\t/* the packet was already parsed */\n\t\t\tsbuf_prepare_skip(sbuf, pkt->len);\n\t\t\treturn true;\n\t\t} else\n\t\t\treturn false;\n\t}\n\n\tswitch (pkt->type) {\n\tcase PKT_SSLREQ:\n\t\tslog_noise(client, \"C: req SSL\");\n\t\tslog_noise(client, \"P: nak\");\n\n\t\t/* reject SSL attempt */\n\t\tif (!sbuf_answer(&client->sbuf, \"N\", 1)) {\n\t\t\tdisconnect_client(client, false, \"failed to nak SSL\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_STARTUP_V2:\n\t\tdisconnect_client(client, true, \"Old V2 protocol not supported\");\n\t\treturn false;\n\tcase PKT_STARTUP:\n\t\tif (client->pool) {\n\t\t\tdisconnect_client(client, true, \"client re-sent startup pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!decide_startup_pool(client, pkt))\n\t\t\treturn false;\n\n\t\tif (client->pool->db->admin) {\n\t\t\tif (!admin_pre_login(client))\n\t\t\t\treturn false;\n\t\t}\n\n\t\tif (cf_auth_type <= AUTH_TRUST || client->own_user) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (!send_client_authreq(client)) {\n\t\t\t\tdisconnect_client(client, false, \"failed to send auth req\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 'p':\t\t/* PasswordMessage */\n\t\t/* haven't requested it */\n\t\tif (cf_auth_type <= AUTH_TRUST) {\n\t\t\tdisconnect_client(client, true, \"unrequested passwd pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tok = mbuf_get_string(&pkt->data, &passwd);\n\t\tif (ok && check_client_passwd(client, passwd)) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tdisconnect_client(client, true, \"Auth failed\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_CANCEL:\n\t\tif (mbuf_avail_for_read(&pkt->data) == BACKENDKEY_LEN\n\t\t    && mbuf_get_bytes(&pkt->data, BACKENDKEY_LEN, &key))\n\t\t{\n\t\t\tmemcpy(client->cancel_key, key, BACKENDKEY_LEN);\n\t\t\taccept_cancel_request(client);\n\t\t} else\n\t\t\tdisconnect_client(client, false, \"bad cancel request\");\n\t\treturn false;\n\tdefault:\n\t\tdisconnect_client(client, false, \"bad packet\");\n\t\treturn false;\n\t}\n\tsbuf_prepare_skip(sbuf, pkt->len);\n\tclient->request_time = get_cached_time();\n\treturn true;\n}", "func_src_after": "static bool handle_client_startup(PgSocket *client, PktHdr *pkt)\n{\n\tconst char *passwd;\n\tconst uint8_t *key;\n\tbool ok;\n\n\tSBuf *sbuf = &client->sbuf;\n\n\t/* don't tolerate partial packets */\n\tif (incomplete_pkt(pkt)) {\n\t\tdisconnect_client(client, true, \"client sent partial pkt in startup phase\");\n\t\treturn false;\n\t}\n\n\tif (client->wait_for_welcome) {\n\t\tif  (finish_client_login(client)) {\n\t\t\t/* the packet was already parsed */\n\t\t\tsbuf_prepare_skip(sbuf, pkt->len);\n\t\t\treturn true;\n\t\t} else\n\t\t\treturn false;\n\t}\n\n\tswitch (pkt->type) {\n\tcase PKT_SSLREQ:\n\t\tslog_noise(client, \"C: req SSL\");\n\t\tslog_noise(client, \"P: nak\");\n\n\t\t/* reject SSL attempt */\n\t\tif (!sbuf_answer(&client->sbuf, \"N\", 1)) {\n\t\t\tdisconnect_client(client, false, \"failed to nak SSL\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_STARTUP_V2:\n\t\tdisconnect_client(client, true, \"Old V2 protocol not supported\");\n\t\treturn false;\n\tcase PKT_STARTUP:\n\t\tif (client->pool) {\n\t\t\tdisconnect_client(client, true, \"client re-sent startup pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!decide_startup_pool(client, pkt))\n\t\t\treturn false;\n\n\t\tif (client->pool->db->admin) {\n\t\t\tif (!admin_pre_login(client))\n\t\t\t\treturn false;\n\t\t}\n\n\t\tif (cf_auth_type <= AUTH_TRUST || client->own_user) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (!send_client_authreq(client)) {\n\t\t\t\tdisconnect_client(client, false, \"failed to send auth req\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 'p':\t\t/* PasswordMessage */\n\t\t/* too early */\n\t\tif (!client->auth_user) {\n\t\t\tdisconnect_client(client, true, \"client password pkt before startup packet\");\n\t\t\treturn false;\n\t\t}\n\n\t\t/* haven't requested it */\n\t\tif (cf_auth_type <= AUTH_TRUST) {\n\t\t\tdisconnect_client(client, true, \"unrequested passwd pkt\");\n\t\t\treturn false;\n\t\t}\n\n\t\tok = mbuf_get_string(&pkt->data, &passwd);\n\t\tif (ok && check_client_passwd(client, passwd)) {\n\t\t\tif (!finish_client_login(client))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tdisconnect_client(client, true, \"Auth failed\");\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase PKT_CANCEL:\n\t\tif (mbuf_avail_for_read(&pkt->data) == BACKENDKEY_LEN\n\t\t    && mbuf_get_bytes(&pkt->data, BACKENDKEY_LEN, &key))\n\t\t{\n\t\t\tmemcpy(client->cancel_key, key, BACKENDKEY_LEN);\n\t\t\taccept_cancel_request(client);\n\t\t} else\n\t\t\tdisconnect_client(client, false, \"bad cancel request\");\n\t\treturn false;\n\tdefault:\n\t\tdisconnect_client(client, false, \"bad packet\");\n\t\treturn false;\n\t}\n\tsbuf_prepare_skip(sbuf, pkt->len);\n\tclient->request_time = get_cached_time();\n\treturn true;\n}", "commit_link": "github.com/pgbouncer/pgbouncer/commit/74d6e5f7de5ec736f71204b7b422af7380c19ac5", "file_name": "src/client.c", "vul_type": "cwe-476", "description": "Write a C function `handle_client_startup` to process different types of startup packets for a PostgreSQL client connection."}
{"func_name": "(anonymous)", "func_src_before": "(function() {\n  'use strict';\n\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    var iframes = [];\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n        return;\n      }\n\n      iframes[data.id].height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframes.push(iframe);\n\n      var id = iframes.length - 1;\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "func_src_after": "(function() {\n  'use strict';\n\n  /**\n   * @param {() => void} loaded\n   */\n  var ready = function(loaded) {\n    if (['interactive', 'complete'].indexOf(document.readyState) !== -1) {\n      loaded();\n    } else {\n      document.addEventListener('DOMContentLoaded', loaded);\n    }\n  };\n\n  ready(function() {\n    /** @type {Map<number, HTMLIFrameElement>} */\n    var iframes = new Map();\n\n    window.addEventListener('message', function(e) {\n      var data = e.data || {};\n\n      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source) {\n        return;\n      }\n\n      iframe.height = data.height;\n    });\n\n    [].forEach.call(document.querySelectorAll('iframe.mastodon-embed'), function(iframe) {\n      // select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }\n\n      iframes.set(id, iframe);\n\n      iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';\n\n      iframe.onload = function() {\n        iframe.contentWindow.postMessage({\n          type: 'setHeight',\n          id: id,\n        }, '*');\n      };\n\n      iframe.onload();\n    });\n  });\n})();", "line_changes": {"deleted": [{"line_no": 13, "char_start": 262, "char_end": 284, "line": "    var iframes = [];\n"}, {"line_no": 18, "char_start": 370, "char_end": 507, "line": "      if (data.type !== 'setHeight' || !iframes[data.id] || window.location.origin !== e.origin || data.id.toString() === '__proto__') {\n"}, {"line_no": 22, "char_start": 532, "char_end": 577, "line": "      iframes[data.id].height = data.height;\n"}, {"line_no": 26, "char_start": 677, "char_end": 713, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 27, "char_start": 713, "char_end": 753, "line": "      iframe.style.overflow = 'hidden';\n"}, {"line_no": 29, "char_start": 754, "char_end": 782, "line": "      iframes.push(iframe);\n"}, {"line_no": 31, "char_start": 783, "char_end": 818, "line": "      var id = iframes.length - 1;\n"}], "added": [{"line_no": 4, "char_start": 31, "char_end": 37, "line": "  /**\n"}, {"line_no": 5, "char_start": 37, "char_end": 69, "line": "   * @param {() => void} loaded\n"}, {"line_no": 6, "char_start": 69, "char_end": 75, "line": "   */\n"}, {"line_no": 16, "char_start": 306, "char_end": 356, "line": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"line_no": 17, "char_start": 356, "char_end": 385, "line": "    var iframes = new Map();\n"}, {"line_no": 22, "char_start": 471, "char_end": 563, "line": "      if (typeof data !== 'object' || data.type !== 'setHeight' || !iframes.has(data.id)) {\n"}, {"line_no": 23, "char_start": 563, "char_end": 579, "line": "        return;\n"}, {"line_no": 24, "char_start": 579, "char_end": 587, "line": "      }\n"}, {"line_no": 25, "char_start": 587, "char_end": 588, "line": "\n"}, {"line_no": 26, "char_start": 588, "char_end": 629, "line": "      var iframe = iframes.get(data.id);\n"}, {"line_no": 27, "char_start": 629, "char_end": 630, "line": "\n"}, {"line_no": 28, "char_start": 630, "char_end": 694, "line": "      if ('source' in e && iframe.contentWindow !== e.source) {\n"}, {"line_no": 32, "char_start": 719, "char_end": 754, "line": "      iframe.height = data.height;\n"}, {"line_no": 37, "char_start": 896, "char_end": 960, "line": "      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n"}, {"line_no": 38, "char_start": 960, "char_end": 1004, "line": "      while (id === 0 || iframes.has(id)) {\n"}, {"line_no": 39, "char_start": 1004, "char_end": 1054, "line": "        id = crypto.getRandomValues(idBuffer)[0];\n"}, {"line_no": 40, "char_start": 1054, "char_end": 1075, "line": "        failCount++;\n"}, {"line_no": 41, "char_start": 1075, "char_end": 1106, "line": "        if (failCount > 100) {\n"}, {"line_no": 43, "char_start": 1211, "char_end": 1247, "line": "          id = -(iframes.size + 1);\n"}, {"line_no": 44, "char_start": 1247, "char_end": 1264, "line": "          break;\n"}, {"line_no": 45, "char_start": 1264, "char_end": 1274, "line": "        }\n"}, {"line_no": 46, "char_start": 1274, "char_end": 1282, "line": "      }\n"}, {"line_no": 48, "char_start": 1283, "char_end": 1314, "line": "      iframes.set(id, iframe);\n"}, {"line_no": 50, "char_start": 1315, "char_end": 1351, "line": "      iframe.scrolling      = 'no';\n"}, {"line_no": 51, "char_start": 1351, "char_end": 1391, "line": "      iframe.style.overflow = 'hidden';\n"}]}, "char_changes": {"deleted": [{"char_start": 280, "char_end": 282, "chars": "[]"}, {"char_start": 417, "char_end": 418, "chars": "["}, {"char_start": 425, "char_end": 503, "chars": "] || window.location.origin !== e.origin || data.id.toString() === '__proto__'"}, {"char_start": 544, "char_end": 554, "chars": "s[data.id]"}, {"char_start": 683, "char_end": 752, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden';"}, {"char_start": 768, "char_end": 773, "chars": "push("}, {"char_start": 789, "char_end": 816, "chars": "var id = iframes.length - 1"}], "added": [{"char_start": 31, "char_end": 75, "chars": "  /**\n   * @param {() => void} loaded\n   */\n"}, {"char_start": 306, "char_end": 356, "chars": "    /** @type {Map<number, HTMLIFrameElement>} */\n"}, {"char_start": 374, "char_end": 383, "chars": "new Map()"}, {"char_start": 481, "char_end": 509, "chars": "typeof data !== 'object' || "}, {"char_start": 546, "char_end": 551, "chars": ".has("}, {"char_start": 558, "char_end": 690, "chars": ")) {\n        return;\n      }\n\n      var iframe = iframes.get(data.id);\n\n      if ('source' in e && iframe.contentWindow !== e.source"}, {"char_start": 860, "char_end": 1281, "chars": "// select unique id for each iframe\n      var id = 0, failCount = 0, idBuffer = new Uint32Array(1);\n      while (id === 0 || iframes.has(id)) {\n        id = crypto.getRandomValues(idBuffer)[0];\n        failCount++;\n        if (failCount > 100) {\n          // give up and assign (easily guessable) unique number if getRandomValues is broken or no luck\n          id = -(iframes.size + 1);\n          break;\n        }\n      }"}, {"char_start": 1297, "char_end": 1305, "chars": "set(id, "}, {"char_start": 1321, "char_end": 1389, "chars": "iframe.scrolling      = 'no';\n      iframe.style.overflow = 'hidden'"}]}, "commit_link": "github.com/yukimochi/mastodon/commit/6e736f2452d2e6fdd4da6d8f6f2f44da9d83fa4f", "file_name": "embed.js", "vul_type": "cwe-915", "commit_msg": "fix: embed.js doesn't expands iframes height (#18301)\n\nalso including some refactoring:\r\n- add `// @ts-check`\r\n- use Map to completely avoid prototype pollution\r\n- assign random id to each iframe for reduce chance to brute-force attack, and leak of iframe counts\r\n- check iframe.contentWindow and MessageEvent.source to validate message is coming from correct iframe (it works on latest Chrome/Firefox/Safari but I'm not sure this is allowed by spec)\r\n\r\nfollow-up of #17420\r\nfix #18299", "parent_commit": "a01580f09f33c275fcc0ffe616b5b5b403f46cae", "description": "Create a JavaScript function that initializes iframes for embedding content and adjusts their height based on postMessage events."}
{"func_name": "compare_and_update", "func_src_before": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name='{user.first_name}', \"\n                 f\"nickname='{user.nickname}', \"\n                 f\"last_name='{user.last_name}' \"\n                 f\"WHERE chat_id={user.chat_id}\")\n\n        try:\n            db.add(query)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "func_src_after": "    @staticmethod\n    def compare_and_update(user, message):\n        \"\"\"\n        This method compare a user object from the bot and his info from\n        the Telegram message to check whether a user has changed his bio\n        or not. If yes, the user object that represents him in the bot will\n        be updated accordingly. Now this function is called only when a user\n        asks the bot for showing the most popular cams\n\n        :param user: user object that represents a Telegram user in this bot\n        :param message: object from Telegram that contains info about user's\n        message and about himself\n        :return: None\n        \"\"\"\n\n        log.info('Checking whether user have changed his info or not...')\n        msg = message.from_user\n        usr_from_message = User(message.chat.id, msg.first_name, msg.username,\n                                msg.last_name)\n\n        if user.chat_id != usr_from_message.chat_id:\n            log.error(\"Wrong user to compare!\")\n            return\n\n        if user.first_name != usr_from_message.first_name:\n            user.first_name = usr_from_message.first_name\n\n        elif user.nickname != usr_from_message.nickname:\n            user.nickname = usr_from_message.nickname\n\n        elif user.last_name != usr_from_message.last_name:\n            user.last_name = usr_from_message.last_name\n\n        else:\n            log.debug(\"User's info hasn't changed\")\n            return\n\n        log.info(\"User has changed his info\")\n        log.debug(\"Updating user's info in the database...\")\n        query = (f\"UPDATE users \"\n                 f\"SET first_name=%s, \"\n                 f\"nickname=%s, \"\n                 f\"last_name=%s \"\n                 f\"WHERE chat_id=%s\")\n\n        parameters = (user.first_name, user.nickname, user.last_name,\n                      user.chat_id)\n\n        try:\n            db.add(query, parameters)\n        except DatabaseError:\n            log.error(\"Could not update info about %s in the database\",\n                      user)\n        else:\n            log.debug(\"User's info has been updated\")", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "In Python, write a static method to compare a user's information from a Telegram bot with the incoming message data and update the user's info in the database if it has changed."}
{"func_name": "vips_foreign_load_gif_scan_image", "func_src_before": "vips_foreign_load_gif_scan_image( VipsForeignLoadGif *gif ) \n{\n\tVipsObjectClass *class = VIPS_OBJECT_GET_CLASS( gif );\n\tGifFileType *file = gif->file;\n\tColorMapObject *map = file->Image.ColorMap ?\n\t\tfile->Image.ColorMap : file->SColorMap;\n\n\tGifByteType *extension;\n\n\tif( DGifGetImageDesc( gif->file ) == GIF_ERROR ) {\n\t\tvips_foreign_load_gif_error( gif ); \n\t\treturn( -1 );\n\t}\n\n\t/* Check that the frame looks sane. Perhaps giflib checks\n\t * this for us.\n\t */\n\tif( file->Image.Left < 0 ||\n\t\tfile->Image.Width < 1 ||\n\t\tfile->Image.Width > 10000 ||\n\t\tfile->Image.Left + file->Image.Width > file->SWidth ||\n\t\tfile->Image.Top < 0 ||\n\t\tfile->Image.Height < 1 ||\n\t\tfile->Image.Height > 10000 ||\n\t\tfile->Image.Top + file->Image.Height > file->SHeight ) {\n\t\tvips_error( class->nickname, \"%s\", _( \"bad frame size\" ) ); \n\t\treturn( -1 ); \n\t}\n\n\t/* Test for a non-greyscale colourmap for this frame.\n\t */\n\tif( !gif->has_colour &&\n\t\tmap ) {\n\t\tint i;\n\n\t\tfor( i = 0; i < map->ColorCount; i++ ) \n\t\t\tif( map->Colors[i].Red != map->Colors[i].Green ||\n\t\t\t\tmap->Colors[i].Green != map->Colors[i].Blue ) {\n\t\t\t\tgif->has_colour = TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\t/* Step over compressed image data.\n\t */\n\tdo {\n\t\tif( vips_foreign_load_gif_code_next( gif, &extension ) ) \n\t\t\treturn( -1 );\n\t} while( extension != NULL );\n\n\treturn( 0 );\n}", "func_src_after": "vips_foreign_load_gif_scan_image( VipsForeignLoadGif *gif ) \n{\n\tVipsObjectClass *class = VIPS_OBJECT_GET_CLASS( gif );\n\tGifFileType *file = gif->file;\n\n\tColorMapObject *map;\n\tGifByteType *extension;\n\n\tif( DGifGetImageDesc( gif->file ) == GIF_ERROR ) {\n\t\tvips_foreign_load_gif_error( gif ); \n\t\treturn( -1 );\n\t}\n\n\t/* Check that the frame looks sane. Perhaps giflib checks\n\t * this for us.\n\t */\n\tif( file->Image.Left < 0 ||\n\t\tfile->Image.Width < 1 ||\n\t\tfile->Image.Width > 10000 ||\n\t\tfile->Image.Left + file->Image.Width > file->SWidth ||\n\t\tfile->Image.Top < 0 ||\n\t\tfile->Image.Height < 1 ||\n\t\tfile->Image.Height > 10000 ||\n\t\tfile->Image.Top + file->Image.Height > file->SHeight ) {\n\t\tvips_error( class->nickname, \"%s\", _( \"bad frame size\" ) ); \n\t\treturn( -1 ); \n\t}\n\n\t/* Test for a non-greyscale colourmap for this frame.\n\t */\n\tmap = file->Image.ColorMap ? file->Image.ColorMap : file->SColorMap;\n\tif( !gif->has_colour &&\n\t\tmap ) {\n\t\tint i;\n\n\t\tfor( i = 0; i < map->ColorCount; i++ ) \n\t\t\tif( map->Colors[i].Red != map->Colors[i].Green ||\n\t\t\t\tmap->Colors[i].Green != map->Colors[i].Blue ) {\n\t\t\t\tgif->has_colour = TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\t/* Step over compressed image data.\n\t */\n\tdo {\n\t\tif( vips_foreign_load_gif_code_next( gif, &extension ) ) \n\t\t\treturn( -1 );\n\t} while( extension != NULL );\n\n\treturn( 0 );\n}", "commit_link": "github.com/libvips/libvips/commit/ce684dd008532ea0bf9d4a1d89bacb35f4a83f4d", "file_name": "libvips/foreign/gifload.c", "vul_type": "cwe-416", "description": "Write a C function to scan and validate a GIF image frame for the Vips image processing library."}
{"func_name": "ReadPSDImage", "func_src_before": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_src_after": "static Image *ReadPSDImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  Image\n    *image;\n\n  MagickBooleanType\n    has_merged_image,\n    skip_layers;\n\n  MagickOffsetType\n    offset;\n\n  MagickSizeType\n    length;\n\n  MagickBooleanType\n    status;\n\n  PSDInfo\n    psd_info;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *data;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read image header.\n  */\n  image->endian=MSBEndian;\n  count=ReadBlob(image,4,(unsigned char *) psd_info.signature);\n  psd_info.version=ReadBlobMSBShort(image);\n  if ((count == 0) || (LocaleNCompare(psd_info.signature,\"8BPS\",4) != 0) ||\n      ((psd_info.version != 1) && (psd_info.version != 2)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlob(image,6,psd_info.reserved);\n  psd_info.channels=ReadBlobMSBShort(image);\n  if (psd_info.channels > MaxPSDChannels)\n    ThrowReaderException(CorruptImageError,\"MaximumChannelsExceeded\");\n  psd_info.rows=ReadBlobMSBLong(image);\n  psd_info.columns=ReadBlobMSBLong(image);\n  if ((psd_info.version == 1) && ((psd_info.rows > 30000) ||\n      (psd_info.columns > 30000)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.depth=ReadBlobMSBShort(image);\n  if ((psd_info.depth != 1) && (psd_info.depth != 8) && (psd_info.depth != 16))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  psd_info.mode=ReadBlobMSBShort(image);\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  Image is %.20g x %.20g with channels=%.20g, depth=%.20g, mode=%s\",\n      (double) psd_info.columns,(double) psd_info.rows,(double)\n      psd_info.channels,(double) psd_info.depth,ModeToString((PSDImageType)\n      psd_info.mode));\n  /*\n    Initialize image.\n  */\n  image->depth=psd_info.depth;\n  image->columns=psd_info.columns;\n  image->rows=psd_info.rows;\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  if (SetImageBackgroundColor(image,exception) == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  if (psd_info.mode == LabMode)\n    SetImageColorspace(image,LabColorspace,exception);\n  if (psd_info.mode == CMYKMode)\n    {\n      SetImageColorspace(image,CMYKColorspace,exception);\n      image->alpha_trait=psd_info.channels > 4 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else if ((psd_info.mode == BitmapMode) || (psd_info.mode == GrayscaleMode) ||\n      (psd_info.mode == DuotoneMode))\n    {\n      status=AcquireImageColormap(image,psd_info.depth != 16 ? 256 : 65536,\n        exception);\n      if (status == MagickFalse)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  Image colormap allocated\");\n      SetImageColorspace(image,GRAYColorspace,exception);\n      image->alpha_trait=psd_info.channels > 1 ? BlendPixelTrait :\n        UndefinedPixelTrait;\n    }\n  else\n    image->alpha_trait=psd_info.channels > 3 ? BlendPixelTrait :\n      UndefinedPixelTrait;\n  /*\n    Read PSD raster colormap only present for indexed and duotone images.\n  */\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading colormap\");\n      if (psd_info.mode == DuotoneMode)\n        {\n          /*\n            Duotone image data;  the format of this data is undocumented.\n          */\n          data=(unsigned char *) AcquireQuantumMemory((size_t) length,\n            sizeof(*data));\n          if (data == (unsigned char *) NULL)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          (void) ReadBlob(image,(size_t) length,data);\n          data=(unsigned char *) RelinquishMagickMemory(data);\n        }\n      else\n        {\n          size_t\n            number_colors;\n\n          /*\n            Read PSD raster colormap.\n          */\n          number_colors=length/3;\n          if (number_colors > 65536)\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if (AcquireImageColormap(image,number_colors,exception) == MagickFalse)\n            ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          for (i=0; i < (ssize_t) image->colors; i++)\n            image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n              ReadBlobByte(image));\n          image->alpha_trait=UndefinedPixelTrait;\n        }\n    }\n  if ((image->depth == 1) && (image->storage_class != PseudoClass))\n    ThrowReaderException(CorruptImageError, \"ImproperImageHeader\");\n  has_merged_image=MagickTrue;\n  length=ReadBlobMSBLong(image);\n  if (length != 0)\n    {\n      unsigned char\n        *blocks;\n\n      /*\n        Image resources block.\n      */\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  reading image resource blocks - %.20g bytes\",(double)\n          ((MagickOffsetType) length));\n      blocks=(unsigned char *) AcquireQuantumMemory((size_t) length,\n        sizeof(*blocks));\n      if (blocks == (unsigned char *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      count=ReadBlob(image,(size_t) length,blocks);\n      if ((count != (ssize_t) length) ||\n          (LocaleNCompare((char *) blocks,\"8BIM\",4) != 0))\n        {\n          blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n          ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n        }\n      ParseImageResourceBlocks(image,blocks,(size_t) length,&has_merged_image,\n        exception);\n      blocks=(unsigned char *) RelinquishMagickMemory(blocks);\n    }\n  /*\n    Layer and mask block.\n  */\n  length=GetPSDSize(&psd_info,image);\n  if (length == 8)\n    {\n      length=ReadBlobMSBLong(image);\n      length=ReadBlobMSBLong(image);\n    }\n  offset=TellBlob(image);\n  skip_layers=MagickFalse;\n  if ((image_info->number_scenes == 1) && (image_info->scene == 0) &&\n      (has_merged_image != MagickFalse))\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  read composite only\");\n      skip_layers=MagickTrue;\n    }\n  if (length == 0)\n    {\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image has no layers\");\n    }\n  else\n    {\n      if (ReadPSDLayers(image,image_info,&psd_info,skip_layers,exception) !=\n          MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n\n      /*\n         Skip the rest of the layer and mask information.\n      */\n      SeekBlob(image,offset+length,SEEK_SET);\n    }\n  /*\n    If we are only \"pinging\" the image, then we're done - so return.\n  */\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  /*\n    Read the precombined layer, present for PSD < 4 compatibility.\n  */\n  if (image->debug != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  reading the precombined layer\");\n  if ((has_merged_image != MagickFalse) || (GetImageListLength(image) == 1))\n    has_merged_image=(MagickBooleanType) ReadPSDMergedImage(image_info,image,\n      &psd_info,exception);\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) == 1) &&\n      (length != 0))\n    {\n      SeekBlob(image,offset,SEEK_SET);\n      status=ReadPSDLayers(image,image_info,&psd_info,MagickFalse,exception);\n      if (status != MagickTrue)\n        {\n          (void) CloseBlob(image);\n          image=DestroyImageList(image);\n          return((Image *) NULL);\n        }\n    }\n  if ((has_merged_image == MagickFalse) && (GetImageListLength(image) > 1))\n    {\n      Image\n        *merged;\n\n      SetImageAlphaChannel(image,TransparentAlphaChannel,exception);\n      image->background_color.alpha=TransparentAlpha;\n      image->background_color.alpha_trait=BlendPixelTrait;\n      merged=MergeImageLayers(image,FlattenLayer,exception);\n      ReplaceImageInList(&image,merged);\n    }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "commit_link": "github.com/ImageMagick/ImageMagick/commit/198fffab4daf8aea88badd9c629350e5b26ec32f", "file_name": "coders/psd.c", "vul_type": "cwe-125", "description": "Write a C function to read and process a PSD image file."}
{"func_name": "markTokenUsedExternal", "func_src_before": "def markTokenUsedExternal(token, optStr=\"\"):\n    conn, c = connectDB()\n    req = \"UPDATE {} SET \\\"options_selected\\\"='{}' WHERE token='{}'\".format(CFG(\"tokens_table_name\"), \\\n                    optStr, token)\n    c.execute(req)\n    closeDB(conn)", "func_src_after": "def markTokenUsedExternal(token, optStr=\"\"):\n    conn, c = connectDB()\n    req = \"UPDATE {} SET \\\"options_selected\\\"=? WHERE token=?\".format(CFG(\"tokens_table_name\"))\n    c.execute(req, (optStr, token,))\n    closeDB(conn)", "commit_link": "github.com/FAUSheppy/simple-python-poll/commit/186c5ff5cdf58272e253a1bb432419ee50d93109", "file_name": "database.py", "vul_type": "cwe-089", "description": "Write a Python function to update a database record's options_selected field for a given token."}
{"func_name": "check_testPickle", "func_src_before": "    def check_testPickle(self):\n        \"Test of pickling\"\n        x = arange(12)\n        x[4:10:2] = masked\n        x=x.reshape(4,3)\n        f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb')\n        y = pickle.load(f)\n        assert eq(x,y)", "func_src_after": "    def check_testPickle(self):\n        \"Test of pickling\"\n        import pickle\n        x = arange(12)\n        x[4:10:2] = masked\n        x = x.reshape(4,3)\n        s = pickle.dumps(x)\n        y = pickle.loads(s)\n        assert eq(x,y)", "line_changes": {"deleted": [{"line_no": 5, "char_start": 109, "char_end": 134, "line": "        x=x.reshape(4,3)\n"}, {"line_no": 6, "char_start": 134, "char_end": 169, "line": "        f = open('test9.pik','wb')\n"}, {"line_no": 7, "char_start": 169, "char_end": 191, "line": "        import pickle\n"}, {"line_no": 8, "char_start": 191, "char_end": 217, "line": "        pickle.dump(x, f)\n"}, {"line_no": 9, "char_start": 217, "char_end": 235, "line": "        f.close()\n"}, {"line_no": 10, "char_start": 235, "char_end": 271, "line": "        f = open('test9.pik', 'rb')\n"}, {"line_no": 11, "char_start": 271, "char_end": 298, "line": "        y = pickle.load(f)\n"}, {"line_no": 12, "char_start": 298, "char_end": 320, "line": "        assert eq(x,y) \n"}], "added": [{"line_no": 3, "char_start": 59, "char_end": 81, "line": "        import pickle\n"}]}, "char_changes": {"deleted": [{"char_start": 118, "char_end": 119, "chars": "="}, {"char_start": 142, "char_end": 269, "chars": "f = open('test9.pik','wb')\n        import pickle\n        pickle.dump(x, f)\n        f.close()\n        f = open('test9.pik', 'rb'"}, {"char_start": 294, "char_end": 296, "chars": "(f"}], "added": [{"char_start": 59, "char_end": 81, "chars": "        import pickle\n"}, {"char_start": 140, "char_end": 143, "chars": " = "}, {"char_start": 166, "char_end": 167, "chars": "s"}, {"char_start": 170, "char_end": 184, "chars": "pickle.dumps(x"}, {"char_start": 209, "char_end": 212, "chars": "s(s"}]}, "commit_link": "github.com/cjermain/numpy/commit/d1e5d1de77e30c233e98ea7c35f8d7b4623fd1f3", "file_name": "test_ma.py", "vul_type": "cwe-502", "commit_msg": "Use pickle.loads/dumps for test_ma to avoid littering the filesystem with test9.pik files.", "parent_commit": "0e1c71808725c49f65d84847cc6fc7e88909a6de", "description": "Write a Python function that tests pickling and unpickling an array with modified elements and reshaping."}
{"func_name": "nav_path", "func_src_before": "def nav_path(request):\n  \"\"\"Return current path as list of items with \"name\" and \"href\" members\n\n  The href members are view_directory links for directories and view_log\n  links for files, but are set to None when the link would point to\n  the current view\"\"\"\n\n  if not request.repos:\n    return []\n\n  is_dir = request.pathtype == vclib.DIR\n\n  # add root item\n  items = []\n  root_item = _item(name=request.server.escape(request.repos.name), href=None)\n  if request.path_parts or request.view_func is not view_directory:\n    root_item.href = request.get_url(view_func=view_directory,\n                                     where='', pathtype=vclib.DIR,\n                                     params={}, escape=1)\n  items.append(root_item)\n\n  # add path part items\n  path_parts = []\n  for part in request.path_parts:\n    path_parts.append(part)\n    is_last = len(path_parts) == len(request.path_parts)\n\n    item = _item(name=part, href=None)\n\n    if not is_last or (is_dir and request.view_func is not view_directory):\n      item.href = request.get_url(view_func=view_directory,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.DIR,\n                                  params={}, escape=1)\n    elif not is_dir and request.view_func is not view_log:\n      item.href = request.get_url(view_func=view_log,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.FILE,\n                                  params={}, escape=1)\n    items.append(item)\n\n  return items", "func_src_after": "def nav_path(request):\n  \"\"\"Return current path as list of items with \"name\" and \"href\" members\n\n  The href members are view_directory links for directories and view_log\n  links for files, but are set to None when the link would point to\n  the current view\"\"\"\n\n  if not request.repos:\n    return []\n\n  is_dir = request.pathtype == vclib.DIR\n\n  # add root item\n  items = []\n  root_item = _item(name=request.server.escape(request.repos.name), href=None)\n  if request.path_parts or request.view_func is not view_directory:\n    root_item.href = request.get_url(view_func=view_directory,\n                                     where='', pathtype=vclib.DIR,\n                                     params={}, escape=1)\n  items.append(root_item)\n\n  # add path part items\n  path_parts = []\n  for part in request.path_parts:\n    path_parts.append(part)\n    is_last = len(path_parts) == len(request.path_parts)\n\n    item = _item(name=request.server.escape(part), href=None)\n\n    if not is_last or (is_dir and request.view_func is not view_directory):\n      item.href = request.get_url(view_func=view_directory,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.DIR,\n                                  params={}, escape=1)\n    elif not is_dir and request.view_func is not view_log:\n      item.href = request.get_url(view_func=view_log,\n                                  where=_path_join(path_parts),\n                                  pathtype=vclib.FILE,\n                                  params={}, escape=1)\n    items.append(item)\n\n  return items", "commit_link": "github.com/viewvc/viewvc/commit/9dcfc7daa4c940992920d3b2fbd317da20e44aad", "file_name": "lib/viewvc.py", "vul_type": "cwe-079", "description": "Write a Python function that generates a breadcrumb navigation path from a request object."}
{"func_name": "zmi_page_request", "func_src_before": "    def zmi_page_request(self, *args, **kwargs):\r\n      request = self.REQUEST\r\n      RESPONSE = request.RESPONSE\r\n      SESSION = request.SESSION\r\n      self._zmi_page_request()\r\n      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())\r\n      RESPONSE.setHeader('Cache-Control', 'no-cache')\r\n      RESPONSE.setHeader('Pragma', 'no-cache')\r\n      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])\r\n      if not request.get( 'preview'):\r\n        request.set( 'preview','preview')\r\n      langs = self.getLanguages(request)\r\n      if request.get('lang') not in langs:\r\n        request.set('lang',langs[0])\r\n      if request.get('manage_lang') not in self.getLocale().get_manage_langs():\r\n        request.set('manage_lang',self.get_manage_lang())\r\n      if not request.get('manage_tabs_message'):\r\n        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))\r\n      # manage_system\r\n      if request.form.has_key('zmi-manage-system'):\r\n        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))\r\n      # avoid declarative urls\r\n      physical_path = self.getPhysicalPath()\r\n      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')\r\n      path = path_to_handle[:-1]\r\n      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:\r\n        for i in range(len(path)):\r\n          if path[:-(i+1)] != physical_path[:-(i+1)]:\r\n            path[:-(i+1)] = physical_path[:-(i+1)]\r\n        new_path = path+[path_to_handle[-1]]\r\n        if path_to_handle != new_path:\r\n          request.RESPONSE.redirect('/'.join(new_path))", "func_src_after": "    def zmi_page_request(self, *args, **kwargs):\r\n      request = self.REQUEST\r\n      RESPONSE = request.RESPONSE\r\n      SESSION = request.SESSION\r\n      self._zmi_page_request()\r\n      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())\r\n      RESPONSE.setHeader('Cache-Control', 'no-cache')\r\n      RESPONSE.setHeader('Pragma', 'no-cache')\r\n      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])\r\n      if not request.get( 'preview'):\r\n        request.set( 'preview','preview')\r\n      langs = self.getLanguages(request)\r\n      if request.get('lang') not in langs:\r\n        request.set('lang',langs[0])\r\n      if request.get('manage_lang') not in self.getLocale().get_manage_langs():\r\n        request.set('manage_lang',self.get_manage_lang())\r\n      if not request.get('manage_tabs_message'):\r\n        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))\r\n      # manage_system\r\n      if request.form.has_key('zmi-manage-system'):\r\n        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))\r\n      # avoid declarative urls\r\n      physical_path = self.getPhysicalPath()\r\n      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')\r\n      path = path_to_handle[:-1]\r\n      if self.getDocumentElement().id in path and len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:\r\n        for i in range(len(path)):\r\n          if path[:-(i+1)] != physical_path[:-(i+1)]:\r\n            path[:-(i+1)] = physical_path[:-(i+1)]\r\n        new_path = path+[path_to_handle[-1]]\r\n        if path_to_handle != new_path:\r\n          request.RESPONSE.redirect('/'.join(new_path))", "commit_link": "github.com/zms-publishing/zms4/commit/3f28620d475220dfdb06f79787158ac50727c61a", "file_name": "ZMSItem.py", "vul_type": "cwe-022", "description": "Write a Python function named `zmi_page_request` that modifies HTTP response headers, manages session variables, and redirects to a normalized URL if necessary."}
{"func_name": "uas_switch_interface", "func_src_before": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tint alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (alt < 0)\n\t\treturn alt;\n\n\treturn usb_set_interface(udev,\n\t\t\tintf->altsetting[0].desc.bInterfaceNumber, alt);\n}", "func_src_after": "static int uas_switch_interface(struct usb_device *udev,\n\t\t\t\tstruct usb_interface *intf)\n{\n\tstruct usb_host_interface *alt;\n\n\talt = uas_find_uas_alt_setting(intf);\n\tif (!alt)\n\t\treturn -ENODEV;\n\n\treturn usb_set_interface(udev, alt->desc.bInterfaceNumber,\n\t\t\talt->desc.bAlternateSetting);\n}", "commit_link": "github.com/torvalds/linux/commit/786de92b3cb26012d3d0f00ee37adf14527f35c4", "file_name": "drivers/usb/storage/uas.c", "vul_type": "cwe-125", "description": "Write a C function named `uas_switch_interface` that switches the USB interface to an alternate setting for a given USB device and interface."}
{"func_name": "SecurityConfig::configure", "func_src_before": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().disable();\n        //.csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "func_src_after": "    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http.authorizeRequests().anyRequest().authenticated()\n            .and()\n            .formLogin()\n            .and()\n            .httpBasic()\n            .and()\n            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n    }", "line_changes": {"deleted": [{"line_no": 9, "char_start": 250, "char_end": 281, "line": "            .csrf().disable();\n"}], "added": [{"line_no": 9, "char_start": 250, "char_end": 338, "line": "            .csrf().csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse());\n"}]}, "char_changes": {"deleted": [{"char_start": 262, "char_end": 291, "chars": ".csrf().disable();\n        //"}], "added": []}, "commit_link": "github.com/skarsaune/hawtio/commit/95fb0e855e122e618dac7e13523976b1ce4b7726", "file_name": "SecurityConfig.java", "vul_type": "cwe-352", "commit_msg": "chore(examples): enable CSRF protection for springboot-security example", "parent_commit": "bab089fe3b9d8ebf9438288d0a89b6cf51c127d5", "description": "Write a Java method to configure HTTP security, enabling form login, basic authentication, and optionally disabling CSRF protection."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=q.question, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)", "func_src_after": "    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            label = escape(q.question)  # django-bootstrap3 calls mark_safe\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=label, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=label, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)", "commit_link": "github.com/pretix/pretix/commit/affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "file_name": "src/pretix/base/forms/questions.py", "vul_type": "cwe-079", "description": "Write a Python class initializer that processes event-related form fields with dynamic question types and initial values."}
{"func_name": "changedline", "func_src_before": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes in the way */\n}", "func_src_after": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  if (p->lineinfo == NULL)  /* no debug information? */\n    return 0;\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes between positions */\n}", "commit_link": "github.com/lua/lua/commit/ae5b5ba529753c7a653901ffc29b5ea24c3fdf3a", "file_name": "ldebug.c", "vul_type": "cwe-476", "description": "Write a C function named `changedline` that checks if the execution line has changed between two program counters within a Lua function prototype."}
{"func_name": "__rds_rdma_map", "func_src_before": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}", "func_src_after": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0 || !rs->rs_transport) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/f3069c6d33f6ae63a1668737bc78aaaa51bff7ca", "file_name": "net/rds/rdma.c", "vul_type": "cwe-476", "description": "Write a C function named `__rds_rdma_map` that maps a user buffer for RDMA operations and returns a cookie for the mapped memory region."}
{"func_name": "get_last_active_users", "func_src_before": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT {limit}')\n\n        try:\n            cursor = db.execute_query(query)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "func_src_after": "    @staticmethod\n    def get_last_active_users(limit):\n        \"\"\"\n        Get from the database a tuple of users who have been recently using\n        the bot\n        :param limit: integer that specifies how much users to get\n        :return: tuple of tuples with users info\n        \"\"\"\n        log.info('Evaluating last active users with date of '\n                 'last time when they used bot...')\n\n        # From photo_queries_table2 we take chat_id of the last\n        # active users and from 'users' table we take info about these\n        # users by chat_id which is a foreign key\n        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'FROM photo_queries_table2 p '\n                 'INNER JOIN users u '\n                 'ON p.chat_id = u.chat_id '\n                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '\n                 'u.language '\n                 'ORDER BY MAX(time)'\n                 f'DESC LIMIT %s')\n\n        parameters = limit,\n\n        try:\n            cursor = db.execute_query(query, parameters)\n        except DatabaseConnectionError:\n            log.error(\"Cannot get the last active users because of some \"\n                      \"problems with the database\")\n            raise\n\n        last_active_users = cursor.fetchall()\n        return last_active_users", "commit_link": "github.com/RandyRomero/photoGPSbot/commit/0e9f57f13e61863b3672f5730e27f149da00786a", "file_name": "photogpsbot/users.py", "vul_type": "cwe-089", "description": "Write a Python function to fetch a specified number of the most recent active bot users from a database."}
{"func_name": "kmod_module_new_from_name", "func_src_before": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize((char *)name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "func_src_after": "KMOD_EXPORT int kmod_module_new_from_name(struct kmod_ctx *ctx,\n\t\t\t\t\t\tconst char *name,\n\t\t\t\t\t\tstruct kmod_module **mod)\n{\n\tstruct kmod_module *m;\n\tsize_t namelen;\n\tchar name_norm[NAME_MAX];\n\n\tif (ctx == NULL || name == NULL)\n\t\treturn -ENOENT;\n\n\tmodname_normalize(name, name_norm, &namelen);\n\n\tm = kmod_pool_get_module(ctx, name_norm);\n\tif (m != NULL) {\n\t\t*mod = kmod_module_ref(m);\n\t\treturn 0;\n\t}\n\n\tm = calloc(1, sizeof(*m) + namelen + 1);\n\tif (m == NULL) {\n\t\tfree(m);\n\t\treturn -ENOMEM;\n\t}\n\n\tm->ctx = kmod_ref(ctx);\n\tmemcpy(m->name, name_norm, namelen + 1);\n\tm->refcount = 1;\n\n\tkmod_pool_add_module(ctx, m);\n\n\t*mod = m;\n\n\treturn 0;\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 244, "char_end": 299, "line": "\tmodname_normalize((char *)name, name_norm, &namelen);\n"}], "added": [{"line_no": 12, "char_start": 244, "char_end": 291, "line": "\tmodname_normalize(name, name_norm, &namelen);\n"}]}, "char_changes": {"deleted": [{"char_start": 263, "char_end": 271, "chars": "(char *)"}], "added": []}, "commit_link": "github.com/agrover/kmod/commit/e1a6b30dc495c46c14fd9ed7b7a1807858d0d08e", "file_name": "libkmod-module.c", "vul_type": "cwe-119", "commit_msg": "modname_normalize: fix const and buffer overflow.\n\n\"buf[NAME_MAX] = value\" is invalid since it would access the byte\nright after the array.\n\nAlso fix the const of modname, do not mess with it to avoid mistakes.", "parent_commit": "8fc83fe1de2941e1eb0cec1b3b68fbcc14f82f02", "description": "Write a C function to create or reference a module by name in a given context."}
{"func_name": "move", "func_src_before": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        tmp = tempfile.mktemp(suffix='.beets',\n                              prefix=py3_path(b'.' + os.path.basename(dest)),\n                              dir=py3_path(os.path.dirname(dest)))\n        tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)\n            os.replace(tmp, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "func_src_after": "def move(path, dest, replace=False):\n    \"\"\"Rename a file. `dest` may not be a directory. If `dest` already\n    exists, raises an OSError unless `replace` is True. Has no effect if\n    `path` is the same as `dest`. If the paths are on different\n    filesystems (or the rename otherwise fails), a copy is attempted\n    instead, in which case metadata will *not* be preserved. Paths are\n    translated to system paths.\n    \"\"\"\n    if os.path.isdir(path):\n        raise FilesystemError(u'source is directory', 'move', (path, dest))\n    if os.path.isdir(dest):\n        raise FilesystemError(u'destination is directory', 'move',\n                              (path, dest))\n    if samefile(path, dest):\n        return\n    path = syspath(path)\n    dest = syspath(dest)\n    if os.path.exists(dest) and not replace:\n        raise FilesystemError('file exists', 'rename', (path, dest))\n\n    # First, try renaming the file.\n    try:\n        os.replace(path, dest)\n    except OSError:\n        # Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n                                          prefix=b'.' + os.path.basename(dest),\n                                          dir=os.path.dirname(dest),\n                                          delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:\n            os.replace(tmp.name, dest)\n            tmp = None\n            os.remove(path)\n        except OSError as exc:\n            raise FilesystemError(exc, 'move', (path, dest),\n                                  traceback.format_exc())\n        finally:\n            if tmp is not None:\n                os.remove(tmp)", "line_changes": {"deleted": [{"line_no": 25, "char_start": 973, "char_end": 1020, "line": "        tmp = tempfile.mktemp(suffix='.beets',\n"}, {"line_no": 26, "char_start": 1020, "char_end": 1098, "line": "                              prefix=py3_path(b'.' + os.path.basename(dest)),\n"}, {"line_no": 27, "char_start": 1098, "char_end": 1165, "line": "                              dir=py3_path(os.path.dirname(dest)))\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1192, "line": "        tmp = syspath(tmp)\n"}, {"line_no": 30, "char_start": 1205, "char_end": 1244, "line": "            shutil.copyfile(path, tmp)\n"}, {"line_no": 31, "char_start": 1244, "char_end": 1278, "line": "            os.replace(tmp, dest)\n"}], "added": [{"line_no": 26, "char_start": 1025, "char_end": 1085, "line": "        tmp = tempfile.NamedTemporaryFile(suffix=b'.beets',\n"}, {"line_no": 27, "char_start": 1085, "char_end": 1165, "line": "                                          prefix=b'.' + os.path.basename(dest),\n"}, {"line_no": 28, "char_start": 1165, "char_end": 1234, "line": "                                          dir=os.path.dirname(dest),\n"}, {"line_no": 29, "char_start": 1234, "char_end": 1290, "line": "                                          delete=False)\n"}, {"line_no": 31, "char_start": 1303, "char_end": 1343, "line": "            with open(path, 'rb') as f:\n"}, {"line_no": 32, "char_start": 1343, "char_end": 1386, "line": "                shutil.copyfileobj(f, tmp)\n"}, {"line_no": 33, "char_start": 1386, "char_end": 1403, "line": "        finally:\n"}, {"line_no": 34, "char_start": 1403, "char_end": 1427, "line": "            tmp.close()\n"}, {"line_no": 35, "char_start": 1427, "char_end": 1428, "line": "\n"}, {"line_no": 37, "char_start": 1471, "char_end": 1484, "line": "        try:\n"}, {"line_no": 38, "char_start": 1484, "char_end": 1523, "line": "            os.replace(tmp.name, dest)\n"}]}, "char_changes": {"deleted": [{"char_start": 981, "char_end": 1002, "chars": "tmp = tempfile.mktemp"}, {"char_start": 1050, "char_end": 1066, "chars": "prefix=py3_path("}, {"char_start": 1095, "char_end": 1096, "chars": ")"}, {"char_start": 1132, "char_end": 1141, "chars": "py3_path("}, {"char_start": 1162, "char_end": 1164, "chars": "))"}, {"char_start": 1173, "char_end": 1243, "chars": "tmp = syspath(tmp)\n        try:\n            shutil.copyfile(path, tmp)"}], "added": [{"char_start": 981, "char_end": 1066, "chars": "# Copy the file to a temporary destination.\n        tmp = tempfile.NamedTemporaryFile"}, {"char_start": 1074, "char_end": 1075, "chars": "b"}, {"char_start": 1115, "char_end": 1134, "chars": "            prefix="}, {"char_start": 1165, "char_end": 1177, "chars": "            "}, {"char_start": 1232, "char_end": 1233, "chars": ","}, {"char_start": 1242, "char_end": 1483, "chars": "                                  delete=False)\n        try:\n            with open(path, 'rb') as f:\n                shutil.copyfileobj(f, tmp)\n        finally:\n            tmp.close()\n\n        # Move the copied file into place.\n        try:"}, {"char_start": 1510, "char_end": 1515, "chars": ".name"}]}, "commit_link": "github.com/beetbox/beets/commit/4bb695bcdbada9c8153442688e8494199f015f04", "file_name": "__init__.py", "vul_type": "cwe-377", "commit_msg": "Fix copying for atomic file moves\n\nFixes #4168. Also closes #4192, which it supersedes.\n\nThe original problem is that this implementation used bytestrings\nincorrectly to invoke `mktemp`. However, `mktemp` is deprecated, so this\nPR just avoids it altogether. Fortunately, the non-deprecated APIs in\n`tempfile` support all-bytes arguments.", "description": "Write a Python function named `move` that renames a file, with an option to replace the destination file if it already exists."}
{"func_name": "edit_workflow", "func_src_before": "@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "func_src_after": "@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })", "commit_link": "github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "file_name": "apps/oozie/src/oozie/views/editor2.py", "vul_type": "cwe-079", "description": "Write a Python function with a decorator to check user permissions, which retrieves and edits workflow data based on a request, handling credentials and rendering a response."}
{"func_name": "rds_cmsg_atomic", "func_src_before": "int rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,\n\t\t    struct cmsghdr *cmsg)\n{\n\tstruct page *page = NULL;\n\tstruct rds_atomic_args *args;\n\tint ret = 0;\n\n\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_atomic_args))\n\t || rm->atomic.op_active)\n\t\treturn -EINVAL;\n\n\targs = CMSG_DATA(cmsg);\n\n\t/* Nonmasked & masked cmsg ops converted to masked hw ops */\n\tswitch (cmsg->cmsg_type) {\n\tcase RDS_CMSG_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = 0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->m_fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = args->m_fadd.nocarry_mask;\n\t\tbreak;\n\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = ~0;\n\t\trm->atomic.op_m_cswp.swap_mask = ~0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->m_cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->m_cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = args->m_cswp.compare_mask;\n\t\trm->atomic.op_m_cswp.swap_mask = args->m_cswp.swap_mask;\n\t\tbreak;\n\tdefault:\n\t\tBUG(); /* should never happen */\n\t}\n\n\trm->atomic.op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\n\trm->atomic.op_silent = !!(args->flags & RDS_RDMA_SILENT);\n\trm->atomic.op_active = 1;\n\trm->atomic.op_recverr = rs->rs_recverr;\n\trm->atomic.op_sg = rds_message_alloc_sgs(rm, 1);\n\tif (!rm->atomic.op_sg) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* verify 8 byte-aligned */\n\tif (args->local_addr & 0x7) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tret = rds_pin_pages(args->local_addr, 1, &page, 1);\n\tif (ret != 1)\n\t\tgoto err;\n\tret = 0;\n\n\tsg_set_page(rm->atomic.op_sg, page, 8, offset_in_page(args->local_addr));\n\n\tif (rm->atomic.op_notify || rm->atomic.op_recverr) {\n\t\t/* We allocate an uninitialized notifier here, because\n\t\t * we don't want to do that in the completion handler. We\n\t\t * would have to use GFP_ATOMIC there, and don't want to deal\n\t\t * with failed allocations.\n\t\t */\n\t\trm->atomic.op_notifier = kmalloc(sizeof(*rm->atomic.op_notifier), GFP_KERNEL);\n\t\tif (!rm->atomic.op_notifier) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\trm->atomic.op_notifier->n_user_token = args->user_token;\n\t\trm->atomic.op_notifier->n_status = RDS_RDMA_SUCCESS;\n\t}\n\n\trm->atomic.op_rkey = rds_rdma_cookie_key(args->cookie);\n\trm->atomic.op_remote_addr = args->remote_addr + rds_rdma_cookie_offset(args->cookie);\n\n\treturn ret;\nerr:\n\tif (page)\n\t\tput_page(page);\n\tkfree(rm->atomic.op_notifier);\n\n\treturn ret;\n}", "func_src_after": "int rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,\n\t\t    struct cmsghdr *cmsg)\n{\n\tstruct page *page = NULL;\n\tstruct rds_atomic_args *args;\n\tint ret = 0;\n\n\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_atomic_args))\n\t || rm->atomic.op_active)\n\t\treturn -EINVAL;\n\n\targs = CMSG_DATA(cmsg);\n\n\t/* Nonmasked & masked cmsg ops converted to masked hw ops */\n\tswitch (cmsg->cmsg_type) {\n\tcase RDS_CMSG_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = 0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\n\t\trm->atomic.op_m_fadd.add = args->m_fadd.add;\n\t\trm->atomic.op_m_fadd.nocarry_mask = args->m_fadd.nocarry_mask;\n\t\tbreak;\n\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = ~0;\n\t\trm->atomic.op_m_cswp.swap_mask = ~0;\n\t\tbreak;\n\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\trm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\n\t\trm->atomic.op_m_cswp.compare = args->m_cswp.compare;\n\t\trm->atomic.op_m_cswp.swap = args->m_cswp.swap;\n\t\trm->atomic.op_m_cswp.compare_mask = args->m_cswp.compare_mask;\n\t\trm->atomic.op_m_cswp.swap_mask = args->m_cswp.swap_mask;\n\t\tbreak;\n\tdefault:\n\t\tBUG(); /* should never happen */\n\t}\n\n\trm->atomic.op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\n\trm->atomic.op_silent = !!(args->flags & RDS_RDMA_SILENT);\n\trm->atomic.op_active = 1;\n\trm->atomic.op_recverr = rs->rs_recverr;\n\trm->atomic.op_sg = rds_message_alloc_sgs(rm, 1);\n\tif (!rm->atomic.op_sg) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* verify 8 byte-aligned */\n\tif (args->local_addr & 0x7) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tret = rds_pin_pages(args->local_addr, 1, &page, 1);\n\tif (ret != 1)\n\t\tgoto err;\n\tret = 0;\n\n\tsg_set_page(rm->atomic.op_sg, page, 8, offset_in_page(args->local_addr));\n\n\tif (rm->atomic.op_notify || rm->atomic.op_recverr) {\n\t\t/* We allocate an uninitialized notifier here, because\n\t\t * we don't want to do that in the completion handler. We\n\t\t * would have to use GFP_ATOMIC there, and don't want to deal\n\t\t * with failed allocations.\n\t\t */\n\t\trm->atomic.op_notifier = kmalloc(sizeof(*rm->atomic.op_notifier), GFP_KERNEL);\n\t\tif (!rm->atomic.op_notifier) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\trm->atomic.op_notifier->n_user_token = args->user_token;\n\t\trm->atomic.op_notifier->n_status = RDS_RDMA_SUCCESS;\n\t}\n\n\trm->atomic.op_rkey = rds_rdma_cookie_key(args->cookie);\n\trm->atomic.op_remote_addr = args->remote_addr + rds_rdma_cookie_offset(args->cookie);\n\n\treturn ret;\nerr:\n\tif (page)\n\t\tput_page(page);\n\trm->atomic.op_active = 0;\n\tkfree(rm->atomic.op_notifier);\n\n\treturn ret;\n}", "commit_link": "github.com/torvalds/linux/commit/7d11f77f84b27cef452cee332f4e469503084737", "file_name": "net/rds/rdma.c", "vul_type": "cwe-476", "description": "Write a C function named `rds_cmsg_atomic` that processes atomic operations in RDS messages."}
{"func_name": "IRC_PROTOCOL_CALLBACK", "func_src_before": "IRC_PROTOCOL_CALLBACK(352)\n{\n    char *pos_attr, *pos_hopcount, *pos_realname, *str_host;\n    int arg_start, length;\n    struct t_irc_channel *ptr_channel;\n    struct t_irc_nick *ptr_nick;\n\n    IRC_PROTOCOL_MIN_ARGS(5);\n\n    /* silently ignore malformed 352 message (missing infos) */\n    if (argc < 8)\n        return WEECHAT_RC_OK;\n\n    pos_attr = NULL;\n    pos_hopcount = NULL;\n    pos_realname = NULL;\n\n    if (argc > 8)\n    {\n        arg_start = (strcmp (argv[8], \"*\") == 0) ? 9 : 8;\n        if (argv[arg_start][0] == ':')\n        {\n            pos_attr = NULL;\n            pos_hopcount = (argc > arg_start) ? argv[arg_start] + 1 : NULL;\n            pos_realname = (argc > arg_start + 1) ? argv_eol[arg_start + 1] : NULL;\n        }\n        else\n        {\n            pos_attr = argv[arg_start];\n            pos_hopcount = (argc > arg_start + 1) ? argv[arg_start + 1] + 1 : NULL;\n            pos_realname = (argc > arg_start + 2) ? argv_eol[arg_start + 2] : NULL;\n        }\n    }\n\n    ptr_channel = irc_channel_search (server, argv[3]);\n    ptr_nick = (ptr_channel) ?\n        irc_nick_search (server, ptr_channel, argv[7]) : NULL;\n\n    /* update host in nick */\n    if (ptr_nick)\n    {\n        length = strlen (argv[4]) + 1 + strlen (argv[5]) + 1;\n        str_host = malloc (length);\n        if (str_host)\n        {\n            snprintf (str_host, length, \"%s@%s\", argv[4], argv[5]);\n            irc_nick_set_host (ptr_nick, str_host);\n            free (str_host);\n        }\n    }\n\n    /* update away flag in nick */\n    if (ptr_channel && ptr_nick && pos_attr)\n    {\n        irc_nick_set_away (server, ptr_channel, ptr_nick,\n                           (pos_attr[0] == 'G') ? 1 : 0);\n    }\n\n    /* update realname in nick */\n    if (ptr_channel && ptr_nick && pos_realname)\n    {\n        if (ptr_nick->realname)\n            free (ptr_nick->realname);\n        if (pos_realname &&\n            weechat_hashtable_has_key (server->cap_list, \"extended-join\"))\n        {\n            ptr_nick->realname = strdup (pos_realname);\n        }\n        else\n        {\n            ptr_nick->realname = NULL;\n        }\n    }\n\n    /* display output of who (manual who from user) */\n    if (!ptr_channel || (ptr_channel->checking_whox <= 0))\n    {\n        weechat_printf_date_tags (\n            irc_msgbuffer_get_target_buffer (\n                server, NULL, command, \"who\", NULL),\n            date,\n            irc_protocol_tags (command, \"irc_numeric\", NULL, NULL),\n            \"%s%s[%s%s%s] %s%s %s(%s%s@%s%s)%s %s%s%s%s(%s)\",\n            weechat_prefix (\"network\"),\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_CHANNEL,\n            argv[3],\n            IRC_COLOR_CHAT_DELIMITERS,\n            irc_nick_color_for_msg (server, 1, NULL, argv[7]),\n            argv[7],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_HOST,\n            argv[4],\n            argv[5],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_RESET,\n            (pos_attr) ? pos_attr : \"\",\n            (pos_attr) ? \" \" : \"\",\n            (pos_hopcount) ? pos_hopcount : \"\",\n            (pos_hopcount) ? \" \" : \"\",\n            (pos_realname) ? pos_realname : \"\");\n    }\n\n    return WEECHAT_RC_OK;\n}", "func_src_after": "IRC_PROTOCOL_CALLBACK(352)\n{\n    char *pos_attr, *pos_hopcount, *pos_realname, *str_host;\n    int arg_start, length;\n    struct t_irc_channel *ptr_channel;\n    struct t_irc_nick *ptr_nick;\n\n    IRC_PROTOCOL_MIN_ARGS(5);\n\n    /* silently ignore malformed 352 message (missing infos) */\n    if (argc < 8)\n        return WEECHAT_RC_OK;\n\n    pos_attr = NULL;\n    pos_hopcount = NULL;\n    pos_realname = NULL;\n\n    if (argc > 8)\n    {\n        arg_start = ((argc > 9) && (strcmp (argv[8], \"*\") == 0)) ? 9 : 8;\n        if (argv[arg_start][0] == ':')\n        {\n            pos_attr = NULL;\n            pos_hopcount = (argc > arg_start) ? argv[arg_start] + 1 : NULL;\n            pos_realname = (argc > arg_start + 1) ? argv_eol[arg_start + 1] : NULL;\n        }\n        else\n        {\n            pos_attr = argv[arg_start];\n            pos_hopcount = (argc > arg_start + 1) ? argv[arg_start + 1] + 1 : NULL;\n            pos_realname = (argc > arg_start + 2) ? argv_eol[arg_start + 2] : NULL;\n        }\n    }\n\n    ptr_channel = irc_channel_search (server, argv[3]);\n    ptr_nick = (ptr_channel) ?\n        irc_nick_search (server, ptr_channel, argv[7]) : NULL;\n\n    /* update host in nick */\n    if (ptr_nick)\n    {\n        length = strlen (argv[4]) + 1 + strlen (argv[5]) + 1;\n        str_host = malloc (length);\n        if (str_host)\n        {\n            snprintf (str_host, length, \"%s@%s\", argv[4], argv[5]);\n            irc_nick_set_host (ptr_nick, str_host);\n            free (str_host);\n        }\n    }\n\n    /* update away flag in nick */\n    if (ptr_channel && ptr_nick && pos_attr)\n    {\n        irc_nick_set_away (server, ptr_channel, ptr_nick,\n                           (pos_attr[0] == 'G') ? 1 : 0);\n    }\n\n    /* update realname in nick */\n    if (ptr_channel && ptr_nick && pos_realname)\n    {\n        if (ptr_nick->realname)\n            free (ptr_nick->realname);\n        if (pos_realname &&\n            weechat_hashtable_has_key (server->cap_list, \"extended-join\"))\n        {\n            ptr_nick->realname = strdup (pos_realname);\n        }\n        else\n        {\n            ptr_nick->realname = NULL;\n        }\n    }\n\n    /* display output of who (manual who from user) */\n    if (!ptr_channel || (ptr_channel->checking_whox <= 0))\n    {\n        weechat_printf_date_tags (\n            irc_msgbuffer_get_target_buffer (\n                server, NULL, command, \"who\", NULL),\n            date,\n            irc_protocol_tags (command, \"irc_numeric\", NULL, NULL),\n            \"%s%s[%s%s%s] %s%s %s(%s%s@%s%s)%s %s%s%s%s(%s)\",\n            weechat_prefix (\"network\"),\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_CHANNEL,\n            argv[3],\n            IRC_COLOR_CHAT_DELIMITERS,\n            irc_nick_color_for_msg (server, 1, NULL, argv[7]),\n            argv[7],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_CHAT_HOST,\n            argv[4],\n            argv[5],\n            IRC_COLOR_CHAT_DELIMITERS,\n            IRC_COLOR_RESET,\n            (pos_attr) ? pos_attr : \"\",\n            (pos_attr) ? \" \" : \"\",\n            (pos_hopcount) ? pos_hopcount : \"\",\n            (pos_hopcount) ? \" \" : \"\",\n            (pos_realname) ? pos_realname : \"\");\n    }\n\n    return WEECHAT_RC_OK;\n}", "commit_link": "github.com/weechat/weechat/commit/9904cb6d2eb40f679d8ff6557c22d53a3e3dc75a", "file_name": "src/plugins/irc/irc-protocol.c", "vul_type": "cwe-476", "description": "In C, write a function to handle IRC protocol 352 messages, parsing user details and updating channel and nick information."}
{"func_name": "_find_host_from_wwpn", "func_src_before": "    def _find_host_from_wwpn(self, connector):\n        for wwpn in connector['wwpns']:\n            ssh_cmd = 'svcinfo lsfabric -wwpn %s -delim !' % wwpn\n            out, err = self._run_ssh(ssh_cmd)\n\n            if not len(out.strip()):\n                # This WWPN is not in use\n                continue\n\n            host_lines = out.strip().split('\\n')\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('remote_wwpn' in header and\n                                    'name' in header,\n                                    '_find_host_from_wwpn',\n                                    ssh_cmd, out, err)\n            rmt_wwpn_idx = header.index('remote_wwpn')\n            name_idx = header.index('name')\n\n            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)\n\n            if wwpn in wwpns:\n                # All the wwpns will be the mapping for the same\n                # host from this WWPN-based query. Just pick\n                # the name from first line.\n                hostname = host_lines[0].split('!')[name_idx]\n                return hostname\n\n        # Didn't find a host\n        return None", "func_src_after": "    def _find_host_from_wwpn(self, connector):\n        for wwpn in connector['wwpns']:\n            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']\n            out, err = self._run_ssh(ssh_cmd)\n\n            if not len(out.strip()):\n                # This WWPN is not in use\n                continue\n\n            host_lines = out.strip().split('\\n')\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('remote_wwpn' in header and\n                                    'name' in header,\n                                    '_find_host_from_wwpn',\n                                    ssh_cmd, out, err)\n            rmt_wwpn_idx = header.index('remote_wwpn')\n            name_idx = header.index('name')\n\n            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)\n\n            if wwpn in wwpns:\n                # All the wwpns will be the mapping for the same\n                # host from this WWPN-based query. Just pick\n                # the name from first line.\n                hostname = host_lines[0].split('!')[name_idx]\n                return hostname\n\n        # Didn't find a host\n        return None", "commit_link": "github.com/thatsdone/cinder/commit/f752302d181583a95cf44354aea607ce9d9283f4", "file_name": "cinder/volume/drivers/storwize_svc.py", "vul_type": "cwe-078", "description": "In Python, write a function to find a host's name using a WWPN (World Wide Port Name) by executing an SSH command and parsing the output."}
{"func_name": "dumprecord", "func_src_before": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from \" + table)\n\tcheckY(err)\n\n\trows, err := statement.Query()\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "func_src_after": "func dumprecord(w http.ResponseWriter, r *http.Request, parray []string) {\n\n\tdatabase := parray[0]\n\ttable := parray[1]\n\trec, err := strconv.Atoi(parray[2])\n\tcheckY(err)\n\n\tuser, pw := getCredentials(r)\n\tconn, err := sql.Open(\"mysql\", dsn(user, pw, database))\n\tcheckY(err)\n\tdefer conn.Close()\n\n\tstatement, err := conn.Prepare(\"select * from ?\")\n\tcheckY(err)\n\n\trows, err := statement.Query(table)\n\tcheckY(err)\n\tdefer rows.Close()\n\n\tcolumns, err := rows.Columns()\n\tcheckY(err)\n\n\traw := make([]interface{}, len(columns))\n\tval := make([]interface{}, len(columns))\n\n\tfor i := range val {\n\t\traw[i] = &val[i]\n\t}\n\n\tvar n int = 1\n\nrowLoop:\n\tfor rows.Next() {\n\n\t\t// unfortunately we have to iterate up to row of interest\n\t\tif n == rec {\n\t\t\terr = rows.Scan(raw...)\n\t\t\tcheckY(err)\n\n\t\t\tfmt.Fprintln(w, \"<p>\")\n\t\t\tfor i, col := range val {\n\t\t\t\tif col != nil {\n\t\t\t\t\tfmt.Fprintln(w, columns[i], \":\", string(col.([]byte)), \"<br>\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"</p>\")\n\t\t\tbreak rowLoop\n\t\t}\n\t\tn = n + 1\n\t}\n}", "line_changes": {"deleted": [{"line_no": 13, "char_start": 292, "char_end": 350, "line": "\tstatement, err := conn.Prepare(\"select * from \" + table)\n"}, {"line_no": 16, "char_start": 364, "char_end": 396, "line": "\trows, err := statement.Query()\n"}], "added": [{"line_no": 13, "char_start": 292, "char_end": 343, "line": "\tstatement, err := conn.Prepare(\"select * from ?\")\n"}, {"line_no": 16, "char_start": 357, "char_end": 394, "line": "\trows, err := statement.Query(table)\n"}]}, "char_changes": {"deleted": [{"char_start": 340, "char_end": 348, "chars": " + table"}], "added": [{"char_start": 339, "char_end": 340, "chars": "?"}, {"char_start": 387, "char_end": 392, "chars": "table"}]}, "commit_link": "github.com/gophergala/sqldump/commit/45c8dee2eebc35d73ad47ab3d5f1c7e33fe7dcd7", "file_name": "dump.go", "vul_type": "cwe-089", "commit_msg": "Protection against sql injection via composed queries", "parent_commit": "b4a9927f2ef04729ef3b7df6e8676147161a4183", "description": "Write a Go function to fetch and display a specific record from a MySQL database table based on parameters from an HTTP request."}
{"func_name": "amqp_handle_input", "func_src_before": "int amqp_handle_input(amqp_connection_state_t state, amqp_bytes_t received_data,\n                      amqp_frame_t *decoded_frame) {\n  size_t bytes_consumed;\n  void *raw_frame;\n\n  /* Returning frame_type of zero indicates either insufficient input,\n     or a complete, ignored frame was read. */\n  decoded_frame->frame_type = 0;\n\n  if (received_data.len == 0) {\n    return AMQP_STATUS_OK;\n  }\n\n  if (state->state == CONNECTION_STATE_IDLE) {\n    state->state = CONNECTION_STATE_HEADER;\n  }\n\n  bytes_consumed = consume_data(state, &received_data);\n\n  /* do we have target_size data yet? if not, return with the\n     expectation that more will arrive */\n  if (state->inbound_offset < state->target_size) {\n    return (int)bytes_consumed;\n  }\n\n  raw_frame = state->inbound_buffer.bytes;\n\n  switch (state->state) {\n    case CONNECTION_STATE_INITIAL:\n      /* check for a protocol header from the server */\n      if (memcmp(raw_frame, \"AMQP\", 4) == 0) {\n        decoded_frame->frame_type = AMQP_PSEUDOFRAME_PROTOCOL_HEADER;\n        decoded_frame->channel = 0;\n\n        decoded_frame->payload.protocol_header.transport_high =\n            amqp_d8(amqp_offset(raw_frame, 4));\n        decoded_frame->payload.protocol_header.transport_low =\n            amqp_d8(amqp_offset(raw_frame, 5));\n        decoded_frame->payload.protocol_header.protocol_version_major =\n            amqp_d8(amqp_offset(raw_frame, 6));\n        decoded_frame->payload.protocol_header.protocol_version_minor =\n            amqp_d8(amqp_offset(raw_frame, 7));\n\n        return_to_idle(state);\n        return (int)bytes_consumed;\n      }\n\n    /* it's not a protocol header; fall through to process it as a\n       regular frame header */\n\n    case CONNECTION_STATE_HEADER: {\n      amqp_channel_t channel;\n      amqp_pool_t *channel_pool;\n      /* frame length is 3 bytes in */\n      channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      state->target_size =\n          amqp_d32(amqp_offset(raw_frame, 3)) + HEADER_SIZE + FOOTER_SIZE;\n\n      if ((size_t)state->frame_max < state->target_size) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      channel_pool = amqp_get_or_create_channel_pool(state, channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      amqp_pool_alloc_bytes(channel_pool, state->target_size,\n                            &state->inbound_buffer);\n      if (NULL == state->inbound_buffer.bytes) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n      memcpy(state->inbound_buffer.bytes, state->header_buffer, HEADER_SIZE);\n      raw_frame = state->inbound_buffer.bytes;\n\n      state->state = CONNECTION_STATE_BODY;\n\n      bytes_consumed += consume_data(state, &received_data);\n\n      /* do we have target_size data yet? if not, return with the\n         expectation that more will arrive */\n      if (state->inbound_offset < state->target_size) {\n        return (int)bytes_consumed;\n      }\n    }\n    /* fall through to process body */\n\n    case CONNECTION_STATE_BODY: {\n      amqp_bytes_t encoded;\n      int res;\n      amqp_pool_t *channel_pool;\n\n      /* Check frame end marker (footer) */\n      if (amqp_d8(amqp_offset(raw_frame, state->target_size - 1)) !=\n          AMQP_FRAME_END) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      decoded_frame->frame_type = amqp_d8(amqp_offset(raw_frame, 0));\n      decoded_frame->channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      channel_pool =\n          amqp_get_or_create_channel_pool(state, decoded_frame->channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      switch (decoded_frame->frame_type) {\n        case AMQP_FRAME_METHOD:\n          decoded_frame->payload.method.id =\n              amqp_d32(amqp_offset(raw_frame, HEADER_SIZE));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 4);\n          encoded.len = state->target_size - HEADER_SIZE - 4 - FOOTER_SIZE;\n\n          res = amqp_decode_method(decoded_frame->payload.method.id,\n                                   channel_pool, encoded,\n                                   &decoded_frame->payload.method.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_HEADER:\n          decoded_frame->payload.properties.class_id =\n              amqp_d16(amqp_offset(raw_frame, HEADER_SIZE));\n          /* unused 2-byte weight field goes here */\n          decoded_frame->payload.properties.body_size =\n              amqp_d64(amqp_offset(raw_frame, HEADER_SIZE + 4));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 12);\n          encoded.len = state->target_size - HEADER_SIZE - 12 - FOOTER_SIZE;\n          decoded_frame->payload.properties.raw = encoded;\n\n          res = amqp_decode_properties(\n              decoded_frame->payload.properties.class_id, channel_pool, encoded,\n              &decoded_frame->payload.properties.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_BODY:\n          decoded_frame->payload.body_fragment.len =\n              state->target_size - HEADER_SIZE - FOOTER_SIZE;\n          decoded_frame->payload.body_fragment.bytes =\n              amqp_offset(raw_frame, HEADER_SIZE);\n          break;\n\n        case AMQP_FRAME_HEARTBEAT:\n          break;\n\n        default:\n          /* Ignore the frame */\n          decoded_frame->frame_type = 0;\n          break;\n      }\n\n      return_to_idle(state);\n      return (int)bytes_consumed;\n    }\n\n    default:\n      amqp_abort(\"Internal error: invalid amqp_connection_state_t->state %d\",\n                 state->state);\n  }\n}", "func_src_after": "int amqp_handle_input(amqp_connection_state_t state, amqp_bytes_t received_data,\n                      amqp_frame_t *decoded_frame) {\n  size_t bytes_consumed;\n  void *raw_frame;\n\n  /* Returning frame_type of zero indicates either insufficient input,\n     or a complete, ignored frame was read. */\n  decoded_frame->frame_type = 0;\n\n  if (received_data.len == 0) {\n    return AMQP_STATUS_OK;\n  }\n\n  if (state->state == CONNECTION_STATE_IDLE) {\n    state->state = CONNECTION_STATE_HEADER;\n  }\n\n  bytes_consumed = consume_data(state, &received_data);\n\n  /* do we have target_size data yet? if not, return with the\n     expectation that more will arrive */\n  if (state->inbound_offset < state->target_size) {\n    return (int)bytes_consumed;\n  }\n\n  raw_frame = state->inbound_buffer.bytes;\n\n  switch (state->state) {\n    case CONNECTION_STATE_INITIAL:\n      /* check for a protocol header from the server */\n      if (memcmp(raw_frame, \"AMQP\", 4) == 0) {\n        decoded_frame->frame_type = AMQP_PSEUDOFRAME_PROTOCOL_HEADER;\n        decoded_frame->channel = 0;\n\n        decoded_frame->payload.protocol_header.transport_high =\n            amqp_d8(amqp_offset(raw_frame, 4));\n        decoded_frame->payload.protocol_header.transport_low =\n            amqp_d8(amqp_offset(raw_frame, 5));\n        decoded_frame->payload.protocol_header.protocol_version_major =\n            amqp_d8(amqp_offset(raw_frame, 6));\n        decoded_frame->payload.protocol_header.protocol_version_minor =\n            amqp_d8(amqp_offset(raw_frame, 7));\n\n        return_to_idle(state);\n        return (int)bytes_consumed;\n      }\n\n    /* it's not a protocol header; fall through to process it as a\n       regular frame header */\n\n    case CONNECTION_STATE_HEADER: {\n      amqp_channel_t channel;\n      amqp_pool_t *channel_pool;\n      uint32_t frame_size;\n\n      channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      /* frame length is 3 bytes in */\n      frame_size = amqp_d32(amqp_offset(raw_frame, 3));\n      /* To prevent the target_size calculation below from overflowing, check\n       * that the stated frame_size is smaller than a signed 32-bit. Given\n       * the library only allows configuring frame_max as an int32_t, and\n       * frame_size is uint32_t, the math below is safe from overflow. */\n      if (frame_size >= INT32_MAX) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      state->target_size = frame_size + HEADER_SIZE + FOOTER_SIZE;\n      if ((size_t)state->frame_max < state->target_size) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      channel_pool = amqp_get_or_create_channel_pool(state, channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      amqp_pool_alloc_bytes(channel_pool, state->target_size,\n                            &state->inbound_buffer);\n      if (NULL == state->inbound_buffer.bytes) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n      memcpy(state->inbound_buffer.bytes, state->header_buffer, HEADER_SIZE);\n      raw_frame = state->inbound_buffer.bytes;\n\n      state->state = CONNECTION_STATE_BODY;\n\n      bytes_consumed += consume_data(state, &received_data);\n\n      /* do we have target_size data yet? if not, return with the\n         expectation that more will arrive */\n      if (state->inbound_offset < state->target_size) {\n        return (int)bytes_consumed;\n      }\n    }\n    /* fall through to process body */\n\n    case CONNECTION_STATE_BODY: {\n      amqp_bytes_t encoded;\n      int res;\n      amqp_pool_t *channel_pool;\n\n      /* Check frame end marker (footer) */\n      if (amqp_d8(amqp_offset(raw_frame, state->target_size - 1)) !=\n          AMQP_FRAME_END) {\n        return AMQP_STATUS_BAD_AMQP_DATA;\n      }\n\n      decoded_frame->frame_type = amqp_d8(amqp_offset(raw_frame, 0));\n      decoded_frame->channel = amqp_d16(amqp_offset(raw_frame, 1));\n\n      channel_pool =\n          amqp_get_or_create_channel_pool(state, decoded_frame->channel);\n      if (NULL == channel_pool) {\n        return AMQP_STATUS_NO_MEMORY;\n      }\n\n      switch (decoded_frame->frame_type) {\n        case AMQP_FRAME_METHOD:\n          decoded_frame->payload.method.id =\n              amqp_d32(amqp_offset(raw_frame, HEADER_SIZE));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 4);\n          encoded.len = state->target_size - HEADER_SIZE - 4 - FOOTER_SIZE;\n\n          res = amqp_decode_method(decoded_frame->payload.method.id,\n                                   channel_pool, encoded,\n                                   &decoded_frame->payload.method.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_HEADER:\n          decoded_frame->payload.properties.class_id =\n              amqp_d16(amqp_offset(raw_frame, HEADER_SIZE));\n          /* unused 2-byte weight field goes here */\n          decoded_frame->payload.properties.body_size =\n              amqp_d64(amqp_offset(raw_frame, HEADER_SIZE + 4));\n          encoded.bytes = amqp_offset(raw_frame, HEADER_SIZE + 12);\n          encoded.len = state->target_size - HEADER_SIZE - 12 - FOOTER_SIZE;\n          decoded_frame->payload.properties.raw = encoded;\n\n          res = amqp_decode_properties(\n              decoded_frame->payload.properties.class_id, channel_pool, encoded,\n              &decoded_frame->payload.properties.decoded);\n          if (res < 0) {\n            return res;\n          }\n\n          break;\n\n        case AMQP_FRAME_BODY:\n          decoded_frame->payload.body_fragment.len =\n              state->target_size - HEADER_SIZE - FOOTER_SIZE;\n          decoded_frame->payload.body_fragment.bytes =\n              amqp_offset(raw_frame, HEADER_SIZE);\n          break;\n\n        case AMQP_FRAME_HEARTBEAT:\n          break;\n\n        default:\n          /* Ignore the frame */\n          decoded_frame->frame_type = 0;\n          break;\n      }\n\n      return_to_idle(state);\n      return (int)bytes_consumed;\n    }\n\n    default:\n      amqp_abort(\"Internal error: invalid amqp_connection_state_t->state %d\",\n                 state->state);\n  }\n}", "commit_link": "github.com/alanxz/rabbitmq-c/commit/fc85be7123050b91b054e45b91c78d3241a5047a", "file_name": "librabbitmq/amqp_connection.c", "vul_type": "cwe-190", "description": "Write a C function named `amqp_handle_input` that processes incoming AMQP data and decodes it into a frame structure."}
{"func_name": "CompileAndRun", "func_src_before": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "func_src_after": "func (l *Loader) CompileAndRun(name string, input io.Reader) error {\n\tglog.V(2).Infof(\"CompileAndRun %s\", name)\n\tv, errs := Compile(name, input, l.dumpAst, l.dumpAstTypes, l.syslogUseCurrentYear, l.overrideLocation)\n\tif errs != nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"compile failed for %s:\\n%s\", name, errs)\n\t}\n\tif v == nil {\n\t\tProgLoadErrors.Add(name, 1)\n\t\treturn errors.Errorf(\"Internal error: Compilation failed for %s: No program returned, but no errors.\", name)\n\t}\n\n\tif l.dumpBytecode {\n\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n\t}\n\n\t// Load the metrics from the compilation into the global metric storage for export.\n\tfor _, m := range v.m {\n\t\tif !m.Hidden {\n\t\t\tif l.omitMetricSource {\n\t\t\t\tm.Source = \"\"\n\t\t\t}\n\t\t\terr := l.ms.Add(m)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tProgLoads.Add(name, 1)\n\tglog.Infof(\"Loaded program %s\", name)\n\n\tif l.compileOnly {\n\t\treturn nil\n\t}\n\n\tl.handleMu.Lock()\n\tdefer l.handleMu.Unlock()\n\n\tl.handles[name] = v\n\treturn nil\n}", "line_changes": {"deleted": [{"line_no": 14, "char_start": 513, "char_end": 589, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode(name))\n"}], "added": [{"line_no": 14, "char_start": 513, "char_end": 585, "line": "\t\tglog.Info(\"Dumping program objects and bytecode\\n\", v.DumpByteCode())\n"}]}, "char_changes": {"deleted": [{"char_start": 582, "char_end": 586, "chars": "name"}], "added": []}, "commit_link": "github.com/SuperQ/mtail/commit/a90de206158775716feb1f0a1b36636301cd14fa", "file_name": "loader.go", "vul_type": "cwe-079", "commit_msg": "Don't need to pass the name of the prog to the prog dumper.\n\nIt already knows what the name is, and CodeQL thinks this is an XSS.", "parent_commit": "28a3000450fa7a2a34df95ff656db845139606be", "description": "Write a Go function that compiles and runs a program from an input reader, handling errors and metrics."}
{"func_name": "make_tarfile", "func_src_before": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    unzipped_filename = tempfile.mktemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.remove(unzipped_filename)", "func_src_after": "def make_tarfile(\n    output_filename: str,\n    source_dir: str,\n    archive_name: str,\n    custom_filter: Optional[Callable] = None,\n) -> None:\n    # Helper for filtering out modification timestamps\n    def _filter_timestamps(tar_info: \"tarfile.TarInfo\") -> Optional[\"tarfile.TarInfo\"]:\n        tar_info.mtime = 0\n        return tar_info if custom_filter is None else custom_filter(tar_info)\n\n    descriptor, unzipped_filename = tempfile.mkstemp()\n    try:\n        with tarfile.open(unzipped_filename, \"w\") as tar:\n            tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n        # When gzipping the tar, don't include the tar's filename or modification time in the\n        # zipped archive (see https://docs.python.org/3/library/gzip.html#gzip.GzipFile)\n        with gzip.GzipFile(\n            filename=\"\", fileobj=open(output_filename, \"wb\"), mode=\"wb\", mtime=0\n        ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar_file:\n            gzipped_tar.write(tar_file.read())\n    finally:\n        os.close(descriptor)\n        os.remove(unzipped_filename)", "line_changes": {"deleted": [{"line_no": 12, "char_start": 394, "char_end": 436, "line": "    unzipped_filename = tempfile.mktemp()\n"}], "added": [{"line_no": 12, "char_start": 394, "char_end": 449, "line": "    descriptor, unzipped_filename = tempfile.mkstemp()\n"}, {"line_no": 23, "char_start": 1018, "char_end": 1047, "line": "        os.close(descriptor)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 397, "char_end": 409, "chars": " descriptor,"}, {"char_start": 441, "char_end": 442, "chars": "s"}, {"char_start": 1018, "char_end": 1047, "chars": "        os.close(descriptor)\n"}]}, "commit_link": "github.com/wandb/client/commit/7c1306c1851da5628ac96e0f89728d16852611dd", "file_name": "util.py", "vul_type": "cwe-377", "commit_msg": "Port of #3357: Fix/insecure tempfile (#3360)\n\n* Replaced insecure tempfile.mktemp() with tempfile.mkstemp()\r\nCo-authored-by: whokilleddb <whokilleddb@gmail.com>", "description": "Write a Python function to create a gzipped tar file from a directory, optionally applying a custom filter to the files."}
{"func_name": "saveNewOption", "func_src_before": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val=newValF.val();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "func_src_after": "\tsaveNewOption:function(){\n\t\tvar newValF=$('#akSelectValueFieldNew');\n\t\tvar val = $('<div/>').text(newValF.val()).html();\n\t\tif(val=='') {\n\t\t\treturn;\n\t\t}\n\t\tvar ts = 't' + new Date().getTime();\n\t\tvar template=document.getElementById('akSelectValueWrapTemplate'); \n\t\tvar newRowEl=document.createElement('div');\n\t\tnewRowEl.innerHTML=template.innerHTML.replace(/template_clean/ig,ts).replace(/template/ig,val);\n\t\tnewRowEl.id=\"akSelectValueWrap_\"+ts;\n\t\tnewRowEl.className='akSelectValueWrap';\n\t\t$('#attributeValuesWrap').append(newRowEl);\t\t\n\t\tnewValF.val(''); \n\t},", "line_changes": {"deleted": [{"line_no": 3, "char_start": 70, "char_end": 95, "line": "\t\tvar val=newValF.val();\n"}], "added": [{"line_no": 3, "char_start": 70, "char_end": 122, "line": "\t\tvar val = $('<div/>').text(newValF.val()).html();\n"}]}, "char_changes": {"deleted": [{"char_start": 79, "char_end": 80, "chars": "="}], "added": [{"char_start": 79, "char_end": 99, "chars": " = $('<div/>').text("}, {"char_start": 112, "char_end": 120, "chars": ").html()"}]}, "commit_link": "github.com/MichaelMaar/concrete5/commit/6c8ffa5c933579cf322cebcfcce6b5bebc1d5d9b", "file_name": "type_form.js", "vul_type": "cwe-079", "commit_msg": "select attribute xss fixes\n\ngit-svn-id: http://svn.concrete5.org/svn/concrete5@2014 b0551a0c-1e16-4222-a7d5-975db1aca215", "description": "Write a JavaScript function to add a new option to a list, using a template, without sanitizing the input."}
{"func_name": "ResolveStateAndPredicate", "func_src_before": "ResolveStateAndPredicate(ExprDef *expr, enum xkb_match_operation *pred_rtrn,\n                         xkb_mod_mask_t *mods_rtrn, CompatInfo *info)\n{\n    if (expr == NULL) {\n        *pred_rtrn = MATCH_ANY_OR_NONE;\n        *mods_rtrn = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    *pred_rtrn = MATCH_EXACTLY;\n    if (expr->expr.op == EXPR_ACTION_DECL) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->action.name);\n        if (!LookupString(symInterpretMatchMaskNames, pred_txt, pred_rtrn)) {\n            log_err(info->ctx,\n                    \"Illegal modifier predicate \\\"%s\\\"; Ignored\\n\", pred_txt);\n            return false;\n        }\n        expr = expr->action.args;\n    }\n    else if (expr->expr.op == EXPR_IDENT) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->ident.ident);\n        if (pred_txt && istreq(pred_txt, \"any\")) {\n            *pred_rtrn = MATCH_ANY;\n            *mods_rtrn = MOD_REAL_MASK_ALL;\n            return true;\n        }\n    }\n\n    return ExprResolveModMask(info->ctx, expr, MOD_REAL, &info->mods,\n                              mods_rtrn);\n}", "func_src_after": "ResolveStateAndPredicate(ExprDef *expr, enum xkb_match_operation *pred_rtrn,\n                         xkb_mod_mask_t *mods_rtrn, CompatInfo *info)\n{\n    if (expr == NULL) {\n        *pred_rtrn = MATCH_ANY_OR_NONE;\n        *mods_rtrn = MOD_REAL_MASK_ALL;\n        return true;\n    }\n\n    *pred_rtrn = MATCH_EXACTLY;\n    if (expr->expr.op == EXPR_ACTION_DECL) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->action.name);\n        if (!LookupString(symInterpretMatchMaskNames, pred_txt, pred_rtrn) ||\n            !expr->action.args) {\n            log_err(info->ctx,\n                    \"Illegal modifier predicate \\\"%s\\\"; Ignored\\n\", pred_txt);\n            return false;\n        }\n        expr = expr->action.args;\n    }\n    else if (expr->expr.op == EXPR_IDENT) {\n        const char *pred_txt = xkb_atom_text(info->ctx, expr->ident.ident);\n        if (pred_txt && istreq(pred_txt, \"any\")) {\n            *pred_rtrn = MATCH_ANY;\n            *mods_rtrn = MOD_REAL_MASK_ALL;\n            return true;\n        }\n    }\n\n    return ExprResolveModMask(info->ctx, expr, MOD_REAL, &info->mods,\n                              mods_rtrn);\n}", "commit_link": "github.com/xkbcommon/libxkbcommon/commit/96df3106d49438e442510c59acad306e94f3db4d", "file_name": "src/xkbcomp/compat.c", "vul_type": "cwe-476", "description": "In C, write a function `ResolveStateAndPredicate` that processes an expression to determine a matching operation and modifier mask for keyboard input compatibility."}
{"func_name": "add_rpmmd_repo", "func_src_before": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      gz = open(primary_xml)\n      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n      repo.add_rpmmd(fd, nil, 0)\n      pool.createwhatprovides\n    ensure\n      gz&.close\n    end", "func_src_after": "    def add_rpmmd_repo(primary_xml, name)\n      repo = pool.add_repo(name)\n      File.open(primary_xml) do |gz|\n        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n        repo.add_rpmmd(fd, nil, 0)\n        pool.createwhatprovides\n      end\n    end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 75, "char_end": 104, "line": "      gz = open(primary_xml)\n"}, {"line_no": 4, "char_start": 104, "char_end": 154, "line": "      fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 154, "char_end": 187, "line": "      repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 187, "char_end": 217, "line": "      pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 217, "char_end": 228, "line": "    ensure\n"}, {"line_no": 8, "char_start": 228, "char_end": 244, "line": "      gz&.close\n"}], "added": [{"line_no": 3, "char_start": 75, "char_end": 112, "line": "      File.open(primary_xml) do |gz|\n"}, {"line_no": 4, "char_start": 112, "char_end": 164, "line": "        fd = Solv.xfopen_fd(primary_xml, gz.fileno)\n"}, {"line_no": 5, "char_start": 164, "char_end": 199, "line": "        repo.add_rpmmd(fd, nil, 0)\n"}, {"line_no": 6, "char_start": 199, "char_end": 231, "line": "        pool.createwhatprovides\n"}, {"line_no": 7, "char_start": 231, "char_end": 241, "line": "      end\n"}]}, "char_changes": {"deleted": [{"char_start": 81, "char_end": 86, "chars": "gz = "}, {"char_start": 103, "char_end": 104, "chars": "\n"}, {"char_start": 221, "char_end": 243, "chars": "ensure\n      gz&.close"}], "added": [{"char_start": 81, "char_end": 86, "chars": "File."}, {"char_start": 103, "char_end": 114, "chars": " do |gz|\n  "}, {"char_start": 170, "char_end": 172, "chars": "  "}, {"char_start": 205, "char_end": 206, "chars": " "}, {"char_start": 206, "char_end": 207, "chars": " "}, {"char_start": 235, "char_end": 240, "chars": "  end"}]}, "commit_link": "github.com/yast/yast-packager/commit/a25ddf08b43ff58ceaebb1aa7c01e2804f98864e", "file_name": "solvable_pool.rb", "vul_type": "cwe-078", "commit_msg": "use secure File.open instead of Kernel.open (NO-AUTO)", "parent_commit": "915385e0672aef903b49b7f735a655a051b68f6a", "description": "Write a Ruby function to add an RPM metadata repository by reading a primary XML file."}
{"func_name": "__init__", "func_src_before": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion > 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion > 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "func_src_after": "    def __init__(self, library):\n        \"\"\"\n        Build the wrapper\n        \"\"\"\n        self._lib = ctypes.CDLL(library)\n        self._version, self._hexversion, self._cflags = get_version(self._lib)\n\n        self.pointer = ctypes.pointer\n        self.c_int = ctypes.c_int\n        self.byref = ctypes.byref\n        self.create_string_buffer = ctypes.create_string_buffer\n\n        self.BN_new = self._lib.BN_new\n        self.BN_new.restype = ctypes.c_void_p\n        self.BN_new.argtypes = []\n\n        self.BN_free = self._lib.BN_free\n        self.BN_free.restype = None\n        self.BN_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_num_bits = self._lib.BN_num_bits\n        self.BN_num_bits.restype = ctypes.c_int\n        self.BN_num_bits.argtypes = [ctypes.c_void_p]\n\n        self.BN_bn2bin = self._lib.BN_bn2bin\n        self.BN_bn2bin.restype = ctypes.c_int\n        self.BN_bn2bin.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_bin2bn = self._lib.BN_bin2bn\n        self.BN_bin2bn.restype = ctypes.c_void_p\n        self.BN_bin2bn.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                   ctypes.c_void_p]\n\n        self.EC_KEY_free = self._lib.EC_KEY_free\n        self.EC_KEY_free.restype = None\n        self.EC_KEY_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_new_by_curve_name = self._lib.EC_KEY_new_by_curve_name\n        self.EC_KEY_new_by_curve_name.restype = ctypes.c_void_p\n        self.EC_KEY_new_by_curve_name.argtypes = [ctypes.c_int]\n\n        self.EC_KEY_generate_key = self._lib.EC_KEY_generate_key\n        self.EC_KEY_generate_key.restype = ctypes.c_int\n        self.EC_KEY_generate_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_check_key = self._lib.EC_KEY_check_key\n        self.EC_KEY_check_key.restype = ctypes.c_int\n        self.EC_KEY_check_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_private_key = self._lib.EC_KEY_get0_private_key\n        self.EC_KEY_get0_private_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_private_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_public_key = self._lib.EC_KEY_get0_public_key\n        self.EC_KEY_get0_public_key.restype = ctypes.c_void_p\n        self.EC_KEY_get0_public_key.argtypes = [ctypes.c_void_p]\n\n        self.EC_KEY_get0_group = self._lib.EC_KEY_get0_group\n        self.EC_KEY_get0_group.restype = ctypes.c_void_p\n        self.EC_KEY_get0_group.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_get_affine_coordinates_GFp = self._lib.EC_POINT_get_affine_coordinates_GFp\n        self.EC_POINT_get_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_get_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        self.EC_KEY_set_public_key = self._lib.EC_KEY_set_public_key\n        self.EC_KEY_set_public_key.restype = ctypes.c_int\n        self.EC_KEY_set_public_key.argtypes = [ctypes.c_void_p,\n                                               ctypes.c_void_p]\n\n        self.EC_KEY_set_group = self._lib.EC_KEY_set_group\n        self.EC_KEY_set_group.restype = ctypes.c_int\n        self.EC_KEY_set_group.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_set_affine_coordinates_GFp = self._lib.EC_POINT_set_affine_coordinates_GFp\n        self.EC_POINT_set_affine_coordinates_GFp.restype = ctypes.c_int\n        self.EC_POINT_set_affine_coordinates_GFp.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_POINT_new = self._lib.EC_POINT_new\n        self.EC_POINT_new.restype = ctypes.c_void_p\n        self.EC_POINT_new.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_free = self._lib.EC_POINT_free\n        self.EC_POINT_free.restype = None\n        self.EC_POINT_free.argtypes = [ctypes.c_void_p]\n\n        self.BN_CTX_free = self._lib.BN_CTX_free\n        self.BN_CTX_free.restype = None\n        self.BN_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EC_POINT_mul = self._lib.EC_POINT_mul\n        self.EC_POINT_mul.restype = None\n        self.EC_POINT_mul.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EC_KEY_set_private_key = self._lib.EC_KEY_set_private_key\n        self.EC_KEY_set_private_key.restype = ctypes.c_int\n        self.EC_KEY_set_private_key.argtypes = [ctypes.c_void_p,\n                                                ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EC_KEY_OpenSSL = self._lib.EC_KEY_OpenSSL\n            self._lib.EC_KEY_OpenSSL.restype = ctypes.c_void_p\n            self._lib.EC_KEY_OpenSSL.argtypes = []\n            \n            self.EC_KEY_set_method = self._lib.EC_KEY_set_method\n            self._lib.EC_KEY_set_method.restype = ctypes.c_int\n            self._lib.EC_KEY_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        else:\n            self.ECDH_OpenSSL = self._lib.ECDH_OpenSSL\n            self._lib.ECDH_OpenSSL.restype = ctypes.c_void_p\n            self._lib.ECDH_OpenSSL.argtypes = []\n\n            self.ECDH_set_method = self._lib.ECDH_set_method\n            self._lib.ECDH_set_method.restype = ctypes.c_int\n            self._lib.ECDH_set_method.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.BN_CTX_new = self._lib.BN_CTX_new\n        self._lib.BN_CTX_new.restype = ctypes.c_void_p\n        self._lib.BN_CTX_new.argtypes = []\n\n        self.ECDH_compute_key = self._lib.ECDH_compute_key\n        self.ECDH_compute_key.restype = ctypes.c_int\n        self.ECDH_compute_key.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CipherInit_ex = self._lib.EVP_CipherInit_ex\n        self.EVP_CipherInit_ex.restype = ctypes.c_int\n        self.EVP_CipherInit_ex.argtypes = [ctypes.c_void_p,\n                                           ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_new = self._lib.EVP_CIPHER_CTX_new\n        self.EVP_CIPHER_CTX_new.restype = ctypes.c_void_p\n        self.EVP_CIPHER_CTX_new.argtypes = []\n\n        # Cipher\n        self.EVP_aes_128_cfb128 = self._lib.EVP_aes_128_cfb128\n        self.EVP_aes_128_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_128_cfb128.argtypes = []\n\n        self.EVP_aes_256_cfb128 = self._lib.EVP_aes_256_cfb128\n        self.EVP_aes_256_cfb128.restype = ctypes.c_void_p\n        self.EVP_aes_256_cfb128.argtypes = []\n\n        self.EVP_aes_128_cbc = self._lib.EVP_aes_128_cbc\n        self.EVP_aes_128_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_128_cbc.argtypes = []\n\n        self.EVP_aes_256_cbc = self._lib.EVP_aes_256_cbc\n        self.EVP_aes_256_cbc.restype = ctypes.c_void_p\n        self.EVP_aes_256_cbc.argtypes = []\n\n        #self.EVP_aes_128_ctr = self._lib.EVP_aes_128_ctr\n        #self.EVP_aes_128_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_128_ctr.argtypes = []\n\n        #self.EVP_aes_256_ctr = self._lib.EVP_aes_256_ctr\n        #self.EVP_aes_256_ctr.restype = ctypes.c_void_p\n        #self.EVP_aes_256_ctr.argtypes = []\n\n        self.EVP_aes_128_ofb = self._lib.EVP_aes_128_ofb\n        self.EVP_aes_128_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_128_ofb.argtypes = []\n\n        self.EVP_aes_256_ofb = self._lib.EVP_aes_256_ofb\n        self.EVP_aes_256_ofb.restype = ctypes.c_void_p\n        self.EVP_aes_256_ofb.argtypes = []\n\n        self.EVP_bf_cbc = self._lib.EVP_bf_cbc\n        self.EVP_bf_cbc.restype = ctypes.c_void_p\n        self.EVP_bf_cbc.argtypes = []\n\n        self.EVP_bf_cfb64 = self._lib.EVP_bf_cfb64\n        self.EVP_bf_cfb64.restype = ctypes.c_void_p\n        self.EVP_bf_cfb64.argtypes = []\n\n        self.EVP_rc4 = self._lib.EVP_rc4\n        self.EVP_rc4.restype = ctypes.c_void_p\n        self.EVP_rc4.argtypes = []\n \n        if self._hexversion >= 0x10100000:\n            self.EVP_CIPHER_CTX_reset = self._lib.EVP_CIPHER_CTX_reset\n            self.EVP_CIPHER_CTX_reset.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_reset.argtypes = [ctypes.c_void_p]\n        else:\n            self.EVP_CIPHER_CTX_cleanup = self._lib.EVP_CIPHER_CTX_cleanup\n            self.EVP_CIPHER_CTX_cleanup.restype = ctypes.c_int\n            self.EVP_CIPHER_CTX_cleanup.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CIPHER_CTX_free = self._lib.EVP_CIPHER_CTX_free\n        self.EVP_CIPHER_CTX_free.restype = None\n        self.EVP_CIPHER_CTX_free.argtypes = [ctypes.c_void_p]\n\n        self.EVP_CipherUpdate = self._lib.EVP_CipherUpdate\n        self.EVP_CipherUpdate.restype = ctypes.c_int\n        self.EVP_CipherUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_CipherFinal_ex = self._lib.EVP_CipherFinal_ex\n        self.EVP_CipherFinal_ex.restype = ctypes.c_int\n        self.EVP_CipherFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit = self._lib.EVP_DigestInit\n        self.EVP_DigestInit.restype = ctypes.c_int\n        self._lib.EVP_DigestInit.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestInit_ex = self._lib.EVP_DigestInit_ex\n        self.EVP_DigestInit_ex.restype = ctypes.c_int\n        self._lib.EVP_DigestInit_ex.argtypes = 3 * [ctypes.c_void_p]\n        \n        self.EVP_DigestUpdate = self._lib.EVP_DigestUpdate\n        self.EVP_DigestUpdate.restype = ctypes.c_int\n        self.EVP_DigestUpdate.argtypes = [ctypes.c_void_p,\n                                          ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_DigestFinal = self._lib.EVP_DigestFinal\n        self.EVP_DigestFinal.restype = ctypes.c_int\n        self.EVP_DigestFinal.argtypes = [ctypes.c_void_p,\n                                         ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_DigestFinal_ex = self._lib.EVP_DigestFinal_ex\n        self.EVP_DigestFinal_ex.restype = ctypes.c_int\n        self.EVP_DigestFinal_ex.argtypes = [ctypes.c_void_p,\n                                            ctypes.c_void_p, ctypes.c_void_p]\n        \n        self.ECDSA_sign = self._lib.ECDSA_sign\n        self.ECDSA_sign.restype = ctypes.c_int\n        self.ECDSA_sign.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                    ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n\n        self.ECDSA_verify = self._lib.ECDSA_verify\n        self.ECDSA_verify.restype = ctypes.c_int\n        self.ECDSA_verify.argtypes = [ctypes.c_int, ctypes.c_void_p,\n                                      ctypes.c_int, ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p]\n\n        if self._hexversion >= 0x10100000:\n            self.EVP_MD_CTX_new = self._lib.EVP_MD_CTX_new\n            self.EVP_MD_CTX_new.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_new.argtypes = []\n        \n            self.EVP_MD_CTX_reset = self._lib.EVP_MD_CTX_reset\n            self.EVP_MD_CTX_reset.restype = None\n            self.EVP_MD_CTX_reset.argtypes = [ctypes.c_void_p]\n\n            self.EVP_MD_CTX_free = self._lib.EVP_MD_CTX_free\n            self.EVP_MD_CTX_free.restype = None\n            self.EVP_MD_CTX_free.argtypes = [ctypes.c_void_p]\n\n            self.EVP_sha1 = self._lib.EVP_sha1\n            self.EVP_sha1.restype = ctypes.c_void_p\n            self.EVP_sha1.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_sha1\n        else:\n            self.EVP_MD_CTX_create = self._lib.EVP_MD_CTX_create\n            self.EVP_MD_CTX_create.restype = ctypes.c_void_p\n            self.EVP_MD_CTX_create.argtypes = []\n \n            self.EVP_MD_CTX_init = self._lib.EVP_MD_CTX_init\n            self.EVP_MD_CTX_init.restype = None\n            self.EVP_MD_CTX_init.argtypes = [ctypes.c_void_p]\n \n            self.EVP_MD_CTX_destroy = self._lib.EVP_MD_CTX_destroy\n            self.EVP_MD_CTX_destroy.restype = None\n            self.EVP_MD_CTX_destroy.argtypes = [ctypes.c_void_p]\n\n            self.EVP_ecdsa = self._lib.EVP_ecdsa\n            self._lib.EVP_ecdsa.restype = ctypes.c_void_p\n            self._lib.EVP_ecdsa.argtypes = []\n\n            self.digest_ecdsa_sha1 = self.EVP_ecdsa\n\n        self.RAND_bytes = self._lib.RAND_bytes\n        self.RAND_bytes.restype = ctypes.c_int\n        self.RAND_bytes.argtypes = [ctypes.c_void_p, ctypes.c_int]\n\n        self.EVP_sha256 = self._lib.EVP_sha256\n        self.EVP_sha256.restype = ctypes.c_void_p\n        self.EVP_sha256.argtypes = []\n\n        self.i2o_ECPublicKey = self._lib.i2o_ECPublicKey\n        self.i2o_ECPublicKey.restype = ctypes.c_void_p\n        self.i2o_ECPublicKey.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n\n        self.EVP_sha512 = self._lib.EVP_sha512\n        self.EVP_sha512.restype = ctypes.c_void_p\n        self.EVP_sha512.argtypes = []\n\n        self.HMAC = self._lib.HMAC\n        self.HMAC.restype = ctypes.c_void_p\n        self.HMAC.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int,\n                              ctypes.c_void_p, ctypes.c_int, ctypes.c_void_p, ctypes.c_void_p]\n\n        try:\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC\n        except:\n            # The above is not compatible with all versions of OSX.\n            self.PKCS5_PBKDF2_HMAC = self._lib.PKCS5_PBKDF2_HMAC_SHA1\n            \n        self.PKCS5_PBKDF2_HMAC.restype = ctypes.c_int\n        self.PKCS5_PBKDF2_HMAC.argtypes = [ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_void_p, ctypes.c_int,\n                                           ctypes.c_int, ctypes.c_void_p,\n                                           ctypes.c_int, ctypes.c_void_p]\n\n        self._set_ciphers()\n        self._set_curves()", "line_changes": {"deleted": [{"line_no": 105, "char_start": 4704, "char_end": 4746, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 185, "char_start": 8062, "char_end": 8104, "line": "        if self._hexversion > 0x10100000:\n"}, {"line_no": 241, "char_start": 10904, "char_end": 10946, "line": "        if self._hexversion > 0x10100000:\n"}], "added": [{"line_no": 105, "char_start": 4704, "char_end": 4747, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 185, "char_start": 8063, "char_end": 8106, "line": "        if self._hexversion >= 0x10100000:\n"}, {"line_no": 241, "char_start": 10906, "char_end": 10949, "line": "        if self._hexversion >= 0x10100000:\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 4733, "char_end": 4734, "chars": "="}, {"char_start": 8092, "char_end": 8093, "chars": "="}, {"char_start": 10935, "char_end": 10936, "chars": "="}]}, "commit_link": "github.com/bmng-dev/PyBitmessage/commit/59b5ac3a61fcfb1fd55812198ffe208243c3c160", "file_name": "openssl.py", "vul_type": "cwe-327", "commit_msg": "OpenSSL 1.1.0 compatibility fixes\n\n- function check missed 1.1.0 release\n- TLS didn't work with anonymous ciphers", "description": "Create a Python class that wraps cryptographic functions from a dynamic library using ctypes."}
{"func_name": "get_list_context", "func_src_before": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "func_src_after": "def get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context", "commit_link": "github.com/omirajkar/bench_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "file_name": "frappe/website/doctype/blog_post/blog_post.py", "vul_type": "cwe-079", "description": "Write a Python function in Frappe to customize the context of a blog list page based on filters like category, blogger, or search text."}
{"func_name": "rfbHandleAuthResult", "func_src_before": "rfbHandleAuthResult(rfbClient* client)\n{\n    uint32_t authResult=0, reasonLen=0;\n    char *reason=NULL;\n\n    if (!ReadFromRFBServer(client, (char *)&authResult, 4)) return FALSE;\n\n    authResult = rfbClientSwap32IfLE(authResult);\n\n    switch (authResult) {\n    case rfbVncAuthOK:\n      rfbClientLog(\"VNC authentication succeeded\\n\");\n      return TRUE;\n      break;\n    case rfbVncAuthFailed:\n      if (client->major==3 && client->minor>7)\n      {\n        /* we have an error following */\n        if (!ReadFromRFBServer(client, (char *)&reasonLen, 4)) return FALSE;\n        reasonLen = rfbClientSwap32IfLE(reasonLen);\n        reason = malloc((uint64_t)reasonLen+1);\n        if (!ReadFromRFBServer(client, reason, reasonLen)) { free(reason); return FALSE; }\n        reason[reasonLen]=0;\n        rfbClientLog(\"VNC connection failed: %s\\n\",reason);\n        free(reason);\n        return FALSE;\n      }\n      rfbClientLog(\"VNC authentication failed\\n\");\n      return FALSE;\n    case rfbVncAuthTooMany:\n      rfbClientLog(\"VNC authentication failed - too many tries\\n\");\n      return FALSE;\n    }\n\n    rfbClientLog(\"Unknown VNC authentication result: %d\\n\",\n                 (int)authResult);\n    return FALSE;\n}", "func_src_after": "rfbHandleAuthResult(rfbClient* client)\n{\n    uint32_t authResult=0;\n\n    if (!ReadFromRFBServer(client, (char *)&authResult, 4)) return FALSE;\n\n    authResult = rfbClientSwap32IfLE(authResult);\n\n    switch (authResult) {\n    case rfbVncAuthOK:\n      rfbClientLog(\"VNC authentication succeeded\\n\");\n      return TRUE;\n      break;\n    case rfbVncAuthFailed:\n      if (client->major==3 && client->minor>7)\n      {\n        /* we have an error following */\n        ReadReason(client);\n        return FALSE;\n      }\n      rfbClientLog(\"VNC authentication failed\\n\");\n      return FALSE;\n    case rfbVncAuthTooMany:\n      rfbClientLog(\"VNC authentication failed - too many tries\\n\");\n      return FALSE;\n    }\n\n    rfbClientLog(\"Unknown VNC authentication result: %d\\n\",\n                 (int)authResult);\n    return FALSE;\n}", "commit_link": "github.com/LibVNC/libvncserver/commit/e34bcbb759ca5bef85809967a268fdf214c1ad2c", "file_name": "libvncclient/rfbproto.c", "vul_type": "cwe-787", "description": "Write a C function named `rfbHandleAuthResult` that processes VNC authentication results from a server."}
{"func_name": "archive_directory", "func_src_before": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(top_dir, subdir, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in file_list:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n                return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "func_src_after": "def archive_directory(top_dir, subdir, tmpdir):\n    \"\"\"\n    .. function:: archive_directory(top_dir, subdir, tmpdir)\n\n    Given a sub-directory name under the root directory to be archived, archive the contents of the sub-directory\n    to a temporary directory. Then return the full path to the temporary directory.\n    :param top_dir: The root path that will be archived and uploaded to Glacier.\n    :param subdir: The path to the subdirectory that is being archived here, relative to `top_dir`\n    :param tmpdir: The path to the temporary directory to store archives in until they are uploaded to Glacier\n    :return: If the subdirectory contains files, then the full path to the temporary archives; otherwise, None\n    \"\"\"\n    # We're only archiving the *files* in this directory, not the subdirectories.\n\n    files = []\n    full_backup_path = os.path.join(top_dir, subdir)\n    dir_contents = os.listdir(full_backup_path)\n\n    # Only add files to 'files' list, not subdirs\n    for c in dir_contents:\n        fpath = os.path.join(full_backup_path, c)\n        if os.path.isfile(fpath) and not fpath.endswith(\".ini\"):\n            # logger.info(\"Adding to archive list: {0}\".format(c))\n            files.append(fpath)\n\n    if not files:\n        # No point creating empty archives!\n        return None\n\n    os.makedirs(os.path.join(tmpdir, subdir))\n    archive_file_path = os.path.join(tmpdir, subdir) + \".zip\"\n\n    logger.info(\"Archiving %s to %s\" % (subdir, archive_file_path))\n\n    # with open(os.devnull, \"w\") as devnull:\n\n    devnull = open(os.devnull, \"wb\")\n\n    try:\n        with zipfile.ZipFile(archive_file_path, \"w\", compression=zipfile.ZIP_DEFLATED, allowZip64=True) as arch_zip:\n            for f in files:\n                logger.debug(\"Adding {0} to archive {1}\".format(f, archive_file_path))\n                arch_zip.write(f, os.path.basename(f))\n            return archive_file_path\n\n    except Exception, e:\n        logging.error(\"Failed to create archive {0}: {1}\".format(archive_file_path, e.message))\n        logging.debug(\"Error args: {0}\".format(e.args))\n        return None", "line_changes": {"deleted": [{"line_no": 20, "char_start": 1003, "char_end": 1052, "line": "        fpath = os.path.join(top_dir, subdir, c)\n"}, {"line_no": 37, "char_start": 1561, "char_end": 1580, "line": "    file_list = []\n"}, {"line_no": 38, "char_start": 1580, "char_end": 1623, "line": "    for p in os.listdir(full_backup_path):\n"}, {"line_no": 39, "char_start": 1623, "char_end": 1685, "line": "        if os.path.isfile(os.path.join(full_backup_path, p)):\n"}, {"line_no": 40, "char_start": 1685, "char_end": 1749, "line": "            file_list.append(os.path.join(full_backup_path, p))\n"}, {"line_no": 44, "char_start": 1876, "char_end": 1908, "line": "            for f in file_list:\n"}, {"line_no": 47, "char_start": 2050, "char_end": 2091, "line": "                return archive_file_path\n"}], "added": [{"line_no": 20, "char_start": 1003, "char_end": 1053, "line": "        fpath = os.path.join(full_backup_path, c)\n"}, {"line_no": 40, "char_start": 1689, "char_end": 1717, "line": "            for f in files:\n"}, {"line_no": 43, "char_start": 1859, "char_end": 1896, "line": "            return archive_file_path\n"}]}, "char_changes": {"deleted": [{"char_start": 1032, "char_end": 1047, "chars": "top_dir, subdir"}, {"char_start": 1560, "char_end": 1748, "chars": "\n    file_list = []\n    for p in os.listdir(full_backup_path):\n        if os.path.isfile(os.path.join(full_backup_path, p)):\n            file_list.append(os.path.join(full_backup_path, p))"}, {"char_start": 1901, "char_end": 1906, "chars": "_list"}, {"char_start": 2050, "char_end": 2054, "chars": "    "}], "added": [{"char_start": 1032, "char_end": 1048, "chars": "full_backup_path"}, {"char_start": 1714, "char_end": 1715, "chars": "s"}]}, "commit_link": "github.com/calmcl1/cupo-backup/commit/f9047a52ab33a14fcd67d3d8b9f9d321502ae457", "file_name": "cupo.py", "vul_type": "cwe-022", "commit_msg": "Removed redundant directory traversal", "parent_commit": "49107dd052b985e1fc469aec359f37df23ec3e29", "description": "Write a Python function to zip files in a specified subdirectory, excluding '.ini' files, and save the archive to a temporary directory."}
{"func_name": "article", "func_src_before": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n  end", "func_src_after": "  def article\n    @article = Article.find(params[:id])\n    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 55, "char_end": 138, "line": "    @feedbacks = Feedback.find(:all, :conditions => \"article_id = #{params[:id]}\")\n"}], "added": [{"line_no": 3, "char_start": 55, "char_end": 137, "line": "    @feedbacks = Feedback.find(:all, :conditions => {:article_id => params[:id]})\n"}]}, "char_changes": {"deleted": [{"char_start": 107, "char_end": 108, "chars": "\""}, {"char_start": 121, "char_end": 123, "chars": "#{"}, {"char_start": 135, "char_end": 136, "chars": "\""}], "added": [{"char_start": 107, "char_end": 109, "chars": "{:"}, {"char_start": 121, "char_end": 122, "chars": ">"}]}, "commit_link": "github.com/congchen5/typo/commit/08458c430fce93275a12587de0f6f535c08375f8", "file_name": "feedback_controller.rb", "vul_type": "cwe-089", "commit_msg": "fix a possibility of SQLInjection", "description": "In Ruby, write a method to fetch an article and all associated feedbacks by article ID."}
{"func_name": "parse", "func_src_before": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "func_src_after": "    @staticmethod\n    def parse(path, require_exists=True, require_parses=True):\n        if not os.path.isfile(path):\n            if require_exists:\n                raise ConfigError('not found: ' + path)\n            else:\n                return None\n        try:\n            with open(path) as f:\n                return yaml.safe_load(f)\n        except Exception, error:\n            if require_parses:\n                raise ConfigError('parse error: ' + path)", "line_changes": {"deleted": [{"line_no": 10, "char_start": 298, "char_end": 334, "line": "                return yaml.load(f)\n"}], "added": [{"line_no": 10, "char_start": 298, "char_end": 339, "line": "                return yaml.safe_load(f)\n"}]}, "char_changes": {"deleted": [], "added": [{"char_start": 326, "char_end": 331, "chars": "safe_"}]}, "commit_link": "github.com/silas/rock/commit/0852b4f55891e5dfe7cc6af3881942484daf9132", "file_name": "config.py", "vul_type": "cwe-502", "commit_msg": "Use yaml.safe_load instead of yaml.load", "parent_commit": "93a26daa34d92236cfcfe4e7ccf0d4814687c009", "description": "Create a Python function that loads a YAML file, with options to enforce file existence and successful parsing."}
{"func_name": "TestCreateBasket_InvalidName", "func_src_before": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "func_src_after": "func TestCreateBasket_InvalidName(t *testing.T) {\n\tbasket := \">>>\"\n\n\tr, err := http.NewRequest(\"POST\", \"http://localhost:55555/api/baskets/\"+basket, strings.NewReader(\"\"))\n\tif assert.NoError(t, err) {\n\t\tw := httptest.NewRecorder()\n\t\tps := append(make(httprouter.Params, 0), httprouter.Param{Key: \"basket\", Value: basket})\n\t\tCreateBasket(w, r, ps)\n\n\t\t// validate response: 400 - Bad Request\n\t\tassert.Equal(t, 400, w.Code, \"wrong HTTP result code\")\n\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n\t\t\t\"wrong error message\")\n\t\t// validate database\n\t\tassert.Nil(t, basketsDb.Get(basket), \"basket '%v' should not be created\", basket)\n\t}\n}", "line_changes": {"deleted": [{"line_no": 12, "char_start": 447, "char_end": 574, "line": "\t\tassert.Equal(t, \"invalid basket name; [\"+basket+\"] does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}], "added": [{"line_no": 12, "char_start": 447, "char_end": 570, "line": "\t\tassert.Equal(t, \"invalid basket name; the name does not match pattern: \"+validBasketName.String()+\"\\n\", w.Body.String(),\n"}]}, "char_changes": {"deleted": [{"char_start": 487, "char_end": 499, "chars": "[\"+basket+\"]"}], "added": [{"char_start": 487, "char_end": 495, "chars": "the name"}]}, "commit_link": "github.com/darklynx/request-baskets/commit/093f040f79865e9d44ad565a279f32038fb45a2a", "file_name": "handlers_test.go", "vul_type": "cwe-079", "commit_msg": "fixed reflected cross-site scripting issue related to invalid basket name", "parent_commit": "4fe1fdef9e05a3c0061c82e223dcccacfc2211ae", "description": "Write a Go test function to validate that creating a basket with an invalid name results in a bad request and no database entry."}
{"func_name": "_startSSL_pyOpenSSL", "func_src_before": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1 method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "func_src_after": "    def _startSSL_pyOpenSSL(self):\n        log.debug(\"_startSSL_pyOpenSSL called\")\n        tcpsock = self._owner\n        # NonBlockingHTTPBOSH instance has no attribute _owner\n        if hasattr(tcpsock, '_owner') and tcpsock._owner._caller.client_cert \\\n        and os.path.exists(tcpsock._owner._caller.client_cert):\n            conn = tcpsock._owner._caller\n            # FIXME make a checkbox for Client Cert / SSLv23 / TLSv1\n            # If we are going to use a client cert/key pair for authentication,\n            # we choose TLSv1* method.\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n            log.debug('Using client cert and key from %s' % conn.client_cert)\n            try:\n                p12 = OpenSSL.crypto.load_pkcs12(open(conn.client_cert).read(),\n                    conn.client_cert_passphrase)\n            except OpenSSL.crypto.Error as exception_obj:\n                log.warning('Unable to load client pkcs12 certificate from '\n                    'file %s: %s ... Is it a valid PKCS12 cert?' % \\\n                (conn.client_cert, exception_obj.args))\n            except:\n                log.warning('Unknown error while loading certificate from file '\n                    '%s' % conn.client_cert)\n            else:\n                log.info('PKCS12 Client cert loaded OK')\n                try:\n                    tcpsock._sslContext.use_certificate(p12.get_certificate())\n                    tcpsock._sslContext.use_privatekey(p12.get_privatekey())\n                    log.info('p12 cert and key loaded')\n                except OpenSSL.crypto.Error as exception_obj:\n                    log.warning('Unable to extract client certificate from '\n                        'file %s' % conn.client_cert)\n                except Exception as msg:\n                    log.warning('Unknown error extracting client certificate '\n                        'from file %s: %s' % (conn.client_cert, msg))\n                else:\n                    log.info('client cert and key loaded OK')\n        else:\n            # See http://docs.python.org/dev/library/ssl.html\n            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n            try:\n                flags |= OpenSSL.SSL.OP_NO_TICKET\n            except AttributeError as e:\n                # py-OpenSSL < 0.9 or old OpenSSL\n                flags |= 16384\n            tcpsock._sslContext.set_options(flags)\n\n        tcpsock.ssl_errnum = []\n        tcpsock._sslContext.set_verify(OpenSSL.SSL.VERIFY_PEER,\n            self._ssl_verify_callback)\n        tcpsock._sslContext.set_cipher_list('HIGH:!aNULL:!eNULL:RC4-SHA')\n        store = tcpsock._sslContext.get_cert_store()\n        self._load_cert_file(self.cacerts, store)\n        self._load_cert_file(self.mycerts, store)\n        if os.path.isdir('/etc/ssl/certs'):\n            for f in os.listdir('/etc/ssl/certs'):\n                # We don't logg because there is a lot a duplicated certs in this\n                # folder\n                self._load_cert_file(os.path.join('/etc/ssl/certs', f), store,\n                        logg=False)\n\n        tcpsock._sslObj = OpenSSL.SSL.Connection(tcpsock._sslContext,\n                tcpsock._sock)\n        tcpsock._sslObj.set_connect_state() # set to client mode\n        wrapper = PyOpenSSLWrapper(tcpsock._sslObj)\n        tcpsock._recv = wrapper.recv\n        tcpsock._send = wrapper.send\n\n        log.debug(\"Initiating handshake...\")\n        try:\n            tcpsock._sslObj.do_handshake()\n        except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantWriteError) as e:\n            pass\n        except:\n            log.error('Error while TLS handshake: ', exc_info=True)\n            return False\n        self._owner.ssl_lib = PYOPENSSL\n        return True", "line_changes": {"deleted": [{"line_no": 11, "char_start": 548, "char_end": 628, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)\n"}, {"line_no": 40, "char_start": 2190, "char_end": 2234, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2\n"}], "added": [{"line_no": 11, "char_start": 549, "char_end": 630, "line": "            tcpsock._sslContext = OpenSSL.SSL.Context(OpenSSL.SSL.SSLv23_METHOD)\n"}, {"line_no": 12, "char_start": 630, "char_end": 701, "line": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n"}, {"line_no": 13, "char_start": 701, "char_end": 749, "line": "                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n"}, {"line_no": 14, "char_start": 749, "char_end": 800, "line": "            tcpsock._sslContext.set_options(flags)\n"}, {"line_no": 43, "char_start": 2362, "char_end": 2437, "line": "            flags = OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_SINGLE_DH_USE\n"}]}, "char_changes": {"deleted": [{"char_start": 614, "char_end": 619, "chars": "TLSv1"}], "added": [{"char_start": 539, "char_end": 540, "chars": "*"}, {"char_start": 615, "char_end": 621, "chars": "SSLv23"}, {"char_start": 630, "char_end": 800, "chars": "            flags = (OpenSSL.SSL.OP_NO_SSLv2 | OpenSSL.SSL.OP_NO_SSLv3\n                | OpenSSL.SSL.OP_SINGLE_DH_USE)\n            tcpsock._sslContext.set_options(flags)\n"}, {"char_start": 2405, "char_end": 2436, "chars": " | OpenSSL.SSL.OP_SINGLE_DH_USE"}]}, "commit_link": "github.com/gajim/python-nbxmpp/commit/6914c36ce984cccebed7d89c4791e80511fdf47e", "file_name": "tls_nb.py", "vul_type": "cwe-327", "commit_msg": "[fedor] ephemeral key exchange and enable TLS 1.1 and TLS 1.2 when connecting using client cert authentification. Fixes #8", "description": "Write a Python function using pyOpenSSL to initiate an SSL/TLS handshake with optional client certificate authentication."}
{"func_name": "is_case_sensitive", "func_src_before": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = os.tempnam(self.base_folder, '.caseTest_')\n            os.mkdir(path)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "func_src_after": "    def is_case_sensitive(self):\n        if self._case_sensitive is None:\n            lock = self.unlock_path(self.base_folder, unlock_parent=False)\n            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n            if os.path.exists(path.upper()):\n                self._case_sensitive = False\n            else:\n                self._case_sensitive = True\n            os.rmdir(path)\n            self.lock_path(self.base_folder, lock)\n        return self._case_sensitive", "line_changes": {"deleted": [{"line_no": 4, "char_start": 149, "char_end": 211, "line": "            path = os.tempnam(self.base_folder, '.caseTest_')\n"}, {"line_no": 5, "char_start": 211, "char_end": 238, "line": "            os.mkdir(path)\n"}], "added": [{"line_no": 4, "char_start": 149, "char_end": 228, "line": "            path = tempfile.mkdtemp(prefix='.caseTest_', dir=self.base_folder)\n"}]}, "char_changes": {"deleted": [{"char_start": 168, "char_end": 171, "chars": "os."}, {"char_start": 175, "char_end": 236, "chars": "nam(self.base_folder, '.caseTest_')\n            os.mkdir(path"}], "added": [{"char_start": 172, "char_end": 226, "chars": "file.mkdtemp(prefix='.caseTest_', dir=self.base_folder"}]}, "commit_link": "github.com/arameshkumar/nuxeo-drive/commit/8349d04615fc0f02b4b7588f04a24382431e2d3a", "file_name": "local_client.py", "vul_type": "cwe-377", "commit_msg": "NXDRIVE-170: Fix encoding issue when testing for case sensitive OS (and don't use unsafe tempnam)", "description": "Write a Python function to determine if a filesystem is case-sensitive."}
{"func_name": "WavpackVerifySingleBlock", "func_src_before": "int WavpackVerifySingleBlock (unsigned char *buffer, int verify_checksum)\n{\n    WavpackHeader *wphdr = (WavpackHeader *) buffer;\n    uint32_t checksum_passed = 0, bcount, meta_bc;\n    unsigned char *dp, meta_id, c1, c2;\n\n    if (strncmp (wphdr->ckID, \"wvpk\", 4) || wphdr->ckSize + 8 < sizeof (WavpackHeader))\n        return FALSE;\n\n    bcount = wphdr->ckSize - sizeof (WavpackHeader) + 8;\n    dp = (unsigned char *)(wphdr + 1);\n\n    while (bcount >= 2) {\n        meta_id = *dp++;\n        c1 = *dp++;\n\n        meta_bc = c1 << 1;\n        bcount -= 2;\n\n        if (meta_id & ID_LARGE) {\n            if (bcount < 2)\n                return FALSE;\n\n            c1 = *dp++;\n            c2 = *dp++;\n            meta_bc += ((uint32_t) c1 << 9) + ((uint32_t) c2 << 17);\n            bcount -= 2;\n        }\n\n        if (bcount < meta_bc)\n            return FALSE;\n\n        if (verify_checksum && (meta_id & ID_UNIQUE) == ID_BLOCK_CHECKSUM) {\n#ifdef BITSTREAM_SHORTS\n            uint16_t *csptr = (uint16_t*) buffer;\n#else\n            unsigned char *csptr = buffer;\n#endif\n            int wcount = (int)(dp - 2 - buffer) >> 1;\n            uint32_t csum = (uint32_t) -1;\n\n            if ((meta_id & ID_ODD_SIZE) || meta_bc < 2 || meta_bc > 4)\n                return FALSE;\n\n#ifdef BITSTREAM_SHORTS\n            while (wcount--)\n                csum = (csum * 3) + *csptr++;\n#else\n            WavpackNativeToLittleEndian ((WavpackHeader *) buffer, WavpackHeaderFormat);\n\n            while (wcount--) {\n                csum = (csum * 3) + csptr [0] + (csptr [1] << 8);\n                csptr += 2;\n            }\n\n            WavpackLittleEndianToNative ((WavpackHeader *) buffer, WavpackHeaderFormat);\n#endif\n\n            if (meta_bc == 4) {\n                if (*dp++ != (csum & 0xff) || *dp++ != ((csum >> 8) & 0xff) || *dp++ != ((csum >> 16) & 0xff) || *dp++ != ((csum >> 24) & 0xff))\n                    return FALSE;\n            }\n            else {\n                csum ^= csum >> 16;\n\n                if (*dp++ != (csum & 0xff) || *dp++ != ((csum >> 8) & 0xff))\n                    return FALSE;\n            }\n\n            checksum_passed++;\n        }\n\n        bcount -= meta_bc;\n        dp += meta_bc;\n    }\n\n    return (bcount == 0) && (!verify_checksum || !(wphdr->flags & HAS_CHECKSUM) || checksum_passed);\n}", "func_src_after": "int WavpackVerifySingleBlock (unsigned char *buffer, int verify_checksum)\n{\n    WavpackHeader *wphdr = (WavpackHeader *) buffer;\n    uint32_t checksum_passed = 0, bcount, meta_bc;\n    unsigned char *dp, meta_id, c1, c2;\n\n    if (strncmp (wphdr->ckID, \"wvpk\", 4) || wphdr->ckSize + 8 < sizeof (WavpackHeader))\n        return FALSE;\n\n    bcount = wphdr->ckSize - sizeof (WavpackHeader) + 8;\n    dp = (unsigned char *)(wphdr + 1);\n\n    while (bcount >= 2) {\n        meta_id = *dp++;\n        c1 = *dp++;\n\n        meta_bc = c1 << 1;\n        bcount -= 2;\n\n        if (meta_id & ID_LARGE) {\n            if (bcount < 2)\n                return FALSE;\n\n            c1 = *dp++;\n            c2 = *dp++;\n            meta_bc += ((uint32_t) c1 << 9) + ((uint32_t) c2 << 17);\n            bcount -= 2;\n        }\n\n        if (bcount < meta_bc)\n            return FALSE;\n\n        if (verify_checksum && (meta_id & ID_UNIQUE) == ID_BLOCK_CHECKSUM) {\n#ifdef BITSTREAM_SHORTS\n            uint16_t *csptr = (uint16_t*) buffer;\n#else\n            unsigned char *csptr = buffer;\n#endif\n            int wcount = (int)(dp - 2 - buffer) >> 1;\n            uint32_t csum = (uint32_t) -1;\n\n            if ((meta_id & ID_ODD_SIZE) || meta_bc < 2 || meta_bc > 4)\n                return FALSE;\n\n#ifdef BITSTREAM_SHORTS\n            while (wcount--)\n                csum = (csum * 3) + *csptr++;\n#else\n            WavpackNativeToLittleEndian ((WavpackHeader *) buffer, WavpackHeaderFormat);\n\n            while (wcount--) {\n                csum = (csum * 3) + csptr [0] + (csptr [1] << 8);\n                csptr += 2;\n            }\n\n            WavpackLittleEndianToNative ((WavpackHeader *) buffer, WavpackHeaderFormat);\n#endif\n\n            if (meta_bc == 4) {\n                if (*dp != (csum & 0xff) || dp[1] != ((csum >> 8) & 0xff) || dp[2] != ((csum >> 16) & 0xff) || dp[3] != ((csum >> 24) & 0xff))\n                    return FALSE;\n            }\n            else {\n                csum ^= csum >> 16;\n\n                if (*dp != (csum & 0xff) || dp[1] != ((csum >> 8) & 0xff))\n                    return FALSE;\n            }\n\n            checksum_passed++;\n        }\n\n        bcount -= meta_bc;\n        dp += meta_bc;\n    }\n\n    return (bcount == 0) && (!verify_checksum || !(wphdr->flags & HAS_CHECKSUM) || checksum_passed);\n}", "commit_link": "github.com/dbry/WavPack/commit/bba5389dc598a92bdf2b297c3ea34620b6679b5b", "file_name": "src/open_utils.c", "vul_type": "cwe-125", "description": "In C, write a function to verify the integrity of a single Wavpack audio block, optionally checking its checksum."}
{"func_name": "PHP_FUNCTION", "func_src_before": "PHP_FUNCTION(imagegammacorrect)\n{\n\tzval *IM;\n\tgdImagePtr im;\n\tint i;\n\tdouble input, output;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rdd\", &IM, &input, &output) == FAILURE) {\n\t\treturn;\n\t}\n\n\tZEND_FETCH_RESOURCE(im, gdImagePtr, &IM, -1, \"Image\", le_gd);\n\n\tif (gdImageTrueColor(im))\t{\n\t\tint x, y, c;\n\n\t\tfor (y = 0; y < gdImageSY(im); y++)\t{\n\t\t\tfor (x = 0; x < gdImageSX(im); x++)\t{\n\t\t\t\tc = gdImageGetPixel(im, x, y);\n\t\t\t\tgdImageSetPixel(im, x, y,\n\t\t\t\t\tgdTrueColorAlpha(\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetRed(c)   / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetGreen(c) / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetBlue(c)  / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\tgdTrueColorGetAlpha(c)\n\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tRETURN_TRUE;\n\t}\n\n\tfor (i = 0; i < gdImageColorsTotal(im); i++) {\n\t\tim->red[i]   = (int)((pow((pow((im->red[i]   / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->green[i] = (int)((pow((pow((im->green[i] / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->blue[i]  = (int)((pow((pow((im->blue[i]  / 255.0), input)), 1.0 / output) * 255) + .5);\n\t}\n\n\tRETURN_TRUE;\n}", "func_src_after": "PHP_FUNCTION(imagegammacorrect)\n{\n\tzval *IM;\n\tgdImagePtr im;\n\tint i;\n\tdouble input, output;\n\n\tif (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"rdd\", &IM, &input, &output) == FAILURE) {\n\t\treturn;\n\t}\n\n\tif ( input <= 0.0 || output <= 0.0 ) {\n\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Gamma values should be positive\");\n\t\tRETURN_FALSE;\n\t}\n\n\tZEND_FETCH_RESOURCE(im, gdImagePtr, &IM, -1, \"Image\", le_gd);\n\n\tif (gdImageTrueColor(im))\t{\n\t\tint x, y, c;\n\n\t\tfor (y = 0; y < gdImageSY(im); y++)\t{\n\t\t\tfor (x = 0; x < gdImageSX(im); x++)\t{\n\t\t\t\tc = gdImageGetPixel(im, x, y);\n\t\t\t\tgdImageSetPixel(im, x, y,\n\t\t\t\t\tgdTrueColorAlpha(\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetRed(c)   / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetGreen(c) / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\t(int) ((pow((pow((gdTrueColorGetBlue(c)  / 255.0), input)), 1.0 / output) * 255) + .5),\n\t\t\t\t\t\tgdTrueColorGetAlpha(c)\n\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tRETURN_TRUE;\n\t}\n\n\tfor (i = 0; i < gdImageColorsTotal(im); i++) {\n\t\tim->red[i]   = (int)((pow((pow((im->red[i]   / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->green[i] = (int)((pow((pow((im->green[i] / 255.0), input)), 1.0 / output) * 255) + .5);\n\t\tim->blue[i]  = (int)((pow((pow((im->blue[i]  / 255.0), input)), 1.0 / output) * 255) + .5);\n\t}\n\n\tRETURN_TRUE;\n}", "commit_link": "github.com/php/php-src/commit/1bd103df00f49cf4d4ade2cfe3f456ac058a4eae", "file_name": "ext/gd/gd.c", "vul_type": "cwe-787", "description": "Write a PHP function to apply gamma correction to an image resource with given input and output gamma values."}
{"func_name": "encryptPassword", "func_src_before": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n    }", "func_src_after": "    encryptPassword: function(password) {\n        if (!password) return '';\n        return bcrypt.hashSync(password, 10);\n    }", "line_changes": {"deleted": [{"line_no": 3, "char_start": 76, "char_end": 160, "line": "        return crypto.createHmac('sha1', this.salt).update(password).digest('hex');\n"}], "added": [{"line_no": 3, "char_start": 76, "char_end": 122, "line": "        return bcrypt.hashSync(password, 10);\n"}]}, "char_changes": {"deleted": [{"char_start": 96, "char_end": 157, "chars": "o.createHmac('sha1', this.salt).update(password).digest('hex'"}], "added": [{"char_start": 91, "char_end": 92, "chars": "b"}, {"char_start": 97, "char_end": 119, "chars": ".hashSync(password, 10"}]}, "commit_link": "github.com/andela/temari-cfh/commit/e5e4de5f2cc14fcd86464c83b5d110c9e05f2eba", "file_name": "user.js", "vul_type": "cwe-916", "commit_msg": "Improved user password encryption to use bcrypt instead of SHA1.", "parent_commit": "d56dd3474970c1f8b1dbf3599a451c3d2609c13d", "description": "Create a JavaScript function named `encryptPassword` that takes a password string and returns its hashed version."}
{"func_name": "generate_fZ", "func_src_before": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "func_src_after": "    def generate_fZ(self, Obs, TL, currentTimeAbs, mode, hashname):\r\n        \"\"\"Calculates fZ values for all stars over an entire orbit of the sun\r\n        Args:\r\n            Obs (module):\r\n                Observatory module\r\n            TL (module):\r\n                Target List Module\r\n            currentTimeAbs (astropy Time array):\r\n                current absolute time im MJD\r\n            mode (dict):\r\n                Selected observing mode\r\n            hashname (string):\r\n                hashname describing the files specific to the current json script\r\n        Updates Attributes:\r\n            fZ_startSaved[1000, TL.nStars] (astropy Quantity array):\r\n                Surface brightness of zodiacal light in units of 1/arcsec2 for each star over 1 year at discrete points defined by resolution\r\n        \"\"\"\r\n        #Generate cache Name########################################################################\r\n        cachefname = hashname+'starkfZ'\r\n\r\n        #Check if file exists#######################################################################\r\n        if os.path.isfile(cachefname):#check if file exists\r\n            self.vprint(\"Loading cached fZ from %s\"%cachefname)\r\n            with open(cachefname, 'rb') as f:#load from cache\r\n                tmpfZ = pickle.load(f)\r\n            return tmpfZ\r\n\r\n        #IF the Completeness vs dMag for Each Star File Does Not Exist, Calculate It\r\n        else:\r\n            self.vprint(\"Calculating fZ\")\r\n            #OS = self.OpticalSystem#Testing to be sure I can remove this\r\n            #WA = OS.WA0#Testing to be sure I can remove this\r\n            sInds= np.arange(TL.nStars)\r\n            startTime = np.zeros(sInds.shape[0])*u.d + currentTimeAbs#Array of current times\r\n            resolution = [j for j in range(1000)]\r\n            fZ = np.zeros([sInds.shape[0], len(resolution)])\r\n            dt = 365.25/len(resolution)*u.d\r\n            for i in xrange(len(resolution)):#iterate through all times of year\r\n                time = startTime + dt*resolution[i]\r\n                fZ[:,i] = self.fZ(Obs, TL, sInds, time, mode)\r\n            \r\n            with open(cachefname, \"wb\") as fo:\r\n                pickle.dump(fZ,fo)\r\n                self.vprint(\"Saved cached 1st year fZ to %s\"%cachefname)\r\n            return fZ", "line_changes": {"deleted": [{"line_no": 25, "char_start": 1257, "char_end": 1296, "line": "                print(pickle.load(f))\r\n"}, {"line_no": 27, "char_start": 1336, "char_end": 1358, "line": "                try:\r\n"}, {"line_no": 28, "char_start": 1358, "char_end": 1389, "line": "                    f.close()\r\n"}, {"line_no": 29, "char_start": 1389, "char_end": 1414, "line": "                except:\r\n"}, {"line_no": 30, "char_start": 1414, "char_end": 1440, "line": "                    pass\r\n"}, {"line_no": 48, "char_start": 2302, "char_end": 2362, "line": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": []}, "char_changes": {"deleted": [{"char_start": 1273, "char_end": 1438, "chars": "print(pickle.load(f))\r\n                tmpfZ = pickle.load(f)\r\n                try:\r\n                    f.close()\r\n                except:\r\n                    pass"}, {"char_start": 2302, "char_end": 2362, "chars": "                wr = csv.writer(fo, quoting=csv.QUOTE_ALL)\r\n"}], "added": [{"char_start": 1273, "char_end": 1295, "chars": "tmpfZ = pickle.load(f)"}]}, "commit_link": "github.com/dsavransky/EXOSIMS/commit/2df12d23c54a140161c24e92b3c03aaf522c61ec", "file_name": "ZodiacalLight.py", "vul_type": "cwe-502", "commit_msg": "fixed pickle load errors", "parent_commit": "c4660a0de665797559fd4d048b4d971661366f50", "description": "In Python, write a function to calculate and cache surface brightness values for stars, loading from cache if available."}
{"func_name": "search", "func_src_before": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "func_src_after": "  def search\n    escaped = params[:name].gsub('\\\\', '\\\\\\\\\\\\\\\\').gsub('%', '\\%').gsub('_', '\\_')\n    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n    if @searched.empty?\n      @error = \"\u691c\u7d22\u30ef\u30fc\u30c9\u304c\u30d2\u30c3\u30c8\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3082\u3046\u4e00\u5ea6\u5165\u308c\u306a\u304a\u3057\u3066\u4e0b\u3055\u3044\u3002\"\n    end\n  end", "line_changes": {"deleted": [{"line_no": 3, "char_start": 96, "char_end": 203, "line": "    @searched = Restaurant.where(\"name like '%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'\")\n"}], "added": [{"line_no": 3, "char_start": 96, "char_end": 195, "line": "    @searched = Restaurant.where(\"name like ? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%\")\n"}]}, "char_changes": {"deleted": [{"char_start": 140, "char_end": 200, "chars": "'%\" + escaped + \"%'\" + \"or hurigana like '%\" + escaped + \"%'"}], "added": [{"char_start": 140, "char_end": 192, "chars": "? or hurigana like ?\", \"%#{escaped}%\", \"%#{escaped}%"}]}, "commit_link": "github.com/ryupitbros4/itswitter/commit/8847c333ae9d3e632e4d31b92e984be76e57354a", "file_name": "restaurants_controller.rb", "vul_type": "cwe-089", "commit_msg": "Prevent SQL injection.", "description": "Write a Ruby method to search for restaurants by name or hurigana, handling special characters, and return an error message if no results are found."}
{"func_name": "pathRoleWrite", "func_src_before": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 1024 and it can also be 2048 and 4096.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "func_src_after": "func (b *backend) pathRoleWrite(ctx context.Context, req *logical.Request, d *framework.FieldData) (*logical.Response, error) {\n\troleName := d.Get(\"role\").(string)\n\tif roleName == \"\" {\n\t\treturn logical.ErrorResponse(\"missing role name\"), nil\n\t}\n\n\t// Allowed users is an optional field, applicable for both OTP and Dynamic types.\n\tallowedUsers := d.Get(\"allowed_users\").(string)\n\n\t// Validate the CIDR blocks\n\tcidrList := d.Get(\"cidr_list\").(string)\n\tif cidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(cidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate cidr_list: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(\"failed to validate cidr_list\"), nil\n\t\t}\n\t}\n\n\t// Validate the excluded CIDR blocks\n\texcludeCidrList := d.Get(\"exclude_cidr_list\").(string)\n\tif excludeCidrList != \"\" {\n\t\tvalid, err := cidrutil.ValidateCIDRListString(excludeCidrList, \",\")\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate exclude_cidr_list entry: %w\", err)\n\t\t}\n\t\tif !valid {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"failed to validate exclude_cidr_list entry: %v\", err)), nil\n\t\t}\n\t}\n\n\tport := d.Get(\"port\").(int)\n\tif port == 0 {\n\t\tport = 22\n\t}\n\n\tkeyType := d.Get(\"key_type\").(string)\n\tif keyType == \"\" {\n\t\treturn logical.ErrorResponse(\"missing key type\"), nil\n\t}\n\tkeyType = strings.ToLower(keyType)\n\n\tvar roleEntry sshRole\n\tif keyType == KeyTypeOTP {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\n\t\t// Admin user is not used if OTP key type is used because there is\n\t\t// no need to login to remote machine.\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser != \"\" {\n\t\t\treturn logical.ErrorResponse(\"admin user not required for OTP type\"), nil\n\t\t}\n\n\t\t// Below are the only fields used from the role structure for OTP type.\n\t\troleEntry = sshRole{\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tKeyType:         KeyTypeOTP,\n\t\t\tPort:            port,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t}\n\t} else if keyType == KeyTypeDynamic {\n\t\tdefaultUser := d.Get(\"default_user\").(string)\n\t\tif defaultUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing default user\"), nil\n\t\t}\n\t\t// Key name is required by dynamic type and not by OTP type.\n\t\tkeyName := d.Get(\"key\").(string)\n\t\tif keyName == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing key name\"), nil\n\t\t}\n\t\tkeyEntry, err := req.Storage.Get(ctx, fmt.Sprintf(\"keys/%s\", keyName))\n\t\tif err != nil || keyEntry == nil {\n\t\t\treturn logical.ErrorResponse(fmt.Sprintf(\"invalid 'key': %q\", keyName)), nil\n\t\t}\n\n\t\tinstallScript := d.Get(\"install_script\").(string)\n\t\tkeyOptionSpecs := d.Get(\"key_option_specs\").(string)\n\n\t\t// Setting the default script here. The script will install the\n\t\t// generated public key in the authorized_keys file of linux host.\n\t\tif installScript == \"\" {\n\t\t\tinstallScript = DefaultPublicKeyInstallScript\n\t\t}\n\n\t\tadminUser := d.Get(\"admin_user\").(string)\n\t\tif adminUser == \"\" {\n\t\t\treturn logical.ErrorResponse(\"missing admin username\"), nil\n\t\t}\n\n\t\t// This defaults to 2048, but it can also be 1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys.\n\t\tkeyBits := d.Get(\"key_bits\").(int)\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n\t\t}\n\n\t\t// Store all the fields required by dynamic key type\n\t\troleEntry = sshRole{\n\t\t\tKeyName:         keyName,\n\t\t\tAdminUser:       adminUser,\n\t\t\tDefaultUser:     defaultUser,\n\t\t\tCIDRList:        cidrList,\n\t\t\tExcludeCIDRList: excludeCidrList,\n\t\t\tPort:            port,\n\t\t\tKeyType:         KeyTypeDynamic,\n\t\t\tKeyBits:         keyBits,\n\t\t\tInstallScript:   installScript,\n\t\t\tAllowedUsers:    allowedUsers,\n\t\t\tKeyOptionSpecs:  keyOptionSpecs,\n\t\t}\n\t} else if keyType == KeyTypeCA {\n\t\talgorithmSigner := \"\"\n\t\talgorithmSignerRaw, ok := d.GetOk(\"algorithm_signer\")\n\t\tif ok {\n\t\t\talgorithmSigner = algorithmSignerRaw.(string)\n\t\t\tswitch algorithmSigner {\n\t\t\tcase ssh.SigAlgoRSA, ssh.SigAlgoRSASHA2256, ssh.SigAlgoRSASHA2512:\n\t\t\tcase \"\":\n\t\t\t\t// This case is valid, and the sign operation will use the signer's\n\t\t\t\t// default algorithm.\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unknown algorithm signer %q\", algorithmSigner)\n\t\t\t}\n\t\t}\n\n\t\trole, errorResponse := b.createCARole(allowedUsers, d.Get(\"default_user\").(string), algorithmSigner, d)\n\t\tif errorResponse != nil {\n\t\t\treturn errorResponse, nil\n\t\t}\n\t\troleEntry = *role\n\t} else {\n\t\treturn logical.ErrorResponse(\"invalid key type\"), nil\n\t}\n\n\tentry, err := logical.StorageEntryJSON(fmt.Sprintf(\"roles/%s\", roleName), roleEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := req.Storage.Put(ctx, entry); err != nil {\n\t\treturn nil, err\n\t}\n\treturn nil, nil\n}", "line_changes": {"deleted": [{"line_no": 99, "char_start": 3202, "char_end": 3279, "line": "\t\tif keyBits != 0 && keyBits != 1024 && keyBits != 2048 && keyBits != 4096 {\n"}, {"line_no": 100, "char_start": 3279, "char_end": 3342, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 101, "char_start": 3342, "char_end": 3346, "line": "\t\t}\n"}, {"line_no": 102, "char_start": 3346, "char_end": 3347, "line": "\n"}], "added": [{"line_no": 103, "char_start": 3320, "char_end": 3419, "line": "\t\tif keyBits != 1024 && keyBits != 2048 && keyBits != 3072 && keyBits != 4096 && keyBits != 8192 {\n"}, {"line_no": 104, "char_start": 3419, "char_end": 3482, "line": "\t\t\treturn logical.ErrorResponse(\"invalid key_bits field\"), nil\n"}, {"line_no": 105, "char_start": 3482, "char_end": 3486, "line": "\t\t}\n"}]}, "char_changes": {"deleted": [{"char_start": 3126, "char_end": 3134, "chars": "1024 and"}, {"char_start": 3150, "char_end": 3163, "chars": "2048 and 4096"}, {"char_start": 3215, "char_end": 3216, "chars": "!"}, {"char_start": 3220, "char_end": 3222, "chars": "&&"}, {"char_start": 3272, "char_end": 3276, "chars": "4096"}, {"char_start": 3347, "char_end": 3446, "chars": "\t\t// If user has not set this field, default it to 2048\n\t\tif keyBits == 0 {\n\t\t\tkeyBits = 2048\n\t\t}\n\n"}], "added": [{"char_start": 3126, "char_end": 3135, "chars": "2048, but"}, {"char_start": 3151, "char_end": 3239, "chars": "1024, 3072, 4096, or 8192.\n\t\t// In the near future, we should disallow 1024-bit SSH keys"}, {"char_start": 3291, "char_end": 3292, "chars": "="}, {"char_start": 3296, "char_end": 3324, "chars": "{\n\t\t\tkeyBits = 2048\n\t\t}\n\t\tif"}, {"char_start": 3374, "char_end": 3416, "chars": "3072 && keyBits != 4096 && keyBits != 8192"}]}, "commit_link": "github.com/hashicorp/vault/commit/8833875b1071fcb8c2f16d82dc1a5919ff6534eb", "file_name": "path_roles.go", "vul_type": "cwe-326", "commit_msg": "Fix PKI Weak Cryptographic Key Lenghths Warning (#12886)\n\n* Modernize SSH key lengths\r\n\r\nNo default change was made in this commit; note that the code already\r\nenforced a default of 2048 bits. ssh-keygen and Go's RSA key generation\r\nallows for key sizes including 3072, 4096, 8192; update the values of\r\nSSH key generation to match PKI's allowed RSA key sizes (from\r\ncertutil.ValidateKeyTypeLength(...)). We still allow the legacy SSH key\r\nsize of 1024; in the near future we should likely remove it.\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>\r\n\r\n* Ensure minimum of 2048-bit PKI RSA keys\r\n\r\nWhile the stated path is a false-positive, verifying all paths is\r\nnon-trivial. We largely validate API call lengths using\r\ncertutil.ValidateKeyTypeLength(...), but ensuring no other path calls\r\ncertutil.generatePrivateKey(...) --- directly or indirectly --- is\r\nnon-trivial. Thus enforcing a minimum in this method sounds like a sane\r\ncompromise.\r\n\r\nResolves: https://github.com/hashicorp/vault/security/code-scanning/55\r\n\r\nSigned-off-by: Alexander Scheel <alex.scheel@hashicorp.com>", "parent_commit": "14101f866414d2ed7850648b465c746ac8fda621", "description": "Write a Go function to handle creating or updating SSH role configurations based on provided fields."}
{"func_name": "mpeg4_decode_studio_block", "func_src_before": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "func_src_after": "static int mpeg4_decode_studio_block(MpegEncContext *s, int32_t block[64], int n)\n{\n    Mpeg4DecContext *ctx = s->avctx->priv_data;\n\n    int cc, dct_dc_size, dct_diff, code, j, idx = 1, group = 0, run = 0,\n        additional_code_len, sign, mismatch;\n    VLC *cur_vlc = &ctx->studio_intra_tab[0];\n    uint8_t *const scantable = s->intra_scantable.permutated;\n    const uint16_t *quant_matrix;\n    uint32_t flc;\n    const int min = -1 *  (1 << (s->avctx->bits_per_raw_sample + 6));\n    const int max =      ((1 << (s->avctx->bits_per_raw_sample + 6)) - 1);\n\n    mismatch = 1;\n\n    memset(block, 0, 64 * sizeof(int32_t));\n\n    if (n < 4) {\n        cc = 0;\n        dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->intra_matrix;\n    } else {\n        cc = (n & 1) + 1;\n        if (ctx->rgb)\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_luma_dc.table, STUDIO_INTRA_BITS, 2);\n        else\n            dct_dc_size = get_vlc2(&s->gb, ctx->studio_chroma_dc.table, STUDIO_INTRA_BITS, 2);\n        quant_matrix = s->chroma_intra_matrix;\n    }\n\n    if (dct_dc_size < 0) {\n        av_log(s->avctx, AV_LOG_ERROR, \"illegal dct_dc_size vlc\\n\");\n        return AVERROR_INVALIDDATA;\n    } else if (dct_dc_size == 0) {\n        dct_diff = 0;\n    } else {\n        dct_diff = get_xbits(&s->gb, dct_dc_size);\n\n        if (dct_dc_size > 8) {\n            if(!check_marker(s->avctx, &s->gb, \"dct_dc_size > 8\"))\n                return AVERROR_INVALIDDATA;\n        }\n\n    }\n\n    s->last_dc[cc] += dct_diff;\n\n    if (s->mpeg_quant)\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision);\n    else\n        block[0] = s->last_dc[cc] * (8 >> s->intra_dc_precision) * (8 >> s->dct_precision);\n    /* TODO: support mpeg_quant for AC coefficients */\n\n    block[0] = av_clip(block[0], min, max);\n    mismatch ^= block[0];\n\n    /* AC Coefficients */\n    while (1) {\n        group = get_vlc2(&s->gb, cur_vlc->table, STUDIO_INTRA_BITS, 2);\n\n        if (group < 0) {\n            av_log(s->avctx, AV_LOG_ERROR, \"illegal ac coefficient group vlc\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        additional_code_len = ac_state_tab[group][0];\n        cur_vlc = &ctx->studio_intra_tab[ac_state_tab[group][1]];\n\n        if (group == 0) {\n            /* End of Block */\n            break;\n        } else if (group >= 1 && group <= 6) {\n            /* Zero run length (Table B.47) */\n            run = 1 << additional_code_len;\n            if (additional_code_len)\n                run += get_bits(&s->gb, additional_code_len);\n            idx += run;\n            continue;\n        } else if (group >= 7 && group <= 12) {\n            /* Zero run length and +/-1 level (Table B.48) */\n            code = get_bits(&s->gb, additional_code_len);\n            sign = code & 1;\n            code >>= 1;\n            run = (1 << (additional_code_len - 1)) + code;\n            idx += run;\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = sign ? 1 : -1;\n        } else if (group >= 13 && group <= 20) {\n            /* Level value (Table B.49) */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            block[j] = get_xbits(&s->gb, additional_code_len);\n        } else if (group == 21) {\n            /* Escape */\n            if (idx > 63)\n                return AVERROR_INVALIDDATA;\n            j = scantable[idx++];\n            additional_code_len = s->avctx->bits_per_raw_sample + s->dct_precision + 4;\n            flc = get_bits(&s->gb, additional_code_len);\n            if (flc >> (additional_code_len-1))\n                block[j] = -1 * (( flc ^ ((1 << additional_code_len) -1)) + 1);\n            else\n                block[j] = flc;\n        }\n        block[j] = ((8 * 2 * block[j] * quant_matrix[j] * s->qscale) >> s->dct_precision) / 32;\n        block[j] = av_clip(block[j], min, max);\n        mismatch ^= block[j];\n    }\n\n    block[63] ^= mismatch & 1;\n\n    return 0;\n}", "commit_link": "github.com/FFmpeg/FFmpeg/commit/d227ed5d598340e719eff7156b1aa0a4469e9a6a", "file_name": "libavcodec/mpeg4videodec.c", "vul_type": "cwe-125", "description": "Write a C function named `mpeg4_decode_studio_block` that decodes a single block of MPEG4 studio profile video."}
{"func_name": "_inject_admin_password_into_fs", "func_src_before": "def _inject_admin_password_into_fs(admin_passwd, fs, execute=None):\n    \"\"\"Set the root password to admin_passwd\n\n    admin_password is a root password\n    fs is the path to the base of the filesystem into which to inject\n    the key.\n\n    This method modifies the instance filesystem directly,\n    and does not require a guest agent running in the instance.\n\n    \"\"\"\n    # The approach used here is to copy the password and shadow\n    # files from the instance filesystem to local files, make any\n    # necessary changes, and then copy them back.\n\n    admin_user = 'root'\n\n    fd, tmp_passwd = tempfile.mkstemp()\n    os.close(fd)\n    fd, tmp_shadow = tempfile.mkstemp()\n    os.close(fd)\n\n    utils.execute('cp', os.path.join(fs, 'etc', 'passwd'), tmp_passwd,\n                  run_as_root=True)\n    utils.execute('cp', os.path.join(fs, 'etc', 'shadow'), tmp_shadow,\n                  run_as_root=True)\n    _set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)\n    utils.execute('cp', tmp_passwd, os.path.join(fs, 'etc', 'passwd'),\n                  run_as_root=True)\n    os.unlink(tmp_passwd)\n    utils.execute('cp', tmp_shadow, os.path.join(fs, 'etc', 'shadow'),\n                  run_as_root=True)\n    os.unlink(tmp_shadow)", "func_src_after": "def _inject_admin_password_into_fs(admin_passwd, fs, execute=None):\n    \"\"\"Set the root password to admin_passwd\n\n    admin_password is a root password\n    fs is the path to the base of the filesystem into which to inject\n    the key.\n\n    This method modifies the instance filesystem directly,\n    and does not require a guest agent running in the instance.\n\n    \"\"\"\n    # The approach used here is to copy the password and shadow\n    # files from the instance filesystem to local files, make any\n    # necessary changes, and then copy them back.\n\n    admin_user = 'root'\n\n    fd, tmp_passwd = tempfile.mkstemp()\n    os.close(fd)\n    fd, tmp_shadow = tempfile.mkstemp()\n    os.close(fd)\n\n    passwd_path = _join_and_check_path_within_fs(fs, 'etc', 'passwd')\n    shadow_path = _join_and_check_path_within_fs(fs, 'etc', 'shadow')\n\n    utils.execute('cp', passwd_path, tmp_passwd, run_as_root=True)\n    utils.execute('cp', shadow_path, tmp_shadow, run_as_root=True)\n    _set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)\n    utils.execute('cp', tmp_passwd, passwd_path, run_as_root=True)\n    os.unlink(tmp_passwd)\n    utils.execute('cp', tmp_shadow, shadow_path, run_as_root=True)\n    os.unlink(tmp_shadow)", "commit_link": "github.com/openstack/nova/commit/2427d4a99bed35baefd8f17ba422cb7aae8dcca7", "file_name": "nova/virt/disk/api.py", "vul_type": "cwe-022", "description": "Provide a Python function to set the root password on a filesystem without using a guest agent."}
{"func_name": "fetch", "func_src_before": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  os.system('shasum -c %s.sha256' % base)\n  os.system('shasum -c %s.asc.sha256' % base)\n  os.system('gpg --verify %s.asc %s' % (base, base))\n  os.system('keybase verify %s.asc' % base)", "func_src_after": "def fetch(url):\n  '''Download and verify a package url.'''\n  base = os.path.basename(url)\n  print('Fetching %s...' % base)\n  fetch_file(url + '.asc')\n  fetch_file(url)\n  fetch_file(url + '.sha256')\n  fetch_file(url + '.asc.sha256')\n  print('Verifying %s...' % base)\n  # TODO: check for verification failure.\n  subprocess.check_call(['shasum', '-c', base + '.sha256'])\n  subprocess.check_call(['shasum', '-c', base + '.asc.sha256'])\n  subprocess.check_call(['gpg', '--verify', base + '.asc', base])\n  subprocess.check_call(['keybase', 'verify', base + '.asc'])", "commit_link": "github.com/rillian/rust-build/commit/b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb", "file_name": "repack_rust.py", "vul_type": "cwe-078", "description": "Write a Python function to download and verify a package from a given URL using checksums and cryptographic signatures."}
{"func_name": "disk_seqf_stop", "func_src_before": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t}\n}", "func_src_after": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t\tseqf->private = NULL;\n\t}\n}", "commit_link": "github.com/torvalds/linux/commit/77da160530dd1dc94f6ae15a981f24e5f0021e84", "file_name": "block/genhd.c", "vul_type": "cwe-416", "description": "Write a C function named `disk_seqf_stop` that cleans up an iterator for a sequence file, ensuring memory is freed and the iterator is reset if necessary."}
{"func_name": "process_packet_tail", "func_src_before": "void process_packet_tail(struct msg_digest *md)\n{\n\tstruct state *st = md->st;\n\tenum state_kind from_state = md->v1_from_state;\n\tconst struct state_v1_microcode *smc = md->smc;\n\tbool new_iv_set = md->new_iv_set;\n\tbool self_delete = FALSE;\n\n\tif (md->hdr.isa_flags & ISAKMP_FLAGS_v1_ENCRYPTION) {\n\t\tendpoint_buf b;\n\t\tdbg(\"received encrypted packet from %s\", str_endpoint(&md->sender, &b));\n\n\t\tif (st == NULL) {\n\t\t\tlibreswan_log(\n\t\t\t\t\"discarding encrypted message for an unknown ISAKMP SA\");\n\t\t\treturn;\n\t\t}\n\t\tif (st->st_skeyid_e_nss == NULL) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\"discarding encrypted message because we haven't yet negotiated keying material\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* Mark as encrypted */\n\t\tmd->encrypted = TRUE;\n\n\t\t/* do the specified decryption\n\t\t *\n\t\t * IV is from st->st_iv or (if new_iv_set) st->st_new_iv.\n\t\t * The new IV is placed in st->st_new_iv\n\t\t *\n\t\t * See RFC 2409 \"IKE\" Appendix B\n\t\t *\n\t\t * XXX The IV should only be updated really if the packet\n\t\t * is successfully processed.\n\t\t * We should keep this value, check for a success return\n\t\t * value from the parsing routines and then replace.\n\t\t *\n\t\t * Each post phase 1 exchange generates IVs from\n\t\t * the last phase 1 block, not the last block sent.\n\t\t */\n\t\tconst struct encrypt_desc *e = st->st_oakley.ta_encrypt;\n\n\t\tif (pbs_left(&md->message_pbs) % e->enc_blocksize != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS, \"malformed message: not a multiple of encryption blocksize\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* XXX Detect weak keys */\n\n\t\t/* grab a copy of raw packet (for duplicate packet detection) */\n\t\tmd->raw_packet = clone_bytes_as_chunk(md->packet_pbs.start,\n\t\t\t\t\t\t      pbs_room(&md->packet_pbs),\n\t\t\t\t\t\t      \"raw packet\");\n\n\t\t/* Decrypt everything after header */\n\t\tif (!new_iv_set) {\n\t\t\tif (st->st_v1_iv.len == 0) {\n\t\t\t\tinit_phase2_iv(st, &md->hdr.isa_msgid);\n\t\t\t} else {\n\t\t\t\t/* use old IV */\n\t\t\t\trestore_new_iv(st, st->st_v1_iv);\n\t\t\t}\n\t\t}\n\n\t\tpassert(st->st_v1_new_iv.len >= e->enc_blocksize);\n\t\tst->st_v1_new_iv.len = e->enc_blocksize;   /* truncate */\n\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_log(\"decrypting %u bytes using algorithm %s\",\n\t\t\t\t(unsigned) pbs_left(&md->message_pbs),\n\t\t\t\tst->st_oakley.ta_encrypt->common.fqn);\n\t\t\tDBG_dump_hunk(\"IV before:\", st->st_v1_new_iv);\n\t\t}\n\t\te->encrypt_ops->do_crypt(e, md->message_pbs.cur,\n\t\t\t\t\t pbs_left(&md->message_pbs),\n\t\t\t\t\t st->st_enc_key_nss,\n\t\t\t\t\t st->st_v1_new_iv.ptr, FALSE);\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_dump_hunk(\"IV after:\", st->st_v1_new_iv);\n\t\t\tDBG_log(\"decrypted payload (starts at offset %td):\",\n\t\t\t\tmd->message_pbs.cur - md->message_pbs.roof);\n\t\t\tDBG_dump(NULL, md->message_pbs.start,\n\t\t\t\t md->message_pbs.roof - md->message_pbs.start);\n\t\t}\n\t} else {\n\t\t/* packet was not encryped -- should it have been? */\n\n\t\tif (smc->flags & SMF_INPUT_ENCRYPTED) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"packet rejected: should have been encrypted\");\n\t\t\tSEND_NOTIFICATION(INVALID_FLAGS);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Digest the message.\n\t * Padding must be removed to make hashing work.\n\t * Padding comes from encryption (so this code must be after decryption).\n\t * Padding rules are described before the definition of\n\t * struct isakmp_hdr in packet.h.\n\t */\n\t{\n\t\tenum next_payload_types_ikev1 np = md->hdr.isa_np;\n\t\tlset_t needed = smc->req_payloads;\n\t\tconst char *excuse =\n\t\t\tLIN(SMF_PSK_AUTH | SMF_FIRST_ENCRYPTED_INPUT,\n\t\t\t    smc->flags) ?\n\t\t\t\"probable authentication failure (mismatch of preshared secrets?): \"\n\t\t\t:\n\t\t\t\"\";\n\n\t\twhile (np != ISAKMP_NEXT_NONE) {\n\t\t\tstruct_desc *sd = v1_payload_desc(np);\n\n\t\t\tif (md->digest_roof >= elemsof(md->digest)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"more than %zu payloads in message; ignored\",\n\t\t\t\t       elemsof(md->digest));\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tstruct payload_digest *const pd = md->digest + md->digest_roof;\n\n\t\t\t/*\n\t\t\t * only do this in main mode. In aggressive mode, there\n\t\t\t * is no negotiation of NAT-T method. Get it right.\n\t\t\t */\n\t\t\tif (st != NULL && st->st_connection != NULL &&\n\t\t\t    (st->st_connection->policy & POLICY_AGGRESSIVE) == LEMPTY)\n\t\t\t{\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_NATD_RFC:\n\t\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t\tif ((st->hidden_variables.st_nat_traversal & NAT_T_WITH_RFC_VALUES) == LEMPTY) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * don't accept NAT-D/NAT-OA reloc directly in message,\n\t\t\t\t\t\t * unless we're using NAT-T RFC\n\t\t\t\t\t\t */\n\t\t\t\t\t\tDBG(DBG_NATT,\n\t\t\t\t\t\t    DBG_log(\"st_nat_traversal was: %s\",\n\t\t\t\t\t\t\t    bitnamesof(natt_bit_names,\n\t\t\t\t\t\t\t\t       st->hidden_variables.st_nat_traversal)));\n\t\t\t\t\t\tsd = NULL;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sd == NULL) {\n\t\t\t\t/* payload type is out of range or requires special handling */\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\t\t\t/* ??? two kinds of ID payloads */\n\t\t\t\t\tsd = (IS_PHASE1(from_state) ||\n\t\t\t\t\t      IS_PHASE15(from_state)) ?\n\t\t\t\t\t\t&isakmp_identification_desc :\n\t\t\t\t\t\t&isakmp_ipsec_identification_desc;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATD_DRAFTS: /* out of range */\n\t\t\t\t\t/*\n\t\t\t\t\t * ISAKMP_NEXT_NATD_DRAFTS was a private use type before RFC-3947.\n\t\t\t\t\t * Since it has the same format as ISAKMP_NEXT_NATD_RFC,\n\t\t\t\t\t * just rewrite np and sd, and carry on.\n\t\t\t\t\t */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATD_RFC;\n\t\t\t\t\tsd = &isakmp_nat_d_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATOA_DRAFTS: /* out of range */\n\t\t\t\t\t/* NAT-OA was a private use type before RFC-3947 -- same format */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATOA_RFC;\n\t\t\t\t\tsd = &isakmp_nat_oa_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_SAK: /* or ISAKMP_NEXT_NATD_BADDRAFTS */\n\t\t\t\t\t/*\n\t\t\t\t\t * Official standards say that this is ISAKMP_NEXT_SAK,\n\t\t\t\t\t * a part of Group DOI, something we don't implement.\n\t\t\t\t\t * Old non-updated Cisco gear abused this number in ancient NAT drafts.\n\t\t\t\t\t * We ignore (rather than reject) this in support of people\n\t\t\t\t\t * with crufty Cisco machines.\n\t\t\t\t\t */\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage with unsupported payload ISAKMP_NEXT_SAK (or ISAKMP_NEXT_NATD_BADDRAFTS) ignored\",\n\t\t\t\t\t\texcuse);\n\t\t\t\t\t/*\n\t\t\t\t\t * Hack to discard payload, whatever it was.\n\t\t\t\t\t * Since we are skipping the rest of the loop\n\t\t\t\t\t * body we must do some things ourself:\n\t\t\t\t\t * - demarshall the payload\n\t\t\t\t\t * - grab the next payload number (np)\n\t\t\t\t\t * - don't keep payload (don't increment pd)\n\t\t\t\t\t * - skip rest of loop body\n\t\t\t\t\t */\n\t\t\t\t\tif (!in_struct(&pd->payload, &isakmp_ignore_desc, &md->message_pbs,\n\t\t\t\t\t\t       &pd->pbs)) {\n\t\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t\t\t       excuse);\n\t\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\t\t\t/* NOTE: we do not increment pd! */\n\t\t\t\t\tcontinue;  /* skip rest of the loop */\n\n\t\t\t\tdefault:\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains an unknown or unexpected payload type (%s) at the outermost level\",\n\t\t\t\t\t       excuse,\n\t\t\t\t\t       enum_show(&ikev1_payload_names, np));\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tpassert(sd != NULL);\n\t\t\t}\n\n\t\t\tpassert(np < LELEM_ROOF);\n\n\t\t\t{\n\t\t\t\tlset_t s = LELEM(np);\n\n\t\t\t\tif (LDISJOINT(s,\n\t\t\t\t\t      needed | smc->opt_payloads |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_VID) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_N) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_D) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CR) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CERT))) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains a payload type (%s) unexpected by state %s\",\n\t\t\t\t\t\texcuse,\n\t\t\t\t\t\tenum_show(&ikev1_payload_names, np),\n\t\t\t\t\t\tst->st_state->name);\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_log(\"got payload 0x%\" PRIxLSET\"  (%s) needed: 0x%\" PRIxLSET \" opt: 0x%\" PRIxLSET,\n\t\t\t\t\t    s, enum_show(&ikev1_payload_names, np),\n\t\t\t\t\t    needed, smc->opt_payloads));\n\t\t\t\tneeded &= ~s;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Read in the payload recording what type it\n\t\t\t * should be\n\t\t\t */\n\t\t\tpd->payload_type = np;\n\t\t\tif (!in_struct(&pd->payload, sd, &md->message_pbs,\n\t\t\t\t       &pd->pbs)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t       excuse);\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t/* do payload-type specific debugging */\n\t\t\tswitch (np) {\n\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t/* dump ID section */\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_dump(\"     obj: \", pd->pbs.cur,\n\t\t\t\t\t     pbs_left(&pd->pbs)));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\n\t\t\t/*\n\t\t\t * Place payload at the end of the chain for this type.\n\t\t\t * This code appears in ikev1.c and ikev2.c.\n\t\t\t */\n\t\t\t{\n\t\t\t\t/* np is a proper subscript for chain[] */\n\t\t\t\tpassert(np < elemsof(md->chain));\n\t\t\t\tstruct payload_digest **p = &md->chain[np];\n\n\t\t\t\twhile (*p != NULL)\n\t\t\t\t\tp = &(*p)->next;\n\t\t\t\t*p = pd;\n\t\t\t\tpd->next = NULL;\n\t\t\t}\n\n\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\tmd->digest_roof++;\n\n\t\t\t/* since we've digested one payload happily, it is probably\n\t\t\t * the case that any decryption worked.  So we will not suggest\n\t\t\t * encryption failure as an excuse for subsequent payload\n\t\t\t * problems.\n\t\t\t */\n\t\t\texcuse = \"\";\n\t\t}\n\n\t\tDBG(DBG_PARSING, {\n\t\t\t    if (pbs_left(&md->message_pbs) != 0)\n\t\t\t\t    DBG_log(\"removing %d bytes of padding\",\n\t\t\t\t\t    (int) pbs_left(&md->message_pbs));\n\t\t    });\n\n\t\tmd->message_pbs.roof = md->message_pbs.cur;\n\n\t\t/* check that all mandatory payloads appeared */\n\n\t\tif (needed != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"message for %s is missing payloads %s\",\n\t\t\t       finite_states[from_state]->name,\n\t\t\t       bitnamesof(payload_name_ikev1, needed));\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!check_v1_HASH(smc->hash_type, smc->message, st, md)) {\n\t\t/*SEND_NOTIFICATION(INVALID_HASH_INFORMATION);*/\n\t\treturn;\n\t}\n\n\t/* more sanity checking: enforce most ordering constraints */\n\n\tif (IS_PHASE1(from_state) || IS_PHASE15(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5 Exchanges:\n\t\t * \"The SA payload MUST precede all other payloads in a phase 1 exchange.\"\n\t\t */\n\t\tif (md->chain[ISAKMP_NEXT_SA] != NULL &&\n\t\t    md->hdr.isa_np != ISAKMP_NEXT_SA) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Phase 1 message: does not start with an SA payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t} else if (IS_QUICK(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode\n\t\t *\n\t\t * \"In Quick Mode, a HASH payload MUST immediately follow the ISAKMP\n\t\t *  header and a SA payload MUST immediately follow the HASH.\"\n\t\t * [NOTE: there may be more than one SA payload, so this is not\n\t\t *  totally reasonable.  Probably all SAs should be so constrained.]\n\t\t *\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t *\n\t\t * \"With the exception of the HASH, SA, and the optional ID payloads,\n\t\t *  there are no payload ordering restrictions on Quick Mode.\"\n\t\t */\n\n\t\tif (md->hdr.isa_np != ISAKMP_NEXT_HASH) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Quick Mode message: does not start with a HASH payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\t{\n\t\t\tstruct payload_digest *p;\n\t\t\tint i;\n\n\t\t\tp = md->chain[ISAKMP_NEXT_SA];\n\t\t\ti = 1;\n\t\t\twhile (p != NULL) {\n\t\t\t\tif (p != &md->digest[i]) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"malformed Quick Mode message: SA payload is in wrong position\");\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tp = p->next;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode:\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t */\n\t\t{\n\t\t\tstruct payload_digest *id = md->chain[ISAKMP_NEXT_ID];\n\n\t\t\tif (id != NULL) {\n\t\t\t\tif (id->next == NULL ||\n\t\t\t\t    id->next->next != NULL) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: if any ID payload is present, there must be exactly two\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (id + 1 != id->next) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: the ID payloads are not adjacent\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Ignore payloads that we don't handle:\n\t */\n\t/* XXX Handle Notifications */\n\t{\n\t\tstruct payload_digest *p = md->chain[ISAKMP_NEXT_N];\n\n\t\twhile (p != NULL) {\n\t\t\tswitch (p->payload.notification.isan_type) {\n\t\t\tcase R_U_THERE:\n\t\t\tcase R_U_THERE_ACK:\n\t\t\tcase ISAKMP_N_CISCO_LOAD_BALANCE:\n\t\t\tcase PAYLOAD_MALFORMED:\n\t\t\tcase INVALID_MESSAGE_ID:\n\t\t\tcase IPSEC_RESPONDER_LIFETIME:\n\t\t\t\tif (md->hdr.isa_xchg == ISAKMP_XCHG_INFO) {\n\t\t\t\t\t/* these are handled later on in informational() */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/* FALL THROUGH */\n\t\t\tdefault:\n\t\t\t\tif (st == NULL) {\n\t\t\t\t\tDBG(DBG_CONTROL, DBG_log(\n\t\t\t\t\t       \"ignoring informational payload %s, no corresponding state\",\n\t\t\t\t\t       enum_show(& ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type)));\n\t\t\t\t} else {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"ignoring informational payload %s, msgid=%08\" PRIx32 \", length=%d\",\n\t\t\t\t\t       enum_show(&ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type),\n\t\t\t\t\t       st->st_v1_msgid.id,\n\t\t\t\t\t       p->payload.notification.isan_length);\n\t\t\t\t\tDBG_dump_pbs(&p->pbs);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"info:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_D];\n\t\twhile (p != NULL) {\n\t\t\tself_delete |= accept_delete(md, p);\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"del:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\t\t\tif (md->st != st) {\n\t\t\t\tpexpect(md->st == NULL);\n\t\t\t\tdbg(\"zapping ST as accept_delete() zapped MD.ST\");\n\t\t\t\tst = md->st;\n\t\t\t}\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_VID];\n\t\twhile (p != NULL) {\n\t\t\thandle_vendorid(md, (char *)p->pbs.cur,\n\t\t\t\t\tpbs_left(&p->pbs), FALSE);\n\t\t\tp = p->next;\n\t\t}\n\t}\n\n\tif (self_delete) {\n\t\taccept_self_delete(md);\n\t\tst = md->st;\n\t\t/* note: st ought to be NULL from here on */\n\t}\n\n\tpexpect(st == md->st);\n\tstatetime_t start = statetime_start(md->st);\n\t/*\n\t * XXX: danger - the .informational() processor deletes ST;\n\t * and then tunnels this loss through MD.ST.\n\t */\n\tcomplete_v1_state_transition(md, smc->processor(st, md));\n\tstatetime_stop(&start, \"%s()\", __func__);\n\t/* our caller will release_any_md(mdp); */\n}", "func_src_after": "void process_packet_tail(struct msg_digest *md)\n{\n\tstruct state *st = md->st;\n\tenum state_kind from_state = md->v1_from_state;\n\tconst struct state_v1_microcode *smc = md->smc;\n\tbool new_iv_set = md->new_iv_set;\n\tbool self_delete = FALSE;\n\n\tif (md->hdr.isa_flags & ISAKMP_FLAGS_v1_ENCRYPTION) {\n\t\tendpoint_buf b;\n\t\tdbg(\"received encrypted packet from %s\", str_endpoint(&md->sender, &b));\n\n\t\tif (st == NULL) {\n\t\t\tlibreswan_log(\n\t\t\t\t\"discarding encrypted message for an unknown ISAKMP SA\");\n\t\t\treturn;\n\t\t}\n\t\tif (st->st_skeyid_e_nss == NULL) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\"discarding encrypted message because we haven't yet negotiated keying material\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* Mark as encrypted */\n\t\tmd->encrypted = TRUE;\n\n\t\t/* do the specified decryption\n\t\t *\n\t\t * IV is from st->st_iv or (if new_iv_set) st->st_new_iv.\n\t\t * The new IV is placed in st->st_new_iv\n\t\t *\n\t\t * See RFC 2409 \"IKE\" Appendix B\n\t\t *\n\t\t * XXX The IV should only be updated really if the packet\n\t\t * is successfully processed.\n\t\t * We should keep this value, check for a success return\n\t\t * value from the parsing routines and then replace.\n\t\t *\n\t\t * Each post phase 1 exchange generates IVs from\n\t\t * the last phase 1 block, not the last block sent.\n\t\t */\n\t\tconst struct encrypt_desc *e = st->st_oakley.ta_encrypt;\n\n\t\tif (pbs_left(&md->message_pbs) % e->enc_blocksize != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS, \"malformed message: not a multiple of encryption blocksize\");\n\t\t\treturn;\n\t\t}\n\n\t\t/* XXX Detect weak keys */\n\n\t\t/* grab a copy of raw packet (for duplicate packet detection) */\n\t\tmd->raw_packet = clone_bytes_as_chunk(md->packet_pbs.start,\n\t\t\t\t\t\t      pbs_room(&md->packet_pbs),\n\t\t\t\t\t\t      \"raw packet\");\n\n\t\t/* Decrypt everything after header */\n\t\tif (!new_iv_set) {\n\t\t\tif (st->st_v1_iv.len == 0) {\n\t\t\t\tinit_phase2_iv(st, &md->hdr.isa_msgid);\n\t\t\t} else {\n\t\t\t\t/* use old IV */\n\t\t\t\trestore_new_iv(st, st->st_v1_iv);\n\t\t\t}\n\t\t}\n\n\t\tpassert(st->st_v1_new_iv.len >= e->enc_blocksize);\n\t\tst->st_v1_new_iv.len = e->enc_blocksize;   /* truncate */\n\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_log(\"decrypting %u bytes using algorithm %s\",\n\t\t\t\t(unsigned) pbs_left(&md->message_pbs),\n\t\t\t\tst->st_oakley.ta_encrypt->common.fqn);\n\t\t\tDBG_dump_hunk(\"IV before:\", st->st_v1_new_iv);\n\t\t}\n\t\te->encrypt_ops->do_crypt(e, md->message_pbs.cur,\n\t\t\t\t\t pbs_left(&md->message_pbs),\n\t\t\t\t\t st->st_enc_key_nss,\n\t\t\t\t\t st->st_v1_new_iv.ptr, FALSE);\n\t\tif (DBGP(DBG_CRYPT)) {\n\t\t\tDBG_dump_hunk(\"IV after:\", st->st_v1_new_iv);\n\t\t\tDBG_log(\"decrypted payload (starts at offset %td):\",\n\t\t\t\tmd->message_pbs.cur - md->message_pbs.roof);\n\t\t\tDBG_dump(NULL, md->message_pbs.start,\n\t\t\t\t md->message_pbs.roof - md->message_pbs.start);\n\t\t}\n\t} else {\n\t\t/* packet was not encryped -- should it have been? */\n\n\t\tif (smc->flags & SMF_INPUT_ENCRYPTED) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"packet rejected: should have been encrypted\");\n\t\t\tSEND_NOTIFICATION(INVALID_FLAGS);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Digest the message.\n\t * Padding must be removed to make hashing work.\n\t * Padding comes from encryption (so this code must be after decryption).\n\t * Padding rules are described before the definition of\n\t * struct isakmp_hdr in packet.h.\n\t */\n\t{\n\t\tenum next_payload_types_ikev1 np = md->hdr.isa_np;\n\t\tlset_t needed = smc->req_payloads;\n\t\tconst char *excuse =\n\t\t\tLIN(SMF_PSK_AUTH | SMF_FIRST_ENCRYPTED_INPUT,\n\t\t\t    smc->flags) ?\n\t\t\t\"probable authentication failure (mismatch of preshared secrets?): \"\n\t\t\t:\n\t\t\t\"\";\n\n\t\twhile (np != ISAKMP_NEXT_NONE) {\n\t\t\tstruct_desc *sd = v1_payload_desc(np);\n\n\t\t\tif (md->digest_roof >= elemsof(md->digest)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"more than %zu payloads in message; ignored\",\n\t\t\t\t       elemsof(md->digest));\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tstruct payload_digest *const pd = md->digest + md->digest_roof;\n\n\t\t\t/*\n\t\t\t * only do this in main mode. In aggressive mode, there\n\t\t\t * is no negotiation of NAT-T method. Get it right.\n\t\t\t */\n\t\t\tif (st != NULL && st->st_connection != NULL &&\n\t\t\t    (st->st_connection->policy & POLICY_AGGRESSIVE) == LEMPTY)\n\t\t\t{\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_NATD_RFC:\n\t\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t\tif ((st->hidden_variables.st_nat_traversal & NAT_T_WITH_RFC_VALUES) == LEMPTY) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * don't accept NAT-D/NAT-OA reloc directly in message,\n\t\t\t\t\t\t * unless we're using NAT-T RFC\n\t\t\t\t\t\t */\n\t\t\t\t\t\tDBG(DBG_NATT,\n\t\t\t\t\t\t    DBG_log(\"st_nat_traversal was: %s\",\n\t\t\t\t\t\t\t    bitnamesof(natt_bit_names,\n\t\t\t\t\t\t\t\t       st->hidden_variables.st_nat_traversal)));\n\t\t\t\t\t\tsd = NULL;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sd == NULL) {\n\t\t\t\t/* payload type is out of range or requires special handling */\n\t\t\t\tswitch (np) {\n\t\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\t\t\t/* ??? two kinds of ID payloads */\n\t\t\t\t\tsd = (IS_PHASE1(from_state) ||\n\t\t\t\t\t      IS_PHASE15(from_state)) ?\n\t\t\t\t\t\t&isakmp_identification_desc :\n\t\t\t\t\t\t&isakmp_ipsec_identification_desc;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATD_DRAFTS: /* out of range */\n\t\t\t\t\t/*\n\t\t\t\t\t * ISAKMP_NEXT_NATD_DRAFTS was a private use type before RFC-3947.\n\t\t\t\t\t * Since it has the same format as ISAKMP_NEXT_NATD_RFC,\n\t\t\t\t\t * just rewrite np and sd, and carry on.\n\t\t\t\t\t */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATD_RFC;\n\t\t\t\t\tsd = &isakmp_nat_d_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_NATOA_DRAFTS: /* out of range */\n\t\t\t\t\t/* NAT-OA was a private use type before RFC-3947 -- same format */\n\t\t\t\t\tnp = ISAKMP_NEXT_NATOA_RFC;\n\t\t\t\t\tsd = &isakmp_nat_oa_drafts;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase ISAKMP_NEXT_SAK: /* or ISAKMP_NEXT_NATD_BADDRAFTS */\n\t\t\t\t\t/*\n\t\t\t\t\t * Official standards say that this is ISAKMP_NEXT_SAK,\n\t\t\t\t\t * a part of Group DOI, something we don't implement.\n\t\t\t\t\t * Old non-updated Cisco gear abused this number in ancient NAT drafts.\n\t\t\t\t\t * We ignore (rather than reject) this in support of people\n\t\t\t\t\t * with crufty Cisco machines.\n\t\t\t\t\t */\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage with unsupported payload ISAKMP_NEXT_SAK (or ISAKMP_NEXT_NATD_BADDRAFTS) ignored\",\n\t\t\t\t\t\texcuse);\n\t\t\t\t\t/*\n\t\t\t\t\t * Hack to discard payload, whatever it was.\n\t\t\t\t\t * Since we are skipping the rest of the loop\n\t\t\t\t\t * body we must do some things ourself:\n\t\t\t\t\t * - demarshall the payload\n\t\t\t\t\t * - grab the next payload number (np)\n\t\t\t\t\t * - don't keep payload (don't increment pd)\n\t\t\t\t\t * - skip rest of loop body\n\t\t\t\t\t */\n\t\t\t\t\tif (!in_struct(&pd->payload, &isakmp_ignore_desc, &md->message_pbs,\n\t\t\t\t\t\t       &pd->pbs)) {\n\t\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t\t\t       excuse);\n\t\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\t\t\t/* NOTE: we do not increment pd! */\n\t\t\t\t\tcontinue;  /* skip rest of the loop */\n\n\t\t\t\tdefault:\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains an unknown or unexpected payload type (%s) at the outermost level\",\n\t\t\t\t\t       excuse,\n\t\t\t\t\t       enum_show(&ikev1_payload_names, np));\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tpassert(sd != NULL);\n\t\t\t}\n\n\t\t\tpassert(np < LELEM_ROOF);\n\n\t\t\t{\n\t\t\t\tlset_t s = LELEM(np);\n\n\t\t\t\tif (LDISJOINT(s,\n\t\t\t\t\t      needed | smc->opt_payloads |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_VID) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_N) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_D) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CR) |\n\t\t\t\t\t      LELEM(ISAKMP_NEXT_CERT))) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"%smessage ignored because it contains a payload type (%s) unexpected by state %s\",\n\t\t\t\t\t\texcuse,\n\t\t\t\t\t\tenum_show(&ikev1_payload_names, np),\n\t\t\t\t\t\tfinite_states[smc->state]->name);\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(INVALID_PAYLOAD_TYPE);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_log(\"got payload 0x%\" PRIxLSET\"  (%s) needed: 0x%\" PRIxLSET \" opt: 0x%\" PRIxLSET,\n\t\t\t\t\t    s, enum_show(&ikev1_payload_names, np),\n\t\t\t\t\t    needed, smc->opt_payloads));\n\t\t\t\tneeded &= ~s;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Read in the payload recording what type it\n\t\t\t * should be\n\t\t\t */\n\t\t\tpd->payload_type = np;\n\t\t\tif (!in_struct(&pd->payload, sd, &md->message_pbs,\n\t\t\t\t       &pd->pbs)) {\n\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t       \"%smalformed payload in packet\",\n\t\t\t\t       excuse);\n\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t}\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t/* do payload-type specific debugging */\n\t\t\tswitch (np) {\n\t\t\tcase ISAKMP_NEXT_ID:\n\t\t\tcase ISAKMP_NEXT_NATOA_RFC:\n\t\t\t\t/* dump ID section */\n\t\t\t\tDBG(DBG_PARSING,\n\t\t\t\t    DBG_dump(\"     obj: \", pd->pbs.cur,\n\t\t\t\t\t     pbs_left(&pd->pbs)));\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\n\t\t\t/*\n\t\t\t * Place payload at the end of the chain for this type.\n\t\t\t * This code appears in ikev1.c and ikev2.c.\n\t\t\t */\n\t\t\t{\n\t\t\t\t/* np is a proper subscript for chain[] */\n\t\t\t\tpassert(np < elemsof(md->chain));\n\t\t\t\tstruct payload_digest **p = &md->chain[np];\n\n\t\t\t\twhile (*p != NULL)\n\t\t\t\t\tp = &(*p)->next;\n\t\t\t\t*p = pd;\n\t\t\t\tpd->next = NULL;\n\t\t\t}\n\n\t\t\tnp = pd->payload.generic.isag_np;\n\t\t\tmd->digest_roof++;\n\n\t\t\t/* since we've digested one payload happily, it is probably\n\t\t\t * the case that any decryption worked.  So we will not suggest\n\t\t\t * encryption failure as an excuse for subsequent payload\n\t\t\t * problems.\n\t\t\t */\n\t\t\texcuse = \"\";\n\t\t}\n\n\t\tDBG(DBG_PARSING, {\n\t\t\t    if (pbs_left(&md->message_pbs) != 0)\n\t\t\t\t    DBG_log(\"removing %d bytes of padding\",\n\t\t\t\t\t    (int) pbs_left(&md->message_pbs));\n\t\t    });\n\n\t\tmd->message_pbs.roof = md->message_pbs.cur;\n\n\t\t/* check that all mandatory payloads appeared */\n\n\t\tif (needed != 0) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"message for %s is missing payloads %s\",\n\t\t\t       finite_states[from_state]->name,\n\t\t\t       bitnamesof(payload_name_ikev1, needed));\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!check_v1_HASH(smc->hash_type, smc->message, st, md)) {\n\t\t/*SEND_NOTIFICATION(INVALID_HASH_INFORMATION);*/\n\t\treturn;\n\t}\n\n\t/* more sanity checking: enforce most ordering constraints */\n\n\tif (IS_PHASE1(from_state) || IS_PHASE15(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5 Exchanges:\n\t\t * \"The SA payload MUST precede all other payloads in a phase 1 exchange.\"\n\t\t */\n\t\tif (md->chain[ISAKMP_NEXT_SA] != NULL &&\n\t\t    md->hdr.isa_np != ISAKMP_NEXT_SA) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Phase 1 message: does not start with an SA payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t} else if (IS_QUICK(from_state)) {\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode\n\t\t *\n\t\t * \"In Quick Mode, a HASH payload MUST immediately follow the ISAKMP\n\t\t *  header and a SA payload MUST immediately follow the HASH.\"\n\t\t * [NOTE: there may be more than one SA payload, so this is not\n\t\t *  totally reasonable.  Probably all SAs should be so constrained.]\n\t\t *\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t *\n\t\t * \"With the exception of the HASH, SA, and the optional ID payloads,\n\t\t *  there are no payload ordering restrictions on Quick Mode.\"\n\t\t */\n\n\t\tif (md->hdr.isa_np != ISAKMP_NEXT_HASH) {\n\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t       \"malformed Quick Mode message: does not start with a HASH payload\");\n\t\t\tif (!md->encrypted) {\n\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\t{\n\t\t\tstruct payload_digest *p;\n\t\t\tint i;\n\n\t\t\tp = md->chain[ISAKMP_NEXT_SA];\n\t\t\ti = 1;\n\t\t\twhile (p != NULL) {\n\t\t\t\tif (p != &md->digest[i]) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"malformed Quick Mode message: SA payload is in wrong position\");\n\t\t\t\t\tif (!md->encrypted) {\n\t\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\t}\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tp = p->next;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\n\t\t/* rfc2409: The Internet Key Exchange (IKE), 5.5 Phase 2 - Quick Mode:\n\t\t * \"If ISAKMP is acting as a client negotiator on behalf of another\n\t\t *  party, the identities of the parties MUST be passed as IDci and\n\t\t *  then IDcr.\"\n\t\t */\n\t\t{\n\t\t\tstruct payload_digest *id = md->chain[ISAKMP_NEXT_ID];\n\n\t\t\tif (id != NULL) {\n\t\t\t\tif (id->next == NULL ||\n\t\t\t\t    id->next->next != NULL) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: if any ID payload is present, there must be exactly two\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (id + 1 != id->next) {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t\t\"malformed Quick Mode message: the ID payloads are not adjacent\");\n\t\t\t\t\tSEND_NOTIFICATION(PAYLOAD_MALFORMED);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Ignore payloads that we don't handle:\n\t */\n\t/* XXX Handle Notifications */\n\t{\n\t\tstruct payload_digest *p = md->chain[ISAKMP_NEXT_N];\n\n\t\twhile (p != NULL) {\n\t\t\tswitch (p->payload.notification.isan_type) {\n\t\t\tcase R_U_THERE:\n\t\t\tcase R_U_THERE_ACK:\n\t\t\tcase ISAKMP_N_CISCO_LOAD_BALANCE:\n\t\t\tcase PAYLOAD_MALFORMED:\n\t\t\tcase INVALID_MESSAGE_ID:\n\t\t\tcase IPSEC_RESPONDER_LIFETIME:\n\t\t\t\tif (md->hdr.isa_xchg == ISAKMP_XCHG_INFO) {\n\t\t\t\t\t/* these are handled later on in informational() */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/* FALL THROUGH */\n\t\t\tdefault:\n\t\t\t\tif (st == NULL) {\n\t\t\t\t\tDBG(DBG_CONTROL, DBG_log(\n\t\t\t\t\t       \"ignoring informational payload %s, no corresponding state\",\n\t\t\t\t\t       enum_show(& ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type)));\n\t\t\t\t} else {\n\t\t\t\t\tloglog(RC_LOG_SERIOUS,\n\t\t\t\t\t       \"ignoring informational payload %s, msgid=%08\" PRIx32 \", length=%d\",\n\t\t\t\t\t       enum_show(&ikev1_notify_names,\n\t\t\t\t\t\t\t p->payload.notification.isan_type),\n\t\t\t\t\t       st->st_v1_msgid.id,\n\t\t\t\t\t       p->payload.notification.isan_length);\n\t\t\t\t\tDBG_dump_pbs(&p->pbs);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"info:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_D];\n\t\twhile (p != NULL) {\n\t\t\tself_delete |= accept_delete(md, p);\n\t\t\tif (DBGP(DBG_BASE)) {\n\t\t\t\tDBG_dump(\"del:\", p->pbs.cur,\n\t\t\t\t\t pbs_left(&p->pbs));\n\t\t\t}\n\t\t\tif (md->st != st) {\n\t\t\t\tpexpect(md->st == NULL);\n\t\t\t\tdbg(\"zapping ST as accept_delete() zapped MD.ST\");\n\t\t\t\tst = md->st;\n\t\t\t}\n\t\t\tp = p->next;\n\t\t}\n\n\t\tp = md->chain[ISAKMP_NEXT_VID];\n\t\twhile (p != NULL) {\n\t\t\thandle_vendorid(md, (char *)p->pbs.cur,\n\t\t\t\t\tpbs_left(&p->pbs), FALSE);\n\t\t\tp = p->next;\n\t\t}\n\t}\n\n\tif (self_delete) {\n\t\taccept_self_delete(md);\n\t\tst = md->st;\n\t\t/* note: st ought to be NULL from here on */\n\t}\n\n\tpexpect(st == md->st);\n\tstatetime_t start = statetime_start(md->st);\n\t/*\n\t * XXX: danger - the .informational() processor deletes ST;\n\t * and then tunnels this loss through MD.ST.\n\t */\n\tcomplete_v1_state_transition(md, smc->processor(st, md));\n\tstatetime_stop(&start, \"%s()\", __func__);\n\t/* our caller will release_any_md(mdp); */\n}", "commit_link": "github.com/libreswan/libreswan/commit/471a3e41a449d7c753bc4edbba4239501bb62ba8", "file_name": "programs/pluto/ikev1.c", "vul_type": "cwe-125", "description": "Write a C function named `process_packet_tail` that processes an ISAKMP packet's tail end, handling encryption, payload digestion, and state transitions."}
{"func_name": "get_asset_and_volume", "func_src_before": "@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid='\"+asset_id+\"'\"\n    cur.execute(query)\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])", "func_src_after": "@app.route('/get_asset_and_volume')\ndef get_asset_and_volume():\n    asset_id = request.args.get('asset_id')\n\n    if not isObject(asset_id):\n        ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"lookup_asset_symbols\",[[\"' + asset_id + '\"], 0]]}')\n        result_l = ws.recv()\n        j_l = json.loads(result_l)\n        asset_id = j_l[\"result\"][0][\"id\"]\n\n    #print asset_id\n    ws.send('{\"id\":1, \"method\":\"call\", \"params\":[0,\"get_assets\",[[\"' + asset_id + '\"], 0]]}')\n    result = ws.recv()\n    j = json.loads(result)\n\n    dynamic_asset_data_id =  j[\"result\"][0][\"dynamic_asset_data_id\"]\n\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+dynamic_asset_data_id+'\"]]]}')\n    result2 = ws.recv()\n    j2 = json.loads(result2)\n    #print j2[\"result\"][0][\"current_supply\"]\n\n    j[\"result\"][0][\"current_supply\"] = j2[\"result\"][0][\"current_supply\"]\n    j[\"result\"][0][\"confidential_supply\"] = j2[\"result\"][0][\"confidential_supply\"]\n    #print j[\"result\"]\n\n    j[\"result\"][0][\"accumulated_fees\"] = j2[\"result\"][0][\"accumulated_fees\"]\n    j[\"result\"][0][\"fee_pool\"] = j2[\"result\"][0][\"fee_pool\"]\n\n    issuer = j[\"result\"][0][\"issuer\"]\n    ws.send('{\"id\": 1, \"method\": \"call\", \"params\": [0, \"get_objects\", [[\"'+issuer+'\"]]]}')\n    result3 = ws.recv()\n    j3 = json.loads(result3)\n    j[\"result\"][0][\"issuer_name\"] = j3[\"result\"][0][\"name\"]\n\n\n    con = psycopg2.connect(**config.POSTGRES)\n    cur = con.cursor()\n\n    query = \"SELECT volume, mcap FROM assets WHERE aid=%s\"\n    cur.execute(query, (asset_id,))\n    results = cur.fetchall()\n    con.close()\n    try:\n        j[\"result\"][0][\"volume\"] = results[0][0]\n        j[\"result\"][0][\"mcap\"] = results[0][1]\n    except:\n        j[\"result\"][0][\"volume\"] = 0\n        j[\"result\"][0][\"mcap\"] = 0\n\n    return jsonify(j[\"result\"])", "commit_link": "github.com/VinChain/vinchain-python-api-backend/commit/b78088a551fbb712121269c6eb7f43ede120ff60", "file_name": "api.py", "vul_type": "cwe-089", "description": "In Python, create a Flask endpoint '/get_asset_and_volume' that retrieves and combines asset details from a WebSocket connection and volume data from a PostgreSQL database."}
{"func_name": "check", "func_src_before": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=\"%s\"' % current_num)\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "func_src_after": "def check(current_num):\n    try:\n        cursor.execute('SELECT * FROM comics WHERE num=?', (current_num,))\n    except sqlite3.OperationalError:\n        cursor.execute('CREATE TABLE comics (num text)')\n        return False\n    else:\n        return False if cursor.fetchone() is None else True", "commit_link": "github.com/lord63/a_bunch_of_code/commit/c0d67a1312306fd1257c354bfb5d6cac7643aa29", "file_name": "comics/check_comics.py", "vul_type": "cwe-089", "description": "Write a Python function named `check` that queries a SQLite database for a specific record and creates a table if it doesn't exist."}
